{
    "author": "cyyever",
    "message": "Replace Optional and Union typing with | in some source files (#42294)\n\n* Replace Optional and Union typing with | in some source files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Replace Optional and Union typing with | in some source files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Replace Optional and Union typing with | in some source files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "73a9bc3756cb15b86c10202a62c128121315d347",
    "files": [
        {
            "sha": "da08aabe27c134170ee54f13d490ef26db8e6cda",
            "filename": "src/transformers/cli/add_new_model_like.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -19,7 +19,7 @@\n from collections.abc import Callable\n from datetime import date\n from pathlib import Path\n-from typing import Annotated, Any, Optional, Union\n+from typing import Annotated, Any\n \n import typer\n \n@@ -95,7 +95,7 @@ def visit_SimpleStatementLine(self, node: cst.SimpleStatementLine):\n \n def add_new_model_like(\n     repo_path: Annotated[\n-        Optional[str], typer.Argument(help=\"When not using an editable install, the path to the Transformers repo.\")\n+        str | None, typer.Argument(help=\"When not using an editable install, the path to the Transformers repo.\")\n     ] = None,\n ):\n     \"\"\"\n@@ -156,7 +156,7 @@ def __init__(self, lowercase_name: str):\n         self.processor_class = PROCESSOR_MAPPING_NAMES.get(self.lowercase_name, None)\n \n \n-def add_content_to_file(file_name: Union[str, os.PathLike], new_content: str, add_after: str):\n+def add_content_to_file(file_name: str | os.PathLike, new_content: str, add_after: str):\n     \"\"\"\n     A utility to add some content inside a given file.\n \n@@ -614,9 +614,9 @@ def _add_new_model_like_internal(\n \n def get_user_field(\n     question: str,\n-    default_value: Optional[str] = None,\n-    convert_to: Optional[Callable] = None,\n-    fallback_message: Optional[str] = None,\n+    default_value: str | None = None,\n+    convert_to: Callable | None = None,\n+    fallback_message: str | None = None,\n ) -> Any:\n     \"\"\"\n     A utility function that asks a question to the user to get an answer, potentially looping until it gets a valid"
        },
        {
            "sha": "b5009c2804eaf2d8b88a827366dcef9fcdd37908",
            "filename": "src/transformers/cli/chat.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fchat.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -19,7 +19,7 @@\n import string\n import time\n from collections.abc import AsyncIterator\n-from typing import Annotated, Optional\n+from typing import Annotated\n \n import click\n import typer\n@@ -214,7 +214,7 @@ def __init__(\n         base_url: Annotated[str, typer.Argument(help=\"Base url to connect to (e.g. http://localhost:8000/v1).\")],\n         model_id: Annotated[str, typer.Argument(help=\"ID of the model to use (e.g. 'HuggingFaceTB/SmolLM3-3B').\")],\n         generate_flags: Annotated[\n-            Optional[list[str]],\n+            list[str] | None,\n             typer.Argument(\n                 help=(\n                     \"Flags to pass to `generate`, using a space as a separator between flags. Accepts booleans, numbers, \"\n@@ -227,15 +227,15 @@ def __init__(\n         ] = None,\n         # General settings\n         user: Annotated[\n-            Optional[str],\n+            str | None,\n             typer.Option(help=\"Username to display in chat interface. Defaults to the current user's name.\"),\n         ] = None,\n-        system_prompt: Annotated[Optional[str], typer.Option(help=\"System prompt.\")] = None,\n+        system_prompt: Annotated[str | None, typer.Option(help=\"System prompt.\")] = None,\n         save_folder: Annotated[str, typer.Option(help=\"Folder to save chat history.\")] = \"./chat_history/\",\n-        examples_path: Annotated[Optional[str], typer.Option(help=\"Path to a yaml file with examples.\")] = None,\n+        examples_path: Annotated[str | None, typer.Option(help=\"Path to a yaml file with examples.\")] = None,\n         # Generation settings\n         generation_config: Annotated[\n-            Optional[str],\n+            str | None,\n             typer.Option(\n                 help=\"Path to a local generation config file or to a HuggingFace repo containing a `generation_config.json` file. Other generation settings passed as CLI arguments will be applied on top of this generation config.\"\n             ),\n@@ -455,7 +455,7 @@ async def _inner_run(self):\n                     break\n \n \n-def load_generation_config(generation_config: Optional[str]) -> GenerationConfig:\n+def load_generation_config(generation_config: str | None) -> GenerationConfig:\n     if generation_config is None:\n         return GenerationConfig()\n \n@@ -467,7 +467,7 @@ def load_generation_config(generation_config: Optional[str]) -> GenerationConfig\n         return GenerationConfig.from_pretrained(generation_config)\n \n \n-def parse_generate_flags(generate_flags: Optional[list[str]]) -> dict:\n+def parse_generate_flags(generate_flags: list[str] | None) -> dict:\n     \"\"\"Parses the generate flags from the user input into a dictionary of `generate` kwargs.\"\"\"\n     if generate_flags is None or len(generate_flags) == 0:\n         return {}\n@@ -521,7 +521,7 @@ def is_number(s: str) -> bool:\n     return processed_generate_flags\n \n \n-def new_chat_history(system_prompt: Optional[str] = None) -> list[dict]:\n+def new_chat_history(system_prompt: str | None = None) -> list[dict]:\n     \"\"\"Returns a new chat conversation.\"\"\"\n     return [{\"role\": \"system\", \"content\": system_prompt}] if system_prompt else []\n "
        },
        {
            "sha": "4f56e50cae5bdcbccc0c1a3edc31a6a272e6b99a",
            "filename": "src/transformers/cli/download.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fdownload.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fdownload.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fdownload.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,14 +11,14 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Annotated, Optional\n+from typing import Annotated\n \n import typer\n \n \n def download(\n     model_id: Annotated[str, typer.Argument(help=\"The model ID to download\")],\n-    cache_dir: Annotated[Optional[str], typer.Option(help=\"Directory where to save files.\")] = None,\n+    cache_dir: Annotated[str | None, typer.Option(help=\"Directory where to save files.\")] = None,\n     force_download: Annotated[\n         bool, typer.Option(help=\"If set, the files will be downloaded even if they are already cached locally.\")\n     ] = False,"
        },
        {
            "sha": "7e9927b72dcafebccc066178b630eca588a0e8f0",
            "filename": "src/transformers/cli/run.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Frun.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Frun.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Frun.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from enum import Enum\n-from typing import Annotated, Optional\n+from typing import Annotated\n \n import typer\n \n@@ -28,27 +28,27 @@\n \n def run(\n     task: Annotated[TaskEnum, typer.Argument(help=\"Task to run\", case_sensitive=False)],  # type: ignore\n-    input: Annotated[Optional[str], typer.Option(help=\"Path to the file to use for inference\")] = None,\n+    input: Annotated[str | None, typer.Option(help=\"Path to the file to use for inference\")] = None,\n     output: Annotated[\n-        Optional[str], typer.Option(help=\"Path to the file that will be used post to write results.\")\n+        str | None, typer.Option(help=\"Path to the file that will be used post to write results.\")\n     ] = None,\n     model: Annotated[\n-        Optional[str],\n+        str | None,\n         typer.Option(\n             help=\"Name or path to the model to instantiate. If not provided, will use the default model for that task.\"\n         ),\n     ] = None,\n     config: Annotated[\n-        Optional[str],\n+        str | None,\n         typer.Option(\n             help=\"Name or path to the model's config to instantiate. If not provided, will use the model's one.\"\n         ),\n     ] = None,\n     tokenizer: Annotated[\n-        Optional[str], typer.Option(help=\"Name of the tokenizer to use. If not provided, will use the model's one.\")\n+        str | None, typer.Option(help=\"Name of the tokenizer to use. If not provided, will use the model's one.\")\n     ] = None,\n     column: Annotated[\n-        Optional[str],\n+        str | None,\n         typer.Option(help=\"Name of the column to use as input. For multi columns input use 'column1,columns2'\"),\n     ] = None,\n     format: Annotated[FormatEnum, typer.Option(help=\"Input format to read from\", case_sensitive=False)] = \"pipe\",  # type: ignore"
        },
        {
            "sha": "c13fee361d47198b57c6211650395bf2269d51ee",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -305,7 +305,7 @@ def __init__(\n         self,\n         model: \"PreTrainedModel\",\n         timeout_seconds: int,\n-        processor: Optional[Union[\"ProcessorMixin\", \"PreTrainedTokenizerFast\"]] = None,\n+        processor: Union[\"ProcessorMixin\", \"PreTrainedTokenizerFast\"] | None = None,\n     ):\n         self.model = model\n         self._name_or_path = str(model.name_or_path)\n@@ -363,7 +363,7 @@ def __init__(\n             ),\n         ] = \"auto\",\n         dtype: Annotated[\n-            Optional[str],\n+            str | None,\n             typer.Option(\n                 help=\"Override the default `torch.dtype` and load the model under this dtype. If `'auto'` is passed, the dtype will be automatically derived from the model's weights.\"\n             ),\n@@ -372,7 +372,7 @@ def __init__(\n             bool, typer.Option(help=\"Whether to trust remote code when loading a model.\")\n         ] = False,\n         attn_implementation: Annotated[\n-            Optional[str],\n+            str | None,\n             typer.Option(\n                 help=\"Which attention implementation to use; you can run --attn_implementation=flash_attention_2, in which case you must install this manually by running `pip install flash-attn --no-build-isolation`.\"\n             ),\n@@ -390,7 +390,7 @@ def __init__(\n             str, typer.Option(help=\"Logging level as a string. Example: 'info' or 'warning'.\")\n         ] = \"info\",\n         default_seed: Annotated[\n-            Optional[int], typer.Option(help=\"The default seed for torch, should be an integer.\")\n+            int | None, typer.Option(help=\"The default seed for torch, should be an integer.\")\n         ] = None,\n         enable_cors: Annotated[\n             bool,\n@@ -400,7 +400,7 @@ def __init__(\n         ] = False,\n         input_validation: Annotated[bool, typer.Option(help=\"Whether to turn on strict input validation.\")] = False,\n         force_model: Annotated[\n-            Optional[str],\n+            str | None,\n             typer.Option(\n                 help=\"Name of the model to be forced on all requests. This is useful for testing Apps that don't allow changing models in the request.\"\n             ),\n@@ -445,7 +445,7 @@ def __init__(\n         # Internal state:\n         # 1. Tracks models in memory, to prevent reloading the model unnecessarily\n         self.loaded_models: dict[str, TimedModel] = {}\n-        self.running_continuous_batching_manager: Optional[ContinuousBatchingManager] = None\n+        self.running_continuous_batching_manager: ContinuousBatchingManager | None = None\n \n         # 2. preserves information about the last call and last KV cache, to determine whether we can reuse the KV\n         # cache and avoid re-running prefill\n@@ -648,13 +648,13 @@ def validate_transcription_request(self, request: dict):\n     def build_chat_completion_chunk(\n         self,\n         request_id: str = \"\",\n-        content: Optional[int] = None,\n-        model: Optional[str] = None,\n-        role: Optional[str] = None,\n-        finish_reason: Optional[str] = None,\n-        tool_calls: Optional[list[\"ChoiceDeltaToolCall\"]] = None,\n-        decode_stream: Optional[DecodeStream] = None,\n-        tokenizer: Optional[PreTrainedTokenizerFast] = None,\n+        content: int | None = None,\n+        model: str | None = None,\n+        role: str | None = None,\n+        finish_reason: str | None = None,\n+        tool_calls: list[\"ChoiceDeltaToolCall\"] | None = None,\n+        decode_stream: DecodeStream | None = None,\n+        tokenizer: PreTrainedTokenizerFast | None = None,\n     ) -> ChatCompletionChunk:\n         \"\"\"\n         Builds a chunk of a streaming OpenAI Chat Completion response."
        },
        {
            "sha": "65180c4647246f3fdbd25ad2e902e7feb0d69a5e",
            "filename": "src/transformers/cli/system.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fsystem.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fcli%2Fsystem.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fsystem.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -22,7 +22,7 @@\n import io\n import os\n import platform\n-from typing import Annotated, Optional\n+from typing import Annotated\n \n import huggingface_hub\n import typer\n@@ -40,7 +40,7 @@\n \n def env(\n     accelerate_config_file: Annotated[\n-        Optional[str],\n+        str | None,\n         typer.Argument(help=\"The accelerate config file to use for the default values in the launching script.\"),\n     ] = None,\n ) -> None:"
        },
        {
            "sha": "366cdeb95bbddb661fc04afb397ce0dc05b25e59",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 27,
            "deletions": 27,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -17,7 +17,7 @@\n from collections.abc import Callable, Mapping\n from dataclasses import dataclass\n from random import randint\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n \n@@ -35,7 +35,7 @@\n \n \n class DataCollatorMixin:\n-    def __call__(self, features, return_tensors: Optional[str] = None):\n+    def __call__(self, features, return_tensors: str | None = None):\n         if return_tensors is None:\n             return_tensors = self.return_tensors\n         if return_tensors == \"pt\":\n@@ -216,9 +216,9 @@ class DataCollatorWithPadding:\n     \"\"\"\n \n     tokenizer: PreTrainedTokenizerBase\n-    padding: Union[bool, str, PaddingStrategy] = True\n-    max_length: Optional[int] = None\n-    pad_to_multiple_of: Optional[int] = None\n+    padding: bool | str | PaddingStrategy = True\n+    max_length: int | None = None\n+    pad_to_multiple_of: int | None = None\n     return_tensors: str = \"pt\"\n \n     def __call__(self, features: list[dict[str, Any]]) -> dict[str, Any]:\n@@ -270,9 +270,9 @@ class DataCollatorForTokenClassification(DataCollatorMixin):\n     \"\"\"\n \n     tokenizer: PreTrainedTokenizerBase\n-    padding: Union[bool, str, PaddingStrategy] = True\n-    max_length: Optional[int] = None\n-    pad_to_multiple_of: Optional[int] = None\n+    padding: bool | str | PaddingStrategy = True\n+    max_length: int | None = None\n+    pad_to_multiple_of: int | None = None\n     label_pad_token_id: int = -100\n     return_tensors: str = \"pt\"\n \n@@ -347,7 +347,7 @@ def numpy_call(self, features):\n         return batch\n \n \n-def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n+def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of: int | None = None):\n     \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n     import torch\n \n@@ -384,7 +384,7 @@ def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int]\n     return result\n \n \n-def _numpy_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n+def _numpy_collate_batch(examples, tokenizer, pad_to_multiple_of: int | None = None):\n     \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n     # Tensorize if necessary.\n     if isinstance(examples[0], (list, tuple)):\n@@ -447,9 +447,9 @@ class DataCollatorForMultipleChoice(DataCollatorMixin):\n     \"\"\"\n \n     tokenizer: PreTrainedTokenizerBase\n-    padding: Union[bool, str, PaddingStrategy] = True\n-    max_length: Optional[int] = None\n-    pad_to_multiple_of: Optional[int] = None\n+    padding: bool | str | PaddingStrategy = True\n+    max_length: int | None = None\n+    pad_to_multiple_of: int | None = None\n     return_tensors: str = \"pt\"\n \n     def torch_call(self, examples: list[dict[str, Any]]):  # Refactored implementation from the docs.\n@@ -519,10 +519,10 @@ class DataCollatorForSeq2Seq:\n     \"\"\"\n \n     tokenizer: PreTrainedTokenizerBase\n-    model: Optional[Any] = None\n-    padding: Union[bool, str, PaddingStrategy] = True\n-    max_length: Optional[int] = None\n-    pad_to_multiple_of: Optional[int] = None\n+    model: Any | None = None\n+    padding: bool | str | PaddingStrategy = True\n+    max_length: int | None = None\n+    pad_to_multiple_of: int | None = None\n     label_pad_token_id: int = -100\n     return_tensors: str = \"pt\"\n \n@@ -682,12 +682,12 @@ class DataCollatorForLanguageModeling(DataCollatorMixin):\n     tokenizer: PreTrainedTokenizerBase\n     mlm: bool = True\n     whole_word_mask: bool = False\n-    mlm_probability: Optional[float] = 0.15\n+    mlm_probability: float | None = 0.15\n     mask_replace_prob: float = 0.8\n     random_replace_prob: float = 0.1\n-    pad_to_multiple_of: Optional[int] = None\n+    pad_to_multiple_of: int | None = None\n     return_tensors: str = \"pt\"\n-    seed: Optional[int] = None\n+    seed: int | None = None\n \n     def __post_init__(self):\n         if self.mlm:\n@@ -762,7 +762,7 @@ def create_rng(self):\n \n             self.generator = self.get_generator(self.seed + worker_info.id)\n \n-    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n+    def torch_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]:\n         # Handle dict or lists with proper padding and conversion to tensor.\n \n         if self.seed and self.generator is None:\n@@ -794,7 +794,7 @@ def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> d\n         return batch\n \n     def torch_mask_tokens(\n-        self, inputs: Any, special_tokens_mask: Optional[Any] = None, offset_mapping: Optional[Any] = None\n+        self, inputs: Any, special_tokens_mask: Any | None = None, offset_mapping: Any | None = None\n     ) -> tuple[Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling.\n@@ -856,7 +856,7 @@ def torch_mask_tokens(\n         # The rest of the time ((1-random_replace_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n-    def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n+    def numpy_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]:\n         # Handle dict or lists with proper padding and conversion to tensor.\n \n         if self.seed and self.generator is None:\n@@ -890,8 +890,8 @@ def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> d\n     def numpy_mask_tokens(\n         self,\n         inputs: Any,\n-        special_tokens_mask: Optional[Any] = None,\n-        offset_mapping: Optional[Any] = None,\n+        special_tokens_mask: Any | None = None,\n+        offset_mapping: Any | None = None,\n     ) -> tuple[Any, Any]:\n         \"\"\"\n         Prepare masked tokens inputs/labels for masked language modeling.\n@@ -1149,14 +1149,14 @@ class DataCollatorForPermutationLanguageModeling(DataCollatorMixin):\n     max_span_length: int = 5  # maximum length of a span of masked tokens\n     return_tensors: str = \"pt\"\n \n-    def torch_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n+    def torch_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]:\n         if isinstance(examples[0], Mapping):\n             examples = [e[\"input_ids\"] for e in examples]\n         batch = _torch_collate_batch(examples, self.tokenizer)\n         inputs, perm_mask, target_mapping, labels = self.torch_mask_tokens(batch)\n         return {\"input_ids\": inputs, \"perm_mask\": perm_mask, \"target_mapping\": target_mapping, \"labels\": labels}\n \n-    def numpy_call(self, examples: list[Union[list[int], Any, dict[str, Any]]]) -> dict[str, Any]:\n+    def numpy_call(self, examples: list[list[int] | Any | dict[str, Any]]) -> dict[str, Any]:\n         if isinstance(examples[0], Mapping):\n             examples = [e[\"input_ids\"] for e in examples]\n         batch = _numpy_collate_batch(examples, self.tokenizer)"
        },
        {
            "sha": "51058a696101d767c72e6df147aa29a8f0a61749",
            "filename": "src/transformers/data/datasets/glue.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fglue.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -17,7 +17,6 @@\n import warnings\n from dataclasses import dataclass, field\n from enum import Enum\n-from typing import Optional, Union\n \n import torch\n from filelock import FileLock\n@@ -77,9 +76,9 @@ def __init__(\n         self,\n         args: GlueDataTrainingArguments,\n         tokenizer: PreTrainedTokenizerBase,\n-        limit_length: Optional[int] = None,\n-        mode: Union[str, Split] = Split.train,\n-        cache_dir: Optional[str] = None,\n+        limit_length: int | None = None,\n+        mode: str | Split = Split.train,\n+        cache_dir: str | None = None,\n     ):\n         warnings.warn(\n             \"This dataset will be removed from the library soon, preprocessing should be handled with the Hugging Face Datasets \""
        },
        {
            "sha": "8eb9bf4dfaab8f91607f9c2c5cd97a456faef834",
            "filename": "src/transformers/data/datasets/squad.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdatasets%2Fsquad.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -16,7 +16,6 @@\n import time\n from dataclasses import dataclass, field\n from enum import Enum\n-from typing import Optional, Union\n \n import torch\n from filelock import FileLock\n@@ -116,10 +115,10 @@ def __init__(\n         self,\n         args: SquadDataTrainingArguments,\n         tokenizer: PreTrainedTokenizer,\n-        limit_length: Optional[int] = None,\n-        mode: Union[str, Split] = Split.train,\n+        limit_length: int | None = None,\n+        mode: str | Split = Split.train,\n         is_language_sensitive: bool = False,\n-        cache_dir: Optional[str] = None,\n+        cache_dir: str | None = None,\n         dataset_format: str = \"pt\",\n     ):\n         self.args = args"
        },
        {
            "sha": "8ed669aed8310e86d0c5352c653b6371985e34b4",
            "filename": "src/transformers/data/processors/glue.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fglue.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -18,7 +18,6 @@\n import os\n import warnings\n from enum import Enum\n-from typing import Optional, Union\n \n from ...tokenization_utils import PreTrainedTokenizer\n from ...utils import logging\n@@ -37,7 +36,7 @@\n def glue_convert_examples_to_features(\n     examples: list[InputExample],\n     tokenizer: PreTrainedTokenizer,\n-    max_length: Optional[int] = None,\n+    max_length: int | None = None,\n     task=None,\n     label_list=None,\n     output_mode=None,\n@@ -66,7 +65,7 @@ def glue_convert_examples_to_features(\n def _glue_convert_examples_to_features(\n     examples: list[InputExample],\n     tokenizer: PreTrainedTokenizer,\n-    max_length: Optional[int] = None,\n+    max_length: int | None = None,\n     task=None,\n     label_list=None,\n     output_mode=None,\n@@ -85,7 +84,7 @@ def _glue_convert_examples_to_features(\n \n     label_map = {label: i for i, label in enumerate(label_list)}\n \n-    def label_from_example(example: InputExample) -> Union[int, float, None]:\n+    def label_from_example(example: InputExample) -> int | float | None:\n         if example.label is None:\n             return None\n         if output_mode == \"classification\":"
        },
        {
            "sha": "e3064b9d2134b35e21b472c91a4b039c9e13320d",
            "filename": "src/transformers/data/processors/squad.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -17,7 +17,6 @@\n from functools import partial\n from multiprocessing import Pool, cpu_count\n from multiprocessing.pool import ThreadPool\n-from typing import Optional\n \n import numpy as np\n from tqdm import tqdm\n@@ -693,8 +692,8 @@ def __init__(\n         start_position,\n         end_position,\n         is_impossible,\n-        qas_id: Optional[str] = None,\n-        encoding: Optional[BatchEncoding] = None,\n+        qas_id: str | None = None,\n+        encoding: BatchEncoding | None = None,\n     ):\n         self.input_ids = input_ids\n         self.attention_mask = attention_mask"
        },
        {
            "sha": "d99e3887d2411e910e1cb5256c49f1dc1868b058",
            "filename": "src/transformers/data/processors/utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Futils.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -18,7 +18,6 @@\n import dataclasses\n import json\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n from ...utils import is_torch_available, logging\n \n@@ -43,8 +42,8 @@ class InputExample:\n \n     guid: str\n     text_a: str\n-    text_b: Optional[str] = None\n-    label: Optional[str] = None\n+    text_b: str | None = None\n+    label: str | None = None\n \n     def to_json_string(self):\n         \"\"\"Serializes this instance to a JSON string.\"\"\"\n@@ -68,9 +67,9 @@ class InputFeatures:\n     \"\"\"\n \n     input_ids: list[int]\n-    attention_mask: Optional[list[int]] = None\n-    token_type_ids: Optional[list[int]] = None\n-    label: Optional[Union[int, float]] = None\n+    attention_mask: list[int] | None = None\n+    token_type_ids: list[int] | None = None\n+    label: int | float | None = None\n \n     def to_json_string(self):\n         \"\"\"Serializes this instance to a JSON string.\"\"\""
        },
        {
            "sha": "652639af235cb8f998c1bc48851d1b1958126ea9",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -41,7 +41,7 @@\n class CandidateGenerator:\n     \"\"\"Abstract base class for all candidate generators that can be applied during assisted generation.\"\"\"\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -106,7 +106,7 @@ def __init__(\n         assistant_model: \"PreTrainedModel\",\n         generation_config: \"GenerationConfig\",\n         model_kwargs: dict,\n-        inputs_tensor: Optional[torch.Tensor] = None,\n+        inputs_tensor: torch.Tensor | None = None,\n         logits_processor: Optional[\"LogitsProcessorList\"] = None,\n     ):\n         # Make sure all data at the same device as assistant model\n@@ -195,7 +195,7 @@ def __init__(\n             self.probs = []\n             self.matches = []\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -314,7 +314,7 @@ def _prepare_generation_args(self, input_ids: torch.LongTensor, min_new_tokens:\n             \"logits_processor\": self.logits_processor,\n         }\n \n-    def _generate_candidates(self, generation_args: dict) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def _generate_candidates(self, generation_args: dict) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n         \"\"\"Generate candidate sequences using the assistant model.\"\"\"\n         assistant_output = self.assistant_model.generate(**generation_args, **self.assistant_kwargs)\n         self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n@@ -374,14 +374,14 @@ def __init__(\n         assistant_tokenizer: \"PreTrainedTokenizerBase\",\n         generation_config: \"GenerationConfig\",\n         model_kwargs: dict,\n-        inputs_tensor: Optional[torch.Tensor] = None,\n+        inputs_tensor: torch.Tensor | None = None,\n         logits_processor: Optional[\"LogitsProcessorList\"] = None,\n     ):\n         super().__init__(input_ids, assistant_model, generation_config, model_kwargs, inputs_tensor, logits_processor)\n \n         self.target_tokenizer = target_tokenizer\n         self.assistant_tokenizer = assistant_tokenizer\n-        self.prev_target_ids_len: Optional[int] = None\n+        self.prev_target_ids_len: int | None = None\n         self.prev_assistant_ids = None\n         self.target_lookbehind = assistant_model.generation_config.target_lookbehind\n         self.assistant_lookbehind = assistant_model.generation_config.assistant_lookbehind\n@@ -494,7 +494,7 @@ def convert_source_tokens_to_target_tokens(\n         dest_ids = destination_tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n         return dest_ids.to(input_ids.device)\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -712,7 +712,7 @@ def __init__(\n             self._get_assistant_to_target_input_ids()\n         )\n         self._suppress_input_ids: list[int] = self._get_suppress_input_ids()\n-        self.logits_processors: Optional[LogitsProcessorList] = None\n+        self.logits_processors: LogitsProcessorList | None = None\n         self.assistant_prune_lm_head = assistant_prune_lm_head and assistant_model is not None\n         if len(self._suppress_input_ids) > 0:\n             # the assistant vocab is not a subset of the target vocab\n@@ -900,7 +900,7 @@ def __init__(\n         generation_config: \"GenerationConfig\",\n         model_kwargs: dict,\n         atm_translator: AssistantToTargetTranslator,\n-        inputs_tensor: Optional[torch.Tensor] = None,\n+        inputs_tensor: torch.Tensor | None = None,\n         logits_processor: Optional[\"LogitsProcessorList\"] = None,\n     ):\n         # Initialize translator before parent class\n@@ -917,9 +917,9 @@ def __init__(\n         )\n         # Track sequence lengths and previous assistant IDs\n         self._target_seq_len_with_candidates: int = 0\n-        self._prev_assistant_ids: Optional[torch.LongTensor] = None\n+        self._prev_assistant_ids: torch.LongTensor | None = None\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n         \"\"\"\n         Simplified version of get_candidates that uses the translator cache for token conversion.\n         \"\"\"\n@@ -1028,12 +1028,12 @@ class PromptLookupCandidateGenerator(CandidateGenerator):\n \n     def __init__(\n         self,\n-        eos_token_id: Optional[torch.Tensor] = None,\n+        eos_token_id: torch.Tensor | None = None,\n         num_output_tokens: int = 10,\n         max_matching_ngram_size: int = 2,\n         max_length: int = 20,\n         logits_processor: Optional[\"LogitsProcessorList\"] = None,\n-        vocab_size: Optional[int] = None,\n+        vocab_size: int | None = None,\n     ):\n         self.num_output_tokens = num_output_tokens\n         self.max_matching_ngram_size = max_matching_ngram_size\n@@ -1045,7 +1045,7 @@ def __init__(\n         if self.max_matching_ngram_size <= 0 or self.num_output_tokens <= 0:\n             raise ValueError(\"Invalid max_matching_ngram_size or num_output_tokens\")\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -1186,7 +1186,7 @@ def __init__(\n         assistant_model: \"PreTrainedModel\",\n         generation_config: \"GenerationConfig\",\n         model_kwargs: dict,\n-        inputs_tensor: Optional[torch.Tensor] = None,\n+        inputs_tensor: torch.Tensor | None = None,\n         logits_processor: Optional[\"LogitsProcessorList\"] = None,\n     ):\n         super().__init__(\n@@ -1202,7 +1202,7 @@ def __init__(\n         self.assistant_early_exit = self.generation_config.assistant_early_exit\n         self.generation_config.assistant_early_exit = None\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n         # Temporarily sets the number of hidden layers to the early exit value\n         base_model = getattr(self.assistant_model, self.assistant_model.base_model_prefix)\n         original_num_hidden_layers = base_model.config.num_hidden_layers"
        },
        {
            "sha": "2168dc0bbe191ef17ce1c76da8aeb6688a2f54ae",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -20,7 +20,7 @@\n from abc import ABC, abstractmethod\n from collections.abc import Callable\n from dataclasses import dataclass, is_dataclass\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional\n \n from .. import __version__\n from ..configuration_utils import PreTrainedConfig\n@@ -703,8 +703,8 @@ def validate(self, strict=False):\n \n     def save_pretrained(\n         self,\n-        save_directory: Union[str, os.PathLike],\n-        config_file_name: Optional[Union[str, os.PathLike]] = None,\n+        save_directory: str | os.PathLike,\n+        config_file_name: str | os.PathLike | None = None,\n         push_to_hub: bool = False,\n         **kwargs,\n     ):\n@@ -763,12 +763,12 @@ def save_pretrained(\n     @classmethod\n     def from_pretrained(\n         cls,\n-        pretrained_model_name: Union[str, os.PathLike],\n-        config_file_name: Optional[Union[str, os.PathLike]] = None,\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        pretrained_model_name: str | os.PathLike,\n+        config_file_name: str | os.PathLike | None = None,\n+        cache_dir: str | os.PathLike | None = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         revision: str = \"main\",\n         **kwargs,\n     ) -> \"GenerationConfig\":\n@@ -930,7 +930,7 @@ def from_pretrained(\n             return config\n \n     @classmethod\n-    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n+    def _dict_from_json_file(cls, json_file: str | os.PathLike):\n         with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n             text = reader.read()\n         return json.loads(text)\n@@ -1071,7 +1071,7 @@ def convert_dataclass_to_dict(obj):\n \n         return json.dumps(config_dict, indent=2, sort_keys=True) + \"\\n\"\n \n-    def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True):\n+    def to_json_file(self, json_file_path: str | os.PathLike, use_diff: bool = True):\n         \"\"\"\n         Save this instance to a JSON file.\n \n@@ -1182,7 +1182,7 @@ def from_dict(cls, config_dict, **kwargs):\n             kwargs.pop(key, None)\n         return config\n \n-    def to_json_file(self, json_file_path: Union[str, os.PathLike]):\n+    def to_json_file(self, json_file_path: str | os.PathLike):\n         \"\"\"\n         Save this instance to a JSON file.\n \n@@ -1447,10 +1447,10 @@ class CompileConfig:\n     \"\"\"\n \n     fullgraph: bool = False\n-    dynamic: Optional[bool] = None\n-    backend: Union[str, Callable] = \"inductor\"\n+    dynamic: bool | None = None\n+    backend: str | Callable = \"inductor\"\n     mode: str = \"reduce-overhead\"\n-    options: Optional[dict] = None\n+    options: dict | None = None\n     # Used to flag our `generate` call to compile on e.g. CPU. Often not optimal, but useful for testing purposes.\n     _compile_all_devices = None\n "
        },
        {
            "sha": "b31177fafdf98703323be9a860ef7c1f96450a55",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from math import floor, gcd, sqrt\n-from typing import Optional\n \n import torch\n \n@@ -121,7 +120,7 @@ def __init__(\n         generation_config: GenerationConfig,\n         device: torch.device,\n         dtype: torch.dtype = torch.float16,\n-        tp_size: Optional[int] = None,\n+        tp_size: int | None = None,\n         allow_prefix_sharing: bool = True,\n     ) -> None:\n         \"\"\"Initialize a paged attention cache for efficient memory usage. Also turns in prefix sharing if the model has\n@@ -460,8 +459,8 @@ def get_available_memory(max_memory_percent: float = 1.0) -> int:\n \n     def infer_num_blocks_and_max_batch_tokens(\n         self,\n-        num_blocks: Optional[int] = None,\n-        max_batch_tokens: Optional[int] = None,\n+        num_blocks: int | None = None,\n+        max_batch_tokens: int | None = None,\n         max_memory_percent: float = 0.8,  # FIXME: it seems we overcommit memory, was changed from 0.9 which caused OOMs in our benchmarking CI\n         cache_dtype: torch.dtype = torch.float16,\n     ) -> tuple[int, int]:\n@@ -613,8 +612,8 @@ def compute_num_blocks(\n \n     def compute_memory_footprint(\n         self,\n-        num_blocks: Optional[int] = None,\n-        max_batch_tokens: Optional[int] = None,\n+        num_blocks: int | None = None,\n+        max_batch_tokens: int | None = None,\n         cache_dtype: torch.dtype = torch.float16,\n     ) -> tuple[int, int, int]:\n         \"\"\"Calculate the memory footprint breakdown for a given number of blocks and maximum batch tokens. The memory"
        },
        {
            "sha": "9d7ff2ccfe64a9bfc8e364bd705c0b1b98596d87",
            "filename": "src/transformers/generation/continuous_batching/cache_manager.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -16,7 +16,7 @@\n from collections import deque\n from collections.abc import Iterator\n from math import ceil\n-from typing import Optional, TypeVar\n+from typing import TypeVar\n \n from .requests import logger\n \n@@ -210,7 +210,7 @@ class CacheAllocator(ABC):\n     block_table: dict[str, list[int]]  # request_id -> list of block_ids allocated to the request\n \n     @abstractmethod\n-    def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> Optional[int]:\n+    def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> int | None:\n         \"\"\"Allocates (n_blocks) for a given (request_id) using the (block_manager). Returns the num of blocks allocated\n         if successful and None otherwise.\"\"\"\n \n@@ -250,7 +250,7 @@ def __init__(self, index: int, block_size: int) -> None:\n         self.block_size = block_size\n         self.block_table = {}\n \n-    def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> Optional[int]:\n+    def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> int | None:\n         \"\"\"Allocate (n_blocks) for a given (request_id) using the (block_manager). Returns the number of blocks\n         allocated if successful and None otherwise. For group of full attention layers, we always allocate the number of\n         requested blocks.\"\"\"\n@@ -320,7 +320,7 @@ def __init__(self, index: int, block_size: int, sliding_window: int) -> None:\n         self._max_blocks_per_request = ceil(self.sliding_window / self.block_size)\n         self.block_table = {}\n \n-    def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> Optional[int]:\n+    def allocate_blocks(self, n_blocks: int, request_id: str, block_manager: BlockManager) -> int | None:\n         \"\"\"Allocate (n_blocks) for a given (request_id) using the (block_manager). Returns the number of blocks\n         allocated otherwise. For group of sliding window attention layers, we only allocate up to the point where we can\n         fit an entire sliding window in the cache tensor.\"\"\""
        },
        {
            "sha": "81f1eb6c30253daced92d0f3f4ea2fc0ee5fb8d8",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 14,
            "deletions": 17,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -22,7 +22,6 @@\n from itertools import count\n from math import ceil\n from time import perf_counter\n-from typing import Optional\n \n import torch\n from torch import nn\n@@ -159,7 +158,7 @@ def build_attention_mask(\n @dataclass\n class PagedAttentionArgs:\n     input_ids: torch.Tensor\n-    attention_mask: Optional[torch.Tensor]\n+    attention_mask: torch.Tensor | None\n     position_ids: torch.Tensor\n     cumulative_seqlens_q: torch.Tensor\n     cumulative_seqlens_k: torch.Tensor\n@@ -221,7 +220,7 @@ def __init__(\n         # Accumulator for batch scheduling\n         self.requests_in_batch: list[RequestState] = []\n         # Cuda graphs for the generation step\n-        self._graphs: Optional[dict[tuple[int, int], torch.cuda.CUDAGraph]] = {} if use_cuda_graph else None\n+        self._graphs: dict[tuple[int, int], torch.cuda.CUDAGraph] | None = {} if use_cuda_graph else None\n \n         # Set up metrics collector\n         self.max_batch_tokens = cache.max_batch_tokens\n@@ -767,10 +766,10 @@ def __init__(\n         self.model.generation_config.top_p = None\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n         self.logit_processor = self.model._get_logits_processor(generation_config)\n-        use_cuda_graph: Optional[bool] = getattr(generation_config, \"use_cuda_graph\", None)\n+        use_cuda_graph: bool | None = getattr(generation_config, \"use_cuda_graph\", None)\n         self.profile = getattr(generation_config, \"profile\", False)  # TODO: not supported yet\n         self.manual_eviction = manual_eviction\n-        self.batch_processor: Optional[ContinuousBatchProcessor] = None\n+        self.batch_processor: ContinuousBatchProcessor | None = None\n \n         self._allow_prefix_sharing = allow_prefix_sharing\n \n@@ -813,7 +812,7 @@ def is_running(self) -> bool:\n         \"\"\"Check if the background generation thread is running.\"\"\"\n         return self._generation_thread is not None and self._generation_thread.is_alive()\n \n-    def stop(self, block: bool = True, timeout: Optional[float] = None) -> None:\n+    def stop(self, block: bool = True, timeout: float | None = None) -> None:\n         \"\"\"Signal the background thread to stop.\n \n         Args:\n@@ -844,7 +843,7 @@ def stop(self, block: bool = True, timeout: Optional[float] = None) -> None:\n \n         self.batch_processor = None\n \n-    def join(self, stop_trigger_time: float, timeout: Optional[float] = None) -> None:\n+    def join(self, stop_trigger_time: float, timeout: float | None = None) -> None:\n         \"\"\"Wait for the background thread to finish.\n \n         Args:\n@@ -862,8 +861,8 @@ def join(self, stop_trigger_time: float, timeout: Optional[float] = None) -> Non\n     def add_request(\n         self,\n         input_ids: list[int],\n-        request_id: Optional[str] = None,\n-        max_new_tokens: Optional[int] = None,\n+        request_id: str | None = None,\n+        max_new_tokens: int | None = None,\n         streaming: bool = False,\n     ) -> str:\n         \"\"\"Add a new generation request to the queue.\n@@ -898,7 +897,7 @@ def add_request(\n         return request_id\n \n     def add_requests(\n-        self, inputs: list[list[int]], max_new_tokens: Optional[int] = None, streaming: bool = False\n+        self, inputs: list[list[int]], max_new_tokens: int | None = None, streaming: bool = False\n     ) -> None:\n         for input_ids in inputs:\n             self.add_request(input_ids, max_new_tokens=max_new_tokens, streaming=streaming)\n@@ -913,9 +912,7 @@ def cancel_request(self, request_id: str) -> None:\n             self.batch_processor.scheduler.set_request_cancellation(request_id)\n \n     # TODO:handle benchmarking properly when updating / fixing the requeue logic\n-    def get_result(\n-        self, request_id: Optional[str] = None, timeout: Optional[float] = None\n-    ) -> Optional[GenerationOutput]:\n+    def get_result(self, request_id: str | None = None, timeout: float | None = None) -> GenerationOutput | None:\n         \"\"\"Retrieve one result from the output queue.\n \n         Args:\n@@ -961,7 +958,7 @@ def _generation_step(self) -> None:\n \n     def _run_generation_loop(self) -> None:\n         \"\"\"Main processing loop running in the background thread.\"\"\"\n-        batch_processor: Optional[ContinuousBatchProcessor] = None\n+        batch_processor: ContinuousBatchProcessor | None = None\n         try:\n             t0 = perf_counter()\n             paged_attention_cache = PagedAttentionCache(\n@@ -1032,7 +1029,7 @@ def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor) -> N\n         batch_processor.update_batch()\n \n     @traced\n-    def _handle_critical_error(self, error: Exception, batch_processor: Optional[ContinuousBatchProcessor]) -> None:\n+    def _handle_critical_error(self, error: Exception, batch_processor: ContinuousBatchProcessor | None) -> None:\n         \"\"\"Handle critical errors that terminate the generation loop.\"\"\"\n         # Signal stop\n         self.stop_event.set()\n@@ -1073,7 +1070,7 @@ def continuous_batching_context_manager(self, **kwargs) -> Generator[ContinuousB\n \n     def init_continuous_batching(\n         self,\n-        generation_config: Optional[GenerationConfig] = None,\n+        generation_config: GenerationConfig | None = None,\n         manual_eviction: bool = False,\n         max_queue_size: int = 0,\n         num_q_cuda_graphs: int = 0,\n@@ -1120,7 +1117,7 @@ def init_continuous_batching(\n     def generate_batch(\n         self,\n         inputs: list[list[int]],\n-        generation_config: Optional[GenerationConfig] = None,\n+        generation_config: GenerationConfig | None = None,\n         progress_bar: bool = True,\n         num_q_cuda_graphs: int = 0,\n         num_kv_cuda_graphs: int = 0,"
        },
        {
            "sha": "9d3412f756121a0b4faefd484036439a6e012938",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -15,7 +15,6 @@\n import time\n from dataclasses import dataclass, field\n from enum import Enum\n-from typing import Optional\n \n import torch\n \n@@ -87,7 +86,7 @@ class GenerationOutput:\n     prompt_ids: list[int] = field(default_factory=list)\n     generated_tokens: list[int] = field(default_factory=list)\n     logprobs: list[float] = field(default_factory=list)\n-    error: Optional[str] = None\n+    error: str | None = None\n     status: RequestStatus = RequestStatus.PENDING\n     created_time: float = field(default_factory=time.time)\n \n@@ -118,8 +117,8 @@ class RequestState:\n \n     # Required fields # TODO: come up with better names / not sure prompt_ids and such are not redundant\n     request_id: str\n-    full_prompt_ids: Optional[list[int]] = None  # Full initial prompt\n-    prompt_ids: Optional[list[int]] = None  # Tokens IDs currently being processed\n+    full_prompt_ids: list[int] | None = None  # Full initial prompt\n+    prompt_ids: list[int] | None = None  # Tokens IDs currently being processed\n     remaining_prompt_ids: list[int] = field(default_factory=list)  # For split requests, prefill left to process\n     static_outputs: list[int] = field(default_factory=list)  # Generated tokens\n     allocated_blocks: int = 0  # Number of blocks allocated to the request\n@@ -129,7 +128,7 @@ class RequestState:\n     eos_token_id: int = -1  # ID of the end-of-sequence token\n     streaming: bool = False  # Whether to stream tokens as they're generated\n     created_time: float = field(default_factory=time.time)  # Time the request was created\n-    error: Optional[str] = None  # Error message if the request failed\n+    error: str | None = None  # Error message if the request failed\n     lifespan: tuple[float, float] = (-1, -1)  # (time request was no longer pending, time request finished)\n \n     @property"
        },
        {
            "sha": "07f422e659aa7cfb06f211f1399919732ffa52ed",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 13,
            "deletions": 15,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -16,7 +16,7 @@\n import inspect\n import math\n from collections.abc import Callable, Iterable\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n import numpy as np\n import torch\n@@ -134,7 +134,7 @@ class MinLengthLogitsProcessor(LogitsProcessor):\n     ```\n     \"\"\"\n \n-    def __init__(self, min_length: int, eos_token_id: Union[int, list[int], torch.Tensor], device: str = \"cpu\"):\n+    def __init__(self, min_length: int, eos_token_id: int | list[int] | torch.Tensor, device: str = \"cpu\"):\n         if not isinstance(min_length, int) or min_length < 0:\n             raise ValueError(f\"`min_length` has to be a non-negative integer, but is {min_length}\")\n \n@@ -197,7 +197,7 @@ def __init__(\n         self,\n         prompt_length_to_skip: int,\n         min_new_tokens: int,\n-        eos_token_id: Union[int, list[int], torch.Tensor],\n+        eos_token_id: int | list[int] | torch.Tensor,\n         device: str = \"cpu\",\n     ):\n         for arg_name, arg_value in [\n@@ -344,7 +344,7 @@ class RepetitionPenaltyLogitsProcessor(LogitsProcessor):\n     ```\n     \"\"\"\n \n-    def __init__(self, penalty: float, prompt_ignore_length: Optional[int] = None):\n+    def __init__(self, penalty: float, prompt_ignore_length: int | None = None):\n         if not isinstance(penalty, float) or not (penalty > 0):\n             raise ValueError(f\"`penalty` has to be a strictly positive float, but is {penalty}\")\n \n@@ -1264,7 +1264,7 @@ class SequenceBiasLogitsProcessor(LogitsProcessor):\n     ```\n     \"\"\"\n \n-    def __init__(self, sequence_bias: list[list[Union[list[int], float]]]):\n+    def __init__(self, sequence_bias: list[list[list[int] | float]]):\n         self.sequence_bias = sequence_bias\n         self._validate_arguments()\n         self._convert_list_arguments_into_dict()\n@@ -1437,9 +1437,7 @@ class NoBadWordsLogitsProcessor(SequenceBiasLogitsProcessor):\n     ```\n     \"\"\"\n \n-    def __init__(\n-        self, bad_words_ids: list[list[int]], eos_token_id: Optional[Union[int, list[int], torch.Tensor]] = None\n-    ):\n+    def __init__(self, bad_words_ids: list[list[int]], eos_token_id: int | list[int] | torch.Tensor | None = None):\n         self.bad_word_ids = bad_words_ids\n         self._validate_arguments()\n \n@@ -1751,7 +1749,7 @@ class ForcedEOSTokenLogitsProcessor(LogitsProcessor):\n     ```\n     \"\"\"\n \n-    def __init__(self, max_length: int, eos_token_id: Union[int, list[int], torch.Tensor], device: str = \"cpu\"):\n+    def __init__(self, max_length: int, eos_token_id: int | list[int] | torch.Tensor, device: str = \"cpu\"):\n         self.max_length = max_length\n \n         if not isinstance(eos_token_id, torch.Tensor):\n@@ -1865,7 +1863,7 @@ class ExponentialDecayLengthPenalty(LogitsProcessor):\n     def __init__(\n         self,\n         exponential_decay_length_penalty: tuple[int, float],\n-        eos_token_id: Union[int, list[int], torch.Tensor],\n+        eos_token_id: int | list[int] | torch.Tensor,\n         input_ids_seq_length: int,\n     ):\n         self.regulation_start = exponential_decay_length_penalty[0] + input_ids_seq_length\n@@ -2087,7 +2085,7 @@ def __init__(\n         self,\n         generate_config: \"GenerationConfig\",\n         begin_index: int,\n-        _detect_timestamp_from_logprob: Optional[bool] = None,\n+        _detect_timestamp_from_logprob: bool | None = None,\n     ):  # support for the kwargs\n         self.no_timestamps_token_id = generate_config.no_timestamps_token_id\n         self.timestamp_begin = generate_config.no_timestamps_token_id + 1\n@@ -2398,8 +2396,8 @@ def __init__(\n         self,\n         guidance_scale: float,\n         model,\n-        unconditional_ids: Optional[torch.LongTensor] = None,\n-        unconditional_attention_mask: Optional[torch.LongTensor] = None,\n+        unconditional_ids: torch.LongTensor | None = None,\n+        unconditional_attention_mask: torch.LongTensor | None = None,\n         use_cache: bool = True,\n     ):\n         self.guidance_scale = guidance_scale\n@@ -2477,7 +2475,7 @@ class BarkEosPrioritizerLogitsProcessor(LogitsProcessor):\n             Minimum end of speech threshold.\n     \"\"\"\n \n-    def __init__(self, eos_token_id: Union[int, list[int], torch.Tensor], min_eos_p: float, device: str = \"cpu\"):\n+    def __init__(self, eos_token_id: int | list[int] | torch.Tensor, min_eos_p: float, device: str = \"cpu\"):\n         if not isinstance(eos_token_id, torch.Tensor):\n             if isinstance(eos_token_id, int):\n                 eos_token_id = [eos_token_id]\n@@ -3146,7 +3144,7 @@ class DiaClassifierFreeGuidanceLogitsProcessor(LogitsProcessor):\n             the logits of the combined CFG output, but the conditioned output only.\n     \"\"\"\n \n-    def __init__(self, guidance_scale: float, guidance_top_k: Optional[int] = None):\n+    def __init__(self, guidance_scale: float, guidance_top_k: int | None = None):\n         if guidance_scale > 1:\n             self.guidance_scale = guidance_scale\n         else:"
        },
        {
            "sha": "5fe0c4c9dcf5d71aca227b84ab2d7023cd944652",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -3,7 +3,6 @@\n from abc import ABC\n from collections import OrderedDict\n from copy import deepcopy\n-from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -68,7 +67,7 @@ class MaxLengthCriteria(StoppingCriteria):\n             The maximum model length, as defined by the model's `config.max_position_embeddings` attribute.\n     \"\"\"\n \n-    def __init__(self, max_length: int, max_position_embeddings: Optional[int] = None):\n+    def __init__(self, max_length: int, max_position_embeddings: int | None = None):\n         self.max_length = max_length\n         self.max_position_embeddings = max_position_embeddings\n \n@@ -98,7 +97,7 @@ class MaxTimeCriteria(StoppingCriteria):\n             The start of the generation allowed time.\n     \"\"\"\n \n-    def __init__(self, max_time: float, initial_timestamp: Optional[float] = None):\n+    def __init__(self, max_time: float, initial_timestamp: float | None = None):\n         self.max_time = max_time\n         self.initial_timestamp = time.time() if initial_timestamp is None else initial_timestamp\n \n@@ -239,7 +238,7 @@ class StopStringCriteria(StoppingCriteria):\n     ```\n     \"\"\"\n \n-    def __init__(self, tokenizer: PreTrainedTokenizerBase, stop_strings: Union[str, list[str]]):\n+    def __init__(self, tokenizer: PreTrainedTokenizerBase, stop_strings: str | list[str]):\n         if isinstance(stop_strings, str):\n             stop_strings = [stop_strings]\n         self.stop_strings: tuple[str, ...] = tuple(stop_strings)\n@@ -459,7 +458,7 @@ class EosTokenCriteria(StoppingCriteria):\n             The id(s) of the *end-of-sequence* token.\n     \"\"\"\n \n-    def __init__(self, eos_token_id: Union[int, list[int], torch.Tensor]):\n+    def __init__(self, eos_token_id: int | list[int] | torch.Tensor):\n         if not isinstance(eos_token_id, torch.Tensor):\n             if isinstance(eos_token_id, int):\n                 eos_token_id = [eos_token_id]\n@@ -503,7 +502,7 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwa\n         return is_done\n \n     @property\n-    def max_length(self) -> Optional[int]:\n+    def max_length(self) -> int | None:\n         for stopping_criterium in self:\n             if isinstance(stopping_criterium, MaxLengthCriteria):\n                 return stopping_criterium.max_length"
        },
        {
            "sha": "09d3555774aedf0f04a5c2e1edd5746941d2ac1f",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 87,
            "deletions": 87,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -175,11 +175,11 @@ class GenerateDecoderOnlyOutput(ModelOutput):\n     \"\"\"\n \n     sequences: torch.LongTensor\n-    scores: Optional[tuple[torch.FloatTensor]] = None\n-    logits: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[Cache] = None\n+    scores: tuple[torch.FloatTensor] | None = None\n+    logits: tuple[torch.FloatTensor] | None = None\n+    attentions: tuple[tuple[torch.FloatTensor]] | None = None\n+    hidden_states: tuple[tuple[torch.FloatTensor]] | None = None\n+    past_key_values: Cache | None = None\n \n \n @dataclass\n@@ -220,14 +220,14 @@ class GenerateEncoderDecoderOutput(ModelOutput):\n     \"\"\"\n \n     sequences: torch.LongTensor\n-    scores: Optional[tuple[torch.FloatTensor]] = None\n-    logits: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[Cache] = None\n+    scores: tuple[torch.FloatTensor] | None = None\n+    logits: tuple[torch.FloatTensor] | None = None\n+    encoder_attentions: tuple[torch.FloatTensor] | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor] | None = None\n+    decoder_attentions: tuple[tuple[torch.FloatTensor]] | None = None\n+    cross_attentions: tuple[tuple[torch.FloatTensor]] | None = None\n+    decoder_hidden_states: tuple[tuple[torch.FloatTensor]] | None = None\n+    past_key_values: Cache | None = None\n \n \n @dataclass\n@@ -265,13 +265,13 @@ class GenerateBeamDecoderOnlyOutput(ModelOutput):\n     \"\"\"\n \n     sequences: torch.LongTensor\n-    sequences_scores: Optional[torch.FloatTensor] = None\n-    scores: Optional[tuple[torch.FloatTensor]] = None\n-    logits: Optional[tuple[torch.FloatTensor]] = None\n-    beam_indices: Optional[torch.LongTensor] = None\n-    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[Cache] = None\n+    sequences_scores: torch.FloatTensor | None = None\n+    scores: tuple[torch.FloatTensor] | None = None\n+    logits: tuple[torch.FloatTensor] | None = None\n+    beam_indices: torch.LongTensor | None = None\n+    attentions: tuple[tuple[torch.FloatTensor]] | None = None\n+    hidden_states: tuple[tuple[torch.FloatTensor]] | None = None\n+    past_key_values: Cache | None = None\n \n \n @dataclass\n@@ -319,16 +319,16 @@ class GenerateBeamEncoderDecoderOutput(ModelOutput):\n     \"\"\"\n \n     sequences: torch.LongTensor\n-    sequences_scores: Optional[torch.FloatTensor] = None\n-    scores: Optional[tuple[torch.FloatTensor]] = None\n-    logits: Optional[tuple[torch.FloatTensor]] = None\n-    beam_indices: Optional[torch.LongTensor] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-    past_key_values: Optional[Cache] = None\n+    sequences_scores: torch.FloatTensor | None = None\n+    scores: tuple[torch.FloatTensor] | None = None\n+    logits: tuple[torch.FloatTensor] | None = None\n+    beam_indices: torch.LongTensor | None = None\n+    encoder_attentions: tuple[torch.FloatTensor] | None = None\n+    encoder_hidden_states: tuple[torch.FloatTensor] | None = None\n+    decoder_attentions: tuple[tuple[torch.FloatTensor]] | None = None\n+    cross_attentions: tuple[tuple[torch.FloatTensor]] | None = None\n+    decoder_hidden_states: tuple[tuple[torch.FloatTensor]] | None = None\n+    past_key_values: Cache | None = None\n \n \n # Typing shortcuts\n@@ -430,8 +430,8 @@ def adjust_generation_fn(\n \n     def load_custom_generate(\n         self,\n-        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]] = None,\n-        trust_remote_code: Optional[bool] = None,\n+        pretrained_model_name_or_path: str | os.PathLike | None = None,\n+        trust_remote_code: bool | None = None,\n         **kwargs,\n     ) -> Callable:\n         \"\"\"\n@@ -492,8 +492,8 @@ def load_custom_generate(\n     def _cache_dependant_input_preparation(\n         self,\n         input_ids: torch.LongTensor,\n-        inputs_embeds: Optional[torch.FloatTensor],\n-        cache_position: Optional[torch.LongTensor],\n+        inputs_embeds: torch.FloatTensor | None,\n+        cache_position: torch.LongTensor | None,\n     ) -> tuple[torch.FloatTensor, torch.LongTensor]:\n         \"\"\"\n         Generic cache-dependent input preparation\n@@ -526,8 +526,8 @@ def _cache_dependant_input_preparation(\n     def _cache_dependant_input_preparation_exporting(\n         self,\n         input_ids: torch.LongTensor,\n-        inputs_embeds: Optional[torch.FloatTensor],\n-        cache_position: Optional[torch.LongTensor],\n+        inputs_embeds: torch.FloatTensor | None,\n+        cache_position: torch.LongTensor | None,\n     ) -> tuple[torch.FloatTensor, torch.LongTensor]:\n         \"\"\"\n         This method implements method ``_cache_dependant_input_preparation``\n@@ -589,10 +589,10 @@ def branch_3(input_ids, cache_position):\n     def prepare_inputs_for_generation(\n         self,\n         input_ids: torch.LongTensor,\n-        past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n+        past_key_values: Cache | None = None,\n+        attention_mask: torch.LongTensor | None = None,\n+        inputs_embeds: torch.FloatTensor | None = None,\n+        cache_position: torch.LongTensor | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -737,10 +737,10 @@ def prepare_inputs_for_generation(\n \n     def _prepare_model_inputs(\n         self,\n-        inputs: Optional[torch.Tensor] = None,\n-        bos_token_id: Optional[torch.Tensor] = None,\n-        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n-    ) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:\n+        inputs: torch.Tensor | None = None,\n+        bos_token_id: torch.Tensor | None = None,\n+        model_kwargs: dict[str, torch.Tensor] | None = None,\n+    ) -> tuple[torch.Tensor, str | None, dict[str, torch.Tensor]]:\n         \"\"\"\n         This function extracts the model-specific `inputs` for generation.\n         \"\"\"\n@@ -804,9 +804,9 @@ def _prepare_model_inputs(\n \n     def _maybe_initialize_input_ids_for_generation(\n         self,\n-        inputs: Optional[torch.Tensor] = None,\n-        bos_token_id: Optional[torch.Tensor] = None,\n-        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n+        inputs: torch.Tensor | None = None,\n+        bos_token_id: torch.Tensor | None = None,\n+        model_kwargs: dict[str, torch.Tensor] | None = None,\n     ) -> torch.LongTensor:\n         \"\"\"Initializes input ids for generation, if necessary.\"\"\"\n         if inputs is not None:\n@@ -874,7 +874,7 @@ def _prepare_encoder_decoder_kwargs_for_generation(\n         self,\n         inputs_tensor: torch.Tensor,\n         model_kwargs,\n-        model_input_name: Optional[str],\n+        model_input_name: str | None,\n         generation_config: GenerationConfig,\n     ) -> dict[str, Any]:\n         # 1. get encoder\n@@ -917,7 +917,7 @@ def _prepare_decoder_input_ids_for_generation(\n         model_input_name: str,\n         model_kwargs: dict[str, torch.Tensor],\n         decoder_start_token_id: torch.Tensor,\n-        device: Optional[torch.device] = None,\n+        device: torch.device | None = None,\n     ) -> tuple[torch.LongTensor, dict[str, torch.Tensor]]:\n         \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n         # 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,\n@@ -974,7 +974,7 @@ def _prepare_decoder_input_ids_for_generation(\n     def _expand_inputs_for_generation(\n         expand_size: int = 1,\n         is_encoder_decoder: bool = False,\n-        input_ids: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n         **model_kwargs,\n     ) -> tuple[torch.LongTensor, dict[str, Any]]:\n         \"\"\"Expands tensors from [batch_size, ...] to [batch_size * expand_size, ...]\"\"\"\n@@ -1139,14 +1139,14 @@ def _get_candidate_generator(\n     def _get_logits_processor(\n         self,\n         generation_config: GenerationConfig,\n-        input_ids_seq_length: Optional[int] = None,\n-        encoder_input_ids: Optional[torch.LongTensor] = None,\n-        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n-        logits_processor: Optional[LogitsProcessorList] = None,\n-        device: Optional[str] = None,\n-        model_kwargs: Optional[dict[str, Any]] = None,\n-        negative_prompt_ids: Optional[torch.Tensor] = None,\n-        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n+        input_ids_seq_length: int | None = None,\n+        encoder_input_ids: torch.LongTensor | None = None,\n+        prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], list[int]] | None = None,\n+        logits_processor: LogitsProcessorList | None = None,\n+        device: str | None = None,\n+        model_kwargs: dict[str, Any] | None = None,\n+        negative_prompt_ids: torch.Tensor | None = None,\n+        negative_prompt_attention_mask: torch.Tensor | None = None,\n     ) -> LogitsProcessorList:\n         \"\"\"\n         This class returns a [`LogitsProcessorList`] list object that contains all relevant [`LogitsProcessor`]\n@@ -1362,7 +1362,7 @@ def _get_logits_processor(\n     def _get_stopping_criteria(\n         self,\n         generation_config: GenerationConfig,\n-        stopping_criteria: Optional[StoppingCriteriaList],\n+        stopping_criteria: StoppingCriteriaList | None,\n         tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n     ) -> StoppingCriteriaList:\n         criteria = StoppingCriteriaList()\n@@ -1399,9 +1399,9 @@ def _get_stopping_criteria(\n \n     def _merge_criteria_processor_list(\n         self,\n-        default_list: Union[LogitsProcessorList, StoppingCriteriaList],\n-        custom_list: Union[LogitsProcessorList, StoppingCriteriaList],\n-    ) -> Union[LogitsProcessorList, StoppingCriteriaList]:\n+        default_list: LogitsProcessorList | StoppingCriteriaList,\n+        custom_list: LogitsProcessorList | StoppingCriteriaList,\n+    ) -> LogitsProcessorList | StoppingCriteriaList:\n         \"\"\"\n         Merge user-defined processors/criteria with the ones instantiated inside `generate`. In case the same\n         processor/criteria is present on both lists, use the user-defined one.\n@@ -1438,7 +1438,7 @@ def compute_transition_scores(\n         self,\n         sequences: torch.Tensor,\n         scores: tuple[torch.Tensor],\n-        beam_indices: Optional[torch.Tensor] = None,\n+        beam_indices: torch.Tensor | None = None,\n         normalize_logits: bool = False,\n     ) -> torch.Tensor:\n         \"\"\"\n@@ -1759,8 +1759,8 @@ def _prepare_generated_length(\n \n     def _prepare_generation_config(\n         self,\n-        generation_config: Optional[GenerationConfig],\n-        use_model_defaults: Optional[bool] = None,\n+        generation_config: GenerationConfig | None,\n+        use_model_defaults: bool | None = None,\n         **kwargs: Any,\n     ) -> tuple[GenerationConfig, dict]:\n         \"\"\"\n@@ -2093,8 +2093,8 @@ def _supports_logits_to_keep(self) -> bool:\n     def _prepare_special_tokens(\n         self,\n         generation_config: GenerationConfig,\n-        kwargs_has_attention_mask: Optional[bool] = None,\n-        device: Optional[Union[torch.device, str]] = None,\n+        kwargs_has_attention_mask: bool | None = None,\n+        device: torch.device | str | None = None,\n     ):\n         \"\"\"\n         Prepares the special tokens for generation, overwriting the generation config with their processed versions\n@@ -2229,8 +2229,8 @@ def _get_deprecated_gen_repo(\n         self,\n         generation_mode: GenerationMode,\n         trust_remote_code: bool,\n-        custom_generate: Optional[str] = None,\n-    ) -> Optional[str]:\n+        custom_generate: str | None = None,\n+    ) -> str | None:\n         \"\"\"\n         Returns the Hub repo for a deprecated generation mode, if any.\n         \"\"\"\n@@ -2284,20 +2284,20 @@ def _extract_generation_mode_kwargs(\n     @torch.no_grad()\n     def generate(\n         self,\n-        inputs: Optional[torch.Tensor] = None,\n-        generation_config: Optional[GenerationConfig] = None,\n-        logits_processor: Optional[LogitsProcessorList] = None,\n-        stopping_criteria: Optional[StoppingCriteriaList] = None,\n-        prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n-        synced_gpus: Optional[bool] = None,\n+        inputs: torch.Tensor | None = None,\n+        generation_config: GenerationConfig | None = None,\n+        logits_processor: LogitsProcessorList | None = None,\n+        stopping_criteria: StoppingCriteriaList | None = None,\n+        prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], list[int]] | None = None,\n+        synced_gpus: bool | None = None,\n         assistant_model: Optional[\"PreTrainedModel\"] = None,\n         streamer: Optional[\"BaseStreamer\"] = None,\n-        negative_prompt_ids: Optional[torch.Tensor] = None,\n-        negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n-        use_model_defaults: Optional[bool] = None,\n-        custom_generate: Optional[Union[str, Callable]] = None,\n+        negative_prompt_ids: torch.Tensor | None = None,\n+        negative_prompt_attention_mask: torch.Tensor | None = None,\n+        use_model_defaults: bool | None = None,\n+        custom_generate: str | Callable | None = None,\n         **kwargs,\n-    ) -> Union[GenerateOutput, torch.LongTensor]:\n+    ) -> GenerateOutput | torch.LongTensor:\n         r\"\"\"\n \n         Generates sequences of token ids for models with a language modeling head.\n@@ -2805,7 +2805,7 @@ def _sample(\n         synced_gpus: bool = False,\n         streamer: Optional[\"BaseStreamer\"] = None,\n         **model_kwargs,\n-    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n+    ) -> GenerateNonBeamOutput | torch.LongTensor:\n         r\"\"\"\n         Generates sequences of token ids for models with a language modeling head using **multinomial sampling** and\n         can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n@@ -3014,7 +3014,7 @@ def _check_early_stop_heuristic(\n         cur_len: int,\n         max_length: int,\n         decoder_prompt_len: int,\n-        early_stopping: Union[bool, str],\n+        early_stopping: bool | str,\n         length_penalty: float,\n     ):\n         \"\"\"\n@@ -3057,7 +3057,7 @@ def _beam_search_has_unfinished_sequences(\n         is_early_stop_heuristic_unsatisfied: torch.Tensor,\n         is_sent_finished: torch.Tensor,\n         next_token_hits_stopping_criteria: torch.Tensor,\n-        early_stopping: Union[bool, str],\n+        early_stopping: bool | str,\n     ):\n         \"\"\"\n         Beam Search stopping condition -- halts the generation loop if any of these conditions becomes False\n@@ -3167,7 +3167,7 @@ def _update_finished_beams(\n         cur_len: int,\n         decoder_prompt_len: int,\n         length_penalty: float,\n-        early_stopping: Union[bool, str],\n+        early_stopping: bool | str,\n     ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Updates the finished beams if (and only if) there are new completed sequences that have a higher score than\n@@ -3214,7 +3214,7 @@ def _beam_search(\n         generation_config: GenerationConfig,\n         synced_gpus: bool = False,\n         **model_kwargs,\n-    ) -> Union[GenerateBeamOutput, torch.LongTensor]:\n+    ) -> GenerateBeamOutput | torch.LongTensor:\n         r\"\"\"\n         Generates sequences of token ids for models with a language modeling head using **beam search decoding** and\n         can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n@@ -3555,12 +3555,12 @@ def _assisted_decoding(\n         generation_config: GenerationConfig,\n         synced_gpus: bool = False,\n         streamer: Optional[\"BaseStreamer\"] = None,\n-        inputs_tensor: Optional[torch.FloatTensor] = None,\n+        inputs_tensor: torch.FloatTensor | None = None,\n         assistant_model: Optional[\"PreTrainedModel\"] = None,\n         assistant_tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n         tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n         **model_kwargs,\n-    ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n+    ) -> GenerateNonBeamOutput | torch.LongTensor:\n         r\"\"\"\n         Generates sequences of token ids for models with a language modeling head using **greedy decoding** or\n         **sample** (depending on `do_sample`), assisted by candidate sequences. Assisted generation is an example of a"
        },
        {
            "sha": "40794a0cec79537e499dd0ce67d24eac6d505afc",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -16,7 +16,7 @@\n import collections\n from dataclasses import dataclass\n from functools import lru_cache\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import torch\n@@ -57,13 +57,13 @@ class WatermarkDetectorOutput:\n             Array containing confidence scores of a text being machine-generated for each element in the batch.\n     \"\"\"\n \n-    num_tokens_scored: Optional[np.ndarray] = None\n-    num_green_tokens: Optional[np.ndarray] = None\n-    green_fraction: Optional[np.ndarray] = None\n-    z_score: Optional[np.ndarray] = None\n-    p_value: Optional[np.ndarray] = None\n-    prediction: Optional[np.ndarray] = None\n-    confidence: Optional[np.ndarray] = None\n+    num_tokens_scored: np.ndarray | None = None\n+    num_green_tokens: np.ndarray | None = None\n+    green_fraction: np.ndarray | None = None\n+    z_score: np.ndarray | None = None\n+    p_value: np.ndarray | None = None\n+    prediction: np.ndarray | None = None\n+    confidence: np.ndarray | None = None\n \n \n class WatermarkDetector:\n@@ -122,7 +122,7 @@ def __init__(\n         self,\n         model_config: PreTrainedConfig,\n         device: str,\n-        watermarking_config: Union[WatermarkingConfig, dict],\n+        watermarking_config: WatermarkingConfig | dict,\n         ignore_repeated_ngrams: bool = False,\n         max_cache_size: int = 128,\n     ):\n@@ -191,7 +191,7 @@ def __call__(\n         input_ids: torch.LongTensor,\n         z_threshold: float = 3.0,\n         return_dict: bool = False,\n-    ) -> Union[WatermarkDetectorOutput, np.ndarray]:\n+    ) -> WatermarkDetectorOutput | np.ndarray:\n         \"\"\"\n                 Args:\n                 input_ids (`torch.LongTensor`):\n@@ -253,7 +253,7 @@ class BayesianDetectorConfig(PreTrainedConfig):\n             Prior probability P(w) that a text is watermarked.\n     \"\"\"\n \n-    def __init__(self, watermarking_depth: Optional[int] = None, base_rate: float = 0.5, **kwargs):\n+    def __init__(self, watermarking_depth: int | None = None, base_rate: float = 0.5, **kwargs):\n         self.watermarking_depth = watermarking_depth\n         self.base_rate = base_rate\n         # These can be set later to store information about this detector.\n@@ -279,8 +279,8 @@ class BayesianWatermarkDetectorModelOutput(ModelOutput):\n             Multiple choice classification loss.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    posterior_probabilities: Optional[torch.FloatTensor] = None\n+    loss: torch.FloatTensor | None = None\n+    posterior_probabilities: torch.FloatTensor | None = None\n \n \n class BayesianDetectorWatermarkedLikelihood(nn.Module):\n@@ -436,7 +436,7 @@ def forward(\n         self,\n         g_values: torch.Tensor,\n         mask: torch.Tensor,\n-        labels: Optional[torch.Tensor] = None,\n+        labels: torch.Tensor | None = None,\n         loss_batch_weight=1,\n         return_dict=False,\n     ) -> BayesianWatermarkDetectorModelOutput:"
        },
        {
            "sha": "2fbd6de4feaae547a71bf45403db74133abd7f48",
            "filename": "src/transformers/integrations/eager_paged.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feager_paged.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -1,5 +1,3 @@\n-from typing import Optional\n-\n import torch\n from torch import nn\n \n@@ -23,12 +21,12 @@ def eager_paged_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],  # shape [seqlen_q, seqlen_k]\n+    attention_mask: torch.Tensor | None,  # shape [seqlen_q, seqlen_k]\n     scaling: float,\n     **kwargs,\n ):\n     # Add KV cache to the key and value tensors\n-    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n+    cache: PagedAttentionCache | None = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n         key, value = cache.update("
        },
        {
            "sha": "4ec72d6a0c0e474a695e6d3a53d0d33dd7d7b963",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 29,
            "deletions": 30,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,6 @@\n # specific language governing permissions and limitations under the License.\n \n import logging\n-from typing import Optional\n \n import torch\n \n@@ -193,9 +192,9 @@ class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n     def __init__(\n         self,\n         model: PreTrainedModel,\n-        batch_size: Optional[int] = None,\n-        max_cache_len: Optional[int] = None,\n-        device: Optional[torch.device] = None,\n+        batch_size: int | None = None,\n+        max_cache_len: int | None = None,\n+        device: torch.device | None = None,\n     ) -> None:\n         \"\"\"\n         Initializes the exportable module.\n@@ -225,9 +224,9 @@ def __init__(\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        cache_position: torch.Tensor | None = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n@@ -248,11 +247,11 @@ def forward(\n \n     def export(\n         self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n-        dynamic_shapes: Optional[dict] = None,\n-        strict: Optional[bool] = None,\n+        input_ids: torch.Tensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        cache_position: torch.Tensor | None = None,\n+        dynamic_shapes: dict | None = None,\n+        strict: bool | None = None,\n     ) -> torch.export.ExportedProgram:\n         \"\"\"\n         Export the wrapped module using `torch.export`.\n@@ -461,9 +460,9 @@ class TorchExportableModuleWithStaticCache(torch.nn.Module):\n     def __init__(\n         self,\n         model: PreTrainedModel,\n-        batch_size: Optional[int] = None,\n-        max_cache_len: Optional[int] = None,\n-        device: Optional[torch.device] = None,\n+        batch_size: int | None = None,\n+        max_cache_len: int | None = None,\n+        device: torch.device | None = None,\n     ) -> None:\n         \"\"\"\n         Initializes the wrapper module with the pretrained model.\n@@ -534,9 +533,9 @@ def __init__(\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        cache_position: torch.Tensor | None = None,\n     ):\n         \"\"\"\n         Forward pass of the module, which is compatible with the ExecuTorch runtime.\n@@ -640,9 +639,9 @@ class TorchExportableModuleWithHybridCache(torch.nn.Module):\n     def __init__(\n         self,\n         model: PreTrainedModel,\n-        batch_size: Optional[int] = None,\n-        max_cache_len: Optional[int] = None,\n-        device: Optional[torch.device] = None,\n+        batch_size: int | None = None,\n+        max_cache_len: int | None = None,\n+        device: torch.device | None = None,\n     ) -> None:\n         \"\"\"\n         Initializes the exportable module.\n@@ -702,9 +701,9 @@ def __init__(\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        cache_position: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor | None = None,\n+        inputs_embeds: torch.Tensor | None = None,\n+        cache_position: torch.Tensor | None = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n@@ -733,10 +732,10 @@ def forward(\n \n def convert_and_export_with_cache(\n     model: PreTrainedModel,\n-    example_input_ids: Optional[torch.Tensor] = None,\n-    example_cache_position: Optional[torch.Tensor] = None,\n-    dynamic_shapes: Optional[dict] = None,\n-    strict: Optional[bool] = None,\n+    example_input_ids: torch.Tensor | None = None,\n+    example_cache_position: torch.Tensor | None = None,\n+    dynamic_shapes: dict | None = None,\n+    strict: bool | None = None,\n ):\n     \"\"\"\n     Convert a `PreTrainedModel` into an exportable module and export it using `torch.export`,\n@@ -1003,8 +1002,8 @@ def generate(self, prompt_token_ids, max_new_tokens):\n \n def export_with_dynamic_cache(\n     model: PreTrainedModel,\n-    example_input_ids: Optional[torch.Tensor] = None,\n-    example_attention_mask: Optional[torch.Tensor] = None,\n+    example_input_ids: torch.Tensor | None = None,\n+    example_attention_mask: torch.Tensor | None = None,\n ):\n     \"\"\"\n     Export a model with DynamicCache using `torch.export`, ensuring the exported model is compatible with `ExecuTorch`."
        },
        {
            "sha": "92725e9b26a6b9ac2c38cb609afd60d3974fe109",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -246,7 +246,7 @@ def w8a8_block_fp8_matmul_compile(\n     weight_q: torch.Tensor,  # [out_features, hidden_dim]\n     input_scale: torch.Tensor,  # [batch * seq_len, num_input_groups]\n     weight_scale: torch.Tensor,  # [num_weight_blocks_m, num_weight_blocks_n]\n-    block_size: Optional[tuple[int, int]] = None,  # (M=128, N=128) for weights for example\n+    block_size: tuple[int, int] | None = None,  # (M=128, N=128) for weights for example\n     output_dtype: torch.dtype = torch.float32,\n ) -> torch.Tensor:\n     \"\"\"\n@@ -315,7 +315,7 @@ def __init__(\n         out_features: int,\n         bias: bool = False,\n         dtype=None,\n-        block_size: Optional[tuple[int, int]] = None,\n+        block_size: tuple[int, int] | None = None,\n         device=None,\n         activation_scheme=\"dynamic\",\n     ):"
        },
        {
            "sha": "9b097225bf1532097768745c723c535b0c3261ad",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -1,5 +1,3 @@\n-from typing import Optional\n-\n import torch\n \n from ..modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n@@ -34,12 +32,12 @@ def flash_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     dropout: float = 0.0,\n-    scaling: Optional[float] = None,\n-    sliding_window: Optional[int] = None,\n-    softcap: Optional[float] = None,\n-    is_causal: Optional[bool] = None,\n+    scaling: float | None = None,\n+    sliding_window: int | None = None,\n+    softcap: float | None = None,\n+    is_causal: bool | None = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n     if kwargs.get(\"output_attentions\", False):"
        },
        {
            "sha": "2f0719fb8b05c82d15cba5b67cce14332250c039",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -1,5 +1,3 @@\n-from typing import Optional\n-\n import torch\n \n from ..generation.continuous_batching import PagedAttentionCache\n@@ -28,7 +26,7 @@ def paged_attention_forward(\n     q: torch.Tensor,\n     k: torch.Tensor,\n     v: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor] = None,\n+    attention_mask: torch.Tensor | None = None,\n     cache: PagedAttentionCache = None,\n     cu_seq_lens_q=None,\n     cu_seq_lens_k=None,"
        },
        {
            "sha": "43c216dfef864da7d9e481152695956205f1ff63",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -26,7 +26,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional, Union\n+from typing import Union\n \n import torch\n from packaging import version\n@@ -90,7 +90,7 @@ def compile_friendly_flex_attention(\n     value: torch.Tensor,\n     training=False,\n     **kwargs,\n-) -> Union[torch.Tensor, tuple[torch.Tensor, torch.Tensor]]:\n+) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:\n     # First call initialise singleton wrapper object, second call invokes the object method to return compiled flex attention\n     # Do not use compiled version if already compiling forward (it raises issues)\n     flex_attention_compiled = WrappedFlexAttention(training)() if not is_torchdynamo_compiling() else flex_attention\n@@ -108,11 +108,11 @@ def compile_friendly_flex_attention(\n # TODO: deprecate / rename to make_flex_block_mask for clarity as it's not only causal anymore\n def make_flex_block_causal_mask(\n     attention_mask_2d: torch.Tensor,\n-    attention_chunk_size: Optional[int] = None,\n+    attention_chunk_size: int | None = None,\n     query_length=None,\n     key_length=None,\n-    offsets: Optional[tuple[Offset, Offset]] = None,\n-    is_causal: Optional[bool] = True,\n+    offsets: tuple[Offset, Offset] | None = None,\n+    is_causal: bool | None = True,\n ) -> \"BlockMask\":\n     \"\"\"\n     IMPORTANT NOTICE: This function is deprecated in favor of using the mask primitives in `masking_utils.py`,\n@@ -238,11 +238,11 @@ def flex_attention_forward(\n     key: torch.Tensor,\n     value: torch.Tensor,\n     attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-    scaling: Optional[float] = None,\n-    softcap: Optional[float] = None,\n-    s_aux: Optional[torch.Tensor] = None,\n+    scaling: float | None = None,\n+    softcap: float | None = None,\n+    s_aux: torch.Tensor | None = None,\n     **kwargs,\n-) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+) -> tuple[torch.Tensor, torch.Tensor | None]:\n     if kwargs.get(\"dropout\", 0.0) > 0:\n         raise ValueError(\n             \"`flex_attention` does not support `dropout`. Please use it with inference\""
        },
        {
            "sha": "7e7ab73b1c5f8fb42466c7a140b2dbca631f79b1",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -14,7 +14,6 @@\n \"HIGGS through FLUTE (Flexible Lookup Table Engine for LUT-quantized LLMs) integration file\"\n \n from math import sqrt\n-from typing import Optional\n \n from ..utils import (\n     is_flute_available,\n@@ -497,8 +496,8 @@ def __init__(\n         out_features: int,\n         num_bits: int,\n         bias=True,\n-        dtype: Optional[torch.dtype] = None,\n-        device: Optional[torch.device] = None,\n+        dtype: torch.dtype | None = None,\n+        device: torch.device | None = None,\n         group_size: int = 256,\n         hadamard_size: int = 1024,\n     ):"
        },
        {
            "sha": "0ab866ecbd6dc3c9d57edfe9d875b6cc21240db4",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -16,7 +16,6 @@\n from collections.abc import Callable\n from functools import partial\n from types import ModuleType\n-from typing import Optional, Union\n \n from ..modeling_flash_attention_utils import lazy_import_flash_attention\n from ..utils import ENV_VARS_TRUE_VALUES, logging\n@@ -51,7 +50,7 @@ def use_kernel_forward_from_hub(layer_name: str):\n             )\n             return lambda cls: cls\n \n-    _KERNEL_MAPPING: dict[str, dict[Union[Device, str], LayerRepository]] = {\n+    _KERNEL_MAPPING: dict[str, dict[Device | str, LayerRepository]] = {\n         \"MultiScaleDeformableAttention\": {\n             \"cuda\": LayerRepository(\n                 repo_id=\"kernels-community/deformable-detr\",\n@@ -207,18 +206,18 @@ def register_kernel_mapping(*args, **kwargs):\n     \"causal-conv1d\": {\"repo_id\": \"kernels-community/causal-conv1d\"},\n }\n \n-_KERNEL_MODULE_MAPPING: dict[str, Optional[ModuleType]] = {}\n+_KERNEL_MODULE_MAPPING: dict[str, ModuleType | None] = {}\n \n \n-def is_kernel(attn_implementation: Optional[str]) -> bool:\n+def is_kernel(attn_implementation: str | None) -> bool:\n     \"\"\"Check whether `attn_implementation` matches a kernel pattern from the hub.\"\"\"\n     return (\n         attn_implementation is not None\n         and re.search(r\"^[^/:]+/[^/:]+(?:@[^/:]+)?(?::[^/:]+)?$\", attn_implementation) is not None\n     )\n \n \n-def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: Optional[Callable] = None) -> None:\n+def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: Callable | None = None) -> None:\n     \"\"\"\n     Load and register the kernel associated to `attn_implementation`.\n \n@@ -272,7 +271,7 @@ def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: O\n     ALL_MASK_ATTENTION_FUNCTIONS.register(attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"])\n \n \n-def lazy_load_kernel(kernel_name: str, mapping: dict[str, Optional[ModuleType]] = _KERNEL_MODULE_MAPPING):\n+def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _KERNEL_MODULE_MAPPING):\n     if kernel_name in mapping and isinstance(mapping[kernel_name], ModuleType):\n         return mapping[kernel_name]\n     if kernel_name not in _HUB_KERNEL_MAPPING:"
        },
        {
            "sha": "29de72c05415289b1c2efafd533d297d61d18c82",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -29,7 +29,7 @@\n from dataclasses import fields\n from enum import Enum\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Literal, Optional, Union\n+from typing import TYPE_CHECKING, Any, Literal\n \n import numpy as np\n import packaging.version\n@@ -1491,13 +1491,13 @@ class NeptuneCallback(TrainerCallback):\n     def __init__(\n         self,\n         *,\n-        api_token: Optional[str] = None,\n-        project: Optional[str] = None,\n-        name: Optional[str] = None,\n+        api_token: str | None = None,\n+        project: str | None = None,\n+        name: str | None = None,\n         base_namespace: str = \"finetuning\",\n         run=None,\n         log_parameters: bool = True,\n-        log_checkpoints: Optional[str] = None,\n+        log_checkpoints: str | None = None,\n         **neptune_run_kwargs,\n     ):\n         if not is_neptune_available():\n@@ -1524,7 +1524,7 @@ def __init__(\n         self._base_namespace_path = base_namespace\n         self._log_parameters = log_parameters\n         self._log_checkpoints = log_checkpoints\n-        self._initial_run: Optional[Run] = run\n+        self._initial_run: Run | None = run\n \n         self._run = None\n         self._is_monitoring_run = False\n@@ -1712,7 +1712,7 @@ def get_run(cls, trainer):\n \n         raise Exception(\"The trainer doesn't have a NeptuneCallback configured.\")\n \n-    def on_log(self, args, state, control, logs: Optional[dict[str, float]] = None, **kwargs):\n+    def on_log(self, args, state, control, logs: dict[str, float] | None = None, **kwargs):\n         if not state.is_world_process_zero:\n             return\n \n@@ -2091,8 +2091,8 @@ class DVCLiveCallback(TrainerCallback):\n \n     def __init__(\n         self,\n-        live: Optional[Any] = None,\n-        log_model: Optional[Union[Literal[\"all\"], bool]] = None,\n+        live: Any | None = None,\n+        log_model: Literal[\"all\"] | bool | None = None,\n         **kwargs,\n     ):\n         if not is_dvclive_available():"
        },
        {
            "sha": "3c274804afc6ed22cae05353867a9f06c8d2d4e5",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -17,7 +17,7 @@\n import json\n import os\n import re\n-from typing import Any, Literal, Optional, Union\n+from typing import Any, Literal\n \n from packaging import version\n \n@@ -89,24 +89,24 @@ class PeftAdapterMixin:\n     \"\"\"\n \n     _hf_peft_config_loaded = False\n-    _prepare_peft_hotswap_kwargs: Optional[dict] = None\n+    _prepare_peft_hotswap_kwargs: dict | None = None\n \n     def load_adapter(\n         self,\n-        peft_model_id: Optional[str] = None,\n-        adapter_name: Optional[str] = None,\n-        revision: Optional[str] = None,\n-        token: Optional[str] = None,\n+        peft_model_id: str | None = None,\n+        adapter_name: str | None = None,\n+        revision: str | None = None,\n+        token: str | None = None,\n         device_map: str = \"auto\",\n-        max_memory: Optional[str] = None,\n-        offload_folder: Optional[str] = None,\n-        offload_index: Optional[int] = None,\n-        peft_config: Optional[dict[str, Any]] = None,\n-        adapter_state_dict: Optional[dict[str, \"torch.Tensor\"]] = None,\n+        max_memory: str | None = None,\n+        offload_folder: str | None = None,\n+        offload_index: int | None = None,\n+        peft_config: dict[str, Any] | None = None,\n+        adapter_state_dict: dict[str, \"torch.Tensor\"] | None = None,\n         low_cpu_mem_usage: bool = False,\n         is_trainable: bool = False,\n         hotswap: bool | Literal[\"auto\"] = \"auto\",\n-        adapter_kwargs: Optional[dict[str, Any]] = None,\n+        adapter_kwargs: dict[str, Any] | None = None,\n     ) -> None:\n         \"\"\"\n         Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT methods, we\n@@ -446,7 +446,7 @@ def enable_peft_hotswap(\n         self._hotswap_enabled = True\n         self._prepare_peft_hotswap_kwargs = {\"target_rank\": target_rank, \"check_compiled\": check_compiled}\n \n-    def add_adapter(self, adapter_config, adapter_name: Optional[str] = None) -> None:\n+    def add_adapter(self, adapter_config, adapter_name: str | None = None) -> None:\n         r\"\"\"\n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n         official documentation: https://huggingface.co/docs/peft\n@@ -486,7 +486,7 @@ def add_adapter(self, adapter_config, adapter_name: Optional[str] = None) -> Non\n \n         self.set_adapter(adapter_name)\n \n-    def set_adapter(self, adapter_name: Union[list[str], str]) -> None:\n+    def set_adapter(self, adapter_name: list[str] | str) -> None:\n         \"\"\"\n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n         official documentation: https://huggingface.co/docs/peft\n@@ -608,7 +608,7 @@ def active_adapters(self) -> list[str]:\n \n         return active_adapters\n \n-    def get_adapter_state_dict(self, adapter_name: Optional[str] = None, state_dict: Optional[dict] = None) -> dict:\n+    def get_adapter_state_dict(self, adapter_name: str | None = None, state_dict: dict | None = None) -> dict:\n         \"\"\"\n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n         official documentation: https://huggingface.co/docs/peft\n@@ -640,9 +640,9 @@ def get_adapter_state_dict(self, adapter_name: Optional[str] = None, state_dict:\n     def _dispatch_accelerate_model(\n         self,\n         device_map: str,\n-        max_memory: Optional[int] = None,\n-        offload_folder: Optional[str] = None,\n-        offload_index: Optional[int] = None,\n+        max_memory: int | None = None,\n+        offload_folder: str | None = None,\n+        offload_index: int | None = None,\n     ) -> None:\n         \"\"\"\n         Optional re-dispatch the model and attach new hooks to the model in case the model has been loaded with\n@@ -693,7 +693,7 @@ def _dispatch_accelerate_model(\n             **dispatch_model_kwargs,\n         )\n \n-    def delete_adapter(self, adapter_names: Union[list[str], str]) -> None:\n+    def delete_adapter(self, adapter_names: list[str] | str) -> None:\n         \"\"\"\n         Delete a PEFT adapter from the underlying model.\n "
        },
        {
            "sha": "a44d4f37288f15ba59b6fe3f7c545cf4406c97b5",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -1,5 +1,3 @@\n-from typing import Optional\n-\n import torch\n \n from ..utils import is_torch_npu_available, is_torch_xpu_available, logging\n@@ -27,7 +25,7 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-def use_gqa_in_sdpa(attention_mask: Optional[torch.Tensor], key: torch.Tensor) -> bool:\n+def use_gqa_in_sdpa(attention_mask: torch.Tensor | None, key: torch.Tensor) -> bool:\n     # GQA can only be used under the following conditions\n     # 1.cuda or Ascend NPU\n     #   - torch version >= 2.5\n@@ -44,10 +42,10 @@ def sdpa_attention_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     dropout: float = 0.0,\n-    scaling: Optional[float] = None,\n-    is_causal: Optional[bool] = None,\n+    scaling: float | None = None,\n+    is_causal: bool | None = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n     if kwargs.get(\"output_attentions\", False):"
        },
        {
            "sha": "20c218492fc058aa34499cf7a83209f4a62b9497",
            "filename": "src/transformers/integrations/sdpa_paged.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -1,5 +1,3 @@\n-from typing import Optional\n-\n import torch\n \n from ..generation.continuous_batching.cache import PagedAttentionCache\n@@ -22,13 +20,13 @@ def sdpa_attention_paged_forward(\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: torch.Tensor | None,\n     dropout: float = 0.0,\n-    scaling: Optional[float] = None,\n+    scaling: float | None = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n     # Add KV cache to the key and value tensors\n-    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n+    cache: PagedAttentionCache | None = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n         key, value = cache.update("
        },
        {
            "sha": "52b43f779f35ed0adf8b0fbd42e91ecf46c2408b",
            "filename": "src/transformers/loss/loss_for_object_detection.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_for_object_detection.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional\n \n import torch\n import torch.nn as nn\n@@ -440,7 +439,7 @@ def _max_by_axis(the_list):\n \n \n class NestedTensor:\n-    def __init__(self, tensors, mask: Optional[Tensor]):\n+    def __init__(self, tensors, mask: Tensor | None):\n         self.tensors = tensors\n         self.mask = mask\n "
        },
        {
            "sha": "63cec4c97e7b7d190eb54df7631df625a40e3611",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Optional\n \n import torch\n import torch.nn as nn\n@@ -28,7 +27,7 @@\n def fixed_cross_entropy(\n     source: torch.Tensor,\n     target: torch.Tensor,\n-    num_items_in_batch: Optional[torch.Tensor] = None,\n+    num_items_in_batch: torch.Tensor | None = None,\n     ignore_index: int = -100,\n     **kwargs,\n ) -> torch.Tensor:\n@@ -46,9 +45,9 @@ def ForCausalLMLoss(\n     logits,\n     labels,\n     vocab_size: int,\n-    num_items_in_batch: Optional[torch.Tensor] = None,\n+    num_items_in_batch: torch.Tensor | None = None,\n     ignore_index: int = -100,\n-    shift_labels: Optional[torch.Tensor] = None,\n+    shift_labels: torch.Tensor | None = None,\n     **kwargs,\n ) -> torch.Tensor:\n     # Upcast to float if we need to compute the loss to avoid potential precision issues\n@@ -71,7 +70,7 @@ def ForMaskedLMLoss(\n     logits: torch.Tensor,\n     labels: torch.Tensor,\n     vocab_size: int,\n-    num_items_in_batch: Optional[torch.Tensor] = None,\n+    num_items_in_batch: torch.Tensor | None = None,\n     ignore_index: int = -100,\n     **kwargs,\n ):"
        },
        {
            "sha": "41b3b46dfdbb3990d7ff2954e409c10e9751ef92",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import warnings\n-from typing import Optional, Union\n \n from ..models.auto.configuration_auto import AutoConfig\n from ..utils import logging\n@@ -160,7 +159,7 @@ class AutoHfQuantizer:\n     \"\"\"\n \n     @classmethod\n-    def from_config(cls, quantization_config: Union[QuantizationConfigMixin, dict], **kwargs):\n+    def from_config(cls, quantization_config: QuantizationConfigMixin | dict, **kwargs):\n         # Convert it to a QuantizationConfig if the q_config is a dict\n         if isinstance(quantization_config, dict):\n             quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n@@ -192,8 +191,8 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n     @classmethod\n     def merge_quantization_configs(\n         cls,\n-        quantization_config: Union[dict, QuantizationConfigMixin],\n-        quantization_config_from_args: Optional[QuantizationConfigMixin],\n+        quantization_config: dict | QuantizationConfigMixin,\n+        quantization_config_from_args: QuantizationConfigMixin | None,\n     ):\n         \"\"\"\n         handles situations where both quantization_config from args and quantization_config from model config are present."
        },
        {
            "sha": "642a2c68065f18ba520322e025fed8f1b0aa41be",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n from abc import ABC, abstractmethod\n from copy import deepcopy\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any\n \n from ..utils import is_accelerate_available, is_torch_available, logging\n from ..utils.quantization_config import QuantizationConfigMixin, QuantizationMethod\n@@ -141,7 +141,7 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         \"\"\"\n         return dtype\n \n-    def update_device_map(self, device_map: Optional[dict[str, Any]]) -> Optional[dict[str, Any]]:\n+    def update_device_map(self, device_map: dict[str, Any] | None) -> dict[str, Any] | None:\n         \"\"\"\n         Override this method if you want to pass a override the existing device map with a new\n         one. E.g. for bitsandbytes, since `accelerate` is a hard requirement, if no device_map is\n@@ -206,7 +206,7 @@ def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: lis\n     def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n         return unexpected_keys\n \n-    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n+    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         \"\"\"adjust max_memory argument for infer_auto_device_map() if extra memory is needed for quantization\"\"\"\n         return max_memory\n \n@@ -344,8 +344,8 @@ def get_param_name(self, param_name: str) -> str:\n     @staticmethod\n     def get_modules_to_not_convert(\n         model: \"PreTrainedModel\",\n-        skip_modules: Optional[list[str]] = None,\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        skip_modules: list[str] | None = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         add_default_skips: bool = False,\n     ):\n         if skip_modules is None or add_default_skips:"
        },
        {
            "sha": "9574f3e1fb3472aa444eb5728e7c13f8c7137c78",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import importlib.metadata\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from packaging import version\n \n@@ -108,7 +108,7 @@ def update_dtype(self, dtype):\n         return dtype\n \n     def _process_model_before_weight_loading(\n-        self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[list[str]] = None, **kwargs\n+        self, model: \"PreTrainedModel\", keep_in_fp32_modules: list[str] | None = None, **kwargs\n     ):\n         from ..integrations import replace_quantization_scales, replace_with_awq_linear\n "
        },
        {
            "sha": "b82357d90e5bad5d1e78ac1a4495faa675ccc3a4",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n \n@@ -72,7 +72,7 @@ def validate_environment(self, *args, **kwargs):\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_bitnet_linear\n@@ -88,7 +88,7 @@ def _process_model_before_weight_loading(\n             pre_quantized=self.pre_quantized,\n         )\n \n-    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n+    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n "
        },
        {
            "sha": "8796cdf213b464f083e649457fcb2ae7fe29af4c",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections import defaultdict\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n@@ -199,7 +199,7 @@ def create_quantized_param(\n             module._parameters[tensor_name] = new_value\n \n     # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.adjust_max_memory\n-    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n+    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         # need more space for buffers that are created during quantization\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n@@ -242,7 +242,7 @@ def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n         device_map,\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_bnb_linear"
        },
        {
            "sha": "bdf49b3fb53aad2170441cb581e7930a3e837475",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n \n@@ -97,7 +97,7 @@ def validate_environment(self, *args, **kwargs):\n                     \"for more details. \"\n                 )\n \n-    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n+    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         # need more space for buffers that are created during quantization\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n@@ -195,7 +195,7 @@ def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n         device_map,\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_bnb_linear"
        },
        {
            "sha": "823f652412a9027d3ab50c4c26f1364935f18ec9",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n \n@@ -140,7 +140,7 @@ def create_quantized_param(\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_eetq_linear"
        },
        {
            "sha": "48b81ef1dc8bb600f5b563900deda2000926341b",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n \n@@ -195,7 +195,7 @@ def create_quantized_param(\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_fbgemm_fp8_linear"
        },
        {
            "sha": "293dede7dc52133d01c278169315587b84a42ad8",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from ..utils import is_accelerate_available, is_torch_available, is_torch_xpu_available, logging\n from .base import HfQuantizer\n@@ -153,7 +153,7 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations.finegrained_fp8 import replace_with_fp8_linear"
        },
        {
            "sha": "f780bd5e52abfa4d8687b7cbe6cd081962c40fe4",
            "filename": "src/transformers/quantizers/quantizer_higgs.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from ..utils.logging import tqdm\n from .base import HfQuantizer\n@@ -116,7 +116,7 @@ def create_quantized_param(\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_higgs_linear"
        },
        {
            "sha": "c4e7c16dbf9a9e7449e8957f493c62a3c8ad3944",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n \n@@ -296,7 +296,7 @@ def update_expected_keys(self, model: \"PreTrainedModel\", expected_keys: list[str\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_mxfp4_linear"
        },
        {
            "sha": "cd4bcc60152c8f8d987650a0455b35578d095ac7",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n@@ -112,7 +112,7 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n         else:\n             return False\n \n-    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n+    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n \n@@ -144,7 +144,7 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         return target_dtype\n \n     def _process_model_before_weight_loading(\n-        self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[list[str]] = None, **kwargs\n+        self, model: \"PreTrainedModel\", keep_in_fp32_modules: list[str] | None = None, **kwargs\n     ):\n         from ..integrations import replace_with_quanto_layers\n "
        },
        {
            "sha": "ce549b732ca2ae794ddbe3281a2ab01c9d7f9fe1",
            "filename": "src/transformers/quantizers/quantizer_spqr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n \n@@ -65,7 +65,7 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         self.modules_to_not_convert = self.get_modules_to_not_convert("
        },
        {
            "sha": "d186aff620f98776a69341885a06ae01ecdd2ef0",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -15,7 +15,7 @@\n import re\n import types\n from collections import defaultdict\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from packaging import version\n \n@@ -49,7 +49,7 @@\n logger = logging.get_logger(__name__)\n \n \n-def fuzzy_match_size(config_name: str) -> Optional[str]:\n+def fuzzy_match_size(config_name: str) -> str | None:\n     \"\"\"\n     Extract the size digit from strings like \"4weight\", \"8weight\".\n     Returns the digit as an integer if found, otherwise None.\n@@ -162,7 +162,7 @@ def update_dtype(self, dtype):\n                 dtype = torch.float32\n         return dtype\n \n-    def get_state_dict_and_metadata(self, model, safe_serialization: Optional[bool] = False):\n+    def get_state_dict_and_metadata(self, model, safe_serialization: bool | None = False):\n         \"\"\"\n         If the model is safe serializable, we flatten the state dict of tensor subclasses so that it is compatible with\n         the safetensors format.\n@@ -212,13 +212,13 @@ def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n                 \"`pip install --upgrade accelerate`\"\n             )\n \n-    def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n+    def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:\n         # need more space for the quantization parameters (e.g. scale). Tested with int4 wo and group size = 128\n         max_memory = {key: val * 0.9 for key, val in max_memory.items()}\n         return max_memory\n \n     def _process_model_before_weight_loading(\n-        self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[list[str]] = None, **kwargs\n+        self, model: \"PreTrainedModel\", keep_in_fp32_modules: list[str] | None = None, **kwargs\n     ):\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules"
        },
        {
            "sha": "e5e30152261c415e55173c4291a675f994aefcff",
            "filename": "src/transformers/quantizers/quantizer_vptq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73a9bc3756cb15b86c10202a62c128121315d347/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py?ref=73a9bc3756cb15b86c10202a62c128121315d347",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n from .base import HfQuantizer\n \n@@ -68,7 +68,7 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: Optional[list[str]] = None,\n+        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 815,
        "additions": 393,
        "deletions": 422
    }
}