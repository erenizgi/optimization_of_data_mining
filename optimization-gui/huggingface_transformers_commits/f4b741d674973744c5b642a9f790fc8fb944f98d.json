{
    "author": "muellerzr",
    "message": "Fixup DeepSpeed things (#34007)",
    "sha": "f4b741d674973744c5b642a9f790fc8fb944f98d",
    "files": [
        {
            "sha": "8d6d4bdb480e6715bf7791a050ec045fd43378b7",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f4b741d674973744c5b642a9f790fc8fb944f98d/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f4b741d674973744c5b642a9f790fc8fb944f98d/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=f4b741d674973744c5b642a9f790fc8fb944f98d",
            "patch": "@@ -229,6 +229,7 @@\n if is_accelerate_available():\n     from accelerate import Accelerator, skip_first_batches\n     from accelerate import __version__ as accelerate_version\n+    from accelerate.state import AcceleratorState\n     from accelerate.utils import (\n         DistributedDataParallelKwargs,\n         DistributedType,\n@@ -1676,6 +1677,10 @@ def _hp_search_setup(self, trial: Union[\"optuna.Trial\", Dict[str, Any]]):\n             self.args.hf_deepspeed_config.trainer_config_process(self.args)\n             self.args.deepspeed_plugin = DeepSpeedPlugin(hf_ds_config=self.args.hf_deepspeed_config)\n \n+            # From 1.0 on, we need to fully wipe the DS plugin when doing sweeps.\n+            # Simply calling `_reset_state` is enough and doesn't need a version pin.\n+            AcceleratorState()._reset_state()\n+\n         self.create_accelerator_and_postprocess()\n \n     def _report_to_hp_search(self, trial: Union[\"optuna.Trial\", Dict[str, Any]], step: int, metrics: Dict[str, float]):"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}