{
    "author": "lilin-1",
    "message": "Docs/i18n updates (#42006)\n\n* docs(i18n): Update translations and terminology in Traditional and Simplified Chinese documentation\\n\\n- Correct the translation explanation of \"Trainer\" in Traditional Chinese\\n- Update the terminology for \"Named Entity Recognition\" in Simplified Chinese\\n- Adjust the display format of confidence values\\n- Unify the expression for installation pages\n\n* docs(i18n): Update translations and terminology in Traditional and Simplified Chinese documentation\n\n- Correct the translation explanation of \"Trainer\" in Traditional Chinese\n- Update the terminology for \"Named Entity Recognition\" in Simplified Chinese\n- Adjust the display format of confidence values\n- Unify the expression for installation pages\n\n* docs(i18n): update Simplified Chinese and Traditional Chinese README files\n\nupdate content to reflect latest library features and usage examples\nadd new model badges and installation instructions\nimprove overall structure and clarity",
    "sha": "1fd63dd5328ba4fdc847831c32b618e321945e7e",
    "files": [
        {
            "sha": "fbc65b40a0e7943e33c87439510de0d1dde438a3",
            "filename": "i18n/README_zh-hans.md",
            "status": "modified",
            "additions": 190,
            "deletions": 113,
            "changes": 303,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fd63dd5328ba4fdc847831c32b618e321945e7e/i18n%2FREADME_zh-hans.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fd63dd5328ba4fdc847831c32b618e321945e7e/i18n%2FREADME_zh-hans.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_zh-hans.md?ref=1fd63dd5328ba4fdc847831c32b618e321945e7e",
            "patch": "@@ -50,6 +50,7 @@ checkpoint: æ£€æŸ¥ç‚¹\n </p>\n \n <p align=\"center\">\n+    <a href=\"https://huggingface.co/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n     <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n     <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n     <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n@@ -60,15 +61,15 @@ checkpoint: æ£€æŸ¥ç‚¹\n \n <h4 align=\"center\">\n     <p>\n-        <a href=\"https://github.com/huggingface/transformers/\">English</a> |\n+        <a href=\"https://github.com/huggingface/transformers/blob/main/README.md\">English</a> |\n         <b>ç®€ä½“ä¸­æ–‡</b> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">ç¹é«”ä¸­æ–‡</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">í•œêµ­ì–´</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">EspaÃ±ol</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">æ—¥æœ¬èª</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> |\n-        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">Ğ ortuguÃªs</a> |\n+        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">PortuguÃªs</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">à°¤à±†à°²à±à°—à±</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">FranÃ§ais</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n@@ -81,182 +82,258 @@ checkpoint: æ£€æŸ¥ç‚¹\n </h4>\n \n <h3 align=\"center\">\n-    <p>ä¸º Jaxã€PyTorch å’Œ TensorFlow æ‰“é€ çš„å…ˆè¿›çš„è‡ªç„¶è¯­è¨€å¤„ç†å‡½æ•°åº“</p>\n+    <p>ä¸ºæ–‡æœ¬ã€è§†è§‰ã€éŸ³é¢‘ã€è§†é¢‘ä¸å¤šæ¨¡æ€æä¾›æ¨ç†ä¸è®­ç»ƒçš„å…ˆè¿›é¢„è®­ç»ƒæ¨¡å‹</p>\n </h3>\n \n <h3 align=\"center\">\n-    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png\"/>\n </h3>\n \n-ğŸ¤— Transformers æä¾›äº†æ•°ä»¥åƒè®¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒ 100 å¤šç§è¯­è¨€çš„æ–‡æœ¬åˆ†ç±»ã€ä¿¡æ¯æŠ½å–ã€é—®ç­”ã€æ‘˜è¦ã€ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€‚å®ƒçš„å®—æ—¨æ˜¯è®©æœ€å…ˆè¿›çš„ NLP æŠ€æœ¯äººäººæ˜“ç”¨ã€‚\n+Transformers å……å½“è·¨æ–‡æœ¬ã€è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘ã€è§†é¢‘ä¸å¤šæ¨¡æ€çš„æœ€å…ˆè¿›æœºå™¨å­¦ä¹ æ¨¡å‹çš„ã€Œæ¨¡å‹å®šä¹‰æ¡†æ¶ã€ï¼ŒåŒæ—¶è¦†ç›–æ¨ç†ä¸è®­ç»ƒã€‚\n \n-ğŸ¤— Transformers æä¾›äº†ä¾¿äºå¿«é€Ÿä¸‹è½½å’Œä½¿ç”¨çš„APIï¼Œè®©ä½ å¯ä»¥æŠŠé¢„è®­ç»ƒæ¨¡å‹ç”¨åœ¨ç»™å®šæ–‡æœ¬ã€åœ¨ä½ çš„æ•°æ®é›†ä¸Šå¾®è°ƒç„¶åé€šè¿‡ [model hub](https://huggingface.co/models) ä¸ç¤¾åŒºå…±äº«ã€‚åŒæ—¶ï¼Œæ¯ä¸ªå®šä¹‰çš„ Python æ¨¡å—éƒ½æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œä¾¿äºä¿®æ”¹å’Œå¿«é€Ÿè¿›è¡Œç ”ç©¶å®éªŒã€‚\n+å®ƒå°†æ¨¡å‹çš„å®šä¹‰é›†ä¸­åŒ–ï¼Œä½¿æ•´ä¸ªç”Ÿæ€ç³»ç»Ÿå¯¹è¯¥å®šä¹‰è¾¾æˆä¸€è‡´ã€‚`transformers` æ˜¯è·¨æ¡†æ¶çš„æ¢çº½ï¼šä¸€æ—¦æŸæ¨¡å‹å®šä¹‰è¢«æ”¯æŒï¼Œå®ƒé€šå¸¸å°±èƒ½å…¼å®¹å¤šæ•°è®­ç»ƒæ¡†æ¶ï¼ˆå¦‚ Axolotlã€Unslothã€DeepSpeedã€FSDPã€PyTorchâ€‘Lightning ç­‰ï¼‰ã€æ¨ç†å¼•æ“ï¼ˆå¦‚ vLLMã€SGLangã€TGI ç­‰ï¼‰ï¼Œä»¥åŠä¾èµ– `transformers` æ¨¡å‹å®šä¹‰çš„ç›¸å…³åº“ï¼ˆå¦‚ llama.cppã€mlx ç­‰ï¼‰ã€‚\n \n-ğŸ¤— Transformers æ”¯æŒä¸‰ä¸ªæœ€çƒ­é—¨çš„æ·±åº¦å­¦ä¹ åº“ï¼š [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) ä»¥åŠ [TensorFlow](https://www.tensorflow.org/) â€” å¹¶ä¸ä¹‹æ— ç¼æ•´åˆã€‚ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨ä¸€ä¸ªæ¡†æ¶è®­ç»ƒä½ çš„æ¨¡å‹ç„¶åç”¨å¦ä¸€ä¸ªåŠ è½½å’Œæ¨ç†ã€‚\n+æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æŒç»­æ”¯æŒæ–°çš„æœ€å…ˆè¿›æ¨¡å‹ï¼Œå¹¶é€šè¿‡è®©æ¨¡å‹å®šä¹‰ä¿æŒç®€å•ã€å¯å®šåˆ¶ä¸”é«˜æ•ˆæ¥æ™®åŠå…¶ä½¿ç”¨ã€‚\n \n-## åœ¨çº¿æ¼”ç¤º\n+ç›®å‰åœ¨ [Hugging Face Hub](https://huggingface.com/models) ä¸Šæœ‰è¶…è¿‡ 1M+ ä½¿ç”¨ `transformers` çš„[æ¨¡å‹æ£€æŸ¥ç‚¹](https://huggingface.co/models?library=transformers&sort=trending)ï¼Œå¯éšå–éšç”¨ã€‚\n+ \n+ä»Šå¤©å°±å»æ¢ç´¢ Hubï¼Œæ‰¾åˆ°ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ç”¨ Transformers ç«‹åˆ»å¼€å§‹å§ã€‚\n \n-ä½ å¯ä»¥ç›´æ¥åœ¨æ¨¡å‹é¡µé¢ä¸Šæµ‹è¯•å¤§å¤šæ•° [model hub](https://huggingface.co/models) ä¸Šçš„æ¨¡å‹ã€‚ æˆ‘ä»¬ä¹Ÿæä¾›äº† [ç§æœ‰æ¨¡å‹æ‰˜ç®¡ã€æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ä»¥åŠæ¨ç†API](https://huggingface.co/pricing)ã€‚\n+## å®‰è£…\n+\n+Transformers æ”¯æŒ Python 3.9+ï¼Œä»¥åŠ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+ã€‚\n \n-è¿™é‡Œæ˜¯ä¸€äº›ä¾‹å­ï¼š\n-- [ç”¨ BERT åšæ©ç å¡«è¯](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\n-- [ç”¨ Electra åšå‘½åå®ä½“è¯†åˆ«](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\n-- [ç”¨ GPT-2 åšæ–‡æœ¬ç”Ÿæˆ](https://huggingface.co/openai-community/gpt2?text=A+long+time+ago%2C+)\n-- [ç”¨ RoBERTa åšè‡ªç„¶è¯­è¨€æ¨ç†](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\n-- [ç”¨ BART åšæ–‡æœ¬æ‘˜è¦](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\n-- [ç”¨ DistilBERT åšé—®ç­”](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n-- [ç”¨ T5 åšç¿»è¯‘](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)\n+ä½¿ç”¨ [venv](https://docs.python.org/3/library/venv.html) æˆ– [uv](https://docs.astral.sh/uv/)ï¼ˆä¸€ä¸ªåŸºäº Rust çš„å¿«é€Ÿ Python åŒ…ä¸é¡¹ç›®ç®¡ç†å™¨ï¼‰åˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒï¼š\n \n-**[Write With Transformer](https://transformer.huggingface.co)**ï¼Œç”± Hugging Face å›¢é˜Ÿæ‰“é€ ï¼Œæ˜¯ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆçš„å®˜æ–¹ demoã€‚\n+```py\n+# venv\n+python -m venv .my-env\n+source .my-env/bin/activate\n+# uv\n+uv venv .my-env\n+source .my-env/bin/activate\n+```\n \n-## å¦‚æœä½ åœ¨å¯»æ‰¾ç”± Hugging Face å›¢é˜Ÿæä¾›çš„å®šåˆ¶åŒ–æ”¯æŒæœåŠ¡\n+åœ¨è™šæ‹Ÿç¯å¢ƒä¸­å®‰è£… Transformersï¼š\n \n-<a target=\"_blank\" href=\"https://huggingface.co/support\">\n-    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://huggingface.co/front/thumbnails/support.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n-</a><br>\n+```py\n+# pip\n+pip install \"transformers[torch]\"\n+\n+# uv\n+uv pip install \"transformers[torch]\"\n+```\n+\n+å¦‚æœä½ éœ€è¦åº“ä¸­çš„æœ€æ–°æ”¹åŠ¨æˆ–è®¡åˆ’å‚ä¸è´¡çŒ®ï¼Œå¯ä»æºç å®‰è£…ï¼ˆæ³¨æ„ï¼šæœ€æ–°ç‰ˆå¯èƒ½ä¸ç¨³å®šï¼›å¦‚é‡é”™è¯¯ï¼Œæ¬¢è¿åœ¨ [issues](https://github.com/huggingface/transformers/issues) ä¸­åé¦ˆï¼‰ï¼š\n+\n+```shell\n+git clone https://github.com/huggingface/transformers.git\n+cd transformers\n+\n+# pip\n+pip install '.[torch]'\n+\n+# uv\n+uv pip install '.[torch]'\n+```\n \n ## å¿«é€Ÿä¸Šæ‰‹\n \n-æˆ‘ä»¬ä¸ºå¿«é€Ÿä½¿ç”¨æ¨¡å‹æä¾›äº† `pipeline` APIã€‚Pipeline èšåˆäº†é¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”çš„æ–‡æœ¬é¢„å¤„ç†ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªå¿«é€Ÿä½¿ç”¨ pipeline å»åˆ¤æ–­æ­£è´Ÿé¢æƒ…ç»ªçš„ä¾‹å­ï¼š\n+ä½¿ç”¨ [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API ä¸€æ­¥ä¸Šæ‰‹ã€‚`Pipeline` æ˜¯ä¸€ä¸ªé«˜çº§æ¨ç†ç±»ï¼Œæ”¯æŒæ–‡æœ¬ã€éŸ³é¢‘ã€è§†è§‰ä¸å¤šæ¨¡æ€ä»»åŠ¡ï¼Œè´Ÿè´£è¾“å…¥é¢„å¤„ç†å¹¶è¿”å›é€‚é…çš„è¾“å‡ºã€‚\n+\n+å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ pipelineï¼ŒæŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ã€‚æ¨¡å‹ä¼šè¢«ä¸‹è½½å¹¶ç¼“å­˜ï¼Œæ–¹ä¾¿å¤ç”¨ã€‚æœ€åä¼ å…¥æ–‡æœ¬ä½œä¸ºæç¤ºï¼š\n \n-```python\n->>> from transformers import pipeline\n+```py\n+from transformers import pipeline\n \n-# ä½¿ç”¨æƒ…ç»ªåˆ†æ pipeline\n->>> classifier = pipeline('sentiment-analysis')\n->>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n-[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n+pipeline = pipeline(task=\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\")\n+pipeline(\"the secret to baking a really good cake is \")\n+[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]\n ```\n \n-ç¬¬äºŒè¡Œä»£ç ä¸‹è½½å¹¶ç¼“å­˜äº† pipeline ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè€Œç¬¬ä¸‰è¡Œä»£ç åˆ™åœ¨ç»™å®šçš„æ–‡æœ¬ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚è¿™é‡Œçš„ç­”æ¡ˆ\"æ­£é¢\" (positive) å…·æœ‰ 99 çš„ç½®ä¿¡åº¦ã€‚\n+è¦ä¸æ¨¡å‹è¿›è¡Œã€ŒèŠå¤©ã€ï¼Œç”¨æ³•ä¹Ÿä¸€è‡´ã€‚å”¯ä¸€ä¸åŒæ˜¯éœ€è¦æ„é€ ä¸€æ®µã€ŒèŠå¤©å†å²ã€ï¼ˆå³ `Pipeline` çš„è¾“å…¥ï¼‰ï¼š\n \n-è®¸å¤šçš„ NLP ä»»åŠ¡éƒ½æœ‰å¼€ç®±å³ç”¨çš„é¢„è®­ç»ƒ `pipeline`ã€‚æ¯”å¦‚è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾çš„ä»ç»™å®šæ–‡æœ¬ä¸­æŠ½å–é—®é¢˜ç­”æ¡ˆï¼š\n+> [!TIP]\n+> ä½ ä¹Ÿå¯ä»¥ç›´æ¥åœ¨å‘½ä»¤è¡Œä¸æ¨¡å‹èŠå¤©ï¼š\n+> ```shell\n+> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n+> ```\n \n-``` python\n->>> from transformers import pipeline\n+```py\n+import torch\n+from transformers import pipeline\n \n-# ä½¿ç”¨é—®ç­” pipeline\n->>> question_answerer = pipeline('question-answering')\n->>> question_answerer({\n-...     'question': 'What is the name of the repository ?',\n-...     'context': 'Pipeline has been included in the huggingface/transformers repository'\n-... })\n-{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface/transformers'}\n+chat = [\n+    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n+    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n+]\n \n+pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\n+response = pipeline(chat, max_new_tokens=512)\n+print(response[0][\"generated_text\"][-1][\"content\"])\n ```\n \n-é™¤äº†ç»™å‡ºç­”æ¡ˆï¼Œé¢„è®­ç»ƒæ¨¡å‹è¿˜ç»™å‡ºäº†å¯¹åº”çš„ç½®ä¿¡åº¦åˆ†æ•°ã€ç­”æ¡ˆåœ¨è¯ç¬¦åŒ– (tokenized) åçš„æ–‡æœ¬ä¸­å¼€å§‹å’Œç»“æŸçš„ä½ç½®ã€‚ä½ å¯ä»¥ä»[è¿™ä¸ªæ•™ç¨‹](https://huggingface.co/docs/transformers/task_summary)äº†è§£æ›´å¤š `pipeline` API æ”¯æŒçš„ä»»åŠ¡ã€‚\n+å±•å¼€ä¸‹æ–¹ç¤ºä¾‹ï¼ŒæŸ¥çœ‹ `Pipeline` åœ¨ä¸åŒæ¨¡æ€ä¸ä»»åŠ¡ä¸­çš„ç”¨æ³•ã€‚\n \n-è¦åœ¨ä½ çš„ä»»åŠ¡ä¸Šä¸‹è½½å’Œä½¿ç”¨ä»»æ„é¢„è®­ç»ƒæ¨¡å‹ä¹Ÿå¾ˆç®€å•ï¼Œåªéœ€ä¸‰è¡Œä»£ç ã€‚è¿™é‡Œæ˜¯ PyTorch ç‰ˆçš„ç¤ºä¾‹ï¼š\n-```python\n->>> from transformers import AutoTokenizer, AutoModel\n+<details>\n+<summary>è‡ªåŠ¨è¯­éŸ³è¯†åˆ«</summary>\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n->>> model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n+```py\n+from transformers import pipeline\n \n->>> inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n->>> outputs = model(**inputs)\n+pipeline = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n+pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n+{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n ```\n-è¿™é‡Œæ˜¯ç­‰æ•ˆçš„ TensorFlow ä»£ç ï¼š\n-```python\n->>> from transformers import AutoTokenizer, TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n->>> model = TFAutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n+</details>\n+\n+<details>\n+<summary>å›¾åƒåˆ†ç±»</summary>\n \n->>> inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n->>> outputs = model(**inputs)\n+<h3 align=\"center\">\n+    <a><img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"></a>\n+</h3>\n+\n+```py\n+from transformers import pipeline\n+\n+pipeline = pipeline(task=\"image-classification\", model=\"facebook/dinov2-small-imagenet1k-1-layer\")\n+pipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n+[{\"label\": \"macaw\", \"score\": 0.997848391532898},\n+ {\"label\": \"sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\",\n+  \"score\": 0.0016551691805943847},\n+ {\"label\": \"lorikeet\", \"score\": 0.00018523589824326336},\n+ {\"label\": \"African grey, African gray, Psittacus erithacus\",\n+  \"score\": 7.85409429227002e-05},\n+ {\"label\": \"quail\", \"score\": 5.502637941390276e-05}]\n ```\n \n-è¯ç¬¦åŒ–å™¨ (tokenizer) ä¸ºæ‰€æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹æä¾›äº†é¢„å¤„ç†ï¼Œå¹¶å¯ä»¥ç›´æ¥å¯¹å•ä¸ªå­—ç¬¦ä¸²è¿›è¡Œè°ƒç”¨ï¼ˆæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼‰æˆ–å¯¹åˆ—è¡¨ (list) è°ƒç”¨ã€‚å®ƒä¼šè¾“å‡ºä¸€ä¸ªä½ å¯ä»¥åœ¨ä¸‹æ¸¸ä»£ç é‡Œä½¿ç”¨æˆ–ç›´æ¥é€šè¿‡ `**` è§£åŒ…è¡¨è¾¾å¼ä¼ ç»™æ¨¡å‹çš„è¯å…¸ (dict)ã€‚\n+</details>\n+\n+<details>\n+<summary>è§†è§‰é—®ç­”</summary>\n+\n+<h3 align=\"center\">\n+    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\"></a>\n+</h3>\n \n-æ¨¡å‹æœ¬èº«æ˜¯ä¸€ä¸ªå¸¸è§„çš„ [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) æˆ– [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ï¼ˆå–å†³äºä½ çš„åç«¯ï¼‰ï¼Œå¯ä»¥å¸¸è§„æ–¹å¼ä½¿ç”¨ã€‚ [è¿™ä¸ªæ•™ç¨‹](https://huggingface.co/transformers/training.html)è§£é‡Šäº†å¦‚ä½•å°†è¿™æ ·çš„æ¨¡å‹æ•´åˆåˆ°ç»å…¸çš„ PyTorch æˆ– TensorFlow è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ–æ˜¯å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„ `Trainer` ï¼ˆè®­ç»ƒå™¨ï¼‰API æ¥åœ¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†ä¸Šå¿«é€Ÿå¾®è°ƒã€‚\n+```py\n+from transformers import pipeline\n \n-## ä¸ºä»€ä¹ˆè¦ç”¨ transformersï¼Ÿ\n+pipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\n+pipeline(\n+    image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\n+    question=\"What is in the image?\",\n+)\n+[{\"answer\": \"statue of liberty\"}]\n+```\n \n-1. ä¾¿äºä½¿ç”¨çš„å…ˆè¿›æ¨¡å‹ï¼š\n-    - NLU å’Œ NLG ä¸Šè¡¨ç°ä¼˜è¶Š\n-    - å¯¹æ•™å­¦å’Œå®è·µå‹å¥½ä¸”ä½é—¨æ§›\n-    - é«˜çº§æŠ½è±¡ï¼Œåªéœ€äº†è§£ä¸‰ä¸ªç±»\n-    - å¯¹æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€çš„API\n+</details>\n \n-1. æ›´ä½è®¡ç®—å¼€é”€ï¼Œæ›´å°‘çš„ç¢³æ’æ”¾ï¼š\n-    - ç ”ç©¶äººå‘˜å¯ä»¥åˆ†äº«å·²è®­ç»ƒçš„æ¨¡å‹è€Œéæ¯æ¬¡ä»å¤´å¼€å§‹è®­ç»ƒ\n-    - å·¥ç¨‹å¸ˆå¯ä»¥å‡å°‘è®¡ç®—ç”¨æ—¶å’Œç”Ÿäº§ç¯å¢ƒå¼€é”€\n-    - æ•°åç§æ¨¡å‹æ¶æ„ã€ä¸¤åƒå¤šä¸ªé¢„è®­ç»ƒæ¨¡å‹ã€100å¤šç§è¯­è¨€æ”¯æŒ\n+## ä¸ºä»€ä¹ˆè¦ç”¨ Transformersï¼Ÿ\n \n-1. å¯¹äºæ¨¡å‹ç”Ÿå‘½å‘¨æœŸçš„æ¯ä¸€ä¸ªéƒ¨åˆ†éƒ½é¢é¢ä¿±åˆ°ï¼š\n-    - è®­ç»ƒå…ˆè¿›çš„æ¨¡å‹ï¼Œåªéœ€ 3 è¡Œä»£ç \n-    - æ¨¡å‹åœ¨ä¸åŒæ·±åº¦å­¦ä¹ æ¡†æ¶é—´ä»»æ„è½¬ç§»ï¼Œéšä½ å¿ƒæ„\n-    - ä¸ºè®­ç»ƒã€è¯„ä¼°å’Œç”Ÿäº§é€‰æ‹©æœ€é€‚åˆçš„æ¡†æ¶ï¼Œè¡”æ¥æ— ç¼\n+1. æ˜“äºä½¿ç”¨çš„æœ€å…ˆè¿›æ¨¡å‹ï¼š\n+    - åœ¨è‡ªç„¶è¯­è¨€ç†è§£ä¸ç”Ÿæˆã€è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘ã€è§†é¢‘ä¸å¤šæ¨¡æ€ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜è¶Šã€‚\n+    - å¯¹ç ”ç©¶è€…ã€å·¥ç¨‹å¸ˆä¸å¼€å‘è€…å‹å¥½ä¸”ä½é—¨æ§›ã€‚\n+    - å°‘é‡ç”¨æˆ·ä¾§æŠ½è±¡ï¼Œä»…éœ€å­¦ä¹ ä¸‰ä¸ªç±»ã€‚\n+    - ç»Ÿä¸€çš„ APIï¼Œä½¿ç”¨æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹ä½“éªŒä¸€è‡´ã€‚\n \n-1. ä¸ºä½ çš„éœ€æ±‚è½»æ¾å®šåˆ¶ä¸“å±æ¨¡å‹å’Œç”¨ä¾‹ï¼š\n-    - æˆ‘ä»¬ä¸ºæ¯ç§æ¨¡å‹æ¶æ„æä¾›äº†å¤šä¸ªç”¨ä¾‹æ¥å¤ç°åŸè®ºæ–‡ç»“æœ\n-    - æ¨¡å‹å†…éƒ¨ç»“æ„ä¿æŒé€æ˜ä¸€è‡´\n-    - æ¨¡å‹æ–‡ä»¶å¯å•ç‹¬ä½¿ç”¨ï¼Œæ–¹ä¾¿ä¿®æ”¹å’Œå¿«é€Ÿå®éªŒ\n+1. æ›´ä½è®¡ç®—å¼€é”€ä¸æ›´å°ç¢³è¶³è¿¹ï¼š\n+    - å…±äº«å·²è®­ç»ƒçš„æ¨¡å‹ï¼Œè€Œéæ¯æ¬¡ä»é›¶å¼€å§‹è®­ç»ƒã€‚\n+    - å‡å°‘è®¡ç®—æ—¶é—´ä¸ç”Ÿäº§ç¯å¢ƒæˆæœ¬ã€‚\n+    - è¦†ç›–æ•°åç§æ¨¡å‹æ¶æ„ï¼Œè·¨æ‰€æœ‰æ¨¡æ€æä¾› 1M+ é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚\n \n-## ä»€ä¹ˆæƒ…å†µä¸‹æˆ‘ä¸è¯¥ç”¨ transformersï¼Ÿ\n+1. åœ¨æ¨¡å‹ç”Ÿå‘½å‘¨æœŸçš„æ¯ä¸ªé˜¶æ®µéƒ½å¯ä»¥é€‰ç”¨åˆé€‚çš„æ¡†æ¶ï¼š\n+    - 3 è¡Œä»£ç å³å¯è®­ç»ƒæœ€å…ˆè¿›æ¨¡å‹ã€‚\n+    - åœ¨ PyTorch/JAX/TF2.0 é—´è‡ªç”±è¿ç§»åŒä¸€ä¸ªæ¨¡å‹ã€‚\n+    - ä¸ºè®­ç»ƒã€è¯„ä¼°ä¸ç”Ÿäº§æŒ‘é€‰æœ€åˆé€‚çš„æ¡†æ¶ã€‚\n \n-- æœ¬åº“å¹¶ä¸æ˜¯æ¨¡å—åŒ–çš„ç¥ç»ç½‘ç»œå·¥å…·ç®±ã€‚æ¨¡å‹æ–‡ä»¶ä¸­çš„ä»£ç ç‰¹æ„å‘ˆè‹¥ç’ç‰ï¼Œæœªç»é¢å¤–æŠ½è±¡å°è£…ï¼Œä»¥ä¾¿ç ”ç©¶äººå‘˜å¿«é€Ÿè¿­ä»£ä¿®æ”¹è€Œä¸è‡´æººäºæŠ½è±¡å’Œæ–‡ä»¶è·³è½¬ä¹‹ä¸­ã€‚\n-- `Trainer` API å¹¶éå…¼å®¹ä»»ä½•æ¨¡å‹ï¼Œåªä¸ºæœ¬åº“ä¹‹æ¨¡å‹ä¼˜åŒ–ã€‚è‹¥æ˜¯åœ¨å¯»æ‰¾é€‚ç”¨äºé€šç”¨æœºå™¨å­¦ä¹ çš„è®­ç»ƒå¾ªç¯å®ç°ï¼Œè¯·å¦è§…ä»–åº“ã€‚\n-- å°½ç®¡æˆ‘ä»¬å·²å°½åŠ›è€Œä¸ºï¼Œ[examples ç›®å½•](https://github.com/huggingface/transformers/tree/main/examples)ä¸­çš„è„šæœ¬ä¹Ÿä»…ä¸ºç”¨ä¾‹è€Œå·²ã€‚å¯¹äºä½ çš„ç‰¹å®šé—®é¢˜ï¼Œå®ƒä»¬å¹¶ä¸ä¸€å®šå¼€ç®±å³ç”¨ï¼Œå¯èƒ½éœ€è¦æ”¹å‡ è¡Œä»£ç ä»¥é€‚ä¹‹ã€‚\n+1. è½»æ¾å®šåˆ¶æ¨¡å‹æˆ–ç”¨ä¾‹ï¼š\n+    - ä¸ºæ¯ä¸ªæ¶æ„æä¾›ç¤ºä¾‹ä»¥å¤ç°åŸè®ºæ–‡ç»“æœã€‚\n+    - å°½å¯èƒ½ä¸€è‡´åœ°æš´éœ²æ¨¡å‹å†…éƒ¨ã€‚\n+    - æ¨¡å‹æ–‡ä»¶å¯ç‹¬ç«‹äºåº“ä½¿ç”¨ï¼Œä¾¿äºå¿«é€Ÿå®éªŒã€‚\n \n-## å®‰è£…\n+<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n+    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n+</a><br>\n \n-### ä½¿ç”¨ pip\n+## ä¸ºä»€ä¹ˆæˆ‘ä¸è¯¥ç”¨ Transformersï¼Ÿ\n \n-è¿™ä¸ªä»“åº“å·²åœ¨ Python 3.9+ã€Flax 0.4.1+ã€PyTorch 2.1+ å’Œ TensorFlow 2.6+ ä¸‹ç»è¿‡æµ‹è¯•ã€‚\n+- è¯¥åº“ä¸æ˜¯ä¸€ä¸ªå¯è‡ªç”±æ‹¼æ­çš„ç¥ç»ç½‘ç»œæ¨¡å—åŒ–å·¥å…·ç®±ã€‚æ¨¡å‹æ–‡ä»¶ä¸­çš„ä»£ç åˆ»æ„å‡å°‘é¢å¤–æŠ½è±¡ï¼Œä»¥ä¾¿ç ”ç©¶è€…èƒ½å¿«é€Ÿåœ¨å„ä¸ªæ¨¡å‹ä¸Šè¿­ä»£ï¼Œè€Œæ— éœ€æ·±å…¥æ›´å¤šæŠ½è±¡æˆ–æ–‡ä»¶è·³è½¬ã€‚\n+- è®­ç»ƒ API ä¼˜åŒ–ç”¨äº Transformers æä¾›çš„ PyTorch æ¨¡å‹ã€‚è‹¥éœ€è¦é€šç”¨çš„æœºå™¨å­¦ä¹ è®­ç»ƒå¾ªç¯ï¼Œè¯·ä½¿ç”¨å…¶å®ƒåº“ï¼Œå¦‚ [Accelerate](https://huggingface.co/docs/accelerate)ã€‚\n+- [ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples)åªæ˜¯ã€Œç¤ºä¾‹ã€ã€‚å®ƒä»¬ä¸ä¸€å®šèƒ½ç›´æ¥é€‚é…ä½ çš„å…·ä½“ç”¨ä¾‹ï¼Œéœ€è¦ä½ è¿›è¡Œå¿…è¦çš„æ”¹åŠ¨ã€‚\n \n-ä½ å¯ä»¥åœ¨[è™šæ‹Ÿç¯å¢ƒ](https://docs.python.org/3/library/venv.html)ä¸­å®‰è£… ğŸ¤— Transformersã€‚å¦‚æœä½ è¿˜ä¸ç†Ÿæ‚‰ Python çš„è™šæ‹Ÿç¯å¢ƒï¼Œè¯·é˜…æ­¤[ç”¨æˆ·è¯´æ˜](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ã€‚\n \n-é¦–å…ˆï¼Œç”¨ä½ æ‰“ç®—ä½¿ç”¨çš„ç‰ˆæœ¬çš„ Python åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒå¹¶æ¿€æ´»ã€‚\n+## 100 ä¸ªä½¿ç”¨ Transformers çš„é¡¹ç›®\n \n-ç„¶åï¼Œä½ éœ€è¦å®‰è£… Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ã€‚å…³äºåœ¨ä½ ä½¿ç”¨çš„å¹³å°ä¸Šå®‰è£…è¿™äº›æ¡†æ¶ï¼Œè¯·å‚é˜… [TensorFlow å®‰è£…é¡µ](https://www.tensorflow.org/install/), [PyTorch å®‰è£…é¡µ](https://pytorch.org/get-started/locally/#start-locally) æˆ– [Flax å®‰è£…é¡µ](https://github.com/google/flax#quick-install)ã€‚\n+Transformers ä¸æ­¢æ˜¯ä¸€ä¸ªä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å·¥å…·åŒ…ï¼Œå®ƒè¿˜æ˜¯å›´ç»• Hugging Face Hub æ„å»ºçš„é¡¹ç›®ç¤¾åŒºã€‚æˆ‘ä»¬å¸Œæœ› Transformers èƒ½åŠ©åŠ›å¼€å‘è€…ã€ç ”ç©¶äººå‘˜ã€å­¦ç”Ÿã€è€å¸ˆã€å·¥ç¨‹å¸ˆä¸ä»»ä½•äººæ„å»ºç†æƒ³é¡¹ç›®ã€‚\n \n-å½“è¿™äº›åç«¯ä¹‹ä¸€å®‰è£…æˆåŠŸåï¼Œ ğŸ¤— Transformers å¯ä¾æ­¤å®‰è£…ï¼š\n+ä¸ºåº†ç¥ Transformers è·å¾— 100,000 é¢—æ˜Ÿï¼Œæˆ‘ä»¬åˆ¶ä½œäº† [awesome-transformers](./awesome-transformers.md) é¡µé¢ï¼Œå±•ç¤ºäº† 100 ä¸ªç”±ç¤¾åŒºæ„å»ºçš„ä¼˜ç§€é¡¹ç›®ã€‚\n \n-```bash\n-pip install transformers\n-```\n+å¦‚æœä½ æ‹¥æœ‰æˆ–ä½¿ç”¨æŸä¸ªé¡¹ç›®ï¼Œè®¤ä¸ºå®ƒåº”è¯¥åœ¨åˆ—è¡¨ä¸­å‡ºç°ï¼Œæ¬¢è¿æäº¤ PR æ·»åŠ å®ƒï¼\n \n-å¦‚æœä½ æƒ³è¦è¯•è¯•ç”¨ä¾‹æˆ–è€…æƒ³åœ¨æ­£å¼å‘å¸ƒå‰ä½¿ç”¨æœ€æ–°çš„å¼€å‘ä¸­ä»£ç ï¼Œä½ å¾—[ä»æºä»£ç å®‰è£…](https://huggingface.co/docs/transformers/installation#installing-from-source)ã€‚\n+## ç¤ºä¾‹æ¨¡å‹\n \n-### ä½¿ç”¨ conda\n+ä½ å¯ä»¥ç›´æ¥åœ¨å®ƒä»¬çš„ [Hub æ¨¡å‹é¡µ](https://huggingface.co/models) ä¸Šæµ‹è¯•æˆ‘ä»¬çš„å¤šæ•°æ¨¡å‹ã€‚\n \n-ğŸ¤— Transformers å¯ä»¥é€šè¿‡ conda ä¾æ­¤å®‰è£…ï¼š\n+å±•å¼€æ¯ä¸ªæ¨¡æ€ä»¥æŸ¥çœ‹ä¸åŒç”¨ä¾‹ä¸­çš„éƒ¨åˆ†ç¤ºä¾‹æ¨¡å‹ã€‚\n \n-```shell script\n-conda install conda-forge::transformers\n-```\n+<details>\n+<summary>éŸ³é¢‘</summary>\n \n-> **_ç¬”è®°:_** ä» `huggingface` æ¸ é“å®‰è£… `transformers` å·²è¢«åºŸå¼ƒã€‚\n+- ä½¿ç”¨ [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo) è¿›è¡ŒéŸ³é¢‘åˆ†ç±»\n+- ä½¿ç”¨ [Moonshine](https://huggingface.co/UsefulSensors/moonshine) è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«\n+- ä½¿ç”¨ [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks) è¿›è¡Œå…³é”®è¯æ£€ç´¢\n+- ä½¿ç”¨ [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16) è¿›è¡Œè¯­éŸ³åˆ°è¯­éŸ³ç”Ÿæˆ\n+- ä½¿ç”¨ [MusicGen](https://huggingface.co/facebook/musicgen-large) æ–‡æœ¬åˆ°éŸ³é¢‘ç”Ÿæˆ\n+- ä½¿ç”¨ [Bark](https://huggingface.co/suno/bark) æ–‡æœ¬åˆ°è¯­éŸ³ç”Ÿæˆ\n \n-è¦é€šè¿‡ conda å®‰è£… Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ï¼Œè¯·å‚é˜…å®ƒä»¬å„è‡ªå®‰è£…é¡µçš„è¯´æ˜ã€‚\n+</details>\n \n-## æ¨¡å‹æ¶æ„\n+<details>\n+<summary>è®¡ç®—æœºè§†è§‰</summary>\n \n-ğŸ¤— Transformers æ”¯æŒçš„[**æ‰€æœ‰çš„æ¨¡å‹æ£€æŸ¥ç‚¹**](https://huggingface.co/models)ç”±[ç”¨æˆ·](https://huggingface.co/users)å’Œ[ç»„ç»‡](https://huggingface.co/organizations)ä¸Šä¼ ï¼Œå‡ä¸ huggingface.co [model hub](https://huggingface.co) æ— ç¼æ•´åˆã€‚\n+- ä½¿ç”¨ [SAM](https://huggingface.co/facebook/sam-vit-base) è‡ªåŠ¨ç”Ÿæˆæ©ç \n+- ä½¿ç”¨ [DepthPro](https://huggingface.co/apple/DepthPro-hf) è¿›è¡Œæ·±åº¦ä¼°è®¡\n+- ä½¿ç”¨ [DINO v2](https://huggingface.co/facebook/dinov2-base) è¿›è¡Œå›¾åƒåˆ†ç±»\n+- ä½¿ç”¨ [SuperPoint](https://huggingface.co/magic-leap-community/superpoint) è¿›è¡Œå…³é”®ç‚¹æ£€æµ‹\n+- ä½¿ç”¨ [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor) è¿›è¡Œå…³é”®ç‚¹åŒ¹é…\n+- ä½¿ç”¨ [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd) è¿›è¡Œç›®æ ‡æ£€æµ‹\n+- ä½¿ç”¨ [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple) è¿›è¡Œå§¿æ€ä¼°è®¡\n+- ä½¿ç”¨ [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large) è¿›è¡Œé€šç”¨åˆ†å‰²\n+- ä½¿ç”¨ [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large) è¿›è¡Œè§†é¢‘åˆ†ç±»\n \n-ç›®å‰çš„æ£€æŸ¥ç‚¹æ•°é‡ï¼š ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n+</details>\n \n-ğŸ¤— Transformers ç›®å‰æ”¯æŒå¦‚ä¸‹çš„æ¶æ„: æ¨¡å‹æ¦‚è¿°è¯·é˜…[è¿™é‡Œ](https://huggingface.co/docs/transformers/model_summary).\n+<details>\n+<summary>å¤šæ¨¡æ€</summary>\n \n-è¦æ£€æŸ¥æŸä¸ªæ¨¡å‹æ˜¯å¦å·²æœ‰ Flaxã€PyTorch æˆ– TensorFlow çš„å®ç°ï¼Œæˆ–å…¶æ˜¯å¦åœ¨ ğŸ¤— Tokenizers åº“ä¸­æœ‰å¯¹åº”è¯ç¬¦åŒ–å™¨ï¼ˆtokenizerï¼‰ï¼Œæ•¬è¯·å‚é˜…[æ­¤è¡¨](https://huggingface.co/docs/transformers/index#supported-frameworks)ã€‚\n+- ä½¿ç”¨ [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B) å®ç°éŸ³é¢‘æˆ–æ–‡æœ¬åˆ°æ–‡æœ¬\n+- ä½¿ç”¨ [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base) è¿›è¡Œæ–‡æ¡£é—®ç­”\n+- ä½¿ç”¨ [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct) å®ç°å›¾åƒæˆ–æ–‡æœ¬åˆ°æ–‡æœ¬\n+- ä½¿ç”¨ [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b) è¿›è¡Œå›¾æ–‡æè¿°\n+- ä½¿ç”¨ [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf) è¿›è¡ŒåŸºäº OCR çš„æ–‡æ¡£ç†è§£\n+- ä½¿ç”¨ [TAPAS](https://huggingface.co/google/tapas-base) è¿›è¡Œè¡¨æ ¼é—®ç­”\n+- ä½¿ç”¨ [Emu3](https://huggingface.co/BAAI/Emu3-Gen) è¿›è¡Œç»Ÿä¸€çš„å¤šæ¨¡æ€ç†è§£ä¸ç”Ÿæˆ\n+- ä½¿ç”¨ [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf) è§†è§‰åˆ°æ–‡æœ¬\n+- ä½¿ç”¨ [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf) è¿›è¡Œè§†è§‰é—®ç­”\n+- ä½¿ç”¨ [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224) è¿›è¡Œè§†è§‰æŒ‡ä»£è¡¨è¾¾åˆ†å‰²\n \n-è¿™äº›å®ç°å‡å·²äºå¤šä¸ªæ•°æ®é›†æµ‹è¯•ï¼ˆè¯·å‚çœ‹ç”¨ä¾‹è„šæœ¬ï¼‰å¹¶åº”äºåŸç‰ˆå®ç°è¡¨ç°ç›¸å½“ã€‚ä½ å¯ä»¥åœ¨ç”¨ä¾‹æ–‡æ¡£çš„[æ­¤èŠ‚](https://huggingface.co/docs/transformers/examples)ä¸­äº†è§£è¡¨ç°çš„ç»†èŠ‚ã€‚\n+</details>\n \n+<details>\n+<summary>NLP</summary>\n \n-## äº†è§£æ›´å¤š\n+- ä½¿ç”¨ [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base) è¿›è¡Œæ©ç è¯å¡«å……\n+- ä½¿ç”¨ [Gemma](https://huggingface.co/google/gemma-2-2b) è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰\n+- ä½¿ç”¨ [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) è¿›è¡Œé—®ç­”\n+- ä½¿ç”¨ [BART](https://huggingface.co/facebook/bart-large-cnn) è¿›è¡Œæ‘˜è¦\n+- ä½¿ç”¨ [T5](https://huggingface.co/google-t5/t5-base) è¿›è¡Œç¿»è¯‘\n+- ä½¿ç”¨ [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B) è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ\n+- ä½¿ç”¨ [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B) è¿›è¡Œæ–‡æœ¬åˆ†ç±»\n \n-| ç« èŠ‚ | æè¿° |\n-|-|-|\n-| [æ–‡æ¡£](https://huggingface.co/docs/transformers/) | å®Œæ•´çš„ API æ–‡æ¡£å’Œæ•™ç¨‹ |\n-| [ä»»åŠ¡æ€»ç»“](https://huggingface.co/docs/transformers/task_summary) | ğŸ¤— Transformers æ”¯æŒçš„ä»»åŠ¡ |\n-| [é¢„å¤„ç†æ•™ç¨‹](https://huggingface.co/docs/transformers/preprocessing) | ä½¿ç”¨ `Tokenizer` æ¥ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ® |\n-| [è®­ç»ƒå’Œå¾®è°ƒ](https://huggingface.co/docs/transformers/training) | åœ¨ PyTorch/TensorFlow çš„è®­ç»ƒå¾ªç¯æˆ– `Trainer` API ä¸­ä½¿ç”¨ ğŸ¤— Transformers æä¾›çš„æ¨¡å‹ |\n-| [å¿«é€Ÿä¸Šæ‰‹ï¼šå¾®è°ƒå’Œç”¨ä¾‹è„šæœ¬](https://github.com/huggingface/transformers/tree/main/examples) | ä¸ºå„ç§ä»»åŠ¡æä¾›çš„ç”¨ä¾‹è„šæœ¬ |\n-| [æ¨¡å‹åˆ†äº«å’Œä¸Šä¼ ](https://huggingface.co/docs/transformers/model_sharing) | å’Œç¤¾åŒºä¸Šä¼ å’Œåˆ†äº«ä½ å¾®è°ƒçš„æ¨¡å‹ |\n-| [è¿ç§»](https://huggingface.co/docs/transformers/migration) | ä» `pytorch-transformers` æˆ– `pytorch-pretrained-bert` è¿ç§»åˆ° ğŸ¤— Transformers |\n+</details>\n \n ## å¼•ç”¨\n "
        },
        {
            "sha": "1b6a114d66d0f237c12ae14a37eb6264a7ecf09c",
            "filename": "i18n/README_zh-hant.md",
            "status": "modified",
            "additions": 192,
            "deletions": 153,
            "changes": 345,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fd63dd5328ba4fdc847831c32b618e321945e7e/i18n%2FREADME_zh-hant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fd63dd5328ba4fdc847831c32b618e321945e7e/i18n%2FREADME_zh-hant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/i18n%2FREADME_zh-hant.md?ref=1fd63dd5328ba4fdc847831c32b618e321945e7e",
            "patch": "@@ -14,43 +14,6 @@ See the License for the specific language governing permissions and\n limitations under the License.\n -->\n \n-<!---\n-A useful guide for English-Traditional Chinese translation of Hugging Face documentation\n-- Add space around English words and numbers when they appear between Chinese characters. E.g., å…± 100 å¤šç¨®èªè¨€; ä½¿ç”¨ transformers å‡½å¼åº«ã€‚\n-- Use square quotes, e.g.,ã€Œå¼•ç”¨ã€\n-- Some of terms in the file can be found at National Academy for Educational Research (https://terms.naer.edu.tw/), an official website providing bilingual translations between English and Traditional Chinese.\n-\n-Dictionary\n-\n-API: API (ä¸ç¿»è­¯ï¼‰\n-add: åŠ å…¥\n-checkpoint: æª¢æŸ¥é»\n-code: ç¨‹å¼ç¢¼\n-community: ç¤¾ç¾¤\n-confidence: ä¿¡è³´åº¦\n-dataset: è³‡æ–™é›†\n-documentation: æ–‡ä»¶\n-example: åŸºæœ¬ç¿»è­¯ç‚ºã€Œç¯„ä¾‹ã€ï¼Œæˆ–ä¾èªæ„ç¿»ç‚ºã€Œä¾‹å­ã€\n-finetune: å¾®èª¿\n-Hugging Face: Hugging Faceï¼ˆä¸ç¿»è­¯ï¼‰\n-implementation: å¯¦ä½œ\n-inference: æ¨è«–\n-library: å‡½å¼åº«\n-module: æ¨¡çµ„\n-NLP/Natural Language Processing: ä»¥ NLP å‡ºç¾æ™‚ä¸ç¿»è­¯ï¼Œä»¥ Natural Language Processing å‡ºç¾æ™‚ç¿»è­¯ç‚ºè‡ªç„¶èªè¨€è™•ç†\n-online demos: ç·šä¸ŠDemo\n-pipeline: pipelineï¼ˆä¸ç¿»è­¯ï¼‰\n-pretrained/pretrain: é è¨“ç·´\n-Python data structures (e.g., list, set, dict): ç¿»è­¯ç‚ºä¸²åˆ—ï¼Œé›†åˆï¼Œå­—å…¸ï¼Œä¸¦ç”¨æ‹¬è™Ÿæ¨™è¨»åŸè‹±æ–‡\n-repository: repositoryï¼ˆä¸ç¿»è­¯ï¼‰\n-summary: æ¦‚è¦½\n-token-: token-ï¼ˆä¸ç¿»è­¯ï¼‰\n-Trainer: Trainerï¼ˆä¸ç¿»è­¯ï¼‰\n-transformer: transformerï¼ˆä¸ç¿»è­¯ï¼‰\n-tutorial: æ•™å­¸\n-user: ä½¿ç”¨è€…\n--->\n-\n <p align=\"center\">\n   <picture>\n     <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n@@ -62,6 +25,7 @@ user: ä½¿ç”¨è€…\n </p>\n \n <p align=\"center\">\n+    <a href=\"https://huggingface.com/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n     <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n     <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n     <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n@@ -72,15 +36,15 @@ user: ä½¿ç”¨è€…\n \n <h4 align=\"center\">\n     <p>\n-        <a href=\"https://github.com/huggingface/transformers/\">English</a> |\n+        <a href=\"https://github.com/huggingface/transformers/blob/main/README.md\">English</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">ç®€ä½“ä¸­æ–‡</a> |\n         <b>ç¹é«”ä¸­æ–‡</b> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">í•œêµ­ì–´</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">EspaÃ±ol</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">æ—¥æœ¬èª</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">Ğ ÑƒÑÑĞºĞ¸Ğ¹</a> |\n-        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">Ğ ortuguÃªs</a> |\n+        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">PortuguÃªs</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">à°¤à±†à°²à±à°—à±</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">FranÃ§ais</a> |\n         <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n@@ -93,186 +57,261 @@ user: ä½¿ç”¨è€…\n </h4>\n \n <h3 align=\"center\">\n-    <p>ç‚º Jaxã€PyTorch ä»¥åŠ TensorFlow æ‰“é€ çš„å…ˆé€²è‡ªç„¶èªè¨€è™•ç†å‡½å¼åº«</p>\n+    <p>æœ€å…ˆé€²çš„é è¨“ç·´æ¨¡å‹ï¼Œå°ˆç‚ºæ¨ç†èˆ‡è¨“ç·´è€Œç”Ÿ</p>\n </h3>\n \n <h3 align=\"center\">\n-    <a href=\"https://hf.co/course\"><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png\"></a>\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png\"/>\n </h3>\n \n-ğŸ¤— Transformers æä¾›äº†æ•¸ä»¥åƒè¨ˆçš„é è¨“ç·´æ¨¡å‹ï¼Œæ”¯æ´ 100 å¤šç¨®èªè¨€çš„æ–‡æœ¬åˆ†é¡ã€è³‡è¨Šæ“·å–ã€å•ç­”ã€æ‘˜è¦ã€ç¿»è­¯ã€æ–‡æœ¬ç”Ÿæˆã€‚å®ƒçš„å®—æ—¨æ˜¯è®“æœ€å…ˆé€²çš„ NLP æŠ€è¡“äººäººæ˜“ç”¨ã€‚\n+Transformers æ˜¯ä¸€å€‹ç‚ºæœ€å…ˆé€²çš„æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ï¼ˆæ¶µè“‹æ–‡å­—ã€é›»è…¦è¦–è¦ºã€éŸ³è¨Šã€å½±ç‰‡åŠå¤šæ¨¡æ…‹ï¼‰æä¾›æ¨ç†å’Œè¨“ç·´æ”¯æ´çš„æ¨¡å‹å®šç¾©æ¡†æ¶ã€‚\n+\n+å®ƒå°‡æ¨¡å‹å®šç¾©é›†ä¸­åŒ–ï¼Œä½¿å¾—è©²å®šç¾©åœ¨æ•´å€‹ç”Ÿæ…‹ç³»ä¸­èƒ½å¤ é”æˆå…±è­˜ã€‚`transformers` æ˜¯è²«ç©¿å„å€‹æ¡†æ¶çš„æ¨ç´ï¼šå¦‚æœä¸€å€‹æ¨¡å‹å®šç¾©å—åˆ°æ”¯æ´ï¼Œå®ƒå°‡èˆ‡å¤§å¤šæ•¸è¨“ç·´æ¡†æ¶ï¼ˆå¦‚ Axolotlã€Unslothã€DeepSpeedã€FSDPã€PyTorch-Lightning ç­‰ï¼‰ã€æ¨ç†å¼•æ“ï¼ˆå¦‚ vLLMã€SGLangã€TGI ç­‰ï¼‰ä»¥åŠåˆ©ç”¨ `transformers` æ¨¡å‹å®šç¾©çš„å‘¨é‚Šå»ºæ¨¡å‡½å¼åº«ï¼ˆå¦‚ llama.cppã€mlx ç­‰ï¼‰ç›¸å®¹ã€‚\n \n-ğŸ¤— Transformers æä¾›äº†ä¾¿æ–¼å¿«é€Ÿä¸‹è¼‰å’Œä½¿ç”¨çš„APIï¼Œè®“ä½ å¯ä»¥å°‡é è¨“ç·´æ¨¡å‹ç”¨åœ¨çµ¦å®šæ–‡æœ¬ã€åœ¨ä½ çš„è³‡æ–™é›†ä¸Šå¾®èª¿ç„¶å¾Œç¶“ç”± [model hub](https://huggingface.co/models) èˆ‡ç¤¾ç¾¤å…±äº«ã€‚åŒæ™‚ï¼Œæ¯å€‹å®šç¾©çš„ Python æ¨¡çµ„æ¶æ§‹å‡å®Œå…¨ç¨ç«‹ï¼Œæ–¹ä¾¿ä¿®æ”¹å’Œå¿«é€Ÿç ”ç©¶å¯¦é©—ã€‚\n+æˆ‘å€‘è‡´åŠ›æ–¼æ”¯æ´æœ€æ–°çš„é ‚å°–æ¨¡å‹ï¼Œä¸¦é€éä½¿å…¶æ¨¡å‹å®šç¾©è®Šå¾—ç°¡å–®ã€å¯å®¢è£½åŒ–ä¸”é«˜æ•ˆï¼Œä¾†æ™®åŠå®ƒå€‘çš„æ‡‰ç”¨ã€‚\n \n-ğŸ¤— Transformers æ”¯æ´ä¸‰å€‹æœ€ç†±é–€çš„æ·±åº¦å­¸ç¿’å‡½å¼åº«ï¼š [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) ä»¥åŠ [TensorFlow](https://www.tensorflow.org/) â€” ä¸¦èˆ‡ä¹‹å®Œç¾æ•´åˆã€‚ä½ å¯ä»¥ç›´æ¥ä½¿ç”¨å…¶ä¸­ä¸€å€‹æ¡†æ¶è¨“ç·´ä½ çš„æ¨¡å‹ï¼Œç„¶å¾Œç”¨å¦ä¸€å€‹è¼‰å…¥å’Œæ¨è«–ã€‚\n+åœ¨ [Hugging Face Hub](https://huggingface.com/models) ä¸Šï¼Œæœ‰è¶…é 100 è¬å€‹ Transformers [æ¨¡å‹æª¢æŸ¥é»](https://huggingface.co/models?library=transformers&sort=trending) ä¾›æ‚¨ä½¿ç”¨ã€‚\n \n-## ç·šä¸ŠDemo\n+ç«‹å³æ¢ç´¢ [Hub](https://huggingface.com/)ï¼Œå°‹æ‰¾åˆé©çš„æ¨¡å‹ï¼Œä¸¦ä½¿ç”¨ Transformers å¹«åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹ã€‚\n \n-ä½ å¯ä»¥ç›´æ¥åœ¨ [model hub](https://huggingface.co/models) ä¸Šæ¸¬è©¦å¤§å¤šæ•¸çš„æ¨¡å‹ã€‚æˆ‘å€‘ä¹Ÿæä¾›äº† [ç§æœ‰æ¨¡å‹è¨—ç®¡ã€æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ä»¥åŠæ¨è«–API](https://huggingface.co/pricing)ã€‚\n+## å®‰è£\n \n-é€™è£¡æ˜¯ä¸€äº›ç¯„ä¾‹ï¼š\n-- [ç”¨ BERT åšé®è“‹å¡«è©](https://huggingface.co/google-bert/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)\n-- [ç”¨ Electra åšå°ˆæœ‰åè©è¾¨è­˜](https://huggingface.co/dbmdz/electra-large-discriminator-finetuned-conll03-english?text=My+name+is+Sarah+and+I+live+in+London+city)\n-- [ç”¨ GPT-2 åšæ–‡æœ¬ç”Ÿæˆ](https://huggingface.co/openai-community/gpt2?text=A+long+time+ago%2C+)\n-- [ç”¨ RoBERTa åšè‡ªç„¶èªè¨€æ¨è«–](https://huggingface.co/FacebookAI/roberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+any+animal)\n-- [ç”¨ BART åšæ–‡æœ¬æ‘˜è¦](https://huggingface.co/facebook/bart-large-cnn?text=The+tower+is+324+metres+%281%2C063+ft%29+tall%2C+about+the+same+height+as+an+81-storey+building%2C+and+the+tallest+structure+in+Paris.+Its+base+is+square%2C+measuring+125+metres+%28410+ft%29+on+each+side.+During+its+construction%2C+the+Eiffel+Tower+surpassed+the+Washington+Monument+to+become+the+tallest+man-made+structure+in+the+world%2C+a+title+it+held+for+41+years+until+the+Chrysler+Building+in+New+York+City+was+finished+in+1930.+It+was+the+first+structure+to+reach+a+height+of+300+metres.+Due+to+the+addition+of+a+broadcasting+aerial+at+the+top+of+the+tower+in+1957%2C+it+is+now+taller+than+the+Chrysler+Building+by+5.2+metres+%2817+ft%29.+Excluding+transmitters%2C+the+Eiffel+Tower+is+the+second+tallest+free-standing+structure+in+France+after+the+Millau+Viaduct)\n-- [ç”¨ DistilBERT åšå•ç­”](https://huggingface.co/distilbert/distilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species)\n-- [ç”¨ T5 åšç¿»è­¯](https://huggingface.co/google-t5/t5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)\n+Transformers æ”¯æ´ Python 3.9+ å’Œ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+ã€‚\n \n-**[Write With Transformer](https://transformer.huggingface.co)**ï¼Œç”± Hugging Face åœ˜éšŠæ‰€æ‰“é€ ï¼Œæ˜¯ä¸€å€‹æ–‡æœ¬ç”Ÿæˆçš„å®˜æ–¹ demoã€‚\n+ä½¿ç”¨ [venv](https://docs.python.org/3/library/venv.html) æˆ–åŸºæ–¼ Rust çš„é«˜é€Ÿ Python å¥—ä»¶åŠå°ˆæ¡ˆç®¡ç†å™¨ [uv](https://docs.astral.sh/uv/) ä¾†å»ºç«‹ä¸¦å•Ÿç”¨è™›æ“¬ç’°å¢ƒã€‚\n \n-## å¦‚æœä½ åœ¨å°‹æ‰¾ç”± Hugging Face åœ˜éšŠæ‰€æä¾›çš„å®¢è£½åŒ–æ”¯æ´æœå‹™\n+```py\n+# venv\n+python -m venv .my-env\n+source .my-env/bin/activate\n+# uv\n+uv venv .my-env\n+source .my-env/bin/activate\n+```\n \n-<a target=\"_blank\" href=\"https://huggingface.co/support\">\n-    <img alt=\"HuggingFace Expert Acceleration Program\" src=\"https://huggingface.co/front/thumbnails/support.png\" style=\"max-width: 600px; border: 1px solid #eee; border-radius: 4px; box-shadow: 0 1px 2px 0 rgba(0, 0, 0, 0.05);\">\n-</a><br>\n+åœ¨æ‚¨çš„è™›æ“¬ç’°å¢ƒä¸­å®‰è£ Transformersã€‚\n \n-## å¿«é€Ÿä¸Šæ‰‹\n+```py\n+# pip\n+pip install \"transformers[torch]\"\n \n-æˆ‘å€‘ç‚ºå¿«é€Ÿä½¿ç”¨æ¨¡å‹æä¾›äº† `pipeline` APIã€‚ Pipeline åŒ…å«äº†é è¨“ç·´æ¨¡å‹å’Œå°æ‡‰çš„æ–‡æœ¬é è™•ç†ã€‚ä¸‹é¢æ˜¯ä¸€å€‹å¿«é€Ÿä½¿ç”¨ pipeline å»åˆ¤æ–·æ­£è² é¢æƒ…ç·’çš„ä¾‹å­ï¼š\n+# uv\n+uv pip install \"transformers[torch]\"\n+```\n \n-```python\n->>> from transformers import pipeline\n+å¦‚æœæ‚¨æƒ³ä½¿ç”¨å‡½å¼åº«çš„æœ€æ–°è®Šæ›´æˆ–æœ‰èˆˆè¶£åƒèˆ‡è²¢ç»ï¼Œå¯ä»¥å¾åŸå§‹ç¢¼å®‰è£ Transformersã€‚ç„¶è€Œï¼Œ*æœ€æ–°*ç‰ˆæœ¬å¯èƒ½ä¸ç©©å®šã€‚å¦‚æœæ‚¨é‡åˆ°ä»»ä½•éŒ¯èª¤ï¼Œæ­¡è¿éš¨æ™‚æäº¤ä¸€å€‹ [issue](https://github.com/huggingface/transformers/issues)ã€‚\n \n-# ä½¿ç”¨æƒ…ç·’åˆ†æ pipeline\n->>> classifier = pipeline('sentiment-analysis')\n->>> classifier('We are very happy to introduce pipeline to the transformers repository.')\n-[{'label': 'POSITIVE', 'score': 0.9996980428695679}]\n+```shell\n+git clone https://github.com/huggingface/transformers.git\n+cd transformers\n+\n+# pip\n+pip install '.[torch]'\n+\n+# uv\n+uv pip install '.[torch]'\n ```\n \n-ç¬¬äºŒè¡Œç¨‹å¼ç¢¼ä¸‹è¼‰ä¸¦å¿«å– pipeline ä½¿ç”¨çš„é è¨“ç·´æ¨¡å‹ï¼Œè€Œç¬¬ä¸‰è¡Œç¨‹å¼ç¢¼å‰‡åœ¨çµ¦å®šçš„æ–‡æœ¬ä¸Šé€²è¡Œäº†è©•ä¼°ã€‚é€™è£¡çš„ç­”æ¡ˆâ€œæ­£é¢â€ (positive) å…·æœ‰ 99.97% çš„ä¿¡è³´åº¦ã€‚\n+## å¿«é€Ÿå…¥é–€\n \n-è¨±å¤šçš„ NLP ä»»å‹™éƒ½æœ‰éš¨é¸å³ç”¨çš„é è¨“ç·´ `pipeline`ã€‚ä¾‹å¦‚ï¼Œæˆ‘å€‘å¯ä»¥è¼•é¬†åœ°å¾çµ¦å®šæ–‡æœ¬ä¸­æ“·å–å•é¡Œç­”æ¡ˆï¼š\n+é€é [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API å¿«é€Ÿé–‹å§‹ä½¿ç”¨ Transformersã€‚`Pipeline` æ˜¯ä¸€å€‹é«˜éšçš„æ¨ç†é¡åˆ¥ï¼Œæ”¯æ´æ–‡å­—ã€éŸ³è¨Šã€è¦–è¦ºå’Œå¤šæ¨¡æ…‹ä»»å‹™ã€‚å®ƒè² è²¬è™•ç†è¼¸å…¥è³‡æ–™çš„é è™•ç†ï¼Œä¸¦å›å‚³é©ç•¶çš„è¼¸å‡ºã€‚\n \n-``` python\n->>> from transformers import pipeline\n+å¯¦ä¾‹åŒ–ä¸€å€‹ pipeline ä¸¦æŒ‡å®šç”¨æ–¼æ–‡å­—ç”Ÿæˆçš„æ¨¡å‹ã€‚è©²æ¨¡å‹æœƒè¢«ä¸‹è¼‰ä¸¦å¿«å–ï¼Œæ–¹ä¾¿æ‚¨ä¹‹å¾Œè¼•é¬†è¤‡ç”¨ã€‚æœ€å¾Œï¼Œå‚³å…¥ä¸€äº›æ–‡å­—ä¾†æç¤ºæ¨¡å‹ã€‚\n \n-# ä½¿ç”¨å•ç­” pipeline\n->>> question_answerer = pipeline('question-answering')\n->>> question_answerer({\n-...     'question': 'What is the name of the repository ?',\n-...     'context': 'Pipeline has been included in the huggingface/transformers repository'\n-... })\n-{'score': 0.30970096588134766, 'start': 34, 'end': 58, 'answer': 'huggingface/transformers'}\n+```py\n+from transformers import pipeline\n \n+pipeline = pipeline(task=\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\")\n+pipeline(\"the secret to baking a really good cake is \")\n+[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]\n ```\n \n-é™¤äº†æä¾›å•é¡Œè§£ç­”ï¼Œé è¨“ç·´æ¨¡å‹é‚„æä¾›äº†å°æ‡‰çš„ä¿¡è³´åº¦åˆ†æ•¸ä»¥åŠè§£ç­”åœ¨ tokenized å¾Œçš„æ–‡æœ¬ä¸­é–‹å§‹å’ŒçµæŸçš„ä½ç½®ã€‚ä½ å¯ä»¥å¾[é€™å€‹æ•™å­¸](https://huggingface.co/docs/transformers/task_summary)äº†è§£æ›´å¤š `pipeline` APIæ”¯æ´çš„ä»»å‹™ã€‚\n+èˆ‡æ¨¡å‹é€²è¡ŒèŠå¤©ï¼Œä½¿ç”¨æ¨¡å¼æ˜¯ç›¸åŒçš„ã€‚å”¯ä¸€çš„å€åˆ¥æ˜¯æ‚¨éœ€è¦å»ºæ§‹ä¸€å€‹æ‚¨èˆ‡ç³»çµ±ä¹‹é–“çš„èŠå¤©æ­·å²ï¼ˆä½œç‚º `Pipeline` çš„è¼¸å…¥ï¼‰ã€‚\n \n-è¦åœ¨ä½ çš„ä»»å‹™ä¸­ä¸‹è¼‰å’Œä½¿ç”¨ä»»ä½•é è¨“ç·´æ¨¡å‹å¾ˆç°¡å–®ï¼Œåªéœ€ä¸‰è¡Œç¨‹å¼ç¢¼ã€‚é€™è£¡æ˜¯ PyTorch ç‰ˆçš„ç¯„ä¾‹ï¼š\n-```python\n->>> from transformers import AutoTokenizer, AutoModel\n+> [!TIP]\n+> ä½ ä¹Ÿå¯ä»¥ç›´æ¥åœ¨å‘½ä»¤åˆ—ä¸­èˆ‡æ¨¡å‹èŠå¤©ã€‚\n+> ```shell\n+> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n+> ```\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n->>> model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n+```py\n+import torch\n+from transformers import pipeline\n \n->>> inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n->>> outputs = model(**inputs)\n+chat = [\n+    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n+    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n+]\n+\n+pipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\n+response = pipeline(chat, max_new_tokens=512)\n+print(response[0][\"generated_text\"][-1][\"content\"])\n ```\n-é€™è£¡æ˜¯å°æ‡‰çš„ TensorFlow ç¨‹å¼ç¢¼ï¼š\n-```python\n->>> from transformers import AutoTokenizer, TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n->>> model = TFAutoModel.from_pretrained(\"google-bert/bert-base-uncased\")\n+å±•é–‹ä¸‹é¢çš„ç¯„ä¾‹ï¼ŒæŸ¥çœ‹ `Pipeline` å¦‚ä½•åœ¨ä¸åŒæ¨¡æ…‹å’Œä»»å‹™ä¸Šé‹ä½œã€‚\n+\n+<details>\n+<summary>è‡ªå‹•èªéŸ³è¾¨è­˜</summary>\n \n->>> inputs = tokenizer(\"Hello world!\", return_tensors=\"tf\")\n->>> outputs = model(**inputs)\n+```py\n+from transformers import pipeline\n+\n+pipeline = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n+pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n+{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n ```\n \n-Tokenizer ç‚ºæ‰€æœ‰çš„é è¨“ç·´æ¨¡å‹æä¾›äº†é è™•ç†ï¼Œä¸¦å¯ä»¥ç›´æ¥è½‰æ›å–®ä¸€å­—ä¸²ï¼ˆæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼‰æˆ–ä¸²åˆ— (list)ã€‚å®ƒæœƒè¼¸å‡ºä¸€å€‹çš„å­—å…¸ (dict) è®“ä½ å¯ä»¥åœ¨ä¸‹æ¸¸ç¨‹å¼ç¢¼è£¡ä½¿ç”¨æˆ–ç›´æ¥è—‰ç”± `**` é‹ç®—å¼å‚³çµ¦æ¨¡å‹ã€‚\n+</details>\n \n-æ¨¡å‹æœ¬èº«æ˜¯ä¸€å€‹å¸¸è¦çš„ [Pytorch `nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) æˆ– [TensorFlow `tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)ï¼ˆå–æ±ºæ–¼ä½ çš„å¾Œç«¯ï¼‰ï¼Œå¯ä¾å¸¸è¦æ–¹å¼ä½¿ç”¨ã€‚ [é€™å€‹æ•™å­¸](https://huggingface.co/transformers/training.html)è§£é‡‹äº†å¦‚ä½•å°‡é€™æ¨£çš„æ¨¡å‹æ•´åˆåˆ°ä¸€èˆ¬çš„ PyTorch æˆ– TensorFlow è¨“ç·´è¿´åœˆä¸­ï¼Œæˆ–æ˜¯å¦‚ä½•ä½¿ç”¨æˆ‘å€‘çš„ `Trainer` API åœ¨ä¸€å€‹æ–°çš„è³‡æ–™é›†ä¸Šå¿«é€Ÿé€²è¡Œå¾®èª¿ã€‚\n+<details>\n+<summary>å½±åƒåˆ†é¡</summary>\n \n-## ç‚ºä»€éº¼è¦ç”¨ transformersï¼Ÿ\n+<h3 align=\"center\">\n+    <a><img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"></a>\n+</h3>\n \n-1. ä¾¿æ–¼ä½¿ç”¨çš„å…ˆé€²æ¨¡å‹ï¼š\n-    - NLU å’Œ NLG ä¸Šæ€§èƒ½å“è¶Š\n-    - å°æ•™å­¸å’Œå¯¦ä½œå‹å¥½ä¸”ä½é–€æª»\n-    - é«˜åº¦æŠ½è±¡ï¼Œä½¿ç”¨è€…åªé ˆå­¸ç¿’ 3 å€‹é¡åˆ¥\n-    - å°æ‰€æœ‰æ¨¡å‹ä½¿ç”¨çš„åˆ¶å¼åŒ–API\n+```py\n+from transformers import pipeline\n+\n+pipeline = pipeline(task=\"image-classification\", model=\"facebook/dinov2-small-imagenet1k-1-layer\")\n+pipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n+[{'label': 'macaw', 'score': 0.997848391532898},\n+ {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n+  'score': 0.0016551691805943847},\n+ {'label': 'lorikeet', 'score': 0.00018523589824326336},\n+ {'label': 'African grey, African gray, Psittacus erithacus',\n+  'score': 7.85409429227002e-05},\n+ {'label': 'quail', 'score': 5.502637941390276e-05}]\n+```\n \n-1. æ›´ä½çš„é‹ç®—æˆæœ¬ï¼Œæ›´å°‘çš„ç¢³æ’æ”¾ï¼š\n-    - ç ”ç©¶äººå“¡å¯ä»¥åˆ†äº«å·²è¨“ç·´çš„æ¨¡å‹è€Œéæ¯æ¬¡å¾é ­é–‹å§‹è¨“ç·´\n-    - å·¥ç¨‹å¸«å¯ä»¥æ¸›å°‘è¨ˆç®—æ™‚é–“ä»¥åŠç”Ÿç”¢æˆæœ¬\n-    - æ•¸åç¨®æ¨¡å‹æ¶æ§‹ã€å…©åƒå¤šå€‹é è¨“ç·´æ¨¡å‹ã€100å¤šç¨®èªè¨€æ”¯æ´\n+</details>\n \n-1. å°æ–¼æ¨¡å‹ç”Ÿå‘½é€±æœŸçš„æ¯ä¸€å€‹éƒ¨åˆ†éƒ½é¢é¢ä¿±åˆ°ï¼š\n-    - è¨“ç·´å…ˆé€²çš„æ¨¡å‹ï¼Œåªéœ€ 3 è¡Œç¨‹å¼ç¢¼\n-    - æ¨¡å‹å¯ä»¥åœ¨ä¸åŒæ·±åº¦å­¸ç¿’æ¡†æ¶ä¹‹é–“ä»»æ„è½‰æ›\n-    - ç‚ºè¨“ç·´ã€è©•ä¼°å’Œç”Ÿç”¢é¸æ“‡æœ€é©åˆçš„æ¡†æ¶ï¼Œä¸¦å®Œç¾éŠœæ¥\n+<details>\n+<summary>è¦–è¦ºå•ç­”</summary>\n \n-1. ç‚ºä½ çš„éœ€æ±‚è¼•é¬†å®¢è£½åŒ–å°ˆå±¬æ¨¡å‹å’Œç¯„ä¾‹ï¼š\n-    - æˆ‘å€‘ç‚ºæ¯ç¨®æ¨¡å‹æ¶æ§‹æä¾›äº†å¤šå€‹ç¯„ä¾‹ä¾†é‡ç¾åŸè«–æ–‡çµæœ\n-    - ä¸€è‡´çš„æ¨¡å‹å…§éƒ¨æ¶æ§‹\n-    - æ¨¡å‹æª”æ¡ˆå¯å–®ç¨ä½¿ç”¨ï¼Œä¾¿æ–¼ä¿®æ”¹å’Œå¿«é€Ÿå¯¦é©—\n+<h3 align=\"center\">\n+    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\"></a>\n+</h3>\n \n-## ä»€éº¼æƒ…æ³ä¸‹æˆ‘ä¸è©²ç”¨ transformersï¼Ÿ\n+```py\n+from transformers import pipeline\n \n-- æœ¬å‡½å¼åº«ä¸¦ä¸æ˜¯æ¨¡çµ„åŒ–çš„ç¥ç¶“ç¶²çµ¡å·¥å…·ç®±ã€‚æ¨¡å‹æ–‡ä»¶ä¸­çš„ç¨‹å¼ç¢¼ä¸¦æœªåšé¡å¤–çš„æŠ½è±¡å°è£ï¼Œä»¥ä¾¿ç ”ç©¶äººå“¡å¿«é€Ÿåœ°ç¿»é–±åŠä¿®æ”¹ç¨‹å¼ç¢¼ï¼Œè€Œä¸æœƒæ·±é™·è¤‡é›œçš„é¡åˆ¥åŒ…è£ä¹‹ä¸­ã€‚\n-- `Trainer` API ä¸¦éç›¸å®¹ä»»ä½•æ¨¡å‹ï¼Œå®ƒåªç‚ºæœ¬å‡½å¼åº«ä¸­çš„æ¨¡å‹æœ€ä½³åŒ–ã€‚å°æ–¼ä¸€èˆ¬çš„æ©Ÿå™¨å­¸ç¿’ç”¨é€”ï¼Œè«‹ä½¿ç”¨å…¶ä»–å‡½å¼åº«ã€‚\n-- å„˜ç®¡æˆ‘å€‘å·²ç›¡åŠ›è€Œç‚ºï¼Œ[examples ç›®éŒ„](https://github.com/huggingface/transformers/tree/main/examples)ä¸­çš„è…³æœ¬ä¹Ÿåƒ…ç‚ºç¯„ä¾‹è€Œå·²ã€‚å°æ–¼ç‰¹å®šå•é¡Œï¼Œå®ƒå€‘ä¸¦ä¸ä¸€å®šéš¨é¸å³ç”¨ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹å¹¾è¡Œç¨‹å¼ç¢¼ä»¥ç¬¦åˆéœ€æ±‚ã€‚\n+pipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\n+pipeline(\n+    image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\n+    question=\"What is in the image?\",\n+)\n+[{'answer': 'statue of liberty'}]\n+```\n \n-## å®‰è£\n+</details>\n \n-### ä½¿ç”¨ pip\n+## ç‚ºä»€éº¼æˆ‘æ‡‰è©²ä½¿ç”¨ Transformersï¼Ÿ\n \n-é€™å€‹ Repository å·²åœ¨ Python 3.9+ã€Flax 0.4.1+ã€PyTorch 2.1+ å’Œ TensorFlow 2.6+ ä¸‹ç¶“éæ¸¬è©¦ã€‚\n+1.  æ˜“æ–¼ä½¿ç”¨çš„æœ€å…ˆé€²æ¨¡å‹ï¼š\n+    *   åœ¨è‡ªç„¶èªè¨€ç†è§£èˆ‡ç”Ÿæˆã€é›»è…¦è¦–è¦ºã€éŸ³è¨Šã€å½±ç‰‡å’Œå¤šæ¨¡æ…‹ä»»å‹™ä¸Šè¡¨ç¾å“è¶Šã€‚\n+    *   ç‚ºç ”ç©¶äººå“¡ã€å·¥ç¨‹å¸«èˆ‡é–‹ç™¼è€…æä¾›äº†ä½é–€æª»çš„å…¥é–€é€”å¾‘ã€‚\n+    *   é¢å‘ä½¿ç”¨è€…çš„æŠ½è±¡å±¤ç´šå°‘ï¼Œåªéœ€å­¸ç¿’ä¸‰å€‹æ ¸å¿ƒé¡åˆ¥ã€‚\n+    *   ç‚ºæ‰€æœ‰é è¨“ç·´æ¨¡å‹æä¾›äº†çµ±ä¸€çš„ API ä»‹é¢ã€‚\n \n-ä½ å¯ä»¥åœ¨[è™›æ“¬ç’°å¢ƒ](https://docs.python.org/3/library/venv.html)ä¸­å®‰è£ ğŸ¤— Transformersã€‚å¦‚æœä½ é‚„ä¸ç†Ÿæ‚‰ Python çš„è™›æ“¬ç’°å¢ƒï¼Œè«‹é–±æ­¤[ä½¿ç”¨è€…æŒ‡å¼•](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)ã€‚\n+2.  æ›´ä½çš„é‹ç®—æˆæœ¬ï¼Œæ›´å°çš„ç¢³è¶³è·¡ï¼š\n+    *   åˆ†äº«è¨“ç·´å¥½çš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯å¾é›¶é–‹å§‹è¨“ç·´ã€‚\n+    *   æ¸›å°‘é‹ç®—æ™‚é–“å’Œç”Ÿç”¢æˆæœ¬ã€‚\n+    *   æ“æœ‰æ•¸åç¨®æ¨¡å‹æ¶æ§‹å’Œè¶…é100è¬å€‹æ©«è·¨æ‰€æœ‰æ¨¡æ…‹çš„é è¨“ç·´æª¢æŸ¥é»ã€‚\n \n-é¦–å…ˆï¼Œç”¨ä½ æ‰“ç®—ä½¿ç”¨çš„ç‰ˆæœ¬çš„ Python å‰µå»ºä¸€å€‹è™›æ“¬ç’°å¢ƒä¸¦é€²å…¥ã€‚\n+3.  ç‚ºæ¨¡å‹çš„æ¯å€‹ç”Ÿå‘½é€±æœŸéšæ®µé¸æ“‡åˆé©çš„æ¡†æ¶ï¼š\n+    *   åƒ…ç”¨3è¡Œç¨‹å¼ç¢¼å³å¯è¨“ç·´æœ€å…ˆé€²çš„æ¨¡å‹ã€‚\n+    *   åœ¨PyTorch/JAX/TF2.0æ¡†æ¶ä¹‹é–“è¼•é¬†åˆ‡æ›å–®ä¸€æ¨¡å‹ã€‚\n+    *   ç‚ºè¨“ç·´ã€è©•ä¼°å’Œç”Ÿç”¢é¸æ“‡æœ€åˆé©çš„æ¡†æ¶ã€‚\n \n-ç„¶å¾Œï¼Œä½ éœ€è¦å®‰è£ Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ã€‚å°æ–¼è©²å¦‚ä½•åœ¨ä½ ä½¿ç”¨çš„å¹³å°ä¸Šå®‰è£é€™äº›æ¡†æ¶ï¼Œè«‹åƒé–± [TensorFlow å®‰è£é é¢](https://www.tensorflow.org/install/), [PyTorch å®‰è£é é¢](https://pytorch.org/get-started/locally/#start-locally) æˆ– [Flax å®‰è£é é¢](https://github.com/google/flax#quick-install)ã€‚\n+4.  è¼•é¬†æ ¹æ“šæ‚¨çš„éœ€æ±‚å®¢è£½åŒ–æ¨¡å‹æˆ–ç¯„ä¾‹ï¼š\n+    *   æˆ‘å€‘ç‚ºæ¯å€‹æ¶æ§‹æä¾›äº†ç¯„ä¾‹ï¼Œä»¥é‡ç¾å…¶åŸä½œè€…ç™¼è¡¨çš„çµæœã€‚\n+    *   æ¨¡å‹å…§éƒ¨çµæ§‹ç›¡å¯èƒ½ä¿æŒä¸€è‡´åœ°æš´éœ²çµ¦ä½¿ç”¨è€…ã€‚\n+    *   æ¨¡å‹æª”æ¡ˆå¯ä»¥ç¨ç«‹æ–¼å‡½å¼åº«ä½¿ç”¨ï¼Œä¾¿æ–¼å¿«é€Ÿå¯¦é©—ã€‚\n \n-ç•¶å…¶ä¸­ä¸€å€‹å¾Œç«¯å®‰è£æˆåŠŸå¾Œï¼ŒğŸ¤— Transformers å¯ä¾æ­¤å®‰è£ï¼š\n+<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n+    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n+</a><br>\n \n-```bash\n-pip install transformers\n-```\n+## ç‚ºä»€éº¼æˆ‘ä¸æ‡‰è©²ä½¿ç”¨ Transformersï¼Ÿ\n \n-å¦‚æœä½ æƒ³è¦è©¦è©¦ç¯„ä¾‹æˆ–è€…æƒ³åœ¨æ­£å¼ç™¼å¸ƒå‰ä½¿ç”¨æœ€æ–°é–‹ç™¼ä¸­çš„ç¨‹å¼ç¢¼ï¼Œä½ å¿…é ˆ[å¾åŸå§‹ç¢¼å®‰è£](https://huggingface.co/docs/transformers/installation#installing-from-source)ã€‚\n+-   æœ¬å‡½å¼åº«ä¸¦éä¸€å€‹ç”¨æ–¼å»ºæ§‹ç¥ç¶“ç¶²è·¯çš„æ¨¡çµ„åŒ–å·¥å…·ç®±ã€‚æ¨¡å‹æª”æ¡ˆä¸­çš„ç¨‹å¼ç¢¼ç‚ºäº†è®“ç ”ç©¶äººå“¡èƒ½å¿«é€Ÿåœ¨æ¨¡å‹ä¸Šè¿­ä»£ï¼Œè€Œæ²’æœ‰é€²è¡Œéåº¦çš„æŠ½è±¡é‡æ§‹ï¼Œé¿å…äº†æ·±å…¥é¡å¤–çš„æŠ½è±¡å±¤/æª”æ¡ˆã€‚\n+-   è¨“ç·´ API é‡å° Transformers æä¾›çš„ PyTorch æ¨¡å‹é€²è¡Œäº†æœ€ä½³åŒ–ã€‚å°æ–¼é€šç”¨çš„æ©Ÿå™¨å­¸ç¿’è¿´åœˆï¼Œæ‚¨æ‡‰è©²ä½¿ç”¨åƒ [Accelerate](https://huggingface.co/docs/accelerate) é€™æ¨£çš„å…¶ä»–å‡½å¼åº«ã€‚\n+-   [ç¯„ä¾‹æŒ‡ä»¤ç¨¿](https://github.com/huggingface/transformers/tree/main/examples)åƒ…åƒ…æ˜¯*ç¯„ä¾‹*ã€‚å®ƒå€‘ä¸ä¸€å®šèƒ½åœ¨æ‚¨çš„ç‰¹å®šç”¨ä¾‹ä¸Šé–‹ç®±å³ç”¨ï¼Œæ‚¨å¯èƒ½éœ€è¦ä¿®æ”¹ç¨‹å¼ç¢¼æ‰èƒ½ä½¿å…¶æ­£å¸¸é‹ä½œã€‚\n \n-### ä½¿ç”¨ conda\n+## 100å€‹ä½¿ç”¨ Transformers çš„å°ˆæ¡ˆ\n \n-ğŸ¤— Transformers å¯ä»¥è—‰ç”± conda ä¾æ­¤å®‰è£ï¼š\n+Transformers ä¸åƒ…åƒ…æ˜¯ä¸€å€‹ä½¿ç”¨é è¨“ç·´æ¨¡å‹çš„å·¥å…·åŒ…ï¼Œå®ƒé‚„æ˜¯ä¸€å€‹åœç¹å®ƒå’Œ Hugging Face Hub å»ºæ§‹çš„å°ˆæ¡ˆç¤¾ç¾¤ã€‚æˆ‘å€‘å¸Œæœ› Transformers èƒ½å¤ è³¦èƒ½é–‹ç™¼è€…ã€ç ”ç©¶äººå“¡ã€å­¸ç”Ÿã€æ•™æˆã€å·¥ç¨‹å¸«ä»¥åŠå…¶ä»–ä»»ä½•äººï¼Œå»å»ºæ§‹ä»–å€‘å¤¢æƒ³ä¸­çš„å°ˆæ¡ˆã€‚\n \n-```shell script\n-conda install conda-forge::transformers\n-```\n+ç‚ºäº†æ…¶ç¥ Transformers ç²å¾— 10 è¬é¡†æ˜Ÿæ¨™ï¼Œæˆ‘å€‘å¸Œæœ›é€é [awesome-transformers](./awesome-transformers.md) é é¢ä¾†èšç„¦ç¤¾ç¾¤ï¼Œè©²é é¢åˆ—å‡ºäº†100å€‹åŸºæ–¼ Transformers å»ºæ§‹çš„ç²¾å½©å°ˆæ¡ˆã€‚\n+\n+å¦‚æœæ‚¨æ“æœ‰æˆ–ä½¿ç”¨ä¸€å€‹æ‚¨èªç‚ºæ‡‰è©²è¢«åˆ—å…¥å…¶ä¸­çš„å°ˆæ¡ˆï¼Œè«‹éš¨æ™‚æäº¤ PR å°‡å…¶åŠ å…¥ï¼\n+\n+## ç¯„ä¾‹æ¨¡å‹\n+\n+æ‚¨å¯ä»¥åœ¨æˆ‘å€‘å¤§å¤šæ•¸æ¨¡å‹çš„ [Hub æ¨¡å‹é é¢](https://huggingface.co/models) ä¸Šç›´æ¥é€²è¡Œæ¸¬è©¦ã€‚\n \n-> **_ç­†è¨˜:_** å¾ `huggingface` é »é“å®‰è£ `transformers` å·²è¢«æ·˜æ±°ã€‚\n+å±•é–‹ä¸‹é¢çš„æ¯å€‹æ¨¡æ…‹ï¼ŒæŸ¥çœ‹ä¸€äº›ç”¨æ–¼ä¸åŒç”¨ä¾‹çš„ç¯„ä¾‹æ¨¡å‹ã€‚\n \n-è¦è—‰ç”± conda å®‰è£ Flaxã€PyTorch æˆ– TensorFlow å…¶ä¸­ä¹‹ä¸€ï¼Œè«‹åƒé–±å®ƒå€‘å„è‡ªå®‰è£é é¢çš„èªªæ˜ã€‚\n+<details>\n+<summary>éŸ³è¨Š</summary>\n \n-## æ¨¡å‹æ¶æ§‹\n+-   Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)\n+-   Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)\n+-   Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n+-   Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)\n+-   Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)\n+-   Text to speech with [Bark](https://huggingface.co/suno/bark)\n \n-**ğŸ¤— Transformers æ”¯æ´çš„[æ‰€æœ‰çš„æ¨¡å‹æª¢æŸ¥é»](https://huggingface.co/models)**ï¼Œç”±[ä½¿ç”¨è€…](https://huggingface.co/users)å’Œ[çµ„ç¹”](https://huggingface.co/organizations)ä¸Šå‚³ï¼Œå‡èˆ‡ huggingface.co [model hub](https://huggingface.co) å®Œç¾çµåˆã€‚\n+</details>\n \n-ç›®å‰çš„æª¢æŸ¥é»æ•¸é‡ï¼š ![](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen)\n+<details>\n+<summary>é›»è…¦è¦–è¦º</summary>\n \n-ğŸ¤— Transformers ç›®å‰æ”¯æ´ä»¥ä¸‹çš„æ¶æ§‹: æ¨¡å‹æ¦‚è¦½è«‹åƒé–±[é€™è£¡](https://huggingface.co/docs/transformers/model_summary).\n+-   Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)\n+-   Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)\n+-   Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)\n+-   Keypoint detection with [SuperPoint](https://huggingface.co/magic-leap-community/superpoint)\n+-   Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)\n+-   Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)\n+-   Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)\n+-   Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)\n+-   Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)\n \n-è¦æª¢æŸ¥æŸå€‹æ¨¡å‹æ˜¯å¦å·²æœ‰ Flaxã€PyTorch æˆ– TensorFlow çš„å¯¦ä½œï¼Œæˆ–å…¶æ˜¯å¦åœ¨ğŸ¤— Tokenizers å‡½å¼åº«ä¸­æœ‰å°æ‡‰çš„ tokenizerï¼Œæ•¬è«‹åƒé–±[æ­¤è¡¨](https://huggingface.co/docs/transformers/index#supported-frameworks)ã€‚\n+</details>\n \n-é€™äº›å¯¦ä½œå‡å·²æ–¼å¤šå€‹è³‡æ–™é›†æ¸¬è©¦ï¼ˆè«‹åƒé–±ç¯„ä¾‹è…³æœ¬ï¼‰ä¸¦æ‡‰èˆ‡åŸç‰ˆå¯¦ä½œè¡¨ç¾ç›¸ç•¶ã€‚ä½ å¯ä»¥åœ¨ç¯„ä¾‹æ–‡ä»¶çš„[æ­¤ç¯€](https://huggingface.co/docs/transformers/examples)ä¸­äº†è§£å¯¦ä½œçš„ç´°ç¯€ã€‚\n+<details>\n+<summary>å¤šæ¨¡æ…‹</summary>\n \n+-   Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)\n+-   Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)\n+-   Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n+-   Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n+-   OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n+-   Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)\n+-   Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)\n+-   Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)\n+-   Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n+-   Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)\n \n-## äº†è§£æ›´å¤š\n+</details>\n \n-| ç« ç¯€ | æè¿° |\n-|-|-|\n-| [æ–‡ä»¶](https://huggingface.co/transformers/) | å®Œæ•´çš„ API æ–‡ä»¶å’Œæ•™å­¸ |\n-| [ä»»å‹™æ¦‚è¦½](https://huggingface.co/docs/transformers/task_summary) | ğŸ¤— Transformers æ”¯æ´çš„ä»»å‹™ |\n-| [é è™•ç†æ•™å­¸](https://huggingface.co/docs/transformers/preprocessing) | ä½¿ç”¨ `Tokenizer` ä¾†ç‚ºæ¨¡å‹æº–å‚™è³‡æ–™ |\n-| [è¨“ç·´å’Œå¾®èª¿](https://huggingface.co/docs/transformers/training) | ä½¿ç”¨ PyTorch/TensorFlow çš„å…§å»ºçš„è¨“ç·´æ–¹å¼æˆ–æ–¼ `Trainer` API ä¸­ä½¿ç”¨ ğŸ¤— Transformers æä¾›çš„æ¨¡å‹ |\n-| [å¿«é€Ÿä¸Šæ‰‹ï¼šå¾®èª¿å’Œç¯„ä¾‹è…³æœ¬](https://github.com/huggingface/transformers/tree/main/examples) | ç‚ºå„ç¨®ä»»å‹™æä¾›çš„ç¯„ä¾‹è…³æœ¬ |\n-| [æ¨¡å‹åˆ†äº«å’Œä¸Šå‚³](https://huggingface.co/docs/transformers/model_sharing) | ä¸Šå‚³ä¸¦èˆ‡ç¤¾ç¾¤åˆ†äº«ä½ å¾®èª¿çš„æ¨¡å‹ |\n-| [é·ç§»](https://huggingface.co/docs/transformers/migration) | å¾ `pytorch-transformers` æˆ– `pytorch-pretrained-bert` é·ç§»åˆ° ğŸ¤— Transformers |\n+<details>\n+<summary>è‡ªç„¶èªè¨€è™•ç† (NLP)</summary>\n+\n+-   Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)\n+-   Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)\n+-   Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)\n+-   Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)\n+-   Translation with [T5](https://huggingface.co/google-t5/t5-base)\n+-   Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)\n+-   Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)\n+\n+</details>\n \n ## å¼•ç”¨\n \n-æˆ‘å€‘å·²å°‡æ­¤å‡½å¼åº«çš„[è«–æ–‡](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)æ­£å¼ç™¼è¡¨ã€‚å¦‚æœä½ ä½¿ç”¨äº† ğŸ¤— Transformers å‡½å¼åº«ï¼Œå¯ä»¥å¼•ç”¨ï¼š\n+ç¾åœ¨æˆ‘å€‘æœ‰ä¸€ç¯‡å¯ä¾›æ‚¨å¼•ç”¨çš„é—œæ–¼ ğŸ¤— Transformers å‡½å¼åº«çš„ [è«–æ–‡](https://www.aclweb.org/anthology/2020.emnlp-demos.6/)ï¼š\n ```bibtex\n @inproceedings{wolf-etal-2020-transformers,\n     title = \"Transformers: State-of-the-Art Natural Language Processing\",\n@@ -285,4 +324,4 @@ conda install conda-forge::transformers\n     url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n     pages = \"38--45\"\n }\n-```\n+```\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 648,
        "additions": 382,
        "deletions": 266
    }
}