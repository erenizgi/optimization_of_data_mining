{
    "author": "vasqu",
    "message": "[`FA`] Fix dtype in varlen with position ids (#40295)\n\nfix",
    "sha": "a4e1fee44dc849f2d5ea698c5e730a86c058ba6b",
    "files": [
        {
            "sha": "ee113f7093f0bb65863e674a28c8fc0b60d7679f",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4e1fee44dc849f2d5ea698c5e730a86c058ba6b/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4e1fee44dc849f2d5ea698c5e730a86c058ba6b/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=a4e1fee44dc849f2d5ea698c5e730a86c058ba6b",
            "patch": "@@ -336,9 +336,8 @@ def prepare_fa_kwargs_from_position_ids(position_ids, is_packed_sequence: bool =\n     # If the lengths are not equal, most probably we are in decoding stage with cache\n     # In that case the position ids will not always start with `0` and we need a better way to infer\n     # cumulative seq lengths.\n+    tensor_kwargs = {\"dtype\": torch.int32, \"device\": position_ids.device}\n     if not is_packed_sequence:\n-        tensor_kwargs = {\"dtype\": torch.int32, \"device\": position_ids.device}\n-\n         last_position_ids = position_ids[:, -1]\n         q_len = (\n             torch.ones(position_ids.size(0), **tensor_kwargs)\n@@ -358,8 +357,8 @@ def prepare_fa_kwargs_from_position_ids(position_ids, is_packed_sequence: bool =\n \n         cu_seq_lens_q = torch.cat(\n             (\n-                indices_q,\n-                torch.tensor(position_ids.size(), device=position_ids.device, dtype=torch.int32),\n+                indices_q.to(**tensor_kwargs),\n+                torch.tensor(position_ids.size(), **tensor_kwargs),\n             )\n         )\n         cu_seq_lens_k = cu_seq_lens_q"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 3,
        "deletions": 4
    }
}