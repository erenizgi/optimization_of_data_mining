{
    "author": "cyyever",
    "message": "[2/N] Use pyupgrade --py39-plus to improve code (#36857)\n\nUse pyupgrade --py39-plus to improve code",
    "sha": "00d077267a7add6a068b1ed9393b31b9e46e4ed3",
    "files": [
        {
            "sha": "ea6cc769f640fa13770ca3e054b2e866e0a39785",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 24,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n #\n@@ -20,7 +19,7 @@\n import os\n import re\n import warnings\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n from packaging import version\n \n@@ -196,11 +195,11 @@ class PretrainedConfig(PushToHubMixin):\n \n     model_type: str = \"\"\n     base_config_key: str = \"\"\n-    sub_configs: Dict[str, \"PretrainedConfig\"] = {}\n+    sub_configs: dict[str, \"PretrainedConfig\"] = {}\n     is_composition: bool = False\n-    attribute_map: Dict[str, str] = {}\n-    base_model_tp_plan: Optional[Dict[str, Any]] = None\n-    base_model_pp_plan: Optional[Dict[str, Tuple[List[str]]]] = None\n+    attribute_map: dict[str, str] = {}\n+    base_model_tp_plan: Optional[dict[str, Any]] = None\n+    base_model_pp_plan: Optional[dict[str, tuple[list[str]]]] = None\n     _auto_class: Optional[str] = None\n \n     def __setattr__(self, key, value):\n@@ -574,7 +573,7 @@ def from_pretrained(\n     @classmethod\n     def get_config_dict(\n         cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n-    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n+    ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n         [`PretrainedConfig`] using `from_dict`.\n@@ -609,7 +608,7 @@ def get_config_dict(\n     @classmethod\n     def _get_config_dict(\n         cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n-    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n+    ) -> tuple[dict[str, Any], dict[str, Any]]:\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n         resume_download = kwargs.pop(\"resume_download\", None)\n@@ -667,13 +666,13 @@ def _get_config_dict(\n                 if resolved_config_file is None:\n                     return None, kwargs\n                 commit_hash = extract_commit_hash(resolved_config_file, commit_hash)\n-            except EnvironmentError:\n+            except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n                 raise\n             except Exception:\n                 # For any other exception, we throw a generic error.\n-                raise EnvironmentError(\n+                raise OSError(\n                     f\"Can't load the configuration of '{pretrained_model_name_or_path}'. If you were trying to load it\"\n                     \" from 'https://huggingface.co/models', make sure you don't have a local directory with the same\"\n                     f\" name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory\"\n@@ -689,9 +688,7 @@ def _get_config_dict(\n \n             config_dict[\"_commit_hash\"] = commit_hash\n         except (json.JSONDecodeError, UnicodeDecodeError):\n-            raise EnvironmentError(\n-                f\"It looks like the config file at '{resolved_config_file}' is not a valid JSON file.\"\n-            )\n+            raise OSError(f\"It looks like the config file at '{resolved_config_file}' is not a valid JSON file.\")\n \n         if is_local:\n             logger.info(f\"loading configuration file {resolved_config_file}\")\n@@ -714,7 +711,7 @@ def _get_config_dict(\n         return config_dict, kwargs\n \n     @classmethod\n-    def from_dict(cls, config_dict: Dict[str, Any], **kwargs) -> \"PretrainedConfig\":\n+    def from_dict(cls, config_dict: dict[str, Any], **kwargs) -> \"PretrainedConfig\":\n         \"\"\"\n         Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.\n \n@@ -792,7 +789,7 @@ def from_json_file(cls, json_file: Union[str, os.PathLike]) -> \"PretrainedConfig\n \n     @classmethod\n     def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n-        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n+        with open(json_file, encoding=\"utf-8\") as reader:\n             text = reader.read()\n         return json.loads(text)\n \n@@ -803,10 +800,9 @@ def __repr__(self):\n         return f\"{self.__class__.__name__} {self.to_json_string()}\"\n \n     def __iter__(self):\n-        for attr in self.__dict__:\n-            yield attr\n+        yield from self.__dict__\n \n-    def to_diff_dict(self) -> Dict[str, Any]:\n+    def to_diff_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Removes all attributes from config which correspond to the default config attributes for better readability and\n         serializes to a Python dictionary.\n@@ -874,7 +870,7 @@ def to_diff_dict(self) -> Dict[str, Any]:\n \n         return serializable_config_dict\n \n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary.\n \n@@ -954,7 +950,7 @@ def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool =\n         with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n             writer.write(self.to_json_string(use_diff=use_diff))\n \n-    def update(self, config_dict: Dict[str, Any]):\n+    def update(self, config_dict: dict[str, Any]):\n         \"\"\"\n         Updates attributes of this class with attributes from `config_dict`.\n \n@@ -1002,7 +998,7 @@ def update_from_string(self, update_str: str):\n \n             setattr(self, k, v)\n \n-    def dict_torch_dtype_to_str(self, d: Dict[str, Any]) -> None:\n+    def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n         \"\"\"\n         Checks whether the passed dictionary and its nested dicts have a *torch_dtype* key and if it's not None,\n         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n@@ -1044,7 +1040,7 @@ def register_for_auto_class(cls, auto_class=\"AutoConfig\"):\n         cls._auto_class = auto_class\n \n     @staticmethod\n-    def _get_global_generation_defaults() -> Dict[str, Any]:\n+    def _get_global_generation_defaults() -> dict[str, Any]:\n         return {\n             \"max_length\": 20,\n             \"min_length\": 0,\n@@ -1073,7 +1069,7 @@ def _get_global_generation_defaults() -> Dict[str, Any]:\n             \"begin_suppress_tokens\": None,\n         }\n \n-    def _get_non_default_generation_parameters(self) -> Dict[str, Any]:\n+    def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n         \"\"\"\n         Gets the non-default generation parameters on the PretrainedConfig instance\n         \"\"\"\n@@ -1148,7 +1144,7 @@ def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n         return self\n \n \n-def get_configuration_file(configuration_files: List[str]) -> str:\n+def get_configuration_file(configuration_files: list[str]) -> str:\n     \"\"\"\n     Get the configuration file to use for this version of transformers.\n "
        },
        {
            "sha": "5380c5c639c0ed1099373b822926952beccdab34",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -1587,7 +1587,7 @@ def extract_vocab_merges_from_model(self, tiktoken_url: str):\n             from tiktoken.load import load_tiktoken_bpe\n         except Exception:\n             raise ValueError(\n-                \"`tiktoken` is required to read a `tiktoken` file. Install it with \" \"`pip install tiktoken`.\"\n+                \"`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\"\n             )\n \n         bpe_ranks = load_tiktoken_bpe(tiktoken_url)"
        },
        {
            "sha": "91425d9fed7a3e4626d670b615565e6707b0f870",
            "filename": "src/transformers/debug_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fdebug_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fdebug_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdebug_utils.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -206,7 +206,7 @@ def batch_start_frame(self):\n         self.expand_frame(f\"{'abs min':8} {'abs max':8} metadata\")\n \n     def batch_end_frame(self):\n-        self.expand_frame(f\"{self.prefix} *** Finished batch number={self.batch_number-1} ***\\n\\n\")\n+        self.expand_frame(f\"{self.prefix} *** Finished batch number={self.batch_number - 1} ***\\n\\n\")\n \n     def create_frame(self, module, input, output):\n         self.expand_frame(f\"{self.prefix} {self.module_names[module]} {module.__class__.__name__}\")"
        },
        {
            "sha": "4e8a5b3840585abbda6cc8583407b81b90aab7d9",
            "filename": "src/transformers/dynamic_module_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fdynamic_module_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fdynamic_module_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdynamic_module_utils.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -24,11 +23,10 @@\n import signal\n import sys\n import threading\n-import typing\n import warnings\n from pathlib import Path\n from types import ModuleType\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Optional, Union\n \n from huggingface_hub import try_to_load_from_cache\n \n@@ -84,7 +82,7 @@ def create_dynamic_module(name: Union[str, os.PathLike]) -> None:\n         importlib.invalidate_caches()\n \n \n-def get_relative_imports(module_file: Union[str, os.PathLike]) -> List[str]:\n+def get_relative_imports(module_file: Union[str, os.PathLike]) -> list[str]:\n     \"\"\"\n     Get the list of modules that are relatively imported in a module file.\n \n@@ -94,7 +92,7 @@ def get_relative_imports(module_file: Union[str, os.PathLike]) -> List[str]:\n     Returns:\n         `List[str]`: The list of relative imports in the module.\n     \"\"\"\n-    with open(module_file, \"r\", encoding=\"utf-8\") as f:\n+    with open(module_file, encoding=\"utf-8\") as f:\n         content = f.read()\n \n     # Imports of the form `import .xxx`\n@@ -105,7 +103,7 @@ def get_relative_imports(module_file: Union[str, os.PathLike]) -> List[str]:\n     return list(set(relative_imports))\n \n \n-def get_relative_import_files(module_file: Union[str, os.PathLike]) -> List[str]:\n+def get_relative_import_files(module_file: Union[str, os.PathLike]) -> list[str]:\n     \"\"\"\n     Get the list of all files that are needed for a given module. Note that this function recurses through the relative\n     imports (if a imports b and b imports c, it will return module files for b and c).\n@@ -138,7 +136,7 @@ def get_relative_import_files(module_file: Union[str, os.PathLike]) -> List[str]\n     return all_relative_imports\n \n \n-def get_imports(filename: Union[str, os.PathLike]) -> List[str]:\n+def get_imports(filename: Union[str, os.PathLike]) -> list[str]:\n     \"\"\"\n     Extracts all the libraries (not relative imports this time) that are imported in a file.\n \n@@ -148,7 +146,7 @@ def get_imports(filename: Union[str, os.PathLike]) -> List[str]:\n     Returns:\n         `List[str]`: The list of all packages required to use the input module.\n     \"\"\"\n-    with open(filename, \"r\", encoding=\"utf-8\") as f:\n+    with open(filename, encoding=\"utf-8\") as f:\n         content = f.read()\n \n     # filter out try/except block so in custom code we can have try/except imports\n@@ -168,7 +166,7 @@ def get_imports(filename: Union[str, os.PathLike]) -> List[str]:\n     return list(set(imports))\n \n \n-def check_imports(filename: Union[str, os.PathLike]) -> List[str]:\n+def check_imports(filename: Union[str, os.PathLike]) -> list[str]:\n     \"\"\"\n     Check if the current Python environment contains all the libraries that are imported in a file. Will raise if a\n     library is missing.\n@@ -208,7 +206,7 @@ def get_class_in_module(\n     module_path: Union[str, os.PathLike],\n     *,\n     force_reload: bool = False,\n-) -> typing.Type:\n+) -> type:\n     \"\"\"\n     Import a module on the cache directory for modules and extract a class from it.\n \n@@ -235,7 +233,7 @@ def get_class_in_module(\n         module_spec = importlib.util.spec_from_file_location(name, location=module_file)\n \n         # Hash the module file and all its relative imports to check if we need to reload it\n-        module_files: List[Path] = [module_file] + sorted(map(Path, get_relative_import_files(module_file)))\n+        module_files: list[Path] = [module_file] + sorted(map(Path, get_relative_import_files(module_file)))\n         module_hash: str = hashlib.sha256(b\"\".join(bytes(f) + f.read_bytes() for f in module_files)).hexdigest()\n \n         module: ModuleType\n@@ -258,7 +256,7 @@ def get_cached_module_file(\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n     resume_download: Optional[bool] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n+    proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n     local_files_only: bool = False,\n@@ -358,7 +356,7 @@ def get_cached_module_file(\n         if not is_local and cached_module != resolved_module_file:\n             new_files.append(module_file)\n \n-    except EnvironmentError:\n+    except OSError:\n         logger.error(f\"Could not locate the {module_file} inside {pretrained_model_name_or_path}.\")\n         raise\n \n@@ -434,14 +432,14 @@ def get_class_from_dynamic_module(\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n     resume_download: Optional[bool] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n+    proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n     local_files_only: bool = False,\n     repo_type: Optional[str] = None,\n     code_revision: Optional[str] = None,\n     **kwargs,\n-) -> typing.Type:\n+) -> type:\n     \"\"\"\n     Extracts a class from a module file, present in the local folder or repository of a model.\n \n@@ -553,7 +551,7 @@ def get_class_from_dynamic_module(\n     return get_class_in_module(class_name, final_module, force_reload=force_download)\n \n \n-def custom_object_save(obj: Any, folder: Union[str, os.PathLike], config: Optional[Dict] = None) -> List[str]:\n+def custom_object_save(obj: Any, folder: Union[str, os.PathLike], config: Optional[dict] = None) -> list[str]:\n     \"\"\"\n     Save the modeling files corresponding to a custom model/configuration/tokenizer etc. in a given folder. Optionally\n     adds the proper fields in a config."
        },
        {
            "sha": "ca2a3b5fde31d81554c76e26a24ebb4b806ed052",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2021 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -21,7 +20,7 @@\n import os\n import warnings\n from collections import UserDict\n-from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n import numpy as np\n \n@@ -74,7 +73,7 @@ class BatchFeature(UserDict):\n             initialization.\n     \"\"\"\n \n-    def __init__(self, data: Optional[Dict[str, Any]] = None, tensor_type: Union[None, str, TensorType] = None):\n+    def __init__(self, data: Optional[dict[str, Any]] = None, tensor_type: Union[None, str, TensorType] = None):\n         super().__init__(data)\n         self.convert_to_tensors(tensor_type=tensor_type)\n \n@@ -450,7 +449,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n     @classmethod\n     def get_feature_extractor_dict(\n         cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n-    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n+    ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n         feature extractor of type [`~feature_extraction_utils.FeatureExtractionMixin`] using `from_dict`.\n@@ -521,13 +520,13 @@ def get_feature_extractor_dict(\n                     user_agent=user_agent,\n                     revision=revision,\n                 )\n-            except EnvironmentError:\n+            except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n                 raise\n             except Exception:\n                 # For any other exception, we throw a generic error.\n-                raise EnvironmentError(\n+                raise OSError(\n                     f\"Can't load feature extractor for '{pretrained_model_name_or_path}'. If you were trying to load\"\n                     \" it from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n                     f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n@@ -536,12 +535,12 @@ def get_feature_extractor_dict(\n \n         try:\n             # Load feature_extractor dict\n-            with open(resolved_feature_extractor_file, \"r\", encoding=\"utf-8\") as reader:\n+            with open(resolved_feature_extractor_file, encoding=\"utf-8\") as reader:\n                 text = reader.read()\n             feature_extractor_dict = json.loads(text)\n \n         except json.JSONDecodeError:\n-            raise EnvironmentError(\n+            raise OSError(\n                 f\"It looks like the config file at '{resolved_feature_extractor_file}' is not a valid JSON file.\"\n             )\n \n@@ -565,7 +564,7 @@ def get_feature_extractor_dict(\n         return feature_extractor_dict, kwargs\n \n     @classmethod\n-    def from_dict(cls, feature_extractor_dict: Dict[str, Any], **kwargs) -> PreTrainedFeatureExtractor:\n+    def from_dict(cls, feature_extractor_dict: dict[str, Any], **kwargs) -> PreTrainedFeatureExtractor:\n         \"\"\"\n         Instantiates a type of [`~feature_extraction_utils.FeatureExtractionMixin`] from a Python dictionary of\n         parameters.\n@@ -601,7 +600,7 @@ def from_dict(cls, feature_extractor_dict: Dict[str, Any], **kwargs) -> PreTrain\n         else:\n             return feature_extractor\n \n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary. Returns:\n             `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n@@ -628,7 +627,7 @@ def from_json_file(cls, json_file: Union[str, os.PathLike]) -> PreTrainedFeature\n             A feature extractor of type [`~feature_extraction_utils.FeatureExtractionMixin`]: The feature_extractor\n             object instantiated from that JSON file.\n         \"\"\"\n-        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n+        with open(json_file, encoding=\"utf-8\") as reader:\n             text = reader.read()\n         feature_extractor_dict = json.loads(text)\n         return cls(**feature_extractor_dict)"
        },
        {
            "sha": "5398abe028222b14a372e4e8d3a05a2e1dbf9883",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2020 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -19,7 +18,7 @@\n import os\n import warnings\n from io import BytesIO\n-from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n+from typing import Any, Optional, TypeVar, Union\n \n import numpy as np\n import requests\n@@ -98,7 +97,7 @@ def _set_processor_class(self, processor_class: str):\n \n     @classmethod\n     def from_pretrained(\n-        cls: Type[ImageProcessorType],\n+        cls: type[ImageProcessorType],\n         pretrained_model_name_or_path: Union[str, os.PathLike],\n         cache_dir: Optional[Union[str, os.PathLike]] = None,\n         force_download: bool = False,\n@@ -274,7 +273,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n     @classmethod\n     def get_image_processor_dict(\n         cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n-    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n+    ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n         image processor of type [`~image_processor_utils.ImageProcessingMixin`] using `from_dict`.\n@@ -351,13 +350,13 @@ def get_image_processor_dict(\n                     revision=revision,\n                     subfolder=subfolder,\n                 )\n-            except EnvironmentError:\n+            except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n                 raise\n             except Exception:\n                 # For any other exception, we throw a generic error.\n-                raise EnvironmentError(\n+                raise OSError(\n                     f\"Can't load image processor for '{pretrained_model_name_or_path}'. If you were trying to load\"\n                     \" it from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n                     f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n@@ -366,12 +365,12 @@ def get_image_processor_dict(\n \n         try:\n             # Load image_processor dict\n-            with open(resolved_image_processor_file, \"r\", encoding=\"utf-8\") as reader:\n+            with open(resolved_image_processor_file, encoding=\"utf-8\") as reader:\n                 text = reader.read()\n             image_processor_dict = json.loads(text)\n \n         except json.JSONDecodeError:\n-            raise EnvironmentError(\n+            raise OSError(\n                 f\"It looks like the config file at '{resolved_image_processor_file}' is not a valid JSON file.\"\n             )\n \n@@ -393,7 +392,7 @@ def get_image_processor_dict(\n         return image_processor_dict, kwargs\n \n     @classmethod\n-    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+    def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n         \"\"\"\n         Instantiates a type of [`~image_processing_utils.ImageProcessingMixin`] from a Python dictionary of parameters.\n \n@@ -437,7 +436,7 @@ def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n         else:\n             return image_processor\n \n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary.\n \n@@ -463,7 +462,7 @@ def from_json_file(cls, json_file: Union[str, os.PathLike]):\n             A image processor of type [`~image_processing_utils.ImageProcessingMixin`]: The image_processor object\n             instantiated from that JSON file.\n         \"\"\"\n-        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n+        with open(json_file, encoding=\"utf-8\") as reader:\n             text = reader.read()\n         image_processor_dict = json.loads(text)\n         return cls(**image_processor_dict)\n@@ -529,7 +528,7 @@ def register_for_auto_class(cls, auto_class=\"AutoImageProcessor\"):\n \n         cls._auto_class = auto_class\n \n-    def fetch_images(self, image_url_or_urls: Union[str, List[str]]):\n+    def fetch_images(self, image_url_or_urls: Union[str, list[str]]):\n         \"\"\"\n         Convert a single or a list of urls into the corresponding `PIL.Image` objects.\n "
        },
        {
            "sha": "4476d66f6ce06f4143fbcbb47869b0ee6357d412",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2018 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -20,7 +19,7 @@\n import warnings\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Optional, Union\n \n import requests\n import yaml\n@@ -196,7 +195,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n                 # Load model card\n                 modelcard = cls.from_json_file(resolved_model_card_file)\n \n-            except (EnvironmentError, json.JSONDecodeError):\n+            except (OSError, json.JSONDecodeError):\n                 # We fall back on creating an empty model card\n                 modelcard = cls()\n \n@@ -223,7 +222,7 @@ def from_dict(cls, json_object):\n     @classmethod\n     def from_json_file(cls, json_file):\n         \"\"\"Constructs a `ModelCard` from a json file of parameters.\"\"\"\n-        with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n+        with open(json_file, encoding=\"utf-8\") as reader:\n             text = reader.read()\n         dict_obj = json.loads(text)\n         return cls(**dict_obj)\n@@ -357,18 +356,18 @@ def _get_mapping_values(mapping):\n @dataclass\n class TrainingSummary:\n     model_name: str\n-    language: Optional[Union[str, List[str]]] = None\n+    language: Optional[Union[str, list[str]]] = None\n     license: Optional[str] = None\n-    tags: Optional[Union[str, List[str]]] = None\n+    tags: Optional[Union[str, list[str]]] = None\n     finetuned_from: Optional[str] = None\n-    tasks: Optional[Union[str, List[str]]] = None\n-    dataset: Optional[Union[str, List[str]]] = None\n-    dataset_tags: Optional[Union[str, List[str]]] = None\n-    dataset_args: Optional[Union[str, List[str]]] = None\n-    dataset_metadata: Optional[Dict[str, Any]] = None\n-    eval_results: Optional[Dict[str, float]] = None\n-    eval_lines: Optional[List[str]] = None\n-    hyperparameters: Optional[Dict[str, Any]] = None\n+    tasks: Optional[Union[str, list[str]]] = None\n+    dataset: Optional[Union[str, list[str]]] = None\n+    dataset_tags: Optional[Union[str, list[str]]] = None\n+    dataset_args: Optional[Union[str, list[str]]] = None\n+    dataset_metadata: Optional[dict[str, Any]] = None\n+    eval_results: Optional[dict[str, float]] = None\n+    eval_lines: Optional[list[str]] = None\n+    hyperparameters: Optional[dict[str, Any]] = None\n     source: Optional[str] = \"trainer\"\n \n     def __post_init__(self):"
        },
        {
            "sha": "17632a502ec59ab1112b1cdcf083f45c29fa6374",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 36,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -1,4 +1,3 @@\n-# coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -24,7 +23,7 @@\n import typing\n import warnings\n from pathlib import Path\n-from typing import Any, Callable, Dict, List, Optional, Tuple, TypedDict, Union\n+from typing import Any, Callable, Optional, TypedDict, Union\n \n import numpy as np\n import typing_extensions\n@@ -123,9 +122,9 @@ class TextKwargs(TypedDict, total=False):\n             The side on which padding will be applied.\n     \"\"\"\n \n-    text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]\n-    text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]\n-    text_pair_target: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]\n+    text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n+    text_target: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]\n+    text_pair_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n     add_special_tokens: Optional[bool]\n     padding: Union[bool, str, PaddingStrategy]\n     truncation: Union[bool, str, TruncationStrategy]\n@@ -184,17 +183,17 @@ class methods and docstrings.\n     \"\"\"\n \n     do_resize: Optional[bool]\n-    size: Optional[Dict[str, int]]\n+    size: Optional[dict[str, int]]\n     size_divisor: Optional[int]\n-    crop_size: Optional[Dict[str, int]]\n+    crop_size: Optional[dict[str, int]]\n     resample: Optional[Union[\"PILImageResampling\", int]]\n     do_rescale: Optional[bool]\n     rescale_factor: Optional[float]\n     do_normalize: Optional[bool]\n-    image_mean: Optional[Union[float, List[float]]]\n-    image_std: Optional[Union[float, List[float]]]\n+    image_mean: Optional[Union[float, list[float]]]\n+    image_std: Optional[Union[float, list[float]]]\n     do_pad: Optional[bool]\n-    pad_size: Optional[Dict[str, int]]\n+    pad_size: Optional[dict[str, int]]\n     do_center_crop: Optional[bool]\n     data_format: Optional[ChannelDimension]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n@@ -235,14 +234,14 @@ class VideosKwargs(TypedDict, total=False):\n     \"\"\"\n \n     do_resize: Optional[bool]\n-    size: Optional[Dict[str, int]]\n+    size: Optional[dict[str, int]]\n     size_divisor: Optional[int]\n     resample: Optional[\"PILImageResampling\"]\n     do_rescale: Optional[bool]\n     rescale_factor: Optional[float]\n     do_normalize: Optional[bool]\n-    image_mean: Optional[Union[float, List[float]]]\n-    image_std: Optional[Union[float, List[float]]]\n+    image_mean: Optional[Union[float, list[float]]]\n+    image_std: Optional[Union[float, list[float]]]\n     do_pad: Optional[bool]\n     do_center_crop: Optional[bool]\n     data_format: Optional[ChannelDimension]\n@@ -280,7 +279,7 @@ class AudioKwargs(TypedDict, total=False):\n     \"\"\"\n \n     sampling_rate: Optional[int]\n-    raw_speech: Optional[Union[\"np.ndarray\", List[float], List[\"np.ndarray\"], List[List[float]]]]\n+    raw_speech: Optional[Union[\"np.ndarray\", list[float], list[\"np.ndarray\"], list[list[float]]]]\n     padding: Optional[Union[bool, str, PaddingStrategy]]\n     max_length: Optional[int]\n     truncation: Optional[bool]\n@@ -379,8 +378,8 @@ class TokenizerChatTemplateKwargs(TypedDict, total=False):\n         This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\n     \"\"\"\n \n-    tools: Optional[List[Dict]] = None\n-    documents: Optional[List[Dict[str, str]]] = None\n+    tools: Optional[list[dict]] = None\n+    documents: Optional[list[dict[str, str]]] = None\n     add_generation_prompt: Optional[bool] = False\n     continue_final_message: Optional[bool] = False\n     return_assistant_tokens_mask: Optional[bool] = False\n@@ -435,12 +434,12 @@ class ProcessorMixin(PushToHubMixin):\n \n     attributes = [\"feature_extractor\", \"tokenizer\"]\n     optional_attributes = [\"chat_template\"]\n-    optional_call_args: List[str] = []\n+    optional_call_args: list[str] = []\n     # Names need to be attr_class for attr in attributes\n     feature_extractor_class = None\n     tokenizer_class = None\n     _auto_class = None\n-    valid_kwargs: List[str] = []\n+    valid_kwargs: list[str] = []\n \n     # args have to match the attributes class attribute\n     def __init__(self, *args, **kwargs):\n@@ -481,7 +480,7 @@ def __init__(self, *args, **kwargs):\n \n             setattr(self, attribute_name, arg)\n \n-    def to_dict(self) -> Dict[str, Any]:\n+    def to_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Serializes this instance to a Python dictionary.\n \n@@ -659,7 +658,7 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n     @classmethod\n     def get_processor_dict(\n         cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n-    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n+    ) -> tuple[dict[str, Any], dict[str, Any]]:\n         \"\"\"\n         From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\n         processor of type [`~processing_utils.ProcessingMixin`] using `from_args_and_dict`.\n@@ -764,13 +763,13 @@ def get_processor_dict(\n                     subfolder=subfolder,\n                     _raise_exceptions_for_missing_entries=False,\n                 )\n-            except EnvironmentError:\n+            except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n                 raise\n             except Exception:\n                 # For any other exception, we throw a generic error.\n-                raise EnvironmentError(\n+                raise OSError(\n                     f\"Can't load processor for '{pretrained_model_name_or_path}'. If you were trying to load\"\n                     \" it from 'https://huggingface.co/models', make sure you don't have a local directory with the\"\n                     f\" same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a\"\n@@ -779,11 +778,11 @@ def get_processor_dict(\n \n         # Add chat template as kwarg before returning because most models don't have processor config\n         if resolved_raw_chat_template_file is not None:\n-            with open(resolved_raw_chat_template_file, \"r\", encoding=\"utf-8\") as reader:\n+            with open(resolved_raw_chat_template_file, encoding=\"utf-8\") as reader:\n                 chat_template = reader.read()\n             kwargs[\"chat_template\"] = chat_template\n         elif resolved_chat_template_file is not None:\n-            with open(resolved_chat_template_file, \"r\", encoding=\"utf-8\") as reader:\n+            with open(resolved_chat_template_file, encoding=\"utf-8\") as reader:\n                 text = reader.read()\n             chat_template = json.loads(text)[\"chat_template\"]\n             kwargs[\"chat_template\"] = chat_template\n@@ -801,14 +800,12 @@ def get_processor_dict(\n \n         try:\n             # Load processor dict\n-            with open(resolved_processor_file, \"r\", encoding=\"utf-8\") as reader:\n+            with open(resolved_processor_file, encoding=\"utf-8\") as reader:\n                 text = reader.read()\n             processor_dict = json.loads(text)\n \n         except json.JSONDecodeError:\n-            raise EnvironmentError(\n-                f\"It looks like the config file at '{resolved_processor_file}' is not a valid JSON file.\"\n-            )\n+            raise OSError(f\"It looks like the config file at '{resolved_processor_file}' is not a valid JSON file.\")\n \n         if is_local:\n             logger.info(f\"loading configuration file {resolved_processor_file}\")\n@@ -837,7 +834,7 @@ def get_processor_dict(\n         return processor_dict, kwargs\n \n     @classmethod\n-    def from_args_and_dict(cls, args, processor_dict: Dict[str, Any], **kwargs):\n+    def from_args_and_dict(cls, args, processor_dict: dict[str, Any], **kwargs):\n         \"\"\"\n         Instantiates a type of [`~processing_utils.ProcessingMixin`] from a Python dictionary of parameters.\n \n@@ -882,9 +879,9 @@ def from_args_and_dict(cls, args, processor_dict: Dict[str, Any], **kwargs):\n     def _merge_kwargs(\n         self,\n         ModelProcessorKwargs: ProcessingKwargs,\n-        tokenizer_init_kwargs: Optional[Dict] = None,\n+        tokenizer_init_kwargs: Optional[dict] = None,\n         **kwargs,\n-    ) -> Dict[str, Dict]:\n+    ) -> dict[str, dict]:\n         \"\"\"\n         Method to merge dictionaries of kwargs cleanly separated by modality within a Processor instance.\n         The order of operations is as follows:\n@@ -1236,10 +1233,10 @@ def __call__(\n \n     def _process_messages_for_chat_template(\n         self,\n-        conversation: List[List[Dict[str, str]]],\n-        batch_images: List[ImageInput],\n-        batch_videos: List[VideoInput],\n-        batch_video_metadata: List[List[Dict[str, any]]],\n+        conversation: list[list[dict[str, str]]],\n+        batch_images: list[ImageInput],\n+        batch_videos: list[VideoInput],\n+        batch_video_metadata: list[list[dict[str, any]]],\n         **chat_template_kwargs: Unpack[AllKwargsForChatTemplate],\n     ):\n         \"\"\"\n@@ -1270,7 +1267,7 @@ def _process_messages_for_chat_template(\n \n     def apply_chat_template(\n         self,\n-        conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]]],\n+        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n         chat_template: Optional[str] = None,\n         **kwargs: Unpack[AllKwargsForChatTemplate],\n     ) -> str:"
        },
        {
            "sha": "e12aad9a0bb985fb733d1708622bab5d8ab28d9d",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -950,11 +950,11 @@ def metrics_format(self, metrics: dict[str, float]) -> dict[str, float]:\n     metrics_copy = metrics.copy()\n     for k, v in metrics_copy.items():\n         if \"_mem_\" in k:\n-            metrics_copy[k] = f\"{ v >> 20 }MB\"\n+            metrics_copy[k] = f\"{v >> 20}MB\"\n         elif \"_runtime\" in k:\n             metrics_copy[k] = _secs2timedelta(v)\n         elif k == \"total_flos\":\n-            metrics_copy[k] = f\"{ int(v) >> 30 }GF\"\n+            metrics_copy[k] = f\"{int(v) >> 30}GF\"\n         elif isinstance(metrics_copy[k], float):\n             metrics_copy[k] = round(v, 4)\n "
        },
        {
            "sha": "5c3616dbed8892f8a0ba2bdf2bfe124e2d5c7c59",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00d077267a7add6a068b1ed9393b31b9e46e4ed3/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=00d077267a7add6a068b1ed9393b31b9e46e4ed3",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \n import contextlib\n-import io\n import json\n import math\n import os\n@@ -22,7 +21,7 @@\n from datetime import timedelta\n from enum import Enum\n from pathlib import Path\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Optional, Union\n \n from huggingface_hub import get_full_repo_name\n from packaging import version\n@@ -1138,7 +1137,7 @@ class TrainingArguments:\n             )\n         },\n     )\n-    debug: Union[str, List[DebugOption]] = field(\n+    debug: Union[str, list[DebugOption]] = field(\n         default=\"\",\n         metadata={\n             \"help\": (\n@@ -1198,7 +1197,7 @@ class TrainingArguments:\n     remove_unused_columns: Optional[bool] = field(\n         default=True, metadata={\"help\": \"Remove columns not required by the model when using an nlp.Dataset.\"}\n     )\n-    label_names: Optional[List[str]] = field(\n+    label_names: Optional[list[str]] = field(\n         default=None, metadata={\"help\": \"The list of keys in your dictionary of inputs that correspond to the labels.\"}\n     )\n     load_best_model_at_end: Optional[bool] = field(\n@@ -1225,7 +1224,7 @@ class TrainingArguments:\n             )\n         },\n     )\n-    fsdp: Optional[Union[List[FSDPOption], str]] = field(\n+    fsdp: Optional[Union[list[FSDPOption], str]] = field(\n         default=\"\",\n         metadata={\n             \"help\": (\n@@ -1318,7 +1317,7 @@ class TrainingArguments:\n         default=\"length\",\n         metadata={\"help\": \"Column name with precomputed lengths to use when grouping by length.\"},\n     )\n-    report_to: Union[None, str, List[str]] = field(\n+    report_to: Union[None, str, list[str]] = field(\n         default=None, metadata={\"help\": \"The list of integrations to report the results and logs to.\"}\n     )\n     ddp_find_unused_parameters: Optional[bool] = field(\n@@ -1406,7 +1405,7 @@ class TrainingArguments:\n             \"help\": \"This argument is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `include_for_metrics` instead.\"\n         },\n     )\n-    include_for_metrics: List[str] = field(\n+    include_for_metrics: list[str] = field(\n         default_factory=list,\n         metadata={\n             \"help\": \"List of strings to specify additional data to include in the `compute_metrics` function.\"\n@@ -1534,7 +1533,7 @@ class TrainingArguments:\n         },\n     )\n \n-    optim_target_modules: Union[None, str, List[str]] = field(\n+    optim_target_modules: Union[None, str, list[str]] = field(\n         default=None,\n         metadata={\n             \"help\": \"Target modules for the optimizer defined in the `optim` argument. Only used for the GaLore optimizer at the moment.\"\n@@ -1940,7 +1939,7 @@ def __post_init__(self):\n         if isinstance(self.fsdp_config, str):\n             if len(self.fsdp) == 0:\n                 warnings.warn(\"`--fsdp_config` is useful only when `--fsdp` is specified.\")\n-            with io.open(self.fsdp_config, \"r\", encoding=\"utf-8\") as f:\n+            with open(self.fsdp_config, encoding=\"utf-8\") as f:\n                 self.fsdp_config = json.load(f)\n                 for k in list(self.fsdp_config.keys()):\n                     if k.startswith(\"fsdp_\"):\n@@ -2546,7 +2545,7 @@ def get_warmup_steps(self, num_training_steps: int):\n         )\n         return warmup_steps\n \n-    def _dict_torch_dtype_to_str(self, d: Dict[str, Any]) -> None:\n+    def _dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n         \"\"\"\n         Checks whether the passed dictionary and its nested dicts have a *torch_dtype* key and if it's not None,\n         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n@@ -2586,7 +2585,7 @@ def to_json_string(self):\n         \"\"\"\n         return json.dumps(self.to_dict(), indent=2)\n \n-    def to_sanitized_dict(self) -> Dict[str, Any]:\n+    def to_sanitized_dict(self) -> dict[str, Any]:\n         \"\"\"\n         Sanitized serialization to use with TensorBoardâ€™s hparams\n         \"\"\"\n@@ -2829,7 +2828,7 @@ def set_logging(\n         self,\n         strategy: Union[str, IntervalStrategy] = \"steps\",\n         steps: int = 500,\n-        report_to: Union[str, List[str]] = \"none\",\n+        report_to: Union[str, list[str]] = \"none\",\n         level: str = \"passive\",\n         first_step: bool = False,\n         nan_inf_filter: bool = False,"
        }
    ],
    "stats": {
        "total": 245,
        "additions": 116,
        "deletions": 129
    }
}