{
    "author": "SunMarc",
    "message": "Fix bnb for the weights refactor (#42043)\n\n* small fix\n\n* nits\n\n* ish\n\n* up\n\n* rev\n\n* fix more tie weights keys\n\n* small fixes\n\n* nit\n\n* update\n\n* fix and fix\n\n* fix a test\n\n* glubs\n\n* current shitty changes\n\n* ship validated ones\n\n* more\n\n* more update\n\n* more\n\n* more\n\n* more\n\n* mllama\n\n* more up\n\n* fix ernie\n\n* fix xopies\n\n* up more\n\n* more fixes\n\n* up\n\n* up\n\n* fix-copies\n\n* fix more\n\n* more updates\n\n* AI UPDATE\n\n* up\n\n* hoey\n\n* make it fast\n\n* fix\n\n* lol\n\n* fix asjusting\n\n* more fixes\n\n* _dtype nit\n\n* up\n\n* nit\n\n* update\n\n* update\n\n* remove semaphores\n\n* fix import to avoid jit execution\n\n* try to remove custom tiing logic when its stupid\n\n* fix more individual models\n\n* fix whisper as well\n\n* fix?\n\n* fox umt5\n\n* improve tqdm bar\n\n* cleanup a bit\n\n* oupsi\n\n* some updates\n\n* improve\n\n* remove all buffering -> much faster without it\n\n* remove some tie_weights custome funcs when not needed\n\n* more fixes related to strict matching regex\n\n* remove ALL custom tie weights\n\n* small update\n\n* revert change to init scheme (no need for params)\n\n* fix\n\n* mixtral init\n\n* try less strict source check\n\n* tied weight first shot to the fiiiixxxxxx\n\n* does this help?\n\n* :)\n\n* fix some ppolry defined tied_weights_keys for now\n\n* fixes for more models torch_bc\n\n* nits and fixes\n\n* last update\n\n* Revert \"tied weight first shot to the fiiiixxxxxx\"\n\nThis reverts commit 3fea865810e4dc832919e0a7f853ca5d3d426c72.\n\n* here we go again\n\n* an attempt\n\n* up?\n\n* nits\n\n* Fix bnb loading !\n\n* rm print\n\n* subclass nn.Parameters\n\n* up\n\n* lol\n\n* Ouiiii\n\n* fix led\n\n* fix long cat flash\n\n* fix qwen and long cat flash\n\n* properly fix qwen init\n\n* just push this for now\n\n* propnet is dumb\n\n* update\n\n* rm import\n\n* update\n\n* push\n\n* Update src/transformers/core_model_loading.py\n\nCo-authored-by: Matthew Douglas <38992547+matthewdouglas@users.noreply.github.com>\n\n* remove explict sharing of some tied keys.\n\n* update decoder.bias\n\n* moe case\n\n* Fix loadedparam\n\n* rm report\n\n* more changes to untangle old hardcoded ting\n\n* fixup\n\n* fix big faileurs\n\n* Fix tests single gpu\n\n* should fix it\n\n* fix prophnet\n\n* fix resize token embeddings\n\n* nits\n\n* fix xcodex\n\n* asyncio?\n\n* fix smart apply\n\n* fix data-2-vec\n\n* [build-ci-image]\n\n* checkout\n\n* uupdate\n\n* fix hunyuan\n\n* update error message\n\n* fix deformable detr\n\n* fixes\n\n* fix init weights for non param gate up projs\n\n* shared todo?\n\n* guard needed for compressed-tensors\n\n* deal with buffers\n\n* update some models\n\n* big revert, don't break this behaviour\n\n* ty @SunMarc this fixes the buffers\n\nCo-authored-by: SunMarc <SunMarc@users.noreply.github.com>\n\n* mt5 fuck\n\n* fix lxmbert\n\n* nuke slow test fetcher\n\n* fix\n\n* fix zamba and deepcopy for now\n\n* fix zamba tied weight keys! ~\n\n* fix-copies\n\n* update fetch terst\n\n* fix gradient for test modeling common!\n\n* break \"shared\" for now I will fix tomorrow changes are properly isoalted now :)\n\n* does this fix marian? probably not\n\n* fix some vlms\n\n* D fine seems to handle this well\n\n* glob is fine actually\n\n* fix dab detr\n\n* small steps\n\n* opusy\n\n* fix some more models?\n\n* yups\n\n* better erro\n\n* fix?\n\n* fix double escape\n\n* escape wehere it makes sense\n\n* ??\n\n* fix ibert\n\n* fix tvp as well\n\n* more fxes\n\n* try always download ref PR\n\n* ONONONO\n\n* big fixup\n\n* more fixup\n\n* small step\n\n* small nits\n\n* nits\n\n* brut force some stuff\n\n* fix vilt\n\n* make sure special models that always need tie always tie\n\n* cleaning up\n\n* small nits\n\n* fix zamba and bridge tower!\n\n* just fixup\n\n* potential culprits\n\n* revert bark and fix bridgetower\n\n* remove now non existant tie_weights\n\n* ?\n\n* lol reformer actually had nothing tied!\n\n* wow these two fucking models were really not well made\n\n* fix sam family!\n\n* fix bark revision\n\n* fix speech2test ?\n\n* push this for now....\n\n* upsy\n\n* the fuck\n\n* fix rtdetr\n\n* update\n\n* proper\n\n* wow that one 's annoying\n\n* update\n\n* try to find the culprit\n\n* get some help on common\n\n* nit about general init and cls.padding_idx\n\n* revert num workers update\n\n* remove old loading func\n\n* fix glob\n\n* add annotations\n\n* fix re\n\n* small improvements\n\n* clean some stuff\n\n* improvements\n\n* someone did not understannnnnnd what I tried to dooo or does BNB not support that either?\n\n* gluos\n\n* fix case when `.` is just not there\n\n* for now let's do this\n\n* fix\n\n* fix small test\n\n* style\n\n* fix merge conflits\n\n* style\n\n* 8bit fixed ?\n\n* fix\n\n* fix 8bit dtype\n\n* fix\n\n* rm copy\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* style\n\n* test\n\n* fix\n\n* finally ?\n\n* Apply style fixes\n\n* fix\n\n* fix\n\n* Apply style fixes\n\n* tie weights\n\n* warning\n\n* Apply style fixes\n\n* init\n\n* default\n\n---------\n\nCo-authored-by: Arthur <arthur.zucker@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\nCo-authored-by: Matthew Douglas <38992547+matthewdouglas@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: SunMarc <SunMarc@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "67302b043ea6af670c1c7859bd763c4784edc7d3",
    "files": [
        {
            "sha": "aeabe61c1a4658ffe7d3d3f4c6f217ac06e99325",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 17,
            "deletions": 36,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -46,22 +46,6 @@\n \n logger = logging.get_logger(__name__)\n \n-str_to_torch_dtype = {\n-    \"BOOL\": torch.bool,\n-    \"U8\": torch.uint8,\n-    \"I8\": torch.int8,\n-    \"I16\": torch.int16,\n-    \"F16\": torch.float16,\n-    \"BF16\": torch.bfloat16,\n-    \"I32\": torch.int32,\n-    \"F32\": torch.float32,\n-    \"F64\": torch.float64,\n-    \"I64\": torch.int64,\n-    \"F8_E4M3\": torch.float8_e4m3fn,\n-    \"F8_E5M2\": torch.float8_e5m2,\n-}\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -389,11 +373,15 @@ def set_param_for_module(\n     missing_keys: MutableSet[str],\n     misc: MutableMapping[str, Any],\n     distributed_operation: Optional[TensorParallelLayer],\n+    hf_quantizer: HfQuantizer,\n ):\n     with log_to_misc(layer_name, misc, layer_name):\n         module_path, _, param_name = layer_name.rpartition(\".\")\n         module_obj = model.get_submodule(module_path) if module_path else model\n-        param_value = param_value[0] if isinstance(param_value, list) else param_value[...]\n+        if isinstance(param_value, list):\n+            param_value = param_value[0]\n+        elif not isinstance(param_value, torch.nn.Parameter):\n+            param_value = param_value[...]\n         ref = getattr(module_obj, param_name)\n \n         use_dtensor = hasattr(distributed_operation, \"use_dtensor\") and distributed_operation.use_dtensor\n@@ -415,7 +403,7 @@ def set_param_for_module(\n \n         # Remove from missing keys (it's either mismatched, or all good)\n         missing_keys.discard(layer_name)\n-        if ref is not None and ref.shape != param_value.shape:\n+        if ref is not None and ref.shape != param_value.shape and hf_quantizer is None:\n             mismatch_keys.add((layer_name, param_value.shape, ref.shape))\n             module_obj.param_name._is_hf_initialized = False  # Needs to be initialized\n         else:\n@@ -434,7 +422,7 @@ def convert_and_load_state_dict_in_model(\n     state_dict: dict[str, Any],\n     weight_mapping: dict[str, WeightConverter] | None,\n     tp_plan: dict[str, str] | None,\n-    quantizer: HfQuantizer | None,\n+    hf_quantizer: HfQuantizer | None,\n     dtype: torch.dtype | None = None,\n     device_map: dict | None = None,\n     dtype_plan: dict | None = None,\n@@ -499,20 +487,14 @@ def convert_and_load_state_dict_in_model(\n                 unexpected_keys.add(t)\n                 continue\n \n-            if quantizer is not None and quantizer.param_needs_quantization(model, t):\n-                if quantizer.__class__.__name__ == \"FineGrainedFP8HfQuantizer\":\n-                    from .integrations.finegrained_fp8 import Fp8Quantize\n-\n-                    converter.quantization_operation = Fp8Quantize()  # TODO support other methods\n-                else:\n-                    raise ValueError(\"This quantization method is gonna be supported SOOOON\")\n-            else:\n-                _dtype = dtype\n-                matched_dtype_pattern = match_glob(t, dtype_policy_alt, dtype_policy_by_group_name)\n-                if matched_dtype_pattern is not None:\n-                    _dtype = dtype_plan[matched_dtype_pattern]\n-                elif empty_param.dtype != _dtype:\n-                    _dtype = empty_param.dtype\n+            if hf_quantizer is not None and hf_quantizer.param_needs_quantization(model, t):\n+                converter.quantization_operation = hf_quantizer.get_quantize_ops()\n+            _dtype = dtype\n+            matched_dtype_pattern = match_glob(t, dtype_policy_alt, dtype_policy_by_group_name)\n+            if matched_dtype_pattern is not None:\n+                _dtype = dtype_plan[matched_dtype_pattern]\n+            elif empty_param.dtype != _dtype:\n+                _dtype = empty_param.dtype\n \n         first_target_key = new_target_key[0]\n         target_key = \"|\".join(new_target_key)\n@@ -575,9 +557,7 @@ def convert_and_load_state_dict_in_model(\n                             if op := converter.quantization_operation:\n                                 with log_to_misc(layer_name, misc, op=op):\n                                     realized_value.update(\n-                                        op.convert(\n-                                            {k: realized_value.pop(k)}, quant_config=quantizer.quantization_config\n-                                        )\n+                                        op.convert({k: realized_value.pop(k)}, model=model, missing_keys=missing_keys)\n                                     )\n \n                         for k, output_value in realized_value.items():\n@@ -591,6 +571,7 @@ def convert_and_load_state_dict_in_model(\n                                 missing_keys,\n                                 misc,\n                                 converter.distributed_operation,\n+                                hf_quantizer,\n                             )\n \n                 except SkipLayer:"
        },
        {
            "sha": "15dd7518150c8bb212aefdaff8c5eb7888e5118a",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -32,8 +32,8 @@\n         \"unpack_weights\",\n     ],\n     \"bitsandbytes\": [\n+        \"Bnb4bitQuantize\",\n         \"dequantize_and_replace\",\n-        \"get_keys_to_not_convert\",\n         \"replace_with_bnb_linear\",\n         \"validate_bnb_backend_availability\",\n     ],\n@@ -177,8 +177,8 @@\n         unpack_weights,\n     )\n     from .bitsandbytes import (\n+        Bnb4bitQuantize,\n         dequantize_and_replace,\n-        get_keys_to_not_convert,\n         replace_with_bnb_linear,\n         validate_bnb_backend_availability,\n     )"
        },
        {
            "sha": "64a98d0c2fb625994526266d562a0b4d46e70136",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 402,
            "deletions": 22,
            "changes": 424,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -19,9 +19,9 @@\n import copy\n import inspect\n import os\n-from collections import defaultdict\n+from collections import OrderedDict, defaultdict\n from contextlib import contextmanager\n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Optional, Union\n \n from ..utils import (\n     is_accelerate_available,\n@@ -39,8 +39,9 @@\n     import torch.nn as nn\n \n if is_accelerate_available():\n-    from accelerate import dispatch_model, infer_auto_device_map\n-    from accelerate.utils import check_tied_parameters_on_same_device, get_max_memory\n+    from accelerate import dispatch_model\n+    from accelerate.utils import get_max_memory\n+    from accelerate.utils.modeling import clean_device_map, get_max_layer_size, get_module_size_with_ties\n \n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n@@ -258,7 +259,7 @@ def check_and_set_device_map(device_map: \"torch.device | int | str | dict | None\n \n \n def compute_module_sizes(\n-    model: \"PreTrainedModel\", hf_quantizer: \"HfQuantizer | None\"\n+    model: \"PreTrainedModel\", hf_quantizer: \"HfQuantizer | None\" = None, buffers_only: bool = False\n ) -> tuple[dict[str, int], dict[str, int]]:\n     \"\"\"\n     Compute the size of each submodule of a given model (in bytes).\n@@ -268,7 +269,13 @@ def compute_module_sizes(\n     \"\"\"\n     all_module_sizes = defaultdict(int)\n     leaves_module_sizes = defaultdict(int)\n-    for name, param in model.state_dict().items():\n+\n+    if buffers_only:\n+        named_tensors = model.named_buffers(recurse=True)\n+    else:\n+        named_tensors = model.state_dict().items()\n+\n+    for name, param in named_tensors:\n         if hf_quantizer is not None:\n             dtype_size = hf_quantizer.param_element_size(model, name)\n         else:\n@@ -283,6 +290,14 @@ def compute_module_sizes(\n     return all_module_sizes, leaves_module_sizes\n \n \n+def compute_module_total_buffer_size(model: nn.Module, hf_quantizer: \"HfQuantizer | None\" = None):\n+    \"\"\"\n+    Compute the total size of buffers in each submodule of a given model.\n+    \"\"\"\n+    module_sizes, _ = compute_module_sizes(model, hf_quantizer, buffers_only=True)\n+    return module_sizes.get(\"\", 0)\n+\n+\n def get_balanced_memory(\n     model: \"PreTrainedModel\",\n     max_memory: dict[int | str, int | str] | None = None,\n@@ -393,20 +408,11 @@ def _get_device_map(\n     device_map: dict | str | None,\n     max_memory: dict | None,\n     hf_quantizer: \"HfQuantizer | None\",\n-    dtype: torch.dtype | None,\n ) -> dict:\n     \"\"\"Compute the final `device_map` to use if we passed a value in ['auto', 'balanced', 'balanced_low_0', 'sequential'].\n     Otherwise, we check for any device inconsistencies in the device_map.\n     \"\"\"\n     if isinstance(device_map, str):\n-        special_dtypes = {}\n-        if hf_quantizer is not None:\n-            special_dtypes = hf_quantizer.get_special_dtypes_update(model, dtype)\n-\n-        target_dtype = dtype\n-        if hf_quantizer is not None:\n-            target_dtype = hf_quantizer.adjust_target_dtype(target_dtype)\n-\n         no_split_modules = model._get_no_split_modules(device_map)\n \n         if device_map != \"sequential\":\n@@ -438,19 +444,13 @@ def _get_device_map(\n         device_map = infer_auto_device_map(\n             model,\n             max_memory=inferred_max_memory,\n-            dtype=target_dtype,\n             no_split_module_classes=no_split_modules,\n-            special_dtypes=special_dtypes,\n+            hf_quantizer=hf_quantizer,\n         )\n \n         if hf_quantizer is not None:\n             hf_quantizer.validate_environment(device_map=device_map)\n \n-    elif device_map is not None:\n-        tied_params = find_tied_parameters(model)\n-        # check if we don't have tied param in different devices\n-        check_tied_parameters_on_same_device(tied_params, device_map)\n-\n     return device_map\n \n \n@@ -547,3 +547,383 @@ def accelerate_disk_offload(\n     else:\n         disk_offload_index = {}\n     return disk_offload_index, disk_only_shard_files, is_offloaded_safetensors\n+\n+\n+def _init_infer_auto_device_map(\n+    model: nn.Module,\n+    max_memory: Optional[dict[Union[int, str], Union[int, str]]] = None,\n+    no_split_module_classes: Optional[list[str]] = None,\n+    tied_parameters: Optional[list[list[str]]] = None,\n+    hf_quantizer: \"HfQuantizer | None\" = None,\n+) -> tuple[\n+    list[Union[int, str]],\n+    dict[Union[int, str], Union[int, str]],\n+    list[Union[int, str]],\n+    list[int],\n+    dict[str, int],\n+    list[list[str]],\n+    list[str],\n+    list[tuple[str, nn.Module]],\n+]:\n+    \"\"\"\n+    Initialize variables required for computing the device map for model allocation.\n+    \"\"\"\n+    max_memory = get_max_memory(max_memory)\n+    if no_split_module_classes is None:\n+        no_split_module_classes = []\n+    elif not isinstance(no_split_module_classes, (list, tuple)):\n+        no_split_module_classes = [no_split_module_classes]\n+\n+    devices = list(max_memory.keys())\n+    if \"disk\" not in devices:\n+        devices.append(\"disk\")\n+    gpus = [device for device in devices if device not in [\"cpu\", \"disk\"]]\n+\n+    # Devices that need to keep space for a potential offloaded layer.\n+    if \"mps\" in gpus:\n+        main_devices = [\"mps\"]\n+    elif len(gpus) > 0:\n+        main_devices = [gpus[0], \"cpu\"]\n+    else:\n+        main_devices = [\"cpu\"]\n+\n+    module_sizes, _ = compute_module_sizes(model, hf_quantizer)\n+\n+    if tied_parameters is None:\n+        if len(model.all_tied_weights_keys) > 0:\n+            # create a list of list of tied params\n+            tied_parameters = [list(t) for t in model.all_tied_weights_keys.items()]\n+        else:\n+            tied_parameters = [[]]\n+\n+    # Direct submodules and parameters\n+    modules_to_treat = (\n+        list(model.named_parameters(recurse=False))\n+        + list(model.named_children())\n+        + list(model.named_buffers(recurse=False))\n+    )\n+\n+    return (\n+        devices,\n+        max_memory,\n+        main_devices,\n+        gpus,\n+        module_sizes,\n+        tied_parameters,\n+        no_split_module_classes,\n+        modules_to_treat,\n+    )\n+\n+\n+def infer_auto_device_map(\n+    model: nn.Module,\n+    max_memory: Optional[dict[Union[int, str], Union[int, str]]] = None,\n+    no_split_module_classes: Optional[list[str]] = None,\n+    verbose: bool = False,\n+    clean_result: bool = True,\n+    offload_buffers: bool = False,\n+    tied_parameters: Optional[list[list[str]]] = None,\n+    hf_quantizer: \"HfQuantizer | None\" = None,\n+):\n+    \"\"\"\n+    Compute a device map for a given model giving priority to GPUs, then offload on CPU and finally offload to disk,\n+    such that:\n+    - we don't exceed the memory available of any of the GPU.\n+    - if offload to the CPU is needed, there is always room left on GPU 0 to put back the layer offloaded on CPU that\n+      has the largest size.\n+    - if offload to the CPU is needed,we don't exceed the RAM available on the CPU.\n+    - if offload to the disk is needed, there is always room left on the CPU to put back the layer offloaded on disk\n+      that has the largest size.\n+\n+    <Tip>\n+\n+    All computation is done analyzing sizes and dtypes of the model parameters. As a result, the model can be on the\n+    meta device (as it would if initialized within the `init_empty_weights` context manager).\n+\n+    </Tip>\n+\n+    Args:\n+        model (`torch.nn.Module`):\n+            The model to analyze.\n+        max_memory (`Dict`, *optional*):\n+            A dictionary device identifier to maximum memory. Will default to the maximum memory available if unset.\n+            Example: `max_memory={0: \"1GB\"}`.\n+        no_split_module_classes (`List[str]`, *optional*):\n+            A list of layer class names that should never be split across device (for instance any layer that has a\n+            residual connection).\n+        verbose (`bool`, *optional*, defaults to `False`):\n+            Whether or not to provide debugging statements as the function builds the device_map.\n+        clean_result (`bool`, *optional*, defaults to `True`):\n+            Clean the resulting device_map by grouping all submodules that go on the same device together.\n+        offload_buffers (`bool`, *optional*, defaults to `False`):\n+            In the layers that are offloaded on the CPU or the hard drive, whether or not to offload the buffers as\n+            well as the parameters.\n+    \"\"\"\n+\n+    # Initialize the variables\n+    (\n+        devices,\n+        max_memory,\n+        main_devices,\n+        gpus,\n+        module_sizes,\n+        tied_parameters,\n+        no_split_module_classes,\n+        modules_to_treat,\n+    ) = _init_infer_auto_device_map(model, max_memory, no_split_module_classes, tied_parameters, hf_quantizer)\n+\n+    device_map = OrderedDict()\n+    current_device = 0\n+    device_memory_used = dict.fromkeys(devices, 0)\n+    device_buffer_sizes = {}\n+    device_minimum_assignment_memory = {}\n+\n+    # Initialize maximum largest layer, to know which space to keep in memory\n+    max_layer_size, max_layer_names = get_max_layer_size(modules_to_treat, module_sizes, no_split_module_classes)\n+\n+    # Ready ? This is going to be a bit messy.\n+    while len(modules_to_treat) > 0:\n+        name, module = modules_to_treat.pop(0)\n+        if verbose:\n+            print(f\"\\nTreating module {name}.\")\n+        # Max size in the remaining layers may have changed since we took one, so we maybe update it.\n+        max_layer_names = [n for n in max_layer_names if n != name and not n.startswith(name + \".\")]\n+        if len(max_layer_names) == 0:\n+            max_layer_size, max_layer_names = get_max_layer_size(\n+                [(n, m) for n, m in modules_to_treat if isinstance(m, torch.nn.Module)],\n+                module_sizes,\n+                no_split_module_classes,\n+            )\n+        # Assess size needed\n+        module_size = module_sizes[name]\n+\n+        # We keep relevant tied parameters only: one of the tied parameters in the group is inside the current module\n+        # and the other is not.\n+        # Note: If we are currently processing the name `compute.weight`, an other parameter named\n+        # e.g. `compute.weight_submodule.parameter`\n+        # needs to be considered outside the current module, hence the check with additional dots.\n+        tied_param_groups = [\n+            tied_group\n+            for tied_group in tied_parameters\n+            if any(name + \".\" in k + \".\" for k in tied_group) and not all(name + \".\" in k + \".\" for k in tied_group)\n+        ]\n+\n+        if verbose and len(tied_param_groups) > 0:\n+            print(f\"  Found the relevant tied param groups {tied_param_groups}\")\n+\n+        # Then we keep track of all the parameters that are tied to the current module, but not in the current module\n+        tied_params = sum(\n+            [[p for p in tied_group if name + \".\" not in p + \".\"] for tied_group in tied_param_groups], []\n+        )\n+\n+        if verbose and len(tied_params) > 0:\n+            print(f\"  So those parameters need to be taken into account {tied_params}\")\n+\n+        device = devices[current_device]\n+        current_max_size = max_memory[device] if device != \"disk\" else None\n+        current_memory_reserved = 0\n+        # Reduce max size available by the largest layer.\n+        if devices[current_device] in main_devices:\n+            current_max_size = current_max_size - max_layer_size\n+            current_memory_reserved = max_layer_size\n+\n+        module_size_with_ties, tied_module_names, tied_modules = get_module_size_with_ties(\n+            tied_params, module_size, module_sizes, modules_to_treat\n+        )\n+\n+        # The module and its tied modules fit on the current device.\n+        if current_max_size is None or device_memory_used[device] + module_size_with_ties <= current_max_size:\n+            if verbose:\n+                output = f\"Putting {name}\"\n+\n+                if tied_module_names:\n+                    output += f\" and {tied_module_names}\"\n+                else:\n+                    output += f\" (size={module_size})\"\n+\n+                if current_max_size is not None:\n+                    output += f\" (available={current_max_size - device_memory_used[device]})\"\n+\n+                output += f\" on {device}.\"\n+                print(output)\n+\n+            device_memory_used[device] += module_size_with_ties\n+\n+            # Assign the primary module to the device.\n+            device_map[name] = device\n+\n+            # Assign tied modules if any.\n+            for tied_module_name in tied_module_names:\n+                if tied_module_name in [m[0] for m in modules_to_treat]:\n+                    # Find the index of the tied module in the list\n+                    tied_module_index = next(i for i, (n, _) in enumerate(modules_to_treat) if n == tied_module_name)\n+                    # Remove the tied module from the list to prevent reprocessing\n+                    modules_to_treat.pop(tied_module_index)\n+\n+                # Assign the tied module to the device\n+                device_map[tied_module_name] = device\n+\n+            # Buffer Handling\n+            if not offload_buffers and isinstance(module, nn.Module):\n+                # Compute the total buffer size for the module\n+                current_buffer_size = compute_module_total_buffer_size(module, hf_quantizer)\n+                # Update the buffer size on the device\n+                device_buffer_sizes[device] = device_buffer_sizes.get(device, 0) + current_buffer_size\n+\n+            continue\n+\n+        # The current module itself fits, so we try to split the tied modules.\n+        if len(tied_params) > 0 and device_memory_used[device] + module_size <= current_max_size:\n+            # can we split one of the tied modules to make it smaller or do we need to go on the next device?\n+            if verbose:\n+                print(\n+                    f\"Not enough space on {devices[current_device]} to put {name} and {tied_module_names} (space \"\n+                    f\"available {current_max_size - device_memory_used[device]}, needed size {module_size_with_ties}).\"\n+                )\n+            split_happened = False\n+            for tied_module_name, tied_module in zip(tied_module_names, tied_modules):\n+                tied_module_children = list(tied_module.named_children())\n+                if len(tied_module_children) == 0 or tied_module.__class__.__name__ in no_split_module_classes:\n+                    # can't break this one.\n+                    continue\n+\n+                if verbose:\n+                    print(f\"Splitting {tied_module_name}.\")\n+                tied_module_children = list(tied_module.named_parameters(recurse=False)) + tied_module_children\n+                tied_module_children = [(f\"{tied_module_name}.{n}\", v) for n, v in tied_module_children]\n+                tied_module_index = [i for i, (n, _) in enumerate(modules_to_treat) if n == tied_module_name][0]\n+\n+                modules_to_treat = (\n+                    [(name, module)]\n+                    + modules_to_treat[:tied_module_index]\n+                    + tied_module_children\n+                    + modules_to_treat[tied_module_index + 1 :]\n+                )\n+                # Update the max layer size.\n+                max_layer_size, max_layer_names = get_max_layer_size(\n+                    [(n, m) for n, m in modules_to_treat if isinstance(m, torch.nn.Module)],\n+                    module_sizes,\n+                    no_split_module_classes,\n+                )\n+                split_happened = True\n+                break\n+\n+            if split_happened:\n+                continue\n+\n+            # If the tied module is not split, we go to the next device\n+            if verbose:\n+                print(\"None of the tied module can be split, going to the next device.\")\n+\n+        # The current module itself doesn't fit, so we have to split it or go to the next device.\n+        if device_memory_used[device] + module_size >= current_max_size:\n+            # Split or not split?\n+            modules_children = (\n+                []\n+                if isinstance(module, nn.Parameter) or isinstance(module, torch.Tensor)\n+                else list(module.named_children())\n+            )\n+            if verbose:\n+                print(\n+                    f\"Not enough space on {devices[current_device]} to put {name} (space available \"\n+                    f\"{current_max_size - device_memory_used[device]}, module size {module_size}).\"\n+                )\n+            if len(modules_children) == 0 or module.__class__.__name__ in no_split_module_classes:\n+                # -> no split, we go to the next device\n+                if verbose:\n+                    print(\"This module cannot be split, going to the next device.\")\n+\n+            else:\n+                # -> split, we replace the module studied by its children + parameters\n+                if verbose:\n+                    print(f\"Splitting {name}.\")\n+                modules_children = list(module.named_parameters(recurse=False)) + modules_children\n+                modules_to_treat = [(f\"{name}.{n}\", v) for n, v in modules_children] + modules_to_treat\n+                # Update the max layer size.\n+                max_layer_size, max_layer_names = get_max_layer_size(\n+                    [(n, m) for n, m in modules_to_treat if isinstance(m, torch.nn.Module)],\n+                    module_sizes,\n+                    no_split_module_classes,\n+                )\n+                continue\n+\n+        if device_memory_used[device] == 0:\n+            device_minimum_assignment_memory[device] = module_size_with_ties + current_memory_reserved\n+\n+        #  Neither the current module nor any tied modules can be split, so we move to the next device.\n+        device_memory_used[device] = device_memory_used[device] + current_memory_reserved\n+        current_device += 1\n+        modules_to_treat = [(name, module)] + modules_to_treat\n+\n+    device_memory_used = {device: mem for device, mem in device_memory_used.items() if mem > 0}\n+\n+    if clean_result:\n+        device_map = clean_device_map(device_map)\n+\n+    non_gpu_buffer_size = device_buffer_sizes.get(\"cpu\", 0) + device_buffer_sizes.get(\"disk\", 0)\n+    if non_gpu_buffer_size > 0 and not offload_buffers:\n+        is_buffer_fit_any_gpu = False\n+        for gpu_device, gpu_max_memory in max_memory.items():\n+            if gpu_device == \"cpu\" or gpu_device == \"disk\":\n+                continue\n+\n+            if not is_buffer_fit_any_gpu:\n+                gpu_memory_used = device_memory_used.get(gpu_device, 0)\n+\n+                if gpu_max_memory >= non_gpu_buffer_size + gpu_memory_used:\n+                    is_buffer_fit_any_gpu = True\n+\n+        if len(gpus) > 0 and not is_buffer_fit_any_gpu:\n+            logger.warning(\n+                f\"Current model requires {non_gpu_buffer_size} bytes of buffer for offloaded layers, which seems does \"\n+                f\"not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using \"\n+                f\"offload_buffers=True.\"\n+            )\n+\n+    if device_minimum_assignment_memory:\n+        devices_info = \"\\n\".join(\n+            f\"  - {device}: {mem} bytes required\" for device, mem in device_minimum_assignment_memory.items()\n+        )\n+        logger.info(\n+            f\"Based on the current allocation process, no modules could be assigned to the following devices due to \"\n+            f\"insufficient memory:\\n\"\n+            f\"{devices_info}\\n\"\n+            f\"These minimum requirements are specific to this allocation attempt and may vary. Consider increasing \"\n+            f\"the available memory for these devices to at least the specified minimum, or adjusting the model config.\"\n+        )\n+\n+    check_tied_parameters_on_same_device(tied_parameters, device_map)\n+    return device_map\n+\n+\n+def _get_param_device(param, device_map):\n+    if param in device_map:\n+        return device_map[param]\n+    parent_param = \".\".join(param.split(\".\")[:-1])\n+    if parent_param == param:\n+        raise ValueError(f\"The `device_map` does not contain the module {param}.\")\n+    else:\n+        return _get_param_device(parent_param, device_map)\n+\n+\n+def check_tied_parameters_on_same_device(tied_params, device_map):\n+    \"\"\"\n+    Check if tied parameters are on the same device\n+\n+    Args:\n+        tied_params (`List[List[str]]`):\n+            A list of lists of parameter names being all tied together.\n+\n+        device_map (`Dict[str, Union[int, str, torch.device]]`):\n+            A map that specifies where each submodule should go.\n+\n+    \"\"\"\n+    for tie_param in tied_params:\n+        tie_param_devices = {}\n+        for param in tie_param:\n+            tie_param_devices[param] = _get_param_device(param, device_map)\n+        if len(set(tie_param_devices.values())) > 1:\n+            logger.warning(\n+                f\"Tied parameters are on different devices: {tie_param_devices}. \"\n+                \"Please modify your custom device map or set `device_map='auto'`. \"\n+            )"
        },
        {
            "sha": "a68b19ff7a1f7427c63577bb46d51f162e4bf356",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 140,
            "deletions": 4,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -1,6 +1,10 @@\n import inspect\n+from collections import defaultdict\n from inspect import signature\n+from typing import Optional\n \n+from ..core_model_loading import ConversionOps\n+from ..quantizers.quantizers_utils import get_module_from_name\n from ..utils import (\n     get_available_devices,\n     is_accelerate_available,\n@@ -27,12 +31,112 @@\n logger = logging.get_logger(__name__)\n \n \n+class Bnb4bitQuantize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self, input_dict: torch.Tensor, model: Optional[torch.nn.Module] = None, missing_keys=None, **kwargs\n+    ) -> dict[str, torch.Tensor]:\n+        \"\"\"\n+        we need to store some parameters to create the quantized weight. For example, bnb requires 6 values that are stored in the checkpoint to recover the quantized weight. So we store them in a dict that it stored in hf_quantizer for now as we can't save it in the op since we create an op per tensor.\n+        \"\"\"\n+        target_key, value = tuple(input_dict.items())[0]\n+        value = value[0] if isinstance(value, list) else value\n+\n+        full_name = target_key\n+        # update param name to get the weights instead of the quantized stats\n+        target_key = self.hf_quantizer.get_param_name(target_key)\n+        module, _ = get_module_from_name(model, target_key)\n+\n+        if not self.hf_quantizer.pre_quantized:\n+            # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n+            # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n+            if issubclass(module.source_cls, Conv1D):\n+                value = value.T\n+            old_value = model.get_parameter_or_buffer(target_key)\n+            new_value = bnb.nn.Params4bit(value, requires_grad=False, **old_value.__dict__).to(value.device)\n+            # remove missing keys that were create when initializing Params4bit\n+            for key in new_value.quant_state.as_dict(packed=True).keys():\n+                missing_keys.discard(f\"{full_name}.{key}\")\n+            return {target_key: new_value}\n+        else:\n+            module_name = target_key.rsplit(\".\", 1)[0]\n+            # Save the states for later quantization when they are all gathered\n+            if not hasattr(self.hf_quantizer, \"param_quant_stats\"):\n+                self.hf_quantizer.param_quant_stats = defaultdict(dict)\n+            self.hf_quantizer.param_quant_stats[module_name].update({full_name: value})\n+            missing_keys.discard(full_name)\n+            # We are ready for quantization in this case (note, the +1 is for the weight itself)\n+            if len(self.hf_quantizer.param_quant_stats[module_name]) == len(self.hf_quantizer.bnb_keys) + 1:\n+                weight = self.hf_quantizer.param_quant_stats[module_name].pop(f\"{module_name}.weight\")\n+                new_value = bnb.nn.Params4bit.from_prequantized(\n+                    data=weight,\n+                    quantized_stats=self.hf_quantizer.param_quant_stats[module_name],\n+                    requires_grad=False,\n+                    device=value.device,\n+                    module=module,\n+                )\n+                del self.hf_quantizer.param_quant_stats[module_name]\n+                return {target_key: new_value}\n+            return {}\n+\n+\n+class Bnb8bitQuantize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self, input_dict: torch.Tensor, model: Optional[torch.nn.Module] = None, missing_keys=None, **kwargs\n+    ) -> dict[str, torch.Tensor]:\n+        target_key, value = tuple(input_dict.items())[0]\n+        value = value[0] if isinstance(value, list) else value\n+\n+        module, tensor_name = get_module_from_name(model, target_key)\n+        module_name = target_key.rsplit(\".\", 1)[0]\n+\n+        if not self.hf_quantizer.pre_quantized:\n+            # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n+            # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n+            if issubclass(module.source_cls, Conv1D):\n+                value = value.T\n+            value_device = value.device\n+            kwargs = getattr(module, tensor_name).__dict__\n+            kwargs.pop(\"SCB\", None)\n+            new_value = bnb.nn.Int8Params(value.to(\"cpu\"), requires_grad=False, **kwargs).to(value_device)\n+            missing_keys.discard(f\"{module_name}.weight_format\")\n+            missing_keys.discard(f\"{module_name}.SCB\")\n+            return {target_key: new_value}\n+        else:\n+            missing_keys.discard(target_key)\n+            # useless key that gets saved for no reason\n+            if tensor_name.endswith(\"weight_format\"):\n+                return {}\n+            # Save the states for later quantization when they are all gathered\n+            if not hasattr(self.hf_quantizer, \"param_quant_stats\"):\n+                self.hf_quantizer.param_quant_stats = defaultdict(dict)\n+            self.hf_quantizer.param_quant_stats[module_name].update({target_key: value})\n+            # We are ready for quantization in this case (SCB and the weight)\n+            if len(self.hf_quantizer.param_quant_stats[module_name]) == 2:\n+                weight = self.hf_quantizer.param_quant_stats[module_name].pop(f\"{module_name}.weight\")\n+                kwargs = getattr(module, \"weight\").__dict__\n+                weight_device = weight.device\n+                new_value = bnb.nn.Int8Params(weight.to(\"cpu\"), requires_grad=False, **kwargs).to(weight_device)\n+                setattr(new_value, \"SCB\", self.hf_quantizer.param_quant_stats[module_name][f\"{module_name}.SCB\"])\n+                del self.hf_quantizer.param_quant_stats[module_name]\n+                # sometimes, weight_format is not saved so we need to remove it from missing keys ...\n+                missing_keys.discard(f\"{module_name}.weight_format\")\n+                return {f\"{module_name}.weight\": new_value}\n+            return {}\n+\n+\n def _replace_with_bnb_linear(\n     model,\n     modules_to_not_convert=None,\n     current_key_name=None,\n     quantization_config=None,\n     has_been_replaced=False,\n+    pre_quantized=False,\n ):\n     \"\"\"\n     Private method that wraps the recursion for module replacement.\n@@ -58,13 +162,18 @@ def _replace_with_bnb_linear(\n                         out_features = module.out_features\n \n                     if quantization_config.quantization_method() == \"llm_int8\":\n-                        model._modules[name] = bnb.nn.Linear8bitLt(\n+                        new_module = bnb.nn.Linear8bitLt(\n                             in_features,\n                             out_features,\n                             module.bias is not None,\n                             has_fp16_weights=quantization_config.llm_int8_has_fp16_weight,\n                             threshold=quantization_config.llm_int8_threshold,\n                         )\n+                        # hack to create the correct keys in the state dict with the right dtype\n+                        new_module.weight.SCB = torch.empty(1, dtype=torch.float32)\n+                        if pre_quantized:\n+                            new_module.weight.data = new_module.weight.data.to(dtype=torch.int8)\n+                        model._modules[name] = new_module\n                         has_been_replaced = True\n                     else:\n                         if (\n@@ -78,7 +187,7 @@ def _replace_with_bnb_linear(\n                                 if \"quant_storage\" in list(signature(bnb.nn.Linear4bit).parameters)\n                                 else {}\n                             )\n-                            model._modules[name] = bnb.nn.Linear4bit(\n+                            new_module = bnb.nn.Linear4bit(\n                                 in_features,\n                                 out_features,\n                                 module.bias is not None,\n@@ -87,6 +196,30 @@ def _replace_with_bnb_linear(\n                                 quant_type=quantization_config.bnb_4bit_quant_type,\n                                 **extra_kwargs,\n                             )\n+                            from bitsandbytes.functional import QuantState\n+\n+                            # hack to create the correct keys in the state dict with the right dtype\n+                            absmax_dtype = (\n+                                torch.uint8 if quantization_config.bnb_4bit_use_double_quant else torch.float32\n+                            )\n+                            new_module.weight.quant_state = QuantState(\n+                                absmax=torch.empty(1, dtype=absmax_dtype),\n+                                code=torch.empty(1, dtype=torch.float32),\n+                                shape=(1,),\n+                                offset=torch.empty(1),\n+                                quant_type=quantization_config.bnb_4bit_quant_type,\n+                                state2=QuantState(\n+                                    absmax=torch.empty(1, dtype=torch.float32),\n+                                    code=torch.empty(1, dtype=torch.float32),\n+                                )\n+                                if quantization_config.bnb_4bit_use_double_quant\n+                                else None,\n+                            )\n+                            if pre_quantized:\n+                                # this is kind of an edge case when supporting both loading and quantization ...\n+                                # we need to set the right dtype as we cast the checkpoint with the dtype of the meta model\n+                                new_module.weight.data = new_module.weight.data.to(dtype=torch.uint8)\n+                            model._modules[name] = new_module\n                             has_been_replaced = True\n                     # Store the module class in case we need to transpose the weight later\n                     model._modules[name].source_cls = type(module)\n@@ -99,13 +232,16 @@ def _replace_with_bnb_linear(\n                 current_key_name,\n                 quantization_config,\n                 has_been_replaced=has_been_replaced,\n+                pre_quantized=pre_quantized,\n             )\n         # Remove the last key for recursion\n         current_key_name.pop(-1)\n     return model, has_been_replaced\n \n \n-def replace_with_bnb_linear(model, modules_to_not_convert=None, current_key_name=None, quantization_config=None):\n+def replace_with_bnb_linear(\n+    model, modules_to_not_convert=None, current_key_name=None, quantization_config=None, pre_quantized=False\n+):\n     \"\"\"\n     A helper function to replace all `torch.nn.Linear` modules by `bnb.nn.Linear8bit` modules from the `bitsandbytes`\n     library. This will enable running your models using mixed int8 precision as described by the paper `LLM.int8():\n@@ -137,7 +273,7 @@ def replace_with_bnb_linear(model, modules_to_not_convert=None, current_key_name\n     \"\"\"\n     modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n     model, has_been_replaced = _replace_with_bnb_linear(\n-        model, modules_to_not_convert, current_key_name, quantization_config\n+        model, modules_to_not_convert, current_key_name, quantization_config, pre_quantized=pre_quantized\n     )\n \n     if not has_been_replaced:"
        },
        {
            "sha": "b964228613e5cb95ebe3f930705a2c6e2998e33d",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 11,
            "deletions": 15,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -549,6 +549,8 @@ def replace_with_fp8_linear(\n     quantization_config=None,\n ):\n     \"\"\"Helper function to replace model layers with FP8 versions.\"\"\"\n+    if modules_to_not_convert is None:\n+        modules_to_not_convert = []\n     modules_to_not_convert += [\"lm_head\"]\n \n     if quantization_config.modules_to_not_convert is not None:\n@@ -570,35 +572,29 @@ def replace_with_fp8_linear(\n     return model\n \n \n-class QuantizationOp(ConversionOps):\n-    \"\"\"Base class for quantization operations.\"\"\"\n-\n-    pass\n-\n-\n-class Fp8Quantize(QuantizationOp):\n+class Fp8Quantize(ConversionOps):\n     \"\"\"\n     A quantization operation that creates two tensors, weight and scale out of a weight.\n     \"\"\"\n \n     reverse_op: type[ConversionOps]\n \n-    def __init__(self, block_size: Optional[tuple[int, int]] = None):\n-        self.block_size = block_size\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n         self.reverse_op = Fp8Dequantize\n \n-    def convert(self, input_dict: torch.Tensor, *, quant_config: dict[str, Any]) -> dict[str, torch.Tensor]:\n+    def convert(self, input_dict: torch.Tensor, **kwargs) -> dict[str, torch.Tensor]:\n         # Unpack single key/value (value may be wrapped in a list)\n         target_keys, value = tuple(input_dict.items())[0]\n         value = value[0] if isinstance(value, list) else value\n \n         # Resolve block size (support dict-like or attr-like quant_config)\n         block_size = None\n-        if quant_config is not None:\n-            if isinstance(quant_config, dict):\n-                block_size = quant_config.get(\"weight_block_size\")\n+        if self.hf_quantizer.quantization_config is not None:\n+            if isinstance(self.hf_quantizer.quantization_config, dict):\n+                block_size = self.hf_quantizer.quantization_config.get(\"weight_block_size\")\n             else:\n-                block_size = getattr(quant_config, \"weight_block_size\", None)\n+                block_size = getattr(self.hf_quantizer.quantization_config, \"weight_block_size\", None)\n         if block_size is None:\n             block_size = (value.shape[-2], value.shape[-1])\n \n@@ -656,7 +652,7 @@ def convert(self, input_dict: torch.Tensor, *, quant_config: dict[str, Any]) ->\n         }\n \n \n-class Fp8Dequantize(QuantizationOp):\n+class Fp8Dequantize(ConversionOps):\n     \"\"\"Inverse operation of :class:`Fp8Quantize`. Takes a pair (weight, scale) and reconstructs the fp32 tensor.\"\"\"\n \n     def __init__(self, block_size: Optional[tuple[int, int]] = None):"
        },
        {
            "sha": "2c041bcaf3980a906fbbbb4d78009fbcc2956250",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -47,7 +47,11 @@\n from . import initialization as init\n from .configuration_utils import PreTrainedConfig\n from .conversion_mapping import get_checkpoint_conversion_mapping\n-from .core_model_loading import WeightConverter, convert_and_load_state_dict_in_model, revert_weight_conversion\n+from .core_model_loading import (\n+    WeightConverter,\n+    convert_and_load_state_dict_in_model,\n+    revert_weight_conversion,\n+)\n from .distributed import DistributedConfig\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig\n@@ -4059,7 +4063,7 @@ def from_pretrained(\n \n         # Prepare the full device map\n         if device_map is not None:\n-            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype)\n+            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer)\n \n         # restore default dtype\n         if dtype_orig is not None:\n@@ -4692,6 +4696,9 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n         try:\n             param = model.get_parameter_or_buffer(param_name)\n         except AttributeError:\n+            # TODO: for now let's skip if we can't find the parameters\n+            if hf_quantizer is not None:\n+                continue\n             raise AttributeError(f\"Parameter {param_name} not found in model\")\n \n         # The dtype of different parameters may be different with composite models or `keep_in_fp32_modules`"
        },
        {
            "sha": "dcf3c24ae2a71333215ab27f7ed50a9cc7da8d7b",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -206,23 +206,6 @@ def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: lis\n     def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n         return unexpected_keys\n \n-    def get_special_dtypes_update(self, model, dtype: \"torch.dtype\") -> dict[str, \"torch.dtype\"]:\n-        \"\"\"\n-        returns dtypes for modules that are not quantized - used for the computation of the device_map in case\n-        one passes a str as a device_map. The method will use the `modules_to_not_convert` that is modified\n-        in `_process_model_before_weight_loading`.\n-\n-        Args:\n-            model (`~transformers.PreTrainedModel`):\n-                The model to quantize\n-            dtype (`torch.dtype`):\n-                The dtype passed in `from_pretrained` method.\n-        \"\"\"\n-\n-        return {\n-            name: dtype for name, _ in model.named_parameters() if any(m in name for m in self.modules_to_not_convert)\n-        }\n-\n     def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n         \"\"\"adjust max_memory argument for infer_auto_device_map() if extra memory is needed for quantization\"\"\"\n         return max_memory\n@@ -418,6 +401,11 @@ def _convert_model_for_quantization(self, model):\n                         model.config.get_text_config()\n                     )\n \n+    def get_quantize_ops(self):\n+        raise NotImplementedError(\n+            f\"{self.quantization_config.quant_method} is not available yet and will be supported soon.\"\n+        )\n+\n \n class SequentialLlama4TextExperts(ModuleList):\n     \"\"\""
        },
        {
            "sha": "cf8ddc368b1679c0fdb8a2ec2053c17e7c1d4230",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 17,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from collections import defaultdict\n-from functools import cached_property\n from typing import TYPE_CHECKING, Optional, Union\n \n from .base import HfQuantizer\n@@ -172,17 +171,13 @@ def create_quantized_param(\n \n             # We are ready for quantization in this case (note, the +1 is for the weight itself)\n             if len(self.param_quant_stats[module_name]) == len(self.bnb_keys) + 1:\n-                param_kwargs = {}\n-                if self.is_bnb_supports_quant_storage_module:\n-                    param_kwargs[\"module\"] = module\n-\n                 weight = self.param_quant_stats[module_name].pop(f\"{module_name}.weight\")\n                 new_value = bnb.nn.Params4bit.from_prequantized(\n                     data=weight,\n                     quantized_stats=self.param_quant_stats[module_name],\n                     requires_grad=False,\n                     device=target_device,\n-                    **param_kwargs,\n+                    module=module,\n                 )\n                 # Set it\n                 module._parameters[tensor_name] = new_value\n@@ -269,9 +264,11 @@ def _process_model_before_weight_loading(\n                     \" converted to 8-bit but kept in 32-bit.\"\n                 )\n             self.modules_to_not_convert.extend(keys_on_cpu)\n-\n         model = replace_with_bnb_linear(\n-            model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n+            model,\n+            modules_to_not_convert=self.modules_to_not_convert,\n+            quantization_config=self.quantization_config,\n+            pre_quantized=self.pre_quantized,\n         )\n \n         model.config.quantization_config = self.quantization_config\n@@ -285,15 +282,6 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n     def is_serializable(self, safe_serialization=None):\n         return True\n \n-    @cached_property\n-    def is_bnb_supports_quant_storage_module(self) -> bool:\n-        \"\"\"\n-        determines if the current version of bitsandbytes supports\n-        the `module` parameter in `Params4bit.from_prequantized`\n-        :return:\n-        \"\"\"\n-        return True\n-\n     @property\n     def is_trainable(self) -> bool:\n         return True\n@@ -305,3 +293,8 @@ def _dequantize(self, model):\n             model, self.modules_to_not_convert, quantization_config=self.quantization_config\n         )\n         return model\n+\n+    def get_quantize_ops(self):\n+        from ..integrations.bitsandbytes import Bnb4bitQuantize\n+\n+        return Bnb4bitQuantize(self)"
        },
        {
            "sha": "689c7c65ba339a3e33a96d94480e8933872d11f2",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -219,7 +219,10 @@ def _process_model_before_weight_loading(\n             self.modules_to_not_convert.extend(keys_on_cpu)\n \n         model = replace_with_bnb_linear(\n-            model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n+            model,\n+            modules_to_not_convert=self.modules_to_not_convert,\n+            quantization_config=self.quantization_config,\n+            pre_quantized=self.pre_quantized,\n         )\n \n         model.config.quantization_config = self.quantization_config\n@@ -238,3 +241,8 @@ def _dequantize(self, model):\n             model, self.modules_to_not_convert, quantization_config=self.quantization_config\n         )\n         return model\n+\n+    def get_quantize_ops(self):\n+        from ..integrations.bitsandbytes import Bnb8bitQuantize\n+\n+        return Bnb8bitQuantize(self)"
        },
        {
            "sha": "8ab80165c5ece6e4a8a172f206c9abd349eaf408",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -226,3 +226,8 @@ def is_trainable(self) -> bool:\n     def get_accelerator_warm_up_factor(self):\n         # Pre-processing is done cleanly, so we can allocate everything here\n         return 2\n+\n+    def get_quantize_ops(self):\n+        from ..integrations.finegrained_fp8 import Fp8Quantize\n+\n+        return Fp8Quantize(self)"
        },
        {
            "sha": "4b85f0383420b1825eb4ec97bba1681953534b31",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 37,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -202,22 +202,6 @@ def test_linear_are_4bit(self):\n                     # 4-bit parameters are packed in uint8 variables\n                     self.assertTrue(module.weight.dtype == torch.uint8)\n \n-    def test_rwkv_4bit(self):\n-        r\"\"\"\n-        A simple test to check if 4-bit RWKV inference works as expected.\n-        \"\"\"\n-        model_id = \"RWKV/rwkv-4-169m-pile\"\n-\n-        quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True)\n-\n-        model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n-        tok = AutoTokenizer.from_pretrained(model_id)\n-\n-        text = \"Hello my name is\"\n-        input_ids = tok.encode(text, return_tensors=\"pt\").to(torch_device)\n-\n-        _ = model.generate(input_ids, max_new_tokens=30)\n-\n     def test_generate_quality(self):\n         r\"\"\"\n         Test the generation quality of the quantized model and see that we are matching the expected output.\n@@ -607,7 +591,7 @@ def setUp(self):\n     def test_training(self):\n         # Step 1: freeze all parameters\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), revision=\"refs/pr/40\"\n         )\n \n         if torch_device in [\"cuda\", \"xpu\"]:\n@@ -671,7 +655,7 @@ def tearDown(self):\n         gc.collect()\n         backend_empty_cache(torch_device)\n \n-    def test_serialization(self, quant_type=\"nf4\", double_quant=True, safe_serialization=True):\n+    def test_serialization(self, quant_type=\"nf4\", double_quant=True):\n         r\"\"\"\n         Test whether it is possible to serialize a model in 4-bit. Uses most typical params as default.\n         See ExtendedSerializationTest class for more params combinations.\n@@ -685,14 +669,19 @@ def test_serialization(self, quant_type=\"nf4\", double_quant=True, safe_serializa\n             bnb_4bit_use_double_quant=double_quant,\n             bnb_4bit_compute_dtype=torch.bfloat16,\n         )\n+\n+        # for now, we should be able to fetch those in from_pretrained directly\n+        if self.model_name == \"facebook/opt-125m\":\n+            revision = \"refs/pr/49\"\n+        else:\n+            revision = \"main\"\n+\n         model_0 = AutoModelForCausalLM.from_pretrained(\n-            self.model_name,\n-            quantization_config=self.quantization_config,\n-            device_map=torch_device,\n+            self.model_name, quantization_config=self.quantization_config, device_map=torch_device, revision=revision\n         )\n \n         with tempfile.TemporaryDirectory() as tmpdirname:\n-            model_0.save_pretrained(tmpdirname, safe_serialization=safe_serialization)\n+            model_0.save_pretrained(tmpdirname)\n \n             config = AutoConfig.from_pretrained(tmpdirname)\n             self.assertTrue(hasattr(config, \"quantization_config\"))\n@@ -758,28 +747,16 @@ class ExtendedSerializationTest(BaseSerializationTest):\n     tests more combinations of parameters\n     \"\"\"\n \n-    def test_nf4_single_unsafe(self):\n-        self.test_serialization(quant_type=\"nf4\", double_quant=False, safe_serialization=False)\n-\n     def test_nf4_single_safe(self):\n-        self.test_serialization(quant_type=\"nf4\", double_quant=False, safe_serialization=True)\n-\n-    def test_nf4_double_unsafe(self):\n-        self.test_serialization(quant_type=\"nf4\", double_quant=True, safe_serialization=False)\n+        self.test_serialization(quant_type=\"nf4\", double_quant=False)\n \n     # nf4 double safetensors quantization is tested in test_serialization() method from the parent class\n \n-    def test_fp4_single_unsafe(self):\n-        self.test_serialization(quant_type=\"fp4\", double_quant=False, safe_serialization=False)\n-\n     def test_fp4_single_safe(self):\n-        self.test_serialization(quant_type=\"fp4\", double_quant=False, safe_serialization=True)\n-\n-    def test_fp4_double_unsafe(self):\n-        self.test_serialization(quant_type=\"fp4\", double_quant=True, safe_serialization=False)\n+        self.test_serialization(quant_type=\"fp4\", double_quant=False)\n \n     def test_fp4_double_safe(self):\n-        self.test_serialization(quant_type=\"fp4\", double_quant=True, safe_serialization=True)\n+        self.test_serialization(quant_type=\"fp4\", double_quant=True)\n \n \n class BloomSerializationTest(BaseSerializationTest):"
        },
        {
            "sha": "964e0a41dd9faa0a24d169ee4de19eb5dc46cf8a",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 30,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -148,7 +148,7 @@ def test_get_keys_to_not_convert(self):\n         from accelerate import init_empty_weights\n \n         from transformers import AutoModelForMaskedLM, Blip2ForConditionalGeneration, MptForCausalLM, OPTForCausalLM\n-        from transformers.integrations.bitsandbytes import get_keys_to_not_convert\n+        from transformers.quantizers.base import get_keys_to_not_convert\n \n         model_id = \"mosaicml/mpt-7b\"\n         config = AutoConfig.from_pretrained(model_id, revision=\"72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7\")\n@@ -381,35 +381,6 @@ def test_int8_serialization(self):\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n \n-    def test_int8_serialization_regression(self):\n-        r\"\"\"\n-        Test whether it is possible to serialize a model in 8-bit - using not safetensors\n-        \"\"\"\n-        from bitsandbytes.nn import Int8Params\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            self.model_8bit.save_pretrained(tmpdirname, safe_serialization=False)\n-\n-            # check that the file `quantization_config` is present\n-            config = AutoConfig.from_pretrained(tmpdirname)\n-            self.assertTrue(hasattr(config, \"quantization_config\"))\n-\n-            model_from_saved = AutoModelForCausalLM.from_pretrained(\n-                tmpdirname, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n-            )\n-\n-            linear = get_some_linear_layer(model_from_saved)\n-            self.assertTrue(linear.weight.__class__ == Int8Params)\n-            self.assertTrue(hasattr(linear.weight, \"SCB\"))\n-\n-            # generate\n-            encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n-            output_sequences = model_from_saved.generate(\n-                input_ids=encoded_input[\"input_ids\"].to(model_from_saved.device), max_new_tokens=10\n-            )\n-\n-        self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n-\n     def test_int8_serialization_sharded(self):\n         r\"\"\"\n         Test whether it is possible to serialize a model in 8-bit - sharded version."
        },
        {
            "sha": "ac02aad2c60821721319aff4a75ecb016bc19026",
            "filename": "tests/utils/test_core_model_loading.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/67302b043ea6af670c1c7859bd763c4784edc7d3/tests%2Futils%2Ftest_core_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67302b043ea6af670c1c7859bd763c4784edc7d3/tests%2Futils%2Ftest_core_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_core_model_loading.py?ref=67302b043ea6af670c1c7859bd763c4784edc7d3",
            "patch": "@@ -218,7 +218,7 @@ def test_moe_and_qkv_conversion(self):\n             WeightConverter(\"mlp.w2.weight\", \"mlp.down_proj.weight\"),\n         ]\n         missing, unexpected, mismatch, misc = convert_and_load_state_dict_in_model(\n-            model, state_dict, weight_mapping, tp_plan=None, quantizer=None\n+            model, state_dict, weight_mapping, tp_plan=None, hf_quantizer=None\n         )\n \n         self.assertEqual(\n@@ -368,7 +368,7 @@ def __init__(self):\n         ]\n \n         missing, unexpected, mismatch, misc = convert_and_load_state_dict_in_model(\n-            model, state_dict, weight_mapping, tp_plan=None, quantizer=quantizer\n+            model, state_dict, weight_mapping, tp_plan=None, hf_quantizer=quantizer\n         )\n \n         self.assertEqual(missing, set())"
        }
    ],
    "stats": {
        "total": 812,
        "additions": 627,
        "deletions": 185
    }
}