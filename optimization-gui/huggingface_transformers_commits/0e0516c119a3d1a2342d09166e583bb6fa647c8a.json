{
    "author": "KoichiYasuoka",
    "message": "MODERNBERT_INPUTS_DOCSTRING: past_key_values are ignored (#35513)\n\n* MODERNBERT_INPUTS_DOCSTRING: past_key_values are ignored\r\n\r\n* sync to modular_modernbert.py",
    "sha": "0e0516c119a3d1a2342d09166e583bb6fa647c8a",
    "files": [
        {
            "sha": "9e4677c1fdd099248a63305f120f819f38b8d7b6",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e0516c119a3d1a2342d09166e583bb6fa647c8a/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e0516c119a3d1a2342d09166e583bb6fa647c8a/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=0e0516c119a3d1a2342d09166e583bb6fa647c8a",
            "patch": "@@ -776,9 +776,6 @@ def _pad_modernbert_output(\n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n             [`PreTrainedTokenizer.__call__`] for details.\n \n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n             If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n             and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n             information on the default strategy."
        },
        {
            "sha": "b747940f3b0623778d1e42043afadfba90eb38f4",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e0516c119a3d1a2342d09166e583bb6fa647c8a/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e0516c119a3d1a2342d09166e583bb6fa647c8a/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=0e0516c119a3d1a2342d09166e583bb6fa647c8a",
            "patch": "@@ -929,9 +929,6 @@ def resize_token_embeddings(self, *args, **kwargs):\n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n             [`PreTrainedTokenizer.__call__`] for details.\n \n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n             If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n             and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n             information on the default strategy."
        }
    ],
    "stats": {
        "total": 6,
        "additions": 0,
        "deletions": 6
    }
}