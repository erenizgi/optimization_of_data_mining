{
    "author": "duanjunwen",
    "message": "[Feat] Support npu in modeling models (#37369)",
    "sha": "7ff896c0f22227ae999f991cff500e70452d5dcc",
    "files": [
        {
            "sha": "d64e2746d492a7125a7c0e74f497aae56481579f",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1040,7 +1040,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "0cf23edb7510942932425f818006e9e7b6799e9c",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1331,7 +1331,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "5aa4c8fb405059fbc6682d53b2c4db69743d4dc8",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1098,7 +1098,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "48f89810bd0d62c7fec71ab04429007055fb27db",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -799,7 +799,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "4d950ec668f5e49257590af641745d001b7a9e61",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1441,7 +1441,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "4f06094567ab7636b74e892b2cf50b0aa09053b9",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -645,7 +645,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "fd888c38d7fddf890b52c2848828f717b2a36026",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -689,7 +689,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "9f89398685028ce479c30bc7ac95ec71032fc5a2",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1171,7 +1171,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "c1a020b7c76a06a886790b23f4bdea16825a88d4",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -834,7 +834,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "ed536cbebaf3c779d3842f3a35a61d4ba4403099",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -931,7 +931,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "4646b9f9bdee5c615199c35afadd1d14d0ca1cb4",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1518,7 +1518,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "82a4dfee0871ca8c53d3bf49c98d3afa00c1836c",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1060,7 +1060,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "679bc08698556d1da3ef590f7589b52ea358e881",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -656,7 +656,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "2a41815d7444d6e7eccf644c8358a31c745d5198",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -670,7 +670,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "857735838f0eef20049e766a157f94c551ac4ae2",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -926,7 +926,11 @@ def forward(\n                     # [batch_size, target_length, 1, source_length], not compatible with SDPA, hence this transpose.\n                     self_attention_mask = self_attention_mask.transpose(1, 2)\n \n-                if query_length > 1 and attention_mask is not None and attention_mask.device.type in [\"cuda\", \"xpu\"]:\n+                if (\n+                    query_length > 1\n+                    and attention_mask is not None\n+                    and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+                ):\n                     # From PyTorch 2.1 onwards, F.scaled_dot_product_attention with the memory-efficient attention backend\n                     # produces nans if sequences are completely unattended in the attention mask. Details: https://github.com/pytorch/pytorch/issues/110213\n                     self_attention_mask = AttentionMaskConverter._unmask_unattended("
        },
        {
            "sha": "9151590b9263920be98bb12867634a5b00d38d0d",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -848,7 +848,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "5a0bc1af2f89136f4783f24659cf590272b3eabc",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -657,7 +657,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "e08624bf813d6085d9708abb9cffa6f8ba296899",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -694,7 +694,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "adfe6c584b21d11928100d29a5c91eb850d15c6e",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -947,7 +947,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "bb06757fb83dc9ad564777c80c2c136c1f00ff62",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -671,7 +671,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "9ea68ab0aa1c5ccfb1df51c5e09eae8ea4fc991b",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1147,7 +1147,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "886bed096856a92f4167bab6cec39f682addf799",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1092,7 +1092,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "2597ce27fa94adfe958f979774c2739238afd4fd",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -657,7 +657,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "fb5006d2e2264fdd4be6883999849f439265a7dd",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1423,7 +1423,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "abf4ad4b3dc008a46a95dc0fd49f7016104cdb22",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1372,7 +1372,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path."
        },
        {
            "sha": "70cb8126dcabbef0911978f095af85f52fedd94b",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1151,7 +1151,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "e8dd13952661a86f04792b983207bf8921ae8655",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -661,7 +661,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "4363001f4d28a9a9e0f9eac73c0adb9e9bd2bc29",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -810,7 +810,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and attention_mask.ndim == 4\n             and not output_attentions  # Only unmask for 4d masks\n         ):"
        },
        {
            "sha": "da8bdca3e74019d128e08ddbfb42790da1888fe5",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1657,7 +1657,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "f79fea53c16e314a61573d3eac103fb102d7156a",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1105,7 +1105,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "b235c7441339cec1833f340f22178fdce6b5f67f",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -638,7 +638,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "9aae2e5505484e5d336220f29942a0275f5f620d",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -186,7 +186,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "d6da7b1d780555d6e91b634d54476ed968dd1224",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -766,7 +766,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "64329c2abd09f078cf3aa9aabd78c3f3ac5be0ca",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1108,7 +1108,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "c9e6eb756c07fda26070dd000c0f6ee72d637523",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1014,7 +1014,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "d77d32edf756d2e2e8d5d69fa5cdf2bdf4657b12",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1335,7 +1335,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n@@ -1649,7 +1649,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "0199c7d779733058be69fba34c85f1def07dfd73",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1248,7 +1248,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "31d14fb5da24ed124bc97f59db76b8c9067b8118",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -906,7 +906,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "dc09278fe2ab634eaf744ff2c9befdfa13fbcdd6",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -634,7 +634,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "bd0a47eaf16a7b901f13e7b5df3c9da8c28f5696",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -637,7 +637,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "ce38e57cfda1defe19b61da657551b3cc19ed62d",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1056,7 +1056,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "5a1f15f94cdbb6e45211f02944018bd5901cc3b4",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -695,7 +695,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "029e376eafd06dcb1e1e406dcf16bba8fce782c2",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -706,7 +706,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "664cb571adde8734fd89c567a325efadba527f33",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -627,7 +627,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "e777e5a28b05a554affe8c14b3d6bae99f970be9",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -692,7 +692,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "112dca6e6d863a1db1c1cc85bfbde0d0934b31ae",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1986,7 +1986,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "7e94c27cf9b6bf01aaef9802dd4037c2c67fd75e",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1240,7 +1240,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "da65dbc3697911d60c37269254d664f6b41ed9d6",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1644,7 +1644,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "18f6659dc0acbc351e7119f7d2d00b3e81f2ee1f",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1057,7 +1057,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "c3d735e8c4373e4b3ba6f00dc90d765e2b2bc977",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -651,7 +651,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "ee963a7e278c3e0e3bf0e2ab194a8b7211a94178",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1277,7 +1277,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "42fbdd9c57910eafc9035468dbea498071c06c46",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1101,7 +1101,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "e4593a2b4fb8137090c940299b0bf1a0f35156cf",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1231,7 +1231,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "ea04ded17b4135ec636a58b2c3cbb973679b5dd3",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -678,7 +678,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "2052fc39f8d3869892f587d4ec97668b6dd7fe97",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -780,7 +780,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "3bf5b43706ca1e9222ede0279e264b6ad5877f63",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -764,7 +764,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n                 padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n                 causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)\n \n-        if attention_mask is not None and attention_mask.device.type in [\"cuda\", \"xpu\"]:\n+        if attention_mask is not None and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]:\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213"
        },
        {
            "sha": "69d2ff3c0dcc82ae1d53154559d338f17e4c86c9",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -960,7 +960,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "3d7892656161f70276d4b27d924661d43195cbe5",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -626,7 +626,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "4a2696a61097aefc6a42546e55146560b1532f96",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1191,7 +1191,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "8e3cd8d965f381a74138c09052bd6601ca50457e",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1262,7 +1262,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "5a3ce836d44d9f2eb929efd1676e33034e21ddf3",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1594,7 +1594,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "37015121cf2d8e7ee55a2b8f1bccdff9b972bffc",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -905,7 +905,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "dec1e03937623f264cfe5272198f8e995e6678c1",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1428,7 +1428,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "27d75129e87b7af2fe22cdb1425f42a9a5f2ddb9",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1167,7 +1167,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path."
        },
        {
            "sha": "843420d7db185423740f3760e321ea821c48ca6f",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ff896c0f22227ae999f991cff500e70452d5dcc/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=7ff896c0f22227ae999f991cff500e70452d5dcc",
            "patch": "@@ -1529,7 +1529,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path."
        }
    ],
    "stats": {
        "total": 136,
        "additions": 70,
        "deletions": 66
    }
}