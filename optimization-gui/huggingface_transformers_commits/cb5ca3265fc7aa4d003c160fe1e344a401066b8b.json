{
    "author": "VladOS95-cyber",
    "message": "Add GGUF for starcoder2 (#34094)\n\n* add starcoder2 arch support for gguf\r\n\r\n* fix q6 test",
    "sha": "cb5ca3265fc7aa4d003c160fe1e344a401066b8b",
    "files": [
        {
            "sha": "01583cedbf4110eba25a8caffecd155bc91df3a8",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb5ca3265fc7aa4d003c160fe1e344a401066b8b/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb5ca3265fc7aa4d003c160fe1e344a401066b8b/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=cb5ca3265fc7aa4d003c160fe1e344a401066b8b",
            "patch": "@@ -84,6 +84,7 @@ For now the supported model architectures are the architectures that have been v\n - Falcon\n - StableLM\n - GPT2\n+- Starcoder2\n \n ## Example usage\n "
        },
        {
            "sha": "7b5828176ffcf43930c7835b546b894b0acd8ad2",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb5ca3265fc7aa4d003c160fe1e344a401066b8b/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb5ca3265fc7aa4d003c160fe1e344a401066b8b/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=cb5ca3265fc7aa4d003c160fe1e344a401066b8b",
            "patch": "@@ -176,6 +176,20 @@\n         \"ffn_up\": \"mlp.c_fc\",\n         \"ffn_down\": \"mlp.c_proj\",\n     },\n+    \"starcoder2\": {\n+        \"token_embd\": \"model.embed_tokens\",\n+        \"blk\": \"model.layers\",\n+        \"ffn_up\": \"mlp.c_fc\",\n+        \"ffn_down\": \"mlp.c_proj\",\n+        \"ffn_norm\": \"post_attention_layernorm\",\n+        \"attn_norm\": \"input_layernorm\",\n+        \"attn_q\": \"self_attn.q_proj\",\n+        \"attn_v\": \"self_attn.v_proj\",\n+        \"attn_k\": \"self_attn.k_proj\",\n+        \"attn_output\": \"self_attn.o_proj\",\n+        \"output.weight\": \"lm_head.weight\",\n+        \"output_norm\": \"model.norm\",\n+    },\n }\n \n \n@@ -292,6 +306,15 @@\n         \"attention.head_count\": \"n_head\",\n         \"attention.layer_norm_epsilon\": \"layer_norm_epsilon\",\n     },\n+    \"starcoder2\": {\n+        \"block_count\": \"num_hidden_layers\",\n+        \"context_length\": \"max_position_embeddings\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_epsilon\": \"norm_epsilon\",\n+    },\n }\n \n GGUF_TOKENIZER_MAPPING = {\n@@ -622,6 +645,7 @@ def converted(self) -> Tokenizer:\n     \"falcon\": GGUFGPTConverter,\n     \"stablelm\": GGUFGPTConverter,\n     \"gpt2\": GGUFGPTConverter,\n+    \"starcoder2\": GGUFGPTConverter,\n }\n \n "
        },
        {
            "sha": "6e47d46f07c47ebeef8ba42e3548a0dceefbbb6f",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb5ca3265fc7aa4d003c160fe1e344a401066b8b/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb5ca3265fc7aa4d003c160fe1e344a401066b8b/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=cb5ca3265fc7aa4d003c160fe1e344a401066b8b",
            "patch": "@@ -54,6 +54,9 @@ class GgufIntegrationTests(unittest.TestCase):\n     gpt2_model_id = \"mradermacher/gpt2-GGUF\"\n     gpt2_original_model_id = \"openai-community/gpt2\"\n     gpt2_xl_model_id = \"RichardErkhov/openai-community_-_gpt2-xl-gguf\"\n+    starcoder2_model_id = \"QuantFactory/starcoder2-3b-GGUF\"\n+    starcoder2_fp16_model_id = \"brittlewis12/starcoder2-3b-GGUF\"\n+    starcoder2_original_model_id = \"bigcode/starcoder2-3b\"\n \n     # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n@@ -93,6 +96,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     fp16_gpt2_model_id = \"gpt2.f16.gguf\"\n     q8_gpt2_model_id = \"gpt2.Q8_0.gguf\"\n     q6_k_gpt2_xl_model_id = \"gpt2-xl.Q6_K.gguf\"\n+    q6_k_starcoder2_model_id = \"starcoder2-3b.Q6_K.gguf\"\n+    fp16_starcoder2_gguf_model_id = \"starcoder2-3b.fp16.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -650,6 +655,45 @@ def test_stablelm_weights_conversion_fp16(self):\n                 self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n                 torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n \n+    def test_starcoder2_weights_conversion_fp16(self):\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.starcoder2_original_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_model = AutoModelForCausalLM.from_pretrained(\n+            self.starcoder2_fp16_model_id,\n+            gguf_file=self.fp16_starcoder2_gguf_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_state_dict = converted_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in converted_state_dict and layer_name != \"lm_head.weight\":\n+                # quantized models do not contain \"lm_head.weight\" layer\n+                self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+\n+    def test_starcoder2_q6_k(self):\n+        example_function_text = \"def print_hello_world():\"\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.starcoder2_model_id,\n+            gguf_file=self.q6_k_starcoder2_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.starcoder2_model_id, gguf_file=self.q6_k_starcoder2_model_id)\n+        text = tokenizer(example_function_text, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = 'def print_hello_world():\\n    print(\"Hello World\")\\n\\ndef print'\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n     def test_tokenization_xnli(self):\n         import tqdm\n         from datasets import load_dataset"
        }
    ],
    "stats": {
        "total": 69,
        "additions": 69,
        "deletions": 0
    }
}