{
    "author": "Kuangdd01",
    "message": "Gemma3 processor typo (#36710)\n\n* fix typo when  is on\n\n* tiny\n\n* add test and remove 'text_crops'\n\n* lint",
    "sha": "53742b11f51eb03277550dd5f1730c17c0870507",
    "files": [
        {
            "sha": "1b41d702bd6948b7d834b83546313faecc66ce9e",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/53742b11f51eb03277550dd5f1730c17c0870507/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53742b11f51eb03277550dd5f1730c17c0870507/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=53742b11f51eb03277550dd5f1730c17c0870507",
            "patch": "@@ -384,7 +384,7 @@ def preprocess(\n             images_list = [images for images, _ in images_list_and_num_crops]\n             num_crops = [num_crops for _, num_crops in images_list_and_num_crops]\n         else:\n-            num_crops = [[0] for images in images_list]\n+            num_crops = [[0] for _ in images_list]\n \n         processed_images = []\n         for images in images_list:"
        },
        {
            "sha": "00cece56a2694354ceb473b893b65af09bb4f686",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/53742b11f51eb03277550dd5f1730c17c0870507/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53742b11f51eb03277550dd5f1730c17c0870507/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=53742b11f51eb03277550dd5f1730c17c0870507",
            "patch": "@@ -113,7 +113,6 @@ def __call__(\n \n             # Replace image tokens by the full expanded sequence\n             batch_num_crops = to_py_obj(image_inputs.pop(\"num_crops\"))\n-            text_with_crops = text\n             for batch_idx, (prompt, images, num_crops) in enumerate(zip(text, batched_images, batch_num_crops)):\n                 image_indexes = [m.start() for m in re.finditer(self.boi_token, prompt)]\n \n@@ -130,7 +129,7 @@ def __call__(\n                             + \" \".join([self.boi_token] * num)\n                         )\n                         prompt = prompt[:idx] + formatted_image_text + prompt[idx + len(self.boi_token) :]\n-                        text_with_crops[batch_idx] = prompt\n+                        text[batch_idx] = prompt\n \n             # Expand placeholder image tokens to the full image token sequence\n             text = [prompt.replace(self.boi_token, self.full_image_sequence) for prompt in text]"
        },
        {
            "sha": "f9b7ad10038d93e037c80639d666666f58410ef8",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/53742b11f51eb03277550dd5f1730c17c0870507/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53742b11f51eb03277550dd5f1730c17c0870507/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=53742b11f51eb03277550dd5f1730c17c0870507",
            "patch": "@@ -417,6 +417,39 @@ def test_model_4b_batch(self):\n         ]  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n+    def test_model_4b_crops(self):\n+        model_id = \"gg-hf-g/gemma-3-4b-it\"\n+\n+        model = Gemma3ForConditionalGeneration.from_pretrained(\n+            model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n+        ).to(torch_device)\n+\n+        crop_config = {\n+            \"images_kwargs\": {\n+                \"do_pan_and_scan\": True,\n+                \"pan_and_scan_max_num_crops\": 448,\n+                \"pan_and_scan_min_crop_size\": 32,\n+                \"pan_and_scan_min_ratio_to_activate\": 0.3,\n+            }\n+        }\n+\n+        inputs = self.processor.apply_chat_template(\n+            self.messages,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            add_generation_prompt=True,\n+            **crop_config,\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n+        EXPECTED_TEXTS = [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nDescribe this image in detail.\\nmodel\\nHere's a detailed description of the image:\\n\\n**Overall Impression:**\\n\\nThe image is a close-up shot of a garden scene featuring several\"]  # fmt: skip\n+        self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n+        self.assertEqual(output_text, EXPECTED_TEXTS)\n+\n     def test_model_4b_multiimage(self):\n         model_id = \"gg-hf-g/gemma-3-4b-it\"\n "
        }
    ],
    "stats": {
        "total": 38,
        "additions": 35,
        "deletions": 3
    }
}