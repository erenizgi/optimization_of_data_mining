{
    "author": "zucchini-nlp",
    "message": "VLMs: enable generation tests (#33533)\n\n* add tests\r\n\r\n* fix whisper\r\n\r\n* update\r\n\r\n* nit\r\n\r\n* add qwen2-vl\r\n\r\n* more updates!\r\n\r\n* better this way\r\n\r\n* fix this one\r\n\r\n* fix more tests\r\n\r\n* fix final tests, hope so\r\n\r\n* fix led\r\n\r\n* Update tests/generation/test_utils.py\r\n\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\r\n\r\n* pr comments\r\n\r\n* not pass pixels and extra for low-mem tests, very flaky because of visio tower\r\n\r\n---------\r\n\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "d7975a58749cb5f2102a518858eef43ee3614def",
    "files": [
        {
            "sha": "a4cb000cf462b8101140f12f72b061c3988cf199",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -1154,7 +1154,7 @@ def _validate_assistant(self, assistant_model):\n                     \"Ensure you load the assistant with the correct encoder-decoder class, e.g. `AutoModelForSpeechSeq2Seq` for Whisper.\"\n                 )\n \n-        if not self.config.vocab_size == assistant_model.config.vocab_size:\n+        if not self.config.get_text_config().vocab_size == assistant_model.config.get_text_config().vocab_size:\n             raise ValueError(\"Make sure the main and assistant model use the same tokenizer\")\n \n     def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n@@ -1476,7 +1476,7 @@ def get_layer_device_map(execution_device_map: Optional[dict] = None):\n             layer_device_map = get_layer_device_map(execution_device_map)\n \n             cache_kwargs = {\n-                \"config\": self.config if hasattr(self.config, \"text_config\") else self.config,\n+                \"config\": self.config.get_text_config(),\n                 \"max_batch_size\": batch_size,\n                 \"max_cache_len\": max_cache_len,\n                 \"device\": device,"
        },
        {
            "sha": "4e456b9f08e213d5ec1559c06d06bf4fc9c0c634",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 111,
            "deletions": 15,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -45,6 +45,74 @@\n _CONFIG_FOR_DOC = \"PaliGemmaConfig\"\n \n \n+# Adapted from transformers.models.llama.modeling_llama._prepare_4d_causal_attention_mask_with_cache_position\n+# But Paligemma has no causal mask on prefix\n+def _prepare_4d_causal_attention_mask_with_cache_position(\n+    attention_mask: torch.Tensor,\n+    sequence_length: int,\n+    target_length: int,\n+    dtype: torch.dtype,\n+    device: torch.device,\n+    min_dtype: float,\n+    cache_position: torch.Tensor,\n+    batch_size: int,\n+    is_training: bool,\n+    token_type_ids: torch.Tensor,\n+):\n+    \"\"\"\n+    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+    Args:\n+        attention_mask (`torch.Tensor`):\n+            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+        sequence_length (`int`):\n+            The sequence length being processed.\n+        target_length (`int`):\n+            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+        dtype (`torch.dtype`):\n+            The dtype to use for the 4D attention mask.\n+        device (`torch.device`):\n+            The device to plcae the 4D attention mask on.\n+        min_dtype (`float`):\n+            The minimum value representable with the dtype `dtype`.\n+        cache_position (`torch.Tensor`):\n+            Indices depicting the position of the input sequence tokens in the sequence.\n+        batch_size (`torch.Tensor`):\n+            Batch size.\n+        is_training (`bool`):\n+            Whether the model is in training mode or in inference. The condition is checked by presence/absence of `token_type_ids/labels`\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.dim() == 4:\n+        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+        causal_mask = attention_mask\n+    else:\n+        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n+        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n+        if sequence_length != 1:\n+            if is_training:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            else:\n+                causal_mask = torch.zeros_like(causal_mask)\n+\n+        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+        if attention_mask is not None:\n+            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+            mask_length = attention_mask.shape[-1]\n+            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n+            padding_mask = padding_mask == 0\n+            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                padding_mask, min_dtype\n+            )\n+            # we are training thus we need to create a full mask on the image + prefix but causal on suffix\n+            if is_training:\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n+                )\n+    return causal_mask\n+\n+\n @dataclass\n class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n@@ -285,7 +353,7 @@ def _update_causal_mask(\n         self, attention_mask, token_type_ids, inputs_embeds, past_key_values, cache_position, is_training: bool = False\n     ):\n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        dtype, device = inputs_embeds.dtype, inputs_embeds.device\n+        dtype = inputs_embeds.dtype\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = inputs_embeds.shape[1]\n         if using_static_cache:\n@@ -299,19 +367,19 @@ def _update_causal_mask(\n \n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n-            )\n-            # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n-            if sequence_length != 1:\n-                if is_training:\n-                    causal_mask = torch.triu(causal_mask, diagonal=1)\n-                else:\n-                    causal_mask = torch.zeros_like(causal_mask)\n-\n-        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            return attention_mask\n+\n+        causal_mask = torch.full(\n+            (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+        )\n+        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n+        if sequence_length != 1:\n+            if is_training:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            else:\n+                causal_mask = torch.zeros_like(causal_mask)\n+\n+        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n         causal_mask = causal_mask[None, None, :, :].expand(inputs_embeds.shape[0], 1, -1, -1)\n         if attention_mask is not None:\n             causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n@@ -420,7 +488,8 @@ def forward(\n             image_features = self.multi_modal_projector(selected_image_feature)\n             image_features = image_features / (self.config.hidden_size**0.5)\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if inputs_embeds[special_image_mask].numel() != image_features.numel():\n                 image_tokens_in_text = torch.sum(input_ids == self.config.image_token_index)\n                 raise ValueError(\n@@ -508,11 +577,38 @@ def prepare_inputs_for_generation(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n+            position_ids=position_ids,\n             cache_position=cache_position,\n+            use_cache=use_cache,\n             num_logits_to_keep=num_logits_to_keep,\n             **kwargs,\n         )\n \n+        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            dtype = self.get_output_embeddings().weight.dtype\n+            min_dtype = torch.finfo(dtype).min\n+            is_training = token_type_ids is not None and kwargs.get(\"labels\", None) is not None\n+\n+            model_inputs[\"attention_mask\"] = _prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_length(),\n+                dtype=dtype,\n+                device=device,\n+                min_dtype=min_dtype,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+                is_training=is_training,\n+                token_type_ids=token_type_ids,\n+            )\n+\n         model_inputs[\"token_type_ids\"] = token_type_ids\n \n         # position_ids in Paligemma are 1-indexed"
        },
        {
            "sha": "8e46f6840ad187db92fe2d1e14bf98aeae2b06f2",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -1070,7 +1070,9 @@ def __init__(self, config) -> None:\n         self.blocks = nn.ModuleList(\n             [Qwen2VLVisionBlock(config, config._attn_implementation) for _ in range(config.depth)]\n         )\n-        self.merger = PatchMerger(dim=config.hidden_size, context_dim=config.embed_dim)\n+        self.merger = PatchMerger(\n+            dim=config.hidden_size, context_dim=config.embed_dim, spatial_merge_size=config.spatial_merge_size\n+        )\n \n     def get_dtype(self) -> torch.dtype:\n         return self.blocks[0].mlp.fc2.weight.dtype"
        },
        {
            "sha": "413e920609a812eca50a6afc945af46b895e17e1",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 173,
            "deletions": 87,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -98,10 +98,22 @@ class GenerationTesterMixin:\n \n     def _get_input_ids_and_config(self, batch_size=2):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict[self.input_name]\n+        # TODO: @raushan or @gante, use `model.main_input_name` as the main input instead of relyinn on `input_ids`\n+        input_ids = inputs_dict.pop(self.input_name)[:batch_size, :]\n+        inputs_dict.pop(\"attention_mask\", None)\n \n-        input_ids = input_ids[:batch_size]\n+        # we don't want encoder-decoder models to start from filled decoder ids\n+        inputs_dict.pop(\"decoder_input_ids\", None)\n+        inputs_dict.pop(\"decoder_attention_mask\", None)\n \n+        # we'll set cache use in each test differently\n+        inputs_dict.pop(\"use_cache\", None)\n+\n+        inputs_dict = {\n+            k: v[:batch_size, ...]\n+            for k, v in inputs_dict.items()\n+            if \"head_mask\" not in k and isinstance(v, torch.Tensor)\n+        }\n         if config.eos_token_id is not None and config.pad_token_id is None:\n             # hack to allow generate for models such as GPT2 as is done in `generate()`\n             if isinstance(config.eos_token_id, int):\n@@ -118,7 +130,7 @@ def _get_input_ids_and_config(self, batch_size=2):\n         config.eos_token_id = None\n         config.forced_eos_token_id = None\n \n-        return config, input_ids, attention_mask\n+        return config, input_ids, attention_mask, inputs_dict\n \n     def _get_logits_processor_kwargs(self, do_sample=False):\n         logits_processor_kwargs = {\n@@ -191,6 +203,7 @@ def _greedy_generate(\n         model,\n         input_ids,\n         attention_mask,\n+        inputs_dict,\n         output_scores=False,\n         output_logits=False,\n         output_attentions=False,\n@@ -213,6 +226,7 @@ def _greedy_generate(\n             use_cache=use_cache,\n             **logits_processor_kwargs,\n             **model_kwargs,\n+            **inputs_dict,\n         )\n \n         return output_generate\n@@ -222,6 +236,7 @@ def _sample_generate(\n         model,\n         input_ids,\n         attention_mask,\n+        inputs_dict,\n         num_return_sequences,\n         output_scores=False,\n         output_logits=False,\n@@ -247,6 +262,7 @@ def _sample_generate(\n             use_cache=use_cache,\n             **logits_processor_kwargs,\n             **model_kwargs,\n+            **inputs_dict,\n         )\n \n         return output_generate\n@@ -256,6 +272,7 @@ def _beam_search_generate(\n         model,\n         input_ids,\n         attention_mask,\n+        inputs_dict,\n         beam_kwargs,\n         output_scores=False,\n         output_logits=False,\n@@ -279,6 +296,7 @@ def _beam_search_generate(\n             **beam_kwargs,\n             **logits_processor_kwargs,\n             **model_kwargs,\n+            **inputs_dict,\n         )\n \n         return output_generate\n@@ -288,6 +306,7 @@ def _beam_sample_generate(\n         model,\n         input_ids,\n         attention_mask,\n+        inputs_dict,\n         beam_kwargs,\n         output_scores=False,\n         output_logits=False,\n@@ -312,6 +331,7 @@ def _beam_sample_generate(\n             **beam_kwargs,\n             **logits_processor_kwargs,\n             **model_kwargs,\n+            **inputs_dict,\n         )\n \n         return output_generate\n@@ -321,6 +341,7 @@ def _group_beam_search_generate(\n         model,\n         input_ids,\n         attention_mask,\n+        inputs_dict,\n         beam_kwargs,\n         output_scores=False,\n         output_logits=False,\n@@ -344,6 +365,7 @@ def _group_beam_search_generate(\n             **beam_kwargs,\n             **logits_processor_kwargs,\n             **model_kwargs,\n+            **inputs_dict,\n         )\n \n         return output_generate\n@@ -353,6 +375,7 @@ def _constrained_beam_search_generate(\n         model,\n         input_ids,\n         attention_mask,\n+        inputs_dict,\n         constraints,\n         beam_kwargs,\n         output_scores=False,\n@@ -378,6 +401,7 @@ def _constrained_beam_search_generate(\n             **beam_kwargs,\n             **logits_processor_kwargs,\n             **model_kwargs,\n+            **inputs_dict,\n         )\n \n         return output_generate\n@@ -387,6 +411,7 @@ def _contrastive_generate(\n         model,\n         input_ids,\n         attention_mask,\n+        inputs_dict,\n         output_scores=False,\n         output_logits=False,\n         output_attentions=False,\n@@ -415,17 +440,20 @@ def _contrastive_generate(\n             **logits_processor_kwargs,\n             **model_kwargs,\n             **contrastive_search_kwargs,\n+            **inputs_dict,\n         )\n \n         return output_generate\n \n     @pytest.mark.generate\n     def test_greedy_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n-            output_generate = self._greedy_generate(model=model, input_ids=input_ids, attention_mask=attention_mask)\n+            output_generate = self._greedy_generate(\n+                model=model, input_ids=input_ids, attention_mask=attention_mask, inputs_dict=inputs_dict\n+            )\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n@@ -435,13 +463,14 @@ def test_greedy_generate(self):\n     @pytest.mark.generate\n     def test_greedy_generate_dict_outputs(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 output_scores=True,\n                 output_logits=True,\n                 output_hidden_states=True,\n@@ -466,7 +495,7 @@ def test_greedy_generate_dict_outputs(self):\n     @pytest.mark.generate\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=\"This model doesn't support caching\")\n@@ -479,6 +508,7 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 output_scores=True,\n                 output_logits=True,\n                 output_hidden_states=True,\n@@ -497,13 +527,14 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n     @pytest.mark.generate\n     def test_sample_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._sample_generate(\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 num_return_sequences=1,\n             )\n \n@@ -515,13 +546,14 @@ def test_sample_generate(self):\n     @pytest.mark.generate\n     def test_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._sample_generate(\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 num_return_sequences=2,\n                 output_scores=True,\n                 output_logits=True,\n@@ -547,7 +579,7 @@ def test_sample_generate_dict_output(self):\n     @pytest.mark.generate\n     def test_beam_search_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n \n@@ -556,6 +588,7 @@ def test_beam_search_generate(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n             )\n \n@@ -567,14 +600,15 @@ def test_beam_search_generate(self):\n     @pytest.mark.generate\n     def test_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n             output_generate = self._beam_search_generate(\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n                 output_scores=True,\n                 output_logits=True,\n@@ -602,7 +636,7 @@ def test_beam_search_generate_dict_output(self):\n     def test_beam_search_generate_dict_outputs_use_cache(self):\n         for model_class in self.all_generative_model_classes:\n             # enable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=\"This model doesn't support caching\")\n@@ -618,6 +652,7 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n                 output_scores=True,\n                 output_logits=True,\n@@ -647,7 +682,7 @@ def test_model_parallel_beam_search(self):\n             if model_class._no_split_modules is None:\n                 continue\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).eval()\n             with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -659,19 +694,21 @@ def test_model_parallel_beam_search(self):\n                     attention_mask=attention_mask,\n                     max_new_tokens=self.max_new_tokens,\n                     num_beams=2,\n+                    **inputs_dict,\n                 )\n \n     @pytest.mark.generate\n     def test_beam_sample_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n             output_generate = self._beam_sample_generate(\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n             )\n \n@@ -680,28 +717,34 @@ def test_beam_sample_generate(self):\n             else:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + input_ids.shape[-1])\n \n-            prepare_inputs_for_generation_args = set(inspect.signature(model.prepare_inputs_for_generation).parameters)\n-            # `inputs_embeds` input is well supported when `cache_positions` is used, because it means the modeling\n-            # code is up to date with our most recent standards\n-            if (\n-                \"inputs_embeds\" in prepare_inputs_for_generation_args\n-                and \"cache_positions\" in prepare_inputs_for_generation_args\n-            ):\n-                input_embeds = model.get_input_embeddings()(input_ids)\n-                beam_kwargs.update({\"inputs_embeds\": input_embeds})\n-                output_generate2 = self._beam_sample_generate(\n-                    model=model,\n-                    input_ids=None,\n-                    attention_mask=attention_mask,\n-                    beam_kwargs=beam_kwargs,\n+            # for VLMs inputs embeds won't match input ids unless images are encoded and merged with ids properly\n+            # no quick fix available, since obtaining image embeddings step is very model-specific\n+            if any(name in model.__class__.__name__.lower() for name in (\"blip\", \"llava\", \"paligemma\")):\n+                prepare_inputs_for_generation_args = set(\n+                    inspect.signature(model.prepare_inputs_for_generation).parameters\n                 )\n+                # `inputs_embeds` input is well supported when `cache_positions` is used, because it means the modeling\n+                # code is up to date with our most recent standards\n+                if (\n+                    \"inputs_embeds\" in prepare_inputs_for_generation_args\n+                    and \"cache_positions\" in prepare_inputs_for_generation_args\n+                ):\n+                    input_embeds = model.get_input_embeddings()(input_ids)\n+                    beam_kwargs.update({\"inputs_embeds\": input_embeds})\n+                    output_generate2 = self._beam_sample_generate(\n+                        model=model,\n+                        input_ids=None,\n+                        attention_mask=attention_mask,\n+                        inputs_dict={},\n+                        beam_kwargs=beam_kwargs,\n+                    )\n \n-                torch.testing.assert_close(output_generate[:, input_embeds.shape[1] :], output_generate2)\n+                    torch.testing.assert_close(output_generate[:, input_embeds.shape[1] :], output_generate2)\n \n     @pytest.mark.generate\n     def test_beam_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n@@ -710,6 +753,7 @@ def test_beam_sample_generate_dict_output(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n                 output_scores=True,\n                 output_logits=True,\n@@ -736,7 +780,7 @@ def test_beam_sample_generate_dict_output(self):\n \n     @pytest.mark.generate\n     def test_generate_without_input_ids(self):\n-        config, _, _ = self._get_input_ids_and_config()\n+        config, _, _, _ = self._get_input_ids_and_config()\n \n         # if no bos token id => cannot generate from None\n         if config.bos_token_id is None:\n@@ -758,7 +802,7 @@ def test_generate_without_input_ids(self):\n     @pytest.mark.generate\n     def test_group_beam_search_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n             # check `generate()` and `group_beam_search()` are equal\n@@ -767,6 +811,7 @@ def test_group_beam_search_generate(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n             )\n             if model.config.is_encoder_decoder:\n@@ -781,6 +826,7 @@ def test_group_beam_search_generate(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n             )\n             if model.config.is_encoder_decoder:\n@@ -791,14 +837,15 @@ def test_group_beam_search_generate(self):\n     @pytest.mark.generate\n     def test_group_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_diverse_beam_kwargs()\n             output_generate = self._group_beam_search_generate(\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n                 output_scores=True,\n                 output_logits=True,\n@@ -827,7 +874,7 @@ def test_group_beam_search_generate_dict_output(self):\n     @pytest.mark.generate\n     def test_constrained_beam_search_generate(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n \n@@ -845,6 +892,7 @@ def test_constrained_beam_search_generate(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 constraints=constraints,\n                 beam_kwargs=beam_kwargs,\n             )\n@@ -870,6 +918,7 @@ def test_constrained_beam_search_generate(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 constraints=constraints,\n                 beam_kwargs=beam_kwargs,\n             )\n@@ -885,7 +934,7 @@ def test_constrained_beam_search_generate(self):\n     @pytest.mark.generate\n     def test_constrained_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n \n@@ -902,6 +951,7 @@ def test_constrained_beam_search_generate_dict_output(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 constraints=constraints,\n                 beam_kwargs=beam_kwargs,\n                 output_scores=True,\n@@ -937,7 +987,7 @@ def test_contrastive_generate(self):\n             if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n                 self.skipTest(reason=\"Won't fix: old model with different cache format\")\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -947,7 +997,11 @@ def test_contrastive_generate(self):\n             # test old generation output for backwards compatibility\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._contrastive_generate(\n-                model=model, input_ids=input_ids, attention_mask=attention_mask, use_cache=True\n+                model=model,\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n+                use_cache=True,\n             )\n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[-1] == self.max_new_tokens + 1)\n@@ -964,7 +1018,7 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n             if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n                 self.skipTest(reason=\"Won't fix: old model with different cache format\")\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -976,6 +1030,7 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n                 model=model,\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n+                inputs_dict=inputs_dict,\n                 output_scores=True,\n                 output_logits=True,\n                 output_hidden_states=True,\n@@ -1003,7 +1058,7 @@ def test_contrastive_generate_low_memory(self):\n             if any(model_name in model_class.__name__.lower() for model_name in [\"gptbigcode\"]):\n                 self.skipTest(reason=\"TODO: fix me\")\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config(batch_size=1)\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n \n             # NOTE: contrastive search only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1021,6 +1076,7 @@ def test_contrastive_generate_low_memory(self):\n                 low_memory=True,\n                 max_new_tokens=self.max_new_tokens,\n                 attention_mask=attention_mask,\n+                **inputs_dict,\n                 use_cache=True,\n             )\n \n@@ -1031,6 +1087,7 @@ def test_contrastive_generate_low_memory(self):\n                 low_memory=False,\n                 max_new_tokens=self.max_new_tokens,\n                 attention_mask=attention_mask,\n+                **inputs_dict,\n                 use_cache=True,\n             )\n             self.assertListEqual(low_output.tolist(), high_output.tolist())\n@@ -1055,7 +1112,7 @@ def test_beam_search_low_memory(self):\n                 ]\n             ):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n-            config, input_ids, _ = self._get_input_ids_and_config(batch_size=2)\n+            config, input_ids, _, _ = self._get_input_ids_and_config(batch_size=2)\n             # batch_size=1 is ok, but batch_size>1 will cause non-identical output\n \n             config.use_cache = True\n@@ -1065,7 +1122,12 @@ def test_beam_search_low_memory(self):\n             model = model_class(config).to(torch_device).eval()\n \n             low_output = model.generate(\n-                input_ids, max_new_tokens=8, num_beams=5, early_stopping=True, low_memory=True, use_cache=True\n+                input_ids,\n+                max_new_tokens=8,\n+                num_beams=5,\n+                early_stopping=True,\n+                low_memory=True,\n+                use_cache=True,\n             )\n \n             high_output = model.generate(\n@@ -1114,7 +1176,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n \n             # enable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config(batch_size=1)\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n \n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1140,7 +1202,9 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n                 \"return_dict_in_generate\": True,\n                 \"use_cache\": True,\n             }\n-            output_greedy = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            output_greedy = model.generate(\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n+            )\n \n             # test with the same assistant model or randomly init one\n             # in the first case all candidate tokens are accepted, in the second none is accepted\n@@ -1152,7 +1216,9 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             assistant_model.generation_config.num_assistant_tokens = 2  # see b)\n             assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"  # see b)\n             generation_kwargs.update({\"assistant_model\": assistant_model})\n-            output_assisted = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            output_assisted = model.generate(\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n+            )\n \n             # The two outputs must match and their shape must be as expected\n \n@@ -1187,7 +1253,7 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n \n             # enable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config(batch_size=1)\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n \n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1214,10 +1280,14 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n                 \"use_cache\": True,\n             }\n \n-            output_greedy = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            output_greedy = model.generate(\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n+            )\n \n             generation_kwargs.update({\"prompt_lookup_num_tokens\": 2})  # see b)\n-            output_prompt_lookup = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            output_prompt_lookup = model.generate(\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n+            )\n \n             # The two outputs must match and their shape must be as expected\n \n@@ -1239,7 +1309,7 @@ def test_dola_decoding_sample(self):\n                 self.skipTest(\"DoLa is not supported for models that don't return layerwise hidden states\")\n \n             # enable cache if the model is not openai-gpt, xlnet, cpm, or xlm\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             # Encoder-decoder models are not supported\n             if config.is_encoder_decoder:\n@@ -1267,7 +1337,7 @@ def test_dola_decoding_sample(self):\n             }\n             generation_kwargs.update({\"dola_layers\": \"low\"})\n             model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n-            output_dola = model.generate(input_ids, **model_kwargs, **generation_kwargs)\n+            output_dola = model.generate(input_ids, **model_kwargs, **generation_kwargs, **inputs_dict)\n             self._check_outputs(output_dola, input_ids, model.config, use_cache=hasattr(config, \"use_cache\"))\n \n     @pytest.mark.generate\n@@ -1296,7 +1366,7 @@ def test_assisted_decoding_sample(self):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n \n             # enable cache\n-            config, input_ids, attention_mask = self._get_input_ids_and_config(batch_size=1)\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n \n             # NOTE: assisted generation only works with cache on at the moment.\n             if not hasattr(config, \"use_cache\"):\n@@ -1326,9 +1396,11 @@ def test_assisted_decoding_sample(self):\n                 \"return_dict_in_generate\": True,\n                 \"use_cache\": True,\n             }\n-            output_assisted = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            output_assisted = model.generate(\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n+            )\n \n-            self._check_outputs(output_assisted, input_ids, model.config, use_cache=True)\n+            self._check_outputs(output_assisted, input_ids, config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_prompt_lookup_decoding_stops_at_eos(self):\n@@ -1364,7 +1436,7 @@ def test_generate_with_head_masking(self):\n         \"\"\"Test designed for encoder-decoder models to ensure the attention head masking is used.\"\"\"\n         attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n             # We want to test only encoder-decoder models\n             if not config.is_encoder_decoder:\n                 continue\n@@ -1394,6 +1466,7 @@ def test_generate_with_head_masking(self):\n                     return_dict_in_generate=True,\n                     remove_invalid_values=True,\n                     **{name: mask},\n+                    **inputs_dict,\n                 )\n                 # We check the state of decoder_attentions and cross_attentions just from the last step\n                 attn_weights = out[attn_name] if attn_name == attention_names[0] else out[attn_name][-1]\n@@ -1416,7 +1489,7 @@ def test_left_padding_compatibility(self):\n         # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n         decoder_only_classes = []\n         for model_class in self.all_generative_model_classes:\n-            config, _, _ = self._get_input_ids_and_config()\n+            config, _, _, _ = self._get_input_ids_and_config()\n             if config.is_encoder_decoder:\n                 continue\n             else:\n@@ -1449,7 +1522,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             return model_kwargs\n \n         for model_class in decoder_only_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, _ = self._get_input_ids_and_config()\n             model = model_class(config).to(torch_device).eval()\n             signature = inspect.signature(model.forward).parameters.keys()\n \n@@ -1462,7 +1535,9 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n \n             # With left-padding (length 32)\n             # can hardcode pad_token to be 0 as we'll do attn masking anyway\n-            pad_token_id = config.pad_token_id if getattr(config, \"pad_token_id\") is not None else 0\n+            pad_token_id = (\n+                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n+            )\n             pad_size = (input_ids.shape[0], 32)\n             padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n             padded_input_ids = torch.cat((padding, input_ids), dim=1)\n@@ -1550,7 +1625,7 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n         # When supported, tests that the decoder model can generate from `inputs_embeds` instead of `input_ids`\n         # if fails, you should probably update the `prepare_inputs_for_generation` function\n         for model_class in self.all_generative_model_classes:\n-            config, input_ids, _ = self._get_input_ids_and_config()\n+            config, input_ids, _, _ = self._get_input_ids_and_config()\n \n             # Ignore:\n             # a) eos (to always output 20 tokens) and pad (so we don't try to infer the attn mask from the input_ids,\n@@ -1572,25 +1647,23 @@ def test_generate_from_inputs_embeds_decoder_only(self):\n                 continue\n \n             # Traditional way of generating text\n-            outputs_from_ids = model.generate(input_ids)\n-            self.assertEqual(outputs_from_ids.shape, (2, 20))\n+            outputs_from_ids = model.generate(input_ids, max_new_tokens=5)\n+            self.assertEqual(outputs_from_ids.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n \n             # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n             inputs_embeds = model.get_input_embeddings()(input_ids)\n-            outputs_from_embeds = model.generate(input_ids, inputs_embeds=inputs_embeds)\n+            outputs_from_embeds = model.generate(input_ids, inputs_embeds=inputs_embeds, max_new_tokens=5)\n             self.assertListEqual(outputs_from_ids.tolist(), outputs_from_embeds.tolist())\n \n             # But if we pass different inputs_embeds, we should get different outputs\n             torch.manual_seed(0)\n             random_embeds = torch.rand_like(inputs_embeds)\n-            outputs_from_rand_embeds = model.generate(input_ids, inputs_embeds=random_embeds)\n+            outputs_from_rand_embeds = model.generate(input_ids, inputs_embeds=random_embeds, max_new_tokens=5)\n             with self.assertRaises(AssertionError):\n                 self.assertListEqual(outputs_from_rand_embeds.tolist(), outputs_from_embeds.tolist())\n \n             # input_ids is not a required input -- if we don't pass it, the newly generated tokens will be the same\n-            outputs_from_embeds_wo_ids = model.generate(\n-                inputs_embeds=inputs_embeds, max_new_tokens=20 - inputs_embeds.shape[1]\n-            )\n+            outputs_from_embeds_wo_ids = model.generate(inputs_embeds=inputs_embeds, max_new_tokens=5)\n             self.assertListEqual(\n                 outputs_from_embeds[:, inputs_embeds.shape[1] :].tolist(),\n                 outputs_from_embeds_wo_ids.tolist(),\n@@ -1607,7 +1680,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             if not model_class._supports_static_cache:\n                 self.skipTest(reason=\"This model does not support the static cache format\")\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n             if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n@@ -1621,27 +1694,30 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             max_cache_len = 30\n \n             # here we force to not stop at eos and go until max-length\n-            model.generation_config.eos_token_id = model.config.eos_token_id = -1\n+            model.generation_config.eos_token_id = model.config.get_text_config().eos_token_id = -1\n             generation_kwargs = {\n                 \"max_length\": max_cache_len,\n                 \"cache_implementation\": \"static\",\n                 \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n             }\n \n+            text_config = model.config.get_text_config()\n             head_dim = (\n-                model.config.head_dim\n-                if hasattr(model.config, \"head_dim\")\n-                else model.config.hidden_size // model.config.num_attention_heads\n+                text_config.head_dim\n+                if hasattr(text_config, \"head_dim\")\n+                else text_config.hidden_size // text_config.num_attention_heads\n             )\n             num_key_value_heads = (\n-                model.config.num_attention_heads\n-                if getattr(config, \"num_key_value_heads\", None) is None\n-                else model.config.num_key_value_heads\n+                text_config.num_attention_heads\n+                if getattr(text_config, \"num_key_value_heads\", None) is None\n+                else text_config.num_key_value_heads\n             )\n-            num_hidden_layers = config.num_hidden_layers\n+            num_hidden_layers = text_config.num_hidden_layers\n \n             inputs_embeds = model.get_input_embeddings()(input_ids)\n-            outputs = model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, **generation_kwargs)\n+            outputs = model.generate(\n+                inputs_embeds=inputs_embeds, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n+            )\n \n             # we should get `max_length` in shape, not `max_length - embeds_length`\n             cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n@@ -1742,7 +1818,7 @@ def test_new_cache_format(self, num_beams, do_sample):\n             if not model_class._supports_cache_class:\n                 self.skipTest(reason=\"This model does not support the new cache format\")\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n \n             model = model_class(config).to(torch_device).eval()\n             generation_kwargs = {\n@@ -1757,7 +1833,9 @@ def test_new_cache_format(self, num_beams, do_sample):\n             # Sets seed before calling `generate` for the case with do_sample=True\n             seed = torch.randint(0, 1000000, (1,)).item()\n             set_seed(seed)\n-            legacy_results = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            legacy_results = model.generate(\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict\n+            )\n             set_seed(seed)\n             if config.is_encoder_decoder:\n                 cache_cls = EncoderDecoderCache\n@@ -1766,7 +1844,11 @@ def test_new_cache_format(self, num_beams, do_sample):\n                 cache_cls = DynamicCache\n                 past_key_values = cache_cls()\n             new_results = model.generate(\n-                input_ids, attention_mask=attention_mask, past_key_values=past_key_values, **generation_kwargs\n+                input_ids,\n+                attention_mask=attention_mask,\n+                past_key_values=past_key_values,\n+                **generation_kwargs,\n+                **inputs_dict,\n             )\n \n             # The two sets of generated sequences must match, despite the cache format between forward passes being\n@@ -1810,7 +1892,7 @@ def test_generate_with_static_cache(self):\n             if not model_class._supports_static_cache:\n                 self.skipTest(reason=\"This model does not support the static cache format\")\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n             if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n@@ -1838,7 +1920,7 @@ def test_generate_with_static_cache(self):\n                 else config.num_key_value_heads\n             )\n             num_hidden_layers = config.num_hidden_layers\n-            results = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            results = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict)\n \n             cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n             self.assertTrue(isinstance(results.past_key_values, StaticCache))\n@@ -1852,7 +1934,7 @@ def test_generate_with_quant_cache(self):\n             if not model_class._supports_quantized_cache:\n                 self.skipTest(reason=\"This model does not support the quantized cache format\")\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n             config.is_decoder = True\n \n             model = model_class(config).to(torch_device).eval()\n@@ -1865,7 +1947,7 @@ def test_generate_with_quant_cache(self):\n                 \"use_cache\": True,\n             }\n \n-            results = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            results = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict)\n             self.assertTrue(isinstance(results.past_key_values, QuantoQuantizedCache))\n \n             # passing past key values of different type should raise Error\n@@ -1931,7 +2013,7 @@ def test_generate_methods_with_num_logits_to_keep(self):\n             if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n                 self.skipTest(reason=\"This model does not support `num_logits_to_keep` argument.\")\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n             config.use_cache = True\n             config.is_decoder = True\n \n@@ -1946,10 +2028,12 @@ def test_generate_methods_with_num_logits_to_keep(self):\n \n             # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n             with_all_logits = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, num_logits_to_keep=0\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict, num_logits_to_keep=0\n             )\n             # By default, num_logits_to_keep is automatically set to 1 if not provided (new behavior)\n-            without_all_logits = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            without_all_logits = model.generate(\n+                input_ids, attention_mask=attention_mask, **inputs_dict, **generation_kwargs\n+            )\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n     def test_assisted_decoding_with_num_logits_to_keep(self):\n@@ -1959,7 +2043,7 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n             if model_class._is_stateful:\n                 self.skipTest(reason=\"Stateful models don't support assisted generation\")\n \n-            config, input_ids, attention_mask = self._get_input_ids_and_config(batch_size=1)\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config(batch_size=1)\n             config.use_cache = True\n             config.is_decoder = True\n \n@@ -1976,10 +2060,12 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n \n             # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n             with_all_logits = model.generate(\n-                input_ids, attention_mask=attention_mask, **generation_kwargs, num_logits_to_keep=0\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, **inputs_dict, num_logits_to_keep=0\n             )\n             # By default, num_logits_to_keep is automatically set to 1 if not provided (new behavior)\n-            without_all_logits = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            without_all_logits = model.generate(\n+                input_ids, attention_mask=attention_mask, **inputs_dict, **generation_kwargs\n+            )\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n     def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):"
        },
        {
            "sha": "0f28fc2d67b5823170a56aca2582df912f2717ef",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -289,7 +289,10 @@ def _get_input_ids_and_config(self, batch_size=2):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.attention_type = \"original_full\"\n \n-        input_ids = inputs_dict[self.input_name]\n+        input_ids = inputs_dict.pop(self.input_name)\n+        _ = inputs_dict.pop(\"attention_mask\", None)\n+        _ = inputs_dict.pop(\"decoder_input_ids\", None)\n+        _ = inputs_dict.pop(\"decoder_attention_mask\", None)\n         attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n \n         # cut to half length & take max batch_size 3\n@@ -300,7 +303,7 @@ def _get_input_ids_and_config(self, batch_size=2):\n         if config.eos_token_id is not None and config.pad_token_id is None:\n             # hack to allow generate for models such as GPT2 as is done in `generate()`\n             config.pad_token_id = config.eos_token_id\n-        return config, input_ids, attention_mask\n+        return config, input_ids, attention_mask, inputs_dict\n \n     def setUp(self):\n         self.model_tester = BigBirdPegasusModelTester(self)"
        },
        {
            "sha": "b20012c2a19734c5fd2a92cd80aa34eb79be445a",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -389,10 +389,6 @@ def test_bloom_weight_initialization(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_bloom_weight_initialization(*config_and_inputs)\n \n-    @unittest.skip(reason=\"Bloom has a non-standard KV cache format.\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"bigscience/bigscience-small-testing\""
        },
        {
            "sha": "02a3c033c452f58b05fda9f1c2eba265246aeb1b",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -450,6 +450,53 @@ def test_model_various_embeddings(self):\n             config_and_inputs[0].position_embedding_type = type\n             self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def _check_attentions_for_generate(\n+        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+    ):\n+        # GIT attention shape depends on image inputs, overwrite\n+        self.assertIsInstance(attentions, tuple)\n+        self.assertListEqual(\n+            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n+        )\n+        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n+        image_length = int((config.vision_config.image_size / config.vision_config.patch_size) ** 2 + 1)\n+\n+        for idx, iter_attentions in enumerate(attentions):\n+            tgt_len = min_length + idx + image_length if not use_cache else 1\n+            src_len = min_length + idx + image_length\n+\n+            expected_shape = (\n+                batch_size * num_beam_groups,\n+                config.num_attention_heads,\n+                tgt_len,\n+                src_len,\n+            )\n+            # check attn size\n+            self.assertListEqual(\n+                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n+            )\n+\n+    def _check_hidden_states_for_generate(\n+        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+    ):\n+        # GIT attention shape depends on image inputs, overwrite\n+        self.assertIsInstance(hidden_states, tuple)\n+        self.assertListEqual(\n+            [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n+            [True] * len(hidden_states),\n+        )\n+        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n+        image_length = int((config.vision_config.image_size / config.vision_config.patch_size) ** 2 + 1)\n+\n+        for idx, iter_hidden_states in enumerate(hidden_states):\n+            seq_len = min_length + idx + image_length if not use_cache else 1\n+            expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n+            # check hidden size\n+            self.assertListEqual(\n+                [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n+                [expected_shape] * len(iter_hidden_states),\n+            )\n+\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"microsoft/git-base\"\n@@ -468,10 +515,18 @@ def test_contrastive_generate(self):\n     def test_contrastive_generate_dict_outputs_use_cache(self):\n         pass\n \n+    @unittest.skip(reason=\"GIT has pixel values as additional input\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n     @unittest.skip(reason=\"GIT has pixel values as additional input\")\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         pass\n \n+    @unittest.skip(reason=\"GIT has pixel values as additional input\")\n+    def test_dola_decoding_sample(self):\n+        pass\n+\n \n @require_torch\n @require_vision"
        },
        {
            "sha": "a4d81ab2e1c6db87633362c3e1fd6c60ef9aefc7",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -338,6 +338,14 @@ def test_global_attention(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         self.model_tester.check_global_attention(*config_and_inputs)\n \n+    def _get_input_ids_and_config(self, batch_size=2):\n+        config, input_ids, attention_mask, inputs_dict = GenerationTesterMixin._get_input_ids_and_config(\n+            self, batch_size=batch_size\n+        )\n+        # LED computes attention scores based on mask indices if `is_global`\n+        inputs_dict.pop(\"global_attention_mask\")\n+        return config, input_ids, attention_mask, inputs_dict\n+\n     # LEDForSequenceClassification does not support inputs_embeds\n     def test_inputs_embeds(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "e183c38a59f7d78e929225243c02905bab16a07e",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -36,6 +36,7 @@\n     torch_device,\n )\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n \n@@ -80,7 +81,7 @@ def __init__(\n             \"initializer_range\": 0.02,\n             \"num_labels\": 3,\n             \"num_choices\": 4,\n-            \"pad_token_id\": 0,\n+            \"pad_token_id\": 1,\n         },\n         is_training=True,\n         vision_config={\n@@ -106,7 +107,7 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n         self.text_config = text_config\n         self.vision_config = vision_config\n-        self.seq_length = seq_length\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n \n         self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n         self.vocab_size = text_config[\"vocab_size\"]\n@@ -118,6 +119,8 @@ def __init__(\n         self.num_channels = 3\n         self.image_size = 336\n         self.encoder_seq_length = 231\n+        self.num_image_tokens = 224\n+        self.seq_length = seq_length + self.num_image_tokens\n \n     def get_config(self):\n         return LlavaConfig(\n@@ -128,6 +131,7 @@ def get_config(self):\n             projector_hidden_act=self.projector_hidden_act,\n             vision_feature_select_strategy=self.vision_feature_select_strategy,\n             vision_feature_layer=self.vision_feature_layer,\n+            image_seq_length=self.num_image_tokens,\n         )\n \n     def prepare_config_and_inputs(self):\n@@ -148,8 +152,8 @@ def prepare_config_and_inputs_for_common(self):\n         config, pixel_values = config_and_inputs\n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n         attention_mask = input_ids.ne(1).to(torch_device)\n-        # we are giving 3 images let's make sure we pass in 3 image tokens\n-        input_ids[:, 1] = config.image_token_index\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = config.image_token_index\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n             \"input_ids\": input_ids,\n@@ -172,12 +176,13 @@ def create_and_check_llava_model_fp16_forward(self, config, input_ids, pixel_val\n \n \n @require_torch\n-class LlavaForConditionalGenerationModelTest(ModelTesterMixin, unittest.TestCase):\n+class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     \"\"\"\n     Model tester for `LlavaForConditionalGeneration`.\n     \"\"\"\n \n     all_model_classes = (LlavaForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (LlavaForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-to-text\": LlavaForConditionalGeneration} if is_torch_available() else {}\n     test_pruning = False\n     test_head_masking = False"
        },
        {
            "sha": "772f19e13a4be54dbd75fe466eb1514a17a320b0",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -86,12 +86,12 @@ def __init__(\n             \"initializer_range\": 0.02,\n             \"num_labels\": 3,\n             \"num_choices\": 4,\n-            \"pad_token_id\": 0,\n+            \"pad_token_id\": 1,\n         },\n         is_training=True,\n         vision_config={\n             \"image_size\": 16,\n-            \"patch_size\": 2,\n+            \"patch_size\": 4,\n             \"num_channels\": 3,\n             \"is_training\": True,\n             \"hidden_size\": 32,\n@@ -112,7 +112,7 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n         self.text_config = text_config\n         self.vision_config = vision_config\n-        self.seq_length = seq_length\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n \n         self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n         self.vocab_size = text_config[\"vocab_size\"]\n@@ -123,8 +123,10 @@ def __init__(\n         self.batch_size = 3\n         self.num_channels = 3\n         self.image_size = 30\n-        self.encoder_seq_length = 342\n+        self.encoder_seq_length = 95\n         self.image_grid_pinpoints = [[32, 32]]\n+        self.num_image_tokens = 88\n+        self.seq_length = seq_length + self.num_image_tokens\n \n     def get_config(self):\n         return LlavaNextConfig(\n@@ -136,6 +138,7 @@ def get_config(self):\n             vision_feature_select_strategy=self.vision_feature_select_strategy,\n             vision_feature_layer=self.vision_feature_layer,\n             image_grid_pinpoints=self.image_grid_pinpoints,\n+            image_seq_length=self.num_image_tokens,\n         )\n \n     def prepare_config_and_inputs(self):\n@@ -157,19 +160,17 @@ def prepare_config_and_inputs_for_common(self):\n         config, pixel_values = config_and_inputs\n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 2\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n-        # we are giving 3 images let's make sure we pass in 3 image tokens\n-        input_ids[:, 1] = config.image_token_index\n-        labels = torch.zeros((self.batch_size, self.seq_length), dtype=torch.long, device=torch_device)\n-        # maskout where the image token is\n-        labels[:, 1] == self.ignore_index\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n+\n+        input_ids[:, : self.num_image_tokens] = config.image_token_index\n+\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n             \"image_sizes\": torch.tensor(\n                 [[self.vision_config[\"image_size\"], self.vision_config[\"image_size\"]]] * self.batch_size\n             ),\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n-            \"labels\": labels,\n         }\n         return config, inputs_dict\n \n@@ -214,6 +215,7 @@ class LlavaNextForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n     \"\"\"\n \n     all_model_classes = (LlavaNextForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (LlavaNextForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n "
        },
        {
            "sha": "30eaa7fb050c7c437ad5b8d45b4d2a8cf87e740f",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -87,12 +87,12 @@ def __init__(\n             \"initializer_range\": 0.02,\n             \"num_labels\": 3,\n             \"num_choices\": 4,\n-            \"pad_token_id\": 0,\n+            \"pad_token_id\": 2,\n         },\n         is_training=True,\n         vision_config={\n             \"image_size\": 16,\n-            \"patch_size\": 2,\n+            \"patch_size\": 4,\n             \"num_channels\": 3,\n             \"is_training\": True,\n             \"hidden_size\": 32,\n@@ -114,7 +114,7 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n         self.text_config = text_config\n         self.vision_config = vision_config\n-        self.seq_length = seq_length\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n \n         self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n         self.vocab_size = text_config[\"vocab_size\"]\n@@ -125,8 +125,11 @@ def __init__(\n         self.batch_size = 3\n         self.num_channels = 3\n         self.image_size = 30\n-        self.encoder_seq_length = 469\n+        self.encoder_seq_length = 127\n         self.image_grid_pinpoints = [[32, 32]]\n+        self.num_image_tokens = 88\n+        self.num_video_tokens = 32\n+        self.seq_length = seq_length + self.num_image_tokens + self.num_video_tokens\n \n     def get_config(self):\n         return LlavaNextVideoConfig(\n@@ -139,6 +142,8 @@ def get_config(self):\n             vision_feature_select_strategy=self.vision_feature_select_strategy,\n             vision_feature_layer=self.vision_feature_layer,\n             image_grid_pinpoints=self.image_grid_pinpoints,\n+            video_seq_length=self.num_video_tokens,\n+            image_seq_length=self.num_image_tokens,\n         )\n \n     def prepare_config_and_inputs(self):\n@@ -168,13 +173,12 @@ def prepare_config_and_inputs_for_common(self):\n         config, pixel_values, pixel_values_videos = self.prepare_config_and_inputs()\n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 2\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n-        # we are giving 3 images and videos let's make sure we pass in 3 special tokens\n-        input_ids[:, 1] = config.image_token_index\n-        input_ids[:, 2] = config.video_token_index\n-        labels = torch.zeros((self.batch_size, self.seq_length), dtype=torch.long, device=torch_device)\n-        # maskout where the image/video token is\n-        labels[:, 1] == self.ignore_index\n-        labels[:, 2] == self.ignore_index\n+\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n+        input_ids[input_ids == config.video_token_index] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = config.image_token_index\n+        input_ids[:, self.num_image_tokens : self.num_video_tokens + self.num_image_tokens] = config.video_token_index\n+\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n             \"pixel_values_videos\": pixel_values_videos,\n@@ -183,7 +187,6 @@ def prepare_config_and_inputs_for_common(self):\n             ),\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n-            \"labels\": labels,\n         }\n         return config, inputs_dict\n \n@@ -230,6 +233,7 @@ class LlavaNextVideoForConditionalGenerationModelTest(ModelTesterMixin, Generati\n     \"\"\"\n \n     all_model_classes = (LlavaNextVideoForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (LlavaNextVideoForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n "
        },
        {
            "sha": "0e9c88cb3463fde920e8c68a0f37164ccb1ddf9d",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -60,7 +60,7 @@ def __init__(\n         self,\n         parent,\n         ignore_index=-100,\n-        image_token_index=0,\n+        image_token_index=1,\n         projector_hidden_act=\"gelu\",\n         seq_length=7,\n         vision_feature_select_strategy=\"full\",\n@@ -92,7 +92,7 @@ def __init__(\n         is_training=True,\n         vision_config={\n             \"image_size\": 16,\n-            \"patch_size\": 2,\n+            \"patch_size\": 8,\n             \"num_channels\": 3,\n             \"is_training\": True,\n             \"hidden_size\": 32,\n@@ -113,7 +113,9 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n         self.text_config = text_config\n         self.vision_config = vision_config\n-        self.seq_length = seq_length\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+        self.num_image_tokens = 10\n+        self.seq_length = seq_length + self.num_image_tokens\n \n         self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n         self.vocab_size = text_config[\"vocab_size\"]\n@@ -124,8 +126,7 @@ def __init__(\n         self.batch_size = 3\n         self.num_channels = 3\n         self.image_size = 30\n-        self.encoder_seq_length = 7\n-        self.image_grid_pinpoints = [[32, 32]]\n+        self.image_grid_pinpoints = [[16, 16]]\n \n     def get_config(self):\n         return LlavaOnevisionConfig(\n@@ -143,7 +144,7 @@ def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor(\n             [\n                 self.batch_size,\n-                9,\n+                3,\n                 self.vision_config[\"num_channels\"],\n                 self.vision_config[\"image_size\"],\n                 self.vision_config[\"image_size\"],\n@@ -158,16 +159,16 @@ def prepare_config_and_inputs_for_common(self):\n         config, pixel_values = config_and_inputs\n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 2\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n-        # we are giving 3 images let's make sure we pass in 3 image tokens\n-        input_ids[:, 1] = config.image_token_index\n+\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = config.image_token_index\n+\n         labels = torch.zeros((self.batch_size, self.seq_length), dtype=torch.long, device=torch_device)\n-        # maskout where the image token is\n-        labels[:, 1] == self.ignore_index\n+        labels[:, : self.num_image_tokens] == self.ignore_index\n+\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n-            \"image_sizes\": torch.tensor(\n-                [[self.vision_config[\"image_size\"], self.vision_config[\"image_size\"]]] * self.batch_size\n-            ),\n+            \"image_sizes\": torch.tensor([[45, 45]] * self.batch_size),\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n             \"labels\": labels,"
        },
        {
            "sha": "e143b8ac3c86581582c55d1ffa679913225804d3",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -286,20 +286,27 @@ def _get_input_ids_and_config(self, batch_size=2):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         input_ids = inputs_dict[\"input_ids\"]\n \n+        _ = inputs_dict.pop(\"attention_mask\", None)\n+        inputs_dict = {\n+            k: v[:batch_size, ...]\n+            for k, v in inputs_dict.items()\n+            if \"head_mask\" not in k and isinstance(v, torch.Tensor)\n+        }\n+\n         # take max batch_size\n         sequence_length = input_ids.shape[-1]\n         input_ids = input_ids[: batch_size * config.num_codebooks, :]\n \n         attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)\n-        return config, input_ids, attention_mask\n+        return config, input_ids, attention_mask, inputs_dict\n \n     def _get_logits_processor_kwargs(self, do_sample=False):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n \n     def test_greedy_generate_stereo_outputs(self):\n         for model_class in self.greedy_sample_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, inputs_dict = self._get_input_ids_and_config()\n             config.audio_channels = 2\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(\n@@ -310,6 +317,7 @@ def test_greedy_generate_stereo_outputs(self):\n                 output_hidden_states=True,\n                 output_attentions=True,\n                 return_dict_in_generate=True,\n+                inputs_dict={},\n             )\n \n             self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)"
        },
        {
            "sha": "28c2bf2f168ba90ee20538c66b54dd49f0d3a7bb",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -289,20 +289,27 @@ def _get_input_ids_and_config(self, batch_size=2):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         input_ids = inputs_dict[\"input_ids\"]\n \n+        _ = inputs_dict.pop(\"attention_mask\", None)\n+        inputs_dict = {\n+            k: v[:batch_size, ...]\n+            for k, v in inputs_dict.items()\n+            if \"head_mask\" not in k and isinstance(v, torch.Tensor)\n+        }\n+\n         # take max batch_size\n         sequence_length = input_ids.shape[-1]\n         input_ids = input_ids[: batch_size * config.num_codebooks, :]\n \n         attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)\n-        return config, input_ids, attention_mask\n+        return config, input_ids, attention_mask, inputs_dict\n \n     def _get_logits_processor_kwargs(self, do_sample=False):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n \n     def test_greedy_generate_stereo_outputs(self):\n         for model_class in self.greedy_sample_model_classes:\n-            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config, input_ids, attention_mask, _ = self._get_input_ids_and_config()\n             config.audio_channels = 2\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(\n@@ -313,6 +320,7 @@ def test_greedy_generate_stereo_outputs(self):\n                 output_hidden_states=True,\n                 output_attentions=True,\n                 return_dict_in_generate=True,\n+                inputs_dict={},\n             )\n \n             self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)"
        },
        {
            "sha": "d592205443e1c253cc7cb6172c977dddb658c8f8",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -35,6 +35,7 @@\n     torch_device,\n )\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n \n@@ -82,7 +83,7 @@ def __init__(\n             \"initializer_range\": 0.02,\n             \"num_labels\": 3,\n             \"num_choices\": 4,\n-            \"pad_token_id\": 0,\n+            \"pad_token_id\": 1,\n         },\n         is_training=True,\n         vision_config={\n@@ -115,6 +116,7 @@ def __init__(\n         self.vision_config = vision_config\n         self.seq_length = seq_length\n         self.projection_dim = projection_dim\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n \n         self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n         self.vocab_size = text_config[\"vocab_size\"]\n@@ -160,7 +162,7 @@ def prepare_config_and_inputs_for_common(self):\n         attention_mask = input_ids.ne(1).to(torch_device)\n         # set the 16 first tokens to be image, and ensure that no other tokens are image tokens\n         # do not change this unless you modified image size or patch size\n-        input_ids = torch.where(input_ids == config.image_token_index, 2, input_ids)\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n         input_ids[:, :16] = config.image_token_index\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n@@ -173,12 +175,13 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class PaliGemmaForConditionalGenerationModelTest(ModelTesterMixin, unittest.TestCase):\n+class PaliGemmaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     \"\"\"\n     Model tester for `PaliGemmaForConditionalGeneration`.\n     \"\"\"\n \n     all_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False\n     test_torchscript = False\n@@ -305,6 +308,12 @@ def test_save_load_low_cpu_mem_usage_checkpoints(self):\n     def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n         pass\n \n+    @unittest.skip(\n+        reason=\"VLMs doen't accept inputs embeds and pixel values at the same time. So if the test passed for bacbone LM, it passes for VLM also\"\n+    )\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n \n @slow\n @require_torch"
        },
        {
            "sha": "956243dccebebfb6fc1d3e37f8cd6ca495c60ad0",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 12,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -58,11 +58,11 @@ class Qwen2VLVisionText2TextModelTester:\n     def __init__(\n         self,\n         parent,\n-        batch_size=8,\n+        batch_size=2,\n         seq_length=7,\n         num_channels=3,\n         ignore_index=-100,\n-        image_size=28,\n+        image_size=14,\n         bos_token_id=0,\n         eos_token_id=1,\n         pad_token_id=2,\n@@ -90,7 +90,7 @@ def __init__(\n             \"mlp_ratio\": 4,\n             \"num_heads\": 4,\n             \"patch_size\": 14,\n-            \"spatial_merge_size\": 2,\n+            \"spatial_merge_size\": 1,\n             \"temporal_patch_size\": 2,\n         },\n         rope_scaling={\"type\": \"mrope\", \"mrope_section\": [2, 1, 1]},\n@@ -119,9 +119,10 @@ def __init__(\n         self.batch_size = batch_size\n         self.num_channels = num_channels\n         self.image_size = image_size\n-        self.seq_length = seq_length\n         self.is_training = is_training\n         self.vocab_size = vocab_size\n+        self.num_image_tokens = 32\n+        self.seq_length = seq_length + self.num_image_tokens\n \n     def get_config(self):\n         return Qwen2VLConfig(\n@@ -162,23 +163,19 @@ def prepare_config_and_inputs(self):\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values = config_and_inputs\n-        vision_seqlen = pixel_values.shape[0] // self.batch_size // (self.vision_config[\"spatial_merge_size\"] ** 2)\n-        input_ids = ids_tensor([self.batch_size, self.seq_length - 1 + vision_seqlen], self.vocab_size)\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n \n         input_ids[input_ids == self.image_token_id] = self.pad_token_id\n-        input_ids[:, torch.arange(vision_seqlen, device=torch_device) + 1] = self.image_token_id\n+        input_ids[:, self.num_image_tokens] = self.image_token_id\n         labels = torch.zeros(\n-            (self.batch_size, self.seq_length - 1 + vision_seqlen),\n+            (self.batch_size, self.seq_length),\n             dtype=torch.long,\n             device=torch_device,\n         )\n-        patch_size = self.vision_config[\"patch_size\"]\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n-            \"image_grid_thw\": torch.tensor(\n-                [[1, self.image_size // patch_size, self.image_size // patch_size]] * self.batch_size\n-            ),\n+            \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size),\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n             \"labels\": labels,\n@@ -312,6 +309,12 @@ def test_model_is_small(self):\n     def test_beam_search_low_memory(self):\n         pass\n \n+    @unittest.skip(\n+        reason=\"VLMs can't generate from inputs embeds and pixels. This can be tested as part of bacbone LM, no need to run the tes for VLMs\"\n+    )\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n \n @require_torch\n class Qwen2VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "11c2e821975d0204e06c2e029f16652c95646313",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -689,12 +689,15 @@ def _get_input_ids_and_config(self, batch_size=2):\n         # decreasing the seq_length in tester causes errors for \"training_tests\", those need exactly max seq length\n         # NOTE: seq_length has to be multiple of 4, otherwise it fails for other tests\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict[self.input_name]\n+        input_ids = inputs_dict.pop(self.input_name)\n+        _ = inputs_dict.pop(\"attention_mask\", None)\n+        _ = inputs_dict.pop(\"decoder_input_ids\", None)\n+        _ = inputs_dict.pop(\"decoder_attention_mask\", None)\n         input_ids = input_ids[:batch_size, :16]\n         attention_mask = torch.ones_like(input_ids, dtype=torch.long)[:batch_size, :16]\n         config.eos_token_id = None\n         config.forced_eos_token_id = None\n-        return config, input_ids, attention_mask\n+        return config, input_ids, attention_mask, inputs_dict\n \n \n @require_torch"
        },
        {
            "sha": "9a7b34211c1c4513cf74eb2028273736439e1fb9",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -285,7 +285,7 @@ class Speech2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTest\n     input_name = \"input_features\"\n \n     def _get_input_ids_and_config(self, batch_size=2):\n-        config, input_ids, attention_mask = GenerationTesterMixin._get_input_ids_and_config(self)\n+        config, input_ids, attention_mask, inputs_dict = GenerationTesterMixin._get_input_ids_and_config(self)\n \n         # `input_ids` is actually `input_features` which is a 3D tensor.\n         # We must overwrite the mask to make it 2D since the original `_get_input_ids_and_config` creates an\n@@ -294,7 +294,7 @@ def _get_input_ids_and_config(self, batch_size=2):\n             sequence_length = input_ids.shape[1]\n             attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long, device=attention_mask.device)\n \n-        return config, input_ids, attention_mask\n+        return config, input_ids, attention_mask, inputs_dict\n \n     def setUp(self):\n         self.model_tester = Speech2TextModelTester(self)"
        },
        {
            "sha": "df8fe0b5dca2bfbf57e06987ca683eaae6973e3f",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 26,
            "deletions": 15,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -75,14 +75,14 @@ def __init__(\n             \"initializer_range\": 0.02,\n             \"num_labels\": 3,\n             \"num_choices\": 4,\n-            \"pad_token_id\": 0,\n+            \"pad_token_id\": 3,\n         },\n         is_training=True,\n         vision_config={\n             \"model_type\": \"clip_vision_model\",\n             \"batch_size\": 12,\n             \"image_size\": 30,\n-            \"patch_size\": 2,\n+            \"patch_size\": 6,\n             \"num_channels\": 3,\n             \"is_training\": True,\n             \"hidden_size\": 32,\n@@ -104,8 +104,8 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n         self.text_config = text_config\n         self.vision_config = vision_config\n-        self.seq_length = seq_length\n         self.num_frames = num_frames\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n \n         self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n         self.vocab_size = text_config[\"vocab_size\"]\n@@ -116,7 +116,10 @@ def __init__(\n         self.batch_size = 5\n         self.num_channels = 3\n         self.image_size = 224\n-        self.encoder_seq_length = 2044\n+        self.encoder_seq_length = 64\n+        self.num_image_tokens = 25\n+        self.num_video_tokens = 26\n+        self.seq_length = seq_length + self.num_image_tokens + self.num_video_tokens\n \n     def get_config(self):\n         return VideoLlavaConfig(\n@@ -128,6 +131,8 @@ def get_config(self):\n             projector_hidden_act=self.projector_hidden_act,\n             vision_feature_select_strategy=self.vision_feature_select_strategy,\n             vision_feature_layer=self.vision_feature_layer,\n+            image_seq_length=self.num_image_tokens,\n+            video_seq_length=self.num_video_tokens,\n         )\n \n     def prepare_config_and_inputs(self):\n@@ -159,11 +164,11 @@ def prepare_config_and_inputs_for_common(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n         attention_mask = input_ids.ne(1).to(torch_device)\n \n-        # we are giving 3 videos and 3 images. Need to pass in image and video tokens, both\n-        # also need to make sure no other special tokens are set\n-        input_ids[(input_ids == 0) | (input_ids == 1)] = 3\n-        input_ids[:, 0] = config.video_token_index\n-        input_ids[:, 1:2] = config.image_token_index\n+        input_ids[(input_ids == config.image_token_index) | (input_ids == config.video_token_index)] = (\n+            self.pad_token_id\n+        )\n+        input_ids[:, : self.num_image_tokens] = config.image_token_index\n+        input_ids[:, self.num_image_tokens : self.num_video_tokens + self.num_image_tokens] = config.video_token_index\n         inputs_dict = {\n             \"pixel_values_videos\": pixel_values_videos,\n             \"pixel_values_images\": pixel_values_images,\n@@ -196,6 +201,7 @@ class VideoLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTe\n     \"\"\"\n \n     all_model_classes = (VideoLlavaForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (VideoLlavaForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True\n@@ -242,16 +248,16 @@ def test_mixed_input(self):\n             # if we remove some images from inputs leaving only one\n             # image number mismatch error should raise\n             inputs[\"pixel_values_images\"] = inputs[\"pixel_values_images\"][:1]\n-            with self.assertRaises(ValueError):\n+            with self.assertRaises(RuntimeError):\n                 _ = model(**inputs)\n \n     def test_video_only_input(self):\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device).eval()\n-            # replace video_token with dummy id which is not video token id\n-            # error that video-tokens and num-of-video-inputs mismatch will be raised\n-            inputs[\"input_ids\"][:, 1:2] = 2\n+            # replace image token id with dummy id\n+            # Error will be raised as num-image-tokens and num-of-image-embeds mismatch\n+            inputs[\"input_ids\"][:, : self.model_tester.num_image_tokens] = 2\n             with self.assertRaises(ValueError):\n                 _ = model(**inputs)\n \n@@ -262,8 +268,13 @@ def test_image_only_input(self):\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device).eval()\n-            # set dummy id, which is not image token id, same as above\n-            inputs[\"input_ids\"][:, :1] = 2\n+            # set dummy id, which is not video token id\n+            # Error will be raised as num-video-tokens and num-of-video-embeds mismatch\n+            inputs[\"input_ids\"][\n+                :,\n+                self.model_tester.num_image_tokens : self.model_tester.num_image_tokens\n+                + self.model_tester.num_video_tokens,\n+            ] = 2\n             with self.assertRaises(ValueError):\n                 _ = model(**inputs)\n "
        },
        {
            "sha": "b12f2c30c774a0236b7a0253a4a3b0a71df42693",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -28,6 +28,7 @@\n )\n from transformers.testing_utils import require_bitsandbytes, require_torch, require_torch_gpu, slow, torch_device\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n \n@@ -73,7 +74,7 @@ def __init__(\n             \"initializer_range\": 0.02,\n             \"num_labels\": 3,\n             \"num_choices\": 4,\n-            \"pad_token_id\": 0,\n+            \"pad_token_id\": 1,\n         },\n         is_training=True,\n         vision_config={\n@@ -99,7 +100,7 @@ def __init__(\n         self.vision_feature_layers = vision_feature_layers\n         self.text_config = text_config\n         self.vision_config = vision_config\n-        self.seq_length = seq_length\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n \n         self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n         self.vocab_size = text_config[\"vocab_size\"]\n@@ -111,6 +112,8 @@ def __init__(\n         self.num_channels = 3\n         self.image_size = 336\n         self.encoder_seq_length = 231\n+        self.num_image_tokens = 224\n+        self.seq_length = seq_length + self.num_image_tokens\n \n     def get_config(self):\n         return VipLlavaConfig(\n@@ -120,6 +123,7 @@ def get_config(self):\n             image_token_index=self.image_token_index,\n             projector_hidden_act=self.projector_hidden_act,\n             vision_feature_layers=self.vision_feature_layers,\n+            image_seq_length=self.num_image_tokens,\n         )\n \n     def prepare_config_and_inputs(self):\n@@ -140,8 +144,9 @@ def prepare_config_and_inputs_for_common(self):\n         config, pixel_values = config_and_inputs\n         input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n         attention_mask = input_ids.ne(1).to(torch_device)\n-        # we are giving 3 images let's make sure we pass in 3 image tokens\n-        input_ids[:, 1] = config.image_token_index\n+\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = config.image_token_index\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n             \"input_ids\": input_ids,\n@@ -152,12 +157,13 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n # Copied from transformers.tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest with Llava->VipLlava\n-class VipLlavaForConditionalGenerationModelTest(ModelTesterMixin, unittest.TestCase):\n+class VipLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     \"\"\"\n     Model tester for `VipLlavaForConditionalGeneration`.\n     \"\"\"\n \n     all_model_classes = (VipLlavaForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (VipLlavaForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True"
        },
        {
            "sha": "70b38d3bf38170302d48d0174187c24865169e88",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -497,19 +497,6 @@ def test_encoder_decoder_model_standalone(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         self.model_tester.check_encoder_decoder_model_standalone(*config_and_inputs)\n \n-    def _get_input_ids_and_config(self, batch_size=3):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        input_ids = inputs_dict[self.input_name]\n-\n-        # cut to half length & take max batch_size=batch_size\n-        input_ids = input_ids[:batch_size, :, :]\n-\n-        if config.eos_token_id is not None and config.pad_token_id is None:\n-            # hack to allow generate for models such as GPT2 as is done in `generate()`\n-            config.pad_token_id = config.eos_token_id\n-\n-        return config, input_ids, None\n-\n     def test_inputs_embeds(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "8eab385ad48adf9e3d8890caa508172621be1e71",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7975a58749cb5f2102a518858eef43ee3614def/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=d7975a58749cb5f2102a518858eef43ee3614def",
            "patch": "@@ -4744,7 +4744,7 @@ def test_static_cache_matches_dynamic(self):\n                 output_logits=True,\n                 return_dict_in_generate=True,\n             )\n-            self.assertTrue(torch.allclose(dynamic_out.logits[0], static_out.logits[0], rtol=1e-3, atol=1e-4))\n+            self.assertTrue(torch.allclose(dynamic_out.logits[0], static_out.logits[0], rtol=1e-3, atol=1e-3))\n \n     # For now, Let's focus only on GPU for `torch.compile`\n     @slow"
        }
    ],
    "stats": {
        "total": 709,
        "additions": 501,
        "deletions": 208
    }
}