{
    "author": "yonigozlan",
    "message": "[SAM3] Fix precompute vision_embeds or text_embeds for inference (#42407)\n\nFix precompute vision embeds or text_embeds for inference",
    "sha": "c12dfddba94c04fb6a5f5be4be879af51e354e2f",
    "files": [
        {
            "sha": "1ebeb122114edfe80216040c5e013f2930edda59",
            "filename": "docs/source/en/model_doc/sam3.md",
            "status": "modified",
            "additions": 98,
            "deletions": 0,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/c12dfddba94c04fb6a5f5be4be879af51e354e2f/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c12dfddba94c04fb6a5f5be4be879af51e354e2f/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam3.md?ref=c12dfddba94c04fb6a5f5be4be879af51e354e2f",
            "patch": "@@ -256,6 +256,104 @@ SAM3 also provides semantic segmentation alongside instance masks:\n >>> print(f\"Semantic segmentation: {semantic_seg.shape}\")\n ```\n \n+### Efficient Multi-Prompt Inference on Single Image\n+\n+When running multiple text prompts on the same image, pre-compute vision embeddings to avoid redundant computation:\n+\n+```python\n+>>> from transformers import Sam3Processor, Sam3Model\n+>>> import torch\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+>>> model = Sam3Model.from_pretrained(\"facebook/sam3\").to(device)\n+>>> processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n+\n+>>> # Load image\n+>>> image_url = \"http://images.cocodataset.org/val2017/000000077595.jpg\"\n+>>> image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n+\n+>>> # Pre-process image and compute vision embeddings once\n+>>> img_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n+>>> with torch.no_grad():\n+...     vision_embeds = model.get_vision_features(pixel_values=img_inputs.pixel_values)\n+\n+>>> # Run multiple text prompts efficiently\n+>>> text_prompts = [\"ear\", \"eye\", \"nose\"]\n+>>> all_results = []\n+\n+>>> for prompt in text_prompts:\n+...     text_inputs = processor(text=prompt, return_tensors=\"pt\").to(device)\n+...     with torch.no_grad():\n+...         outputs = model(vision_embeds=vision_embeds, **text_inputs)\n+...\n+...     results = processor.post_process_instance_segmentation(\n+...         outputs,\n+...         threshold=0.5,\n+...         mask_threshold=0.5,\n+...         target_sizes=img_inputs.get(\"original_sizes\").tolist()\n+...     )[0]\n+...     all_results.append({\"prompt\": prompt, \"results\": results})\n+\n+>>> for item in all_results:\n+...     print(f\"Prompt '{item['prompt']}': {len(item['results']['masks'])} objects found\")\n+```\n+\n+### Efficient Single-Prompt Inference on Multiple Images\n+\n+When running the same text prompt on multiple images, pre-compute text embeddings to avoid redundant computation:\n+\n+```python\n+>>> from transformers import Sam3Processor, Sam3Model\n+>>> import torch\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+>>> model = Sam3Model.from_pretrained(\"facebook/sam3\").to(device)\n+>>> processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n+\n+>>> # Pre-compute text embeddings once\n+>>> text_prompt = \"ear\"\n+>>> text_inputs = processor(text=text_prompt, return_tensors=\"pt\").to(device)\n+>>> with torch.no_grad():\n+...     text_embeds = model.get_text_features(**text_inputs)\n+\n+>>> # Load multiple images\n+>>> image_urls = [\n+...     \"http://images.cocodataset.org/val2017/000000077595.jpg\",\n+...     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n+... ]\n+>>> images = [Image.open(requests.get(url, stream=True).raw).convert(\"RGB\") for url in image_urls]\n+\n+>>> # Run inference on each image reusing text embeddings\n+>>> # Note: attention_mask must be passed along with text_embeds for proper masking\n+>>> all_results = []\n+\n+>>> for image in images:\n+...     img_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n+...     with torch.no_grad():\n+...         outputs = model(\n+...             pixel_values=img_inputs.pixel_values,\n+...             text_embeds=text_embeds,\n+...             attention_mask=text_inputs.attention_mask,\n+...         )\n+...\n+...     results = processor.post_process_instance_segmentation(\n+...         outputs,\n+...         threshold=0.5,\n+...         mask_threshold=0.5,\n+...         target_sizes=img_inputs.get(\"original_sizes\").tolist()\n+...     )[0]\n+...     all_results.append(results)\n+\n+>>> for i, results in enumerate(all_results):\n+...     print(f\"Image {i+1}: {len(results['masks'])} '{text_prompt}' objects found\")\n+```\n+\n ### Prompt Label Conventions\n \n SAM3 uses the following label conventions:"
        },
        {
            "sha": "de12b057aaa7e6d3519fb2293d1654010732c99a",
            "filename": "src/transformers/models/sam3/modeling_sam3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c12dfddba94c04fb6a5f5be4be879af51e354e2f/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c12dfddba94c04fb6a5f5be4be879af51e354e2f/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fmodeling_sam3.py?ref=c12dfddba94c04fb6a5f5be4be879af51e354e2f",
            "patch": "@@ -2146,8 +2146,8 @@ def get_text_features(\n         >>> from PIL import Image\n         >>> import requests\n \n-        >>> model = Sam3Model.from_pretrained(\"facebook/sam3-base\")\n-        >>> processor = Sam3Processor.from_pretrained(\"facebook/sam3-base\")\n+        >>> model = Sam3Model.from_pretrained(\"facebook/sam3\")\n+        >>> processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n \n         >>> # Pre-compute text embeddings\n         >>> text_inputs = processor(text=\"cat\", return_tensors=\"pt\")\n@@ -2184,8 +2184,8 @@ def get_vision_features(\n         >>> from PIL import Image\n         >>> import requests\n \n-        >>> model = Sam3Model.from_pretrained(\"facebook/sam3-base\")\n-        >>> processor = Sam3Processor.from_pretrained(\"facebook/sam3-base\")\n+        >>> model = Sam3Model.from_pretrained(\"facebook/sam3\")\n+        >>> processor = Sam3Processor.from_pretrained(\"facebook/sam3\")\n \n         >>> # Pre-compute vision embeddings\n         >>> img_url = \"http://images.cocodataset.org/val2017/000000077595.jpg\"\n@@ -2233,8 +2233,8 @@ def forward(\n         >>> import requests\n         >>> from transformers import AutoModel, AutoProcessor\n \n-        >>> model = AutoModel.from_pretrained(\"facebook/sam3-base\")\n-        >>> processor = AutoProcessor.from_pretrained(\"facebook/sam3-base\")\n+        >>> model = AutoModel.from_pretrained(\"facebook/sam3\")\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/sam3\")\n \n         >>> img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-car.png\"\n         >>> raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")"
        },
        {
            "sha": "41addea80db97c17ca203b23096e799630cfd5e6",
            "filename": "src/transformers/models/sam3/processing_sam3.py",
            "status": "modified",
            "additions": 15,
            "deletions": 17,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/c12dfddba94c04fb6a5f5be4be879af51e354e2f/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c12dfddba94c04fb6a5f5be4be879af51e354e2f/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py?ref=c12dfddba94c04fb6a5f5be4be879af51e354e2f",
            "patch": "@@ -151,8 +151,9 @@ def __call__(\n             - `input_boxes_labels` (`torch.Tensor`): The processed labels for the bounding boxes.\n             - `input_boxes` (`torch.Tensor`): The processed bounding boxes.\n         \"\"\"\n+        encoding = None\n         if images is not None:\n-            encoding_image_processor = self.image_processor(\n+            encoding = self.image_processor(\n                 images,\n                 segmentation_maps=segmentation_maps,\n                 return_tensors=return_tensors,\n@@ -161,24 +162,21 @@ def __call__(\n         elif original_sizes is not None:\n             if isinstance(original_sizes, torch.Tensor):\n                 original_sizes = original_sizes.cpu().tolist()\n-            encoding_image_processor = BatchEncoding({\"original_sizes\": original_sizes}, tensor_type=return_tensors)\n-        else:\n-            raise ValueError(\"Either images or original_sizes must be provided\")\n+            encoding = BatchEncoding({\"original_sizes\": original_sizes}, tensor_type=return_tensors)\n+        elif input_boxes is not None:\n+            raise ValueError(\"Either images or original_sizes must be provided if input_boxes is not None\")\n \n-        original_sizes = encoding_image_processor[\"original_sizes\"]\n-        # Check original_sizes is of length 1 or len(images)\n-        if images is not None and len(original_sizes) != 1 and len(original_sizes) != len(images):\n-            raise ValueError(\n-                \"original_sizes must be of length 1 or len(images). If you are passing a single image, you must pass a single original_size.\"\n-            )\n         text = self._resolve_text_prompts(text, input_boxes)\n-\n-        encoding_image_processor.update(\n-            self.tokenizer(text, return_tensors=return_tensors, padding=\"max_length\", max_length=32)\n-        )\n+        if text is not None:\n+            text_inputs = self.tokenizer(text, return_tensors=return_tensors, padding=\"max_length\", max_length=32)\n+            if encoding is not None:\n+                encoding.update(text_inputs)\n+            else:\n+                encoding = text_inputs\n \n         # Process input boxes if provided\n         if input_boxes is not None:\n+            original_sizes = encoding[\"original_sizes\"]\n             # Validate and convert inputs to standardized format\n             processed_boxes = self._validate_single_input(\n                 input_boxes,\n@@ -215,14 +213,14 @@ def __call__(\n                     final_boxes, original_sizes, is_bounding_box=True, preserve_padding=True\n                 )\n                 final_boxes = box_xyxy_to_cxcywh(final_boxes)\n-                encoding_image_processor.update({\"input_boxes\": final_boxes})\n+                encoding.update({\"input_boxes\": final_boxes})\n \n             if processed_boxes_labels is not None:\n                 padded_boxes_labels = self._pad_nested_list(processed_boxes_labels, boxes_labels_max_dims)\n                 final_boxes_labels = torch.tensor(padded_boxes_labels, dtype=torch.int64)\n-                encoding_image_processor.update({\"input_boxes_labels\": final_boxes_labels})\n+                encoding.update({\"input_boxes_labels\": final_boxes_labels})\n \n-        return encoding_image_processor\n+        return encoding\n \n     def _normalize_coordinates(self, coords: \"torch.Tensor\", original_size, is_bounding_box=False) -> \"torch.Tensor\":\n         \"\"\""
        },
        {
            "sha": "be8e7e6004203f39a92363a589f9b5656efe77f8",
            "filename": "tests/models/sam3/test_modeling_sam3.py",
            "status": "modified",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/c12dfddba94c04fb6a5f5be4be879af51e354e2f/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c12dfddba94c04fb6a5f5be4be879af51e354e2f/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam3%2Ftest_modeling_sam3.py?ref=c12dfddba94c04fb6a5f5be4be879af51e354e2f",
            "patch": "@@ -1447,3 +1447,100 @@ def test_semantic_segmentation_output(self):\n         self.assertEqual(outputs.semantic_seg.shape, (1, 1, 288, 288))\n         # Check that semantic seg has same spatial size as pred_masks\n         self.assertEqual(outputs.semantic_seg.shape[-2:], outputs.pred_masks.shape[-2:])\n+\n+    def test_efficient_multi_prompt_single_image(self):\n+        \"\"\"Test efficient inference with multiple prompts on a single image using get_vision_features.\"\"\"\n+        raw_image = prepare_coco_cat_image()\n+\n+        # Pre-compute vision embeddings once\n+        img_inputs = self.processor(images=raw_image, return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            vision_embeds = self.model.get_vision_features(pixel_values=img_inputs.pixel_values)\n+\n+        # Run multiple text prompts efficiently\n+        text_prompts = [\"ear\", \"eye\"]\n+        all_results = []\n+\n+        for prompt in text_prompts:\n+            text_inputs = self.processor(text=prompt, return_tensors=\"pt\").to(torch_device)\n+            with torch.no_grad():\n+                outputs = self.model(vision_embeds=vision_embeds, **text_inputs)\n+\n+            results = self.processor.post_process_instance_segmentation(\n+                outputs,\n+                threshold=0.5,\n+                mask_threshold=0.5,\n+                target_sizes=img_inputs.get(\"original_sizes\").tolist(),\n+            )[0]\n+            all_results.append(results)\n+\n+        # Check that we get results for both prompts\n+        self.assertEqual(len(all_results), 2)\n+\n+        # Verify outputs are equivalent to running with pixel_values directly\n+        text_inputs = self.processor(text=\"ear\", return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            outputs_with_embeds = self.model(vision_embeds=vision_embeds, **text_inputs)\n+\n+        inputs_direct = self.processor(images=raw_image, text=\"ear\", return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            outputs_direct = self.model(**inputs_direct)\n+\n+        # Outputs should be identical\n+        torch.testing.assert_close(outputs_with_embeds.pred_logits, outputs_direct.pred_logits, atol=1e-5, rtol=1e-5)\n+        torch.testing.assert_close(outputs_with_embeds.pred_boxes, outputs_direct.pred_boxes, atol=1e-5, rtol=1e-5)\n+        torch.testing.assert_close(outputs_with_embeds.pred_masks, outputs_direct.pred_masks, atol=1e-5, rtol=1e-5)\n+\n+    def test_efficient_single_prompt_multi_images(self):\n+        \"\"\"Test efficient inference with same prompt on multiple images using get_text_features.\"\"\"\n+        raw_image1 = prepare_coco_cat_image()\n+        raw_image2 = prepare_coco_kitchen_image()\n+\n+        # Pre-compute text embeddings once\n+        text_prompt = \"handle\"\n+        text_inputs = self.processor(text=text_prompt, return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            text_embeds = self.model.get_text_features(**text_inputs)\n+\n+        # Run inference on multiple images reusing text embeddings\n+        # Note: attention_mask must be passed along with text_embeds for proper masking\n+        images = [raw_image1, raw_image2]\n+        all_results = []\n+\n+        for image in images:\n+            img_inputs = self.processor(images=image, return_tensors=\"pt\").to(torch_device)\n+            with torch.no_grad():\n+                outputs = self.model(\n+                    text_embeds=text_embeds,\n+                    attention_mask=text_inputs.attention_mask,\n+                    **img_inputs,\n+                )\n+\n+            results = self.processor.post_process_instance_segmentation(\n+                outputs,\n+                threshold=0.5,\n+                mask_threshold=0.5,\n+                target_sizes=img_inputs.get(\"original_sizes\").tolist(),\n+            )[0]\n+            all_results.append(results)\n+\n+        # Check that we get results for both images\n+        self.assertEqual(len(all_results), 2)\n+\n+        # Verify outputs are equivalent to running with input_ids directly\n+        img_inputs = self.processor(images=raw_image2, return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            outputs_with_embeds = self.model(\n+                text_embeds=text_embeds,\n+                attention_mask=text_inputs.attention_mask,\n+                **img_inputs,\n+            )\n+\n+        inputs_direct = self.processor(images=raw_image2, text=text_prompt, return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            outputs_direct = self.model(**inputs_direct)\n+\n+        # Outputs should be identical\n+        torch.testing.assert_close(outputs_with_embeds.pred_logits, outputs_direct.pred_logits, atol=1e-5, rtol=1e-5)\n+        torch.testing.assert_close(outputs_with_embeds.pred_boxes, outputs_direct.pred_boxes, atol=1e-5, rtol=1e-5)\n+        torch.testing.assert_close(outputs_with_embeds.pred_masks, outputs_direct.pred_masks, atol=1e-5, rtol=1e-5)"
        }
    ],
    "stats": {
        "total": 239,
        "additions": 216,
        "deletions": 23
    }
}