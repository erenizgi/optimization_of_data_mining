{
    "author": "dataKim1201",
    "message": "Enable customized optimizer for DeepSpeed (#32049)\n\n* transformers: enable custom optimizer for DeepSpeed\r\n\r\n* transformers: modify error message\r\n\r\n---------\r\n\r\nCo-authored-by: datakim1201 <roy.kim@maum.ai>",
    "sha": "55be7c4c483a01a7e03e55a8756fc4385ec08ffc",
    "files": [
        {
            "sha": "14d85f204deef55c47d4efcf1247b04e656ace02",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55be7c4c483a01a7e03e55a8756fc4385ec08ffc/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55be7c4c483a01a7e03e55a8756fc4385ec08ffc/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=55be7c4c483a01a7e03e55a8756fc4385ec08ffc",
            "patch": "@@ -599,11 +599,11 @@ def __init__(\n                     \" `Trainer`. Make sure the lines `import torch_xla.core.xla_model as xm` and\"\n                     \" `model.to(xm.xla_device())` is performed before the optimizer creation in your script.\"\n                 )\n-        if (self.is_deepspeed_enabled or self.is_fsdp_xla_enabled or self.is_fsdp_enabled) and (\n+        if (self.is_fsdp_xla_enabled or self.is_fsdp_enabled) and (\n             self.optimizer is not None or self.lr_scheduler is not None\n         ):\n             raise RuntimeError(\n-                \"Passing `optimizers` is not allowed if Deepspeed or PyTorch FSDP is enabled. \"\n+                \"Passing `optimizers` is not allowed if PyTorch FSDP is enabled. \"\n                 \"You should subclass `Trainer` and override the `create_optimizer_and_scheduler` method.\"\n             )\n         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}