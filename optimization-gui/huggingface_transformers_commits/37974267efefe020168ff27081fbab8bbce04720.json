{
    "author": "stevhliu",
    "message": "[docs] llama.cpp (#43185)\n\nllama.cpp",
    "sha": "37974267efefe020168ff27081fbab8bbce04720",
    "files": [
        {
            "sha": "8f3a0c13b2c7a3302a1ae2c67daee2cb8c2ff002",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/37974267efefe020168ff27081fbab8bbce04720/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/37974267efefe020168ff27081fbab8bbce04720/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=37974267efefe020168ff27081fbab8bbce04720",
            "patch": "@@ -228,12 +228,18 @@\n   title: Quantization\n - isExpanded: false\n   sections:\n-  - local: community_integrations/vllm\n-    title: vLLM\n-  - local: community_integrations/sglang\n-    title: SGLang\n-  - local: community_integrations/transformers_as_backend\n-    title: Building a compatible model backend for inference\n+  - sections:\n+    - local: community_integrations/vllm\n+      title: vLLM\n+    - local: community_integrations/sglang\n+      title: SGLang\n+    - local: community_integrations/transformers_as_backend\n+      title: Building a compatible model backend for inference\n+    title: Inference engines\n+  - sections:\n+    - local: community_integrations/llama_cpp\n+      title: llama.cpp\n+    title: Local deployment\n   title: Community integrations\n - isExpanded: false\n   sections:"
        },
        {
            "sha": "8728053cb4342ac0722145ffdefcff80717ab0d8",
            "filename": "docs/source/en/community_integrations/llama_cpp.md",
            "status": "added",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/37974267efefe020168ff27081fbab8bbce04720/docs%2Fsource%2Fen%2Fcommunity_integrations%2Fllama_cpp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37974267efefe020168ff27081fbab8bbce04720/docs%2Fsource%2Fen%2Fcommunity_integrations%2Fllama_cpp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcommunity_integrations%2Fllama_cpp.md?ref=37974267efefe020168ff27081fbab8bbce04720",
            "patch": "@@ -0,0 +1,60 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# llama.cpp\n+\n+[llama.cpp](https://github.com/ggml-org/llama.cpp) is a C/C++ inference engine for deploying large language models locally. It's lightweight and doesn't require Python, CUDA, or other heavy server infrastructure. llama.cpp uses the [GGUF](https://huggingface.co/blog/ngxson/common-ai-model-formats#gguf) file format. GGUF supports quantized model weights and memory-mapping to reduce memory bandwidth on your device.\n+\n+> [!TIP]\n+> Browse the [Hub](https://huggingface.co/models?apps=llama.cpp&sort=trending) for models already available in GGUF format.\n+\n+llama.cpp can convert and run Transformers models as standalone C++ executables with the [convert_hf_to_gguf.py](https://github.com/ggml-org/llama.cpp/blob/master/convert_hf_to_gguf.py) script.\n+\n+```bash\n+python3 convert_hf_to_gguf.py ./models/openai/gpt-oss-20b\n+```\n+\n+The conversion process works as follows.\n+\n+1. The script loads the model configuration with [`AutoConfig.from_pretrained`] and extracts the \n+vocabulary with [`AutoTokenizer.from_pretrained`].\n+2. Based on the config's architecture field, the script selects a converter class from its internal registry. The registry maps Transformers architecture names (like [`LlamaForCausalLM`]) to corresponding converter classes.\n+3. The converter class extracts config parameters, maps Transformers tensor names to GGUF tensor names, transforms tensors, and packages the vocabulary.\n+4. The output is a single GGUF file containing the model weights, tokenizer, and metadata.\n+\n+Deploy the model locally from the command line with [llama-cli](https://github.com/ggml-org/llama.cpp/tree/master#llama-cli) or start a web UI with [llama-server](https://github.com/ggml-org/llama.cpp/tree/master#llama-server). Add the `-hf` flag to indicate the model is from the Hub.\n+\n+<hfoptions id=\"deploy\">\n+<hfoption id=\"llama-cli\">\n+\n+```bash\n+llama-cli -hf openai/gpt-oss-20b\n+```\n+\n+</hfoption>\n+<hfoption id=\"llama-server\">\n+\n+```bash\n+llama-server -hf ggml-org/gpt-oss-20b-GGUF\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Resources\n+\n+- [llama.cpp](https://github.com/ggml-org/llama.cpp) documentation\n+- [Introduction to ggml](https://huggingface.co/blog/introduction-to-ggml) blog post\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 72,
        "deletions": 6
    }
}