{
    "author": "chrsmcgrr",
    "message": "fix(Wav2Vec2ForCTC): torch export (#34023)\n\n* fix(Wav2Vec2ForCTC): torch export\r\n\r\nResolves the issue described in #34022 by implementing the\r\nmasking of the hidden states using an elementwise multiplication\r\nrather than indexing with assignment.\r\n\r\nThe torch.export functionality seems to mark the tensor as frozen\r\neven though the update is legal.\r\n\r\nThis change is a workaround for now to allow the export of the\r\nmodel as a FxGraph. Further investigation is required to find\r\nthe real solution in pytorch.\r\n\r\n* [run-slow] hubert, unispeech, unispeech_sat, wav2vec2",
    "sha": "b57c7bce21799bcc964c0ab56002f27485be7a13",
    "files": [
        {
            "sha": "57f59cf9aab94fbae5f2b65b24f9f5711da7292c",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b57c7bce21799bcc964c0ab56002f27485be7a13/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b57c7bce21799bcc964c0ab56002f27485be7a13/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=b57c7bce21799bcc964c0ab56002f27485be7a13",
            "patch": "@@ -1040,7 +1040,7 @@ def forward(\n         if attention_mask is not None:\n             # make sure padded tokens are not attended to\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n-            hidden_states[~expand_attention_mask] = 0\n+            hidden_states = hidden_states * expand_attention_mask.to(dtype=hidden_states.dtype)\n             if self._use_flash_attention_2:\n                 # 2d mask is passed through the layers\n                 attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None"
        },
        {
            "sha": "52ba08f5d4eda520d6ede836fade0d6b86e5cc6d",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b57c7bce21799bcc964c0ab56002f27485be7a13/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b57c7bce21799bcc964c0ab56002f27485be7a13/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=b57c7bce21799bcc964c0ab56002f27485be7a13",
            "patch": "@@ -1076,7 +1076,7 @@ def forward(\n         if attention_mask is not None:\n             # make sure padded tokens are not attended to\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n-            hidden_states[~expand_attention_mask] = 0\n+            hidden_states = hidden_states * expand_attention_mask.to(dtype=hidden_states.dtype)\n             if self._use_flash_attention_2:\n                 # 2d mask is passed through the layers\n                 attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None"
        },
        {
            "sha": "52d82ea739426b1a2279032b3b9824e09ca2ab89",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b57c7bce21799bcc964c0ab56002f27485be7a13/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b57c7bce21799bcc964c0ab56002f27485be7a13/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=b57c7bce21799bcc964c0ab56002f27485be7a13",
            "patch": "@@ -1093,7 +1093,7 @@ def forward(\n         if attention_mask is not None:\n             # make sure padded tokens are not attended to\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n-            hidden_states[~expand_attention_mask] = 0\n+            hidden_states = hidden_states * expand_attention_mask.to(dtype=hidden_states.dtype)\n             if self._use_flash_attention_2:\n                 # 2d mask is passed through the layers\n                 attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None"
        },
        {
            "sha": "bf1bb7746ce8021fe22cfe64d341da569e85a49f",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b57c7bce21799bcc964c0ab56002f27485be7a13/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b57c7bce21799bcc964c0ab56002f27485be7a13/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=b57c7bce21799bcc964c0ab56002f27485be7a13",
            "patch": "@@ -1109,7 +1109,7 @@ def forward(\n         if attention_mask is not None:\n             # make sure padded tokens are not attended to\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n-            hidden_states[~expand_attention_mask] = 0\n+            hidden_states = hidden_states * expand_attention_mask.to(dtype=hidden_states.dtype)\n             if self._use_flash_attention_2:\n                 # 2d mask is passed through the layers\n                 attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}