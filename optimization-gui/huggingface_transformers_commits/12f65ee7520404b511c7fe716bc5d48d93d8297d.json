{
    "author": "yao-matrix",
    "message": "enable cpu offloading for Bark on xpu (#37599)\n\n* enable cpu offloading of bark modeling on XPU\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* remove debug print\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix review comments\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* enhance test\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* update\n\n* add deprecate message\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* update\n\n* update\n\n* trigger CI\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "12f65ee7520404b511c7fe716bc5d48d93d8297d",
    "files": [
        {
            "sha": "57a0c4e5a7a53f6af0ae7018e3da5b5f6cf8e121",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 28,
            "deletions": 7,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/12f65ee7520404b511c7fe716bc5d48d93d8297d/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12f65ee7520404b511c7fe716bc5d48d93d8297d/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=12f65ee7520404b511c7fe716bc5d48d93d8297d",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"PyTorch BARK model.\"\"\"\n \n import math\n+import warnings\n from typing import Dict, Optional, Tuple, Union\n \n import numpy as np\n@@ -36,6 +37,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_accelerate_available,\n+    is_torch_accelerator_available,\n     logging,\n )\n from ..auto import AutoModel\n@@ -1598,26 +1600,45 @@ def device(self) -> torch.device:\n             ):\n                 return torch.device(module._hf_hook.execution_device)\n \n-    def enable_cpu_offload(self, gpu_id: Optional[int] = 0):\n+    def enable_cpu_offload(\n+        self,\n+        accelerator_id: Optional[int] = 0,\n+        **kwargs,\n+    ):\n         r\"\"\"\n         Offloads all sub-models to CPU using accelerate, reducing memory usage with a low impact on performance. This\n-        method moves one whole sub-model at a time to the GPU when it is used, and the sub-model remains in GPU until\n-        the next sub-model runs.\n+        method moves one whole sub-model at a time to the accelerator when it is used, and the sub-model remains in accelerator until the next sub-model runs.\n \n         Args:\n-            gpu_id (`int`, *optional*, defaults to 0):\n-                GPU id on which the sub-models will be loaded and offloaded.\n+            accelerator_id (`int`, *optional*, defaults to 0):\n+                accelerator id on which the sub-models will be loaded and offloaded. This argument is deprecated.\n+            kwargs (`dict`, *optional*):\n+                additional keyword arguments:\n+                    `gpu_id`: accelerator id on which the sub-models will be loaded and offloaded.\n         \"\"\"\n         if is_accelerate_available():\n             from accelerate import cpu_offload_with_hook\n         else:\n             raise ImportError(\"`enable_model_cpu_offload` requires `accelerate`.\")\n \n-        device = torch.device(f\"cuda:{gpu_id}\")\n+        gpu_id = kwargs.get(\"gpu_id\", 0)\n+\n+        if gpu_id != 0:\n+            warnings.warn(\n+                \"The argument `gpu_id` is deprecated and will be removed in version 4.54.0 of Transformers. Please use `accelerator_id` instead.\",\n+                FutureWarning,\n+            )\n+            accelerator_id = gpu_id\n+\n+        device_type = \"cuda\"\n+        if is_torch_accelerator_available():\n+            device_type = torch.accelerator.current_accelerator().type\n+        device = torch.device(f\"{device_type}:{accelerator_id}\")\n \n+        torch_accelerator_module = getattr(torch, device_type)\n         if self.device.type != \"cpu\":\n             self.to(\"cpu\")\n-            torch.cuda.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n+            torch_accelerator_module.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n \n         # this layer is used outside the first foward pass of semantic so need to be loaded before semantic\n         self.semantic.input_embeds_layer, _ = cpu_offload_with_hook(self.semantic.input_embeds_layer, device)"
        },
        {
            "sha": "0b3e75cc5ab643265534d3a32dc923f63c3eb08f",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/12f65ee7520404b511c7fe716bc5d48d93d8297d/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12f65ee7520404b511c7fe716bc5d48d93d8297d/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=12f65ee7520404b511c7fe716bc5d48d93d8297d",
            "patch": "@@ -211,6 +211,7 @@\n     is_tiktoken_available,\n     is_timm_available,\n     is_tokenizers_available,\n+    is_torch_accelerator_available,\n     is_torch_available,\n     is_torch_bf16_available,\n     is_torch_bf16_available_on_device,"
        },
        {
            "sha": "2eac33b7a2961b05f626bd429d8b50179be8a406",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 5,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/12f65ee7520404b511c7fe716bc5d48d93d8297d/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12f65ee7520404b511c7fe716bc5d48d93d8297d/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=12f65ee7520404b511c7fe716bc5d48d93d8297d",
            "patch": "@@ -346,16 +346,28 @@ def is_accelerate_available(min_version: str = ACCELERATE_MIN_VERSION):\n     return _accelerate_available and version.parse(_accelerate_version) >= version.parse(min_version)\n \n \n+def is_torch_accelerator_available():\n+    if is_torch_available():\n+        import torch\n+\n+        return hasattr(torch, \"accelerator\")\n+\n+    return False\n+\n+\n def is_torch_deterministic():\n     \"\"\"\n     Check whether pytorch uses deterministic algorithms by looking if torch.set_deterministic_debug_mode() is set to 1 or 2\"\n     \"\"\"\n-    import torch\n+    if is_torch_available():\n+        import torch\n \n-    if torch.get_deterministic_debug_mode() == 0:\n-        return False\n-    else:\n-        return True\n+        if torch.get_deterministic_debug_mode() == 0:\n+            return False\n+        else:\n+            return True\n+\n+    return False\n \n \n def is_hadamard_available():"
        },
        {
            "sha": "1f7ff37e7ded3b02302b6e05553929740267fd62",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/12f65ee7520404b511c7fe716bc5d48d93d8297d/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12f65ee7520404b511c7fe716bc5d48d93d8297d/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=12f65ee7520404b511c7fe716bc5d48d93d8297d",
            "patch": "@@ -36,6 +36,7 @@\n from transformers.testing_utils import (\n     require_flash_attn,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_fp16,\n     require_torch_gpu,\n     slow,\n@@ -1056,7 +1057,8 @@ def processor(self):\n     def inputs(self):\n         input_ids = self.processor(\"In the light of the moon, a little egg lay on a leaf\", voice_preset=\"en_speaker_6\")\n \n-        input_ids = input_ids.to(torch_device)\n+        for k, v in input_ids.items():\n+            input_ids[k] = v.to(torch_device)\n \n         return input_ids\n \n@@ -1295,7 +1297,7 @@ def test_generate_end_to_end_with_sub_models_args(self):\n             len(output_ids_with_min_eos_p[0, :].tolist()), len(output_ids_without_min_eos_p[0, :].tolist())\n         )\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_generate_end_to_end_with_offload(self):\n         input_ids = self.inputs\n@@ -1304,15 +1306,17 @@ def test_generate_end_to_end_with_offload(self):\n             # standard generation\n             output_with_no_offload = self.model.generate(**input_ids, do_sample=False, temperature=1.0)\n \n-            torch.cuda.empty_cache()\n+            torch_accelerator_module = getattr(torch, torch_device, torch.cuda)\n+\n+            torch_accelerator_module.empty_cache()\n \n-            memory_before_offload = torch.cuda.memory_allocated()\n+            memory_before_offload = torch_accelerator_module.memory_allocated()\n             model_memory_footprint = self.model.get_memory_footprint()\n \n             # activate cpu offload\n             self.model.enable_cpu_offload()\n \n-            memory_after_offload = torch.cuda.memory_allocated()\n+            memory_after_offload = torch_accelerator_module.memory_allocated()\n \n             # checks if the model have been offloaded\n "
        }
    ],
    "stats": {
        "total": 72,
        "additions": 55,
        "deletions": 17
    }
}