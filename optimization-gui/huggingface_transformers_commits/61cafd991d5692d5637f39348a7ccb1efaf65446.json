{
    "author": "BenjaminBossan",
    "message": "ENH: Add support for LoRA hotswapping (#41297)\n\nLoRA hotswapping has been available in PEFT since 0.15.0. There is\nalready a diffusers\nintegration (https://github.com/huggingface/diffusers/pull/9453), but\nthe transformers integration was still missing this feature. This PR\nremedies this.\n\nHotswapping allows to swap different LoRA adapters in-place instead of\nloading multiple adapters and switchint between them. Not only can this\nbe advantageous to safe memory and potentially for quicker loading, the\nbiggest advantage is that if the model is compiled, we can hotswap\nwithout triggering recompilation (loading a separate adapter would\nrequire recompilation).\n\nThere are some caveats to using this feature, most notably that only\nLoRA is supported. This was fine for diffusers, as it only works with\nLoRA, but the transformers integration works with other PEFT methods\ntoo. However, LoRA should be by far the most common method, so this\nshould be fine for now. This and other caveats have been documented.\n\nTo make the usage more intuitive, hotswap is now auto-enabled after\ncalling model.enable_peft_hotswap(). For this, we detect if\nenable_peft_hotswap() was called *and* if the adapter being loaded\nis *not* the first adapter (because the first adapter cannot be\nhotswapped, it needs to be loaded normally).",
    "sha": "61cafd991d5692d5637f39348a7ccb1efaf65446",
    "files": [
        {
            "sha": "6b963aeb4bd943bf81bedf7d971e055e51988f5a",
            "filename": "docs/source/en/peft.md",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/61cafd991d5692d5637f39348a7ccb1efaf65446/docs%2Fsource%2Fen%2Fpeft.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/61cafd991d5692d5637f39348a7ccb1efaf65446/docs%2Fsource%2Fen%2Fpeft.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpeft.md?ref=61cafd991d5692d5637f39348a7ccb1efaf65446",
            "patch": "@@ -151,3 +151,49 @@ model.enable_adapters()\n # disable all adapters\n model.disable_adapters()\n ```\n+\n+## Hotswapping adapters\n+\n+A common use case when serving multiple adapters is to load one adapter first, generate output, load another adapter, generate more outputs, load another adapter, etc. This can be inefficient, since each time a new adapter is loaded, new memory is reserved; moreover, if the model is compiled with `torch.compile`, it needs to be re-compiled each time a new adapter is used. When switching frequently, the compilation time may never be amortized.\n+\n+To better support this common workflow, you can \"hotswap\" a LoRA adapter, to avoid accumulating memory and, in some cases, recompilation. It requires an adapter to already be loaded, and the new adapter weights are swapped in-place for the existing adapter. Note that other PEFT methods are not supported yet, only LoRA.\n+\n+Pass `hotswap=True` when loading a LoRA adapter to enable this feature. It is important to indicate the name of the existing adapter (`\"default\"` is the default adapter name) to be swapped.\n+\n+```python\n+model = AutoModel.from_pretrained(...)\n+# load adapter 1 as normal\n+model.load_adapter(file_name_adapter_1)\n+# generate outputs with adapter 1\n+...\n+# now hotswap the 2nd adapter\n+model.load_adapter(file_name_adapter_2, hotswap=True, adapter_name=\"default\")\n+# generate outputs with adapter 2\n+```\n+\n+For compiled models, it is often necessary to call [`~integrations.peft.PeftAdapterMixin.enable_peft_hotswap`] to avoid recompilation. Call this method _before_ loading the first adapter, while `torch.compile` should be called _after_ loading the first adapter.\n+\n+```python\n+model = AutoModel.from_pretrained(...)\n+max_rank = ...  # the highest rank among all LoRAs that you want to load\n+# call *before* compiling and loading the LoRA adapter\n+model.enable_peft_hotswap(target_rank=max_rank)\n+model.load_adapter(file_name_1, adapter_name=\"default\")\n+# optionally compile the model now\n+model = torch.compile(model, ...)\n+output_1 = model(...)\n+# now you can hotswap the 2nd adapter, use the same name as for the 1st\n+model.load_adapter(file_name_2, adapter_name=\"default\")\n+output_2 = model(...)\n+```\n+\n+The `target_rank=max_rank` argument is important for setting the maximum rank among all LoRA adapters that will be loaded. If you have one adapter with rank 8 and another with rank 16, pass `target_rank=16`. You should use a higher value if in doubt. By default, this value is 128.\n+\n+By default, hotswapping is disabled and requires you to pass `hotswap=True` to `load_adapter`. However, if you called `enable_peft_hotswap` first, hotswapping will be enabled by default. If you want to avoid using it, you need to pass `hotswap=False`.\n+\n+However, there can be situations where recompilation is unavoidable. For example, if the hotswapped adapter targets more layers than the initial adapter, then recompilation is triggered. Try to load the adapter that targets the most layers first. Refer to the PEFT docs on [hotswapping](https://huggingface.co/docs/peft/main/en/package_reference/hotswap#peft.utils.hotswap.hotswap_adapter) for more details about the limitations of this feature.\n+\n+> [!Tip]\n+> Move your code inside the `with torch._dynamo.config.patch(error_on_recompile=True)` context manager to detect if a model was recompiled. If you detect recompilation despite following all the steps above, please open an issue with [PEFT](https://github.com/huggingface/peft/issues) with a reproducible example.\n+\n+For an example of how the use of `torch.compile` in combination with hotswapping can improve runtime, check out [this blogpost](https://huggingface.co/blog/lora-fast). Although that example uses Diffusers, similar improvements can be expected here."
        },
        {
            "sha": "5290fc6210c25523655cd0bda1822346dfbe5ada",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 147,
            "deletions": 8,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/61cafd991d5692d5637f39348a7ccb1efaf65446/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61cafd991d5692d5637f39348a7ccb1efaf65446/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=61cafd991d5692d5637f39348a7ccb1efaf65446",
            "patch": "@@ -17,7 +17,7 @@\n import json\n import os\n import re\n-from typing import Any, Optional, Union\n+from typing import Any, Literal, Optional, Union\n \n from packaging import version\n \n@@ -89,6 +89,7 @@ class PeftAdapterMixin:\n     \"\"\"\n \n     _hf_peft_config_loaded = False\n+    _prepare_peft_hotswap_kwargs: Optional[dict] = None\n \n     def load_adapter(\n         self,\n@@ -104,6 +105,7 @@ def load_adapter(\n         adapter_state_dict: Optional[dict[str, \"torch.Tensor\"]] = None,\n         low_cpu_mem_usage: bool = False,\n         is_trainable: bool = False,\n+        hotswap: bool | Literal[\"auto\"] = \"auto\",\n         adapter_kwargs: Optional[dict[str, Any]] = None,\n     ) -> None:\n         \"\"\"\n@@ -159,12 +161,63 @@ def load_adapter(\n             is_trainable (`bool`, *optional*, defaults to `False`):\n                 Whether the adapter should be trainable or not. If `False`, the adapter will be frozen and can only be\n                 used for inference.\n+            hotswap : (`\"auto\"` or `bool`, *optional*, defaults to `\"auto\"`)\n+                Whether to substitute an existing (LoRA) adapter with the newly loaded adapter in-place. This means\n+                that, instead of loading an additional adapter, this will take the existing adapter weights and replace\n+                them with the weights of the new adapter. This can be faster and more memory efficient. However, the\n+                main advantage of hotswapping is that when the model is compiled with torch.compile, loading the new\n+                adapter does not require recompilation of the model. When using hotswapping, the passed `adapter_name`\n+                should be the name of an already loaded adapter.\n+\n+                If the new adapter and the old adapter have different ranks and/or LoRA alphas (i.e. scaling), you need\n+                to call an additional method before loading the adapter:\n+\n+                ```py\n+                model = AutoModel.from_pretrained(...)\n+                max_rank = ...  # the highest rank among all LoRAs that you want to load\n+                # call *before* compiling and loading the LoRA adapter\n+                model.enable_peft_hotswap(target_rank=max_rank)\n+                model.load_adapter(file_name_1, adapter_name=\"default\")\n+                # optionally compile the model now\n+                model = torch.compile(model, ...)\n+                output_1 = model(...)\n+                # now you can hotswap the 2nd adapter, use the same name as for the 1st\n+                # hotswap is activated by default since enable_peft_hotswap was called\n+                model.load_adapter(file_name_2, adapter_name=\"default\")\n+                output_2 = model(...)\n+                ```\n+\n+                By default, hotswap is disabled and requires passing `hotswap=True`. If you called\n+                `enable_peft_hotswap` first, it is enabled. You can still manually disable it in that case by passing\n+                `hotswap=False`.\n+\n+                Note that hotswapping comes with a couple of limitations documented here:\n+                https://huggingface.co/docs/peft/main/en/package_reference/hotswap\n             adapter_kwargs (`dict[str, Any]`, *optional*):\n                 Additional keyword arguments passed along to the `from_pretrained` method of the adapter config and\n                 `find_adapter_config_file` method.\n         \"\"\"\n         check_peft_version(min_version=MIN_PEFT_VERSION)\n \n+        from peft import PeftType\n+\n+        if hotswap == \"auto\":\n+            # if user called model.enable_peft_hotswap and this is not the first adapter, enable hotswap\n+            hotswap_enabled = getattr(self, \"_hotswap_enabled\", False)\n+            not_first_adapter = bool(self._hf_peft_config_loaded and (adapter_name in self.peft_config))\n+            hotswap = hotswap_enabled and not_first_adapter\n+\n+        if hotswap:\n+            min_version_hotswap = \"0.15.0\"\n+            if version.parse(importlib.metadata.version(\"peft\")) < version.parse(min_version_hotswap):\n+                raise ValueError(f\"To hotswap the adapter, you need PEFT >= v{min_version_hotswap}.\")\n+            if (not self._hf_peft_config_loaded) or (adapter_name not in self.peft_config):\n+                raise ValueError(\n+                    \"To hotswap an adapter, there must already be an existing adapter with the same adapter name.\"\n+                )\n+            if any(conf.peft_type != PeftType.LORA for conf in self.peft_config.values()):\n+                raise ValueError(\"Hotswapping is currently only supported for LoRA, please set `hotswap=False`.\")\n+\n         # peft only supports low_cpu_mem_usage starting from v0.13.0\n         peft_load_kwargs = {}\n         key_mapping = adapter_kwargs.pop(\"key_mapping\", None) if adapter_kwargs is not None else None\n@@ -187,8 +240,12 @@ def load_adapter(\n         from peft import PeftConfig, inject_adapter_in_model, load_peft_weights\n         from peft.utils import set_peft_model_state_dict\n \n-        if self._hf_peft_config_loaded and adapter_name in self.peft_config:\n+        if self._hf_peft_config_loaded and (not hotswap) and (adapter_name in self.peft_config):\n             raise ValueError(f\"Adapter with name {adapter_name} already exists. Please use a different name.\")\n+        elif hotswap and ((not self._hf_peft_config_loaded) or (adapter_name not in self.peft_config)):\n+            raise ValueError(\n+                \"To hotswap an adapter, there must already be an existing adapter with the same adapter name.\"\n+            )\n \n         if peft_model_id is None and (adapter_state_dict is None and peft_config is None):\n             raise ValueError(\n@@ -236,9 +293,14 @@ def load_adapter(\n                 **adapter_kwargs,\n             )\n             peft_config.inference_mode = not is_trainable\n-        # TODO: WE NEED TOO APPLY OUR DYNAMIC WEIGHT CONVERSION AT SOME POINT HERE!\n-        # Create and add fresh new adapters into the model.\n-        inject_adapter_in_model(peft_config, self, adapter_name, **peft_load_kwargs)\n+\n+        if peft_config.peft_type != PeftType.LORA:\n+            raise ValueError(\"Hotswapping is currently only supported for LoRA, please set `hotswap=False`.\")\n+\n+        if not hotswap:\n+            # TODO: WE NEED TOO APPLY OUR DYNAMIC WEIGHT CONVERSION AT SOME POINT HERE!\n+            # Create and add fresh new adapters into the model, unless the weights are hotswapped\n+            inject_adapter_in_model(peft_config, self, adapter_name, **peft_load_kwargs)\n \n         if not self._hf_peft_config_loaded:\n             self._hf_peft_config_loaded = True\n@@ -261,12 +323,47 @@ def load_adapter(\n                     # Early exit of the loop\n                     if n_replace > 0:\n                         break\n+\n+            # For hotswapping, we need the adapter name to be present in the state dict keys\n+            if hotswap:\n+                if key.endswith(\"lora_A.weight\") or key.endswith(\"lora_B.weight\"):\n+                    new_key = new_key[: -len(\".weight\")] + f\".{adapter_name}.weight\"\n+                elif key.endswith(\"lora_B.bias\"):  # lora_bias=True option\n+                    new_key = new_key[: -len(\".bias\")] + f\".{adapter_name}.bias\"\n             processed_adapter_state_dict[new_key] = value\n \n         # Load state dict\n-        incompatible_keys = set_peft_model_state_dict(\n-            self, processed_adapter_state_dict, adapter_name, **peft_load_kwargs\n-        )\n+        if not hotswap:\n+            incompatible_keys = set_peft_model_state_dict(\n+                self, processed_adapter_state_dict, adapter_name, **peft_load_kwargs\n+            )\n+\n+            if self._prepare_peft_hotswap_kwargs is not None:\n+                # For hotswapping of compiled models or adapters with different ranks.\n+                # If the user called enable_peft_hotswap, we need to ensure it is called:\n+                # - after the first adapter was loaded\n+                # - before the model is compiled and the 2nd adapter is being hotswapped in\n+                # Therefore, it needs to be called here\n+                from peft.utils.hotswap import prepare_model_for_compiled_hotswap\n+\n+                prepare_model_for_compiled_hotswap(self, config=peft_config, **self._prepare_peft_hotswap_kwargs)\n+                # We only want to call prepare_model_for_compiled_hotswap once\n+                self._prepare_peft_hotswap_kwargs = None\n+        else:\n+            from peft.utils.hotswap import check_hotswap_configs_compatible, hotswap_adapter_from_state_dict\n+\n+            check_hotswap_configs_compatible(self.peft_config[adapter_name], peft_config)\n+            try:\n+                hotswap_adapter_from_state_dict(\n+                    model=self,\n+                    state_dict=processed_adapter_state_dict,\n+                    adapter_name=adapter_name,\n+                    config=peft_config,\n+                )\n+            except Exception as e:\n+                logger.error(f\"Hotswapping {adapter_name} was unsucessful with the following error: \\n{e}\")\n+                raise\n+            incompatible_keys = None\n \n         if incompatible_keys is not None:\n             err_msg = \"\"\n@@ -308,6 +405,47 @@ def load_adapter(\n                 offload_index=offload_index,\n             )\n \n+    def enable_peft_hotswap(\n+        self, target_rank: int = 128, check_compiled: Literal[\"error\", \"warn\", \"ignore\"] = \"error\"\n+    ) -> None:\n+        \"\"\"Enables the possibility to hotswap PEFT adapters with different ranks, or, if the model is compiled, without\n+        triggering recompilation.\n+\n+        Right now, hotswapping is only supported for LoRA.\n+\n+        Calling this method is only required when hotswapping adapters and if the model is compiled or if the ranks of\n+        the loaded adapters differ. If the ranks are all identical and the model is not compiled, hotswapping works\n+        without calling this method first.\n+\n+        Args:\n+            target_rank (`int`, *optional*, defaults to `128`):\n+                The highest rank among all the adapters that will be loaded.\n+            check_compiled (`str`, *optional*, defaults to `\"error\"`):\n+                How to handle the case when the model is already compiled, which should generally be avoided. The\n+                options are:\n+                  - \"error\" (default): raise an error\n+                  - \"warn\": issue a warning\n+                  - \"ignore\": do nothing\n+        \"\"\"\n+        min_version_hotswap = \"0.15.0\"\n+        if version.parse(importlib.metadata.version(\"peft\")) < version.parse(min_version_hotswap):\n+            raise ValueError(f\"To hotswap the adapter, you need PEFT >= v{min_version_hotswap}.\")\n+\n+        if getattr(self, \"peft_config\", {}):\n+            if check_compiled == \"error\":\n+                raise RuntimeError(\"Call `enable_peft_hotswap` before loading the first adapter.\")\n+            elif check_compiled == \"warn\":\n+                logger.warning(\n+                    \"It is recommended to call `enable_peft_hotswap` before loading the first adapter to avoid recompilation.\"\n+                )\n+            elif check_compiled != \"ignore\":\n+                raise ValueError(\n+                    f\"check_compiles should be one of 'error', 'warn', or 'ignore', got '{check_compiled}' instead.\"\n+                )\n+\n+        self._hotswap_enabled = True\n+        self._prepare_peft_hotswap_kwargs = {\"target_rank\": target_rank, \"check_compiled\": check_compiled}\n+\n     def add_adapter(self, adapter_config, adapter_name: Optional[str] = None) -> None:\n         r\"\"\"\n         If you are not familiar with adapters and PEFT methods, we invite you to read more about them on the PEFT\n@@ -343,6 +481,7 @@ def add_adapter(self, adapter_config, adapter_name: Optional[str] = None) -> Non\n         # Retrieve the name or path of the model, one could also use self.config._name_or_path\n         # but to be consistent with what we do in PEFT: https://github.com/huggingface/peft/blob/6e783780ca9df3a623992cc4d1d665001232eae0/src/peft/mapping.py#L100\n         adapter_config.base_model_name_or_path = self.__dict__.get(\"name_or_path\", None)\n+        # TODO: WE NEED TOO APPLY OUR DYNAMIC WEIGHT CONVERSION AT SOME POINT HERE!\n         inject_adapter_in_model(adapter_config, self, adapter_name)\n \n         self.set_adapter(adapter_name)"
        },
        {
            "sha": "b0fb65f56fd9925a8a8d623bb4481260687dfd3f",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 189,
            "deletions": 0,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/61cafd991d5692d5637f39348a7ccb1efaf65446/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61cafd991d5692d5637f39348a7ccb1efaf65446/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=61cafd991d5692d5637f39348a7ccb1efaf65446",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import gc\n import importlib\n import os\n import re\n@@ -941,3 +942,191 @@ def test_peft_pipeline_no_warning(self):\n \n             # Generate text to verify pipeline works\n             _ = lora_generator(text, max_new_tokens=20)\n+\n+\n+@require_peft\n+@require_torch\n+@slow\n+class PeftHotswapIntegrationTest(unittest.TestCase):\n+    def tearDown(self):\n+        # It is critical that the dynamo cache is reset for each test. Otherwise, if the test re-uses the same model,\n+        # there will be recompilation errors, as torch caches the model when run in the same process.\n+        torch.compiler.reset()\n+        gc.collect()\n+\n+    def _check_model_hotswap(self, *, rank1, rank2, do_compile):\n+        # utility method that checks that we can successfully hotswap adapters, with the model outputs corresponding to\n+        # the respective adapters\n+        from peft import LoraConfig\n+\n+        torch.manual_seed(0)\n+        model_id = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n+        input = torch.randint(0, 100, (1, 10)).to(torch_device)\n+        with torch.inference_mode():\n+            base_output = model(input).logits\n+\n+        # create 2 adapters\n+        model.add_adapter(LoraConfig(r=rank1, init_lora_weights=False), adapter_name=\"adapter_1\")\n+        with torch.inference_mode():\n+            lora_1_output = model(input).logits\n+\n+        # second adapter may have a different rank\n+        model.add_adapter(LoraConfig(r=rank2, init_lora_weights=False), adapter_name=\"adapter_2\")\n+        model.set_adapter(\"adapter_2\")\n+        with torch.inference_mode():\n+            lora_2_output = model(input).logits\n+\n+        # sanity checks\n+        self.assertFalse(torch.allclose(base_output, lora_1_output, atol=1e-6, rtol=1e-6))\n+        self.assertFalse(torch.allclose(base_output, lora_2_output, atol=1e-6, rtol=1e-6))\n+        self.assertFalse(torch.allclose(lora_1_output, lora_2_output, atol=1e-6, rtol=1e-6))\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            path_1 = os.path.join(tmpdirname, \"adapter_1\")\n+            path_2 = os.path.join(tmpdirname, \"adapter_2\")\n+            model.set_adapter(\"adapter_1\")\n+            model.save_pretrained(path_1)\n+            model.set_adapter(\"adapter_2\")\n+            model.save_pretrained(path_2)\n+            del model\n+\n+            model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n+            enable_hotswap = do_compile or (rank1 != rank2)\n+            if enable_hotswap:\n+                # calling this is only needed if we want to compile the model or if the ranks are different\n+                model.enable_peft_hotswap(target_rank=max(rank1, rank2))\n+\n+            # load the first adapter without hotswap (hotswap requires an existing adapter)\n+            model.load_adapter(path_1, adapter_name=\"adapter_1\")\n+            if do_compile:\n+                # compile the model after loading the first adapter\n+                model = torch.compile(model, mode=\"reduce-overhead\")\n+\n+            with torch.inference_mode():\n+                lora_1_output_loaded = model(input).logits\n+            self.assertTrue(torch.allclose(lora_1_output, lora_1_output_loaded, atol=1e-6, rtol=1e-6))\n+\n+            # hotswap in adapter_2 again, output should be same as lora_2_output\n+            if enable_hotswap:\n+                # after calling enable_peft_hotswap, hotswap will automatically be enabled\n+                model.load_adapter(path_2, adapter_name=\"adapter_1\")\n+            else:\n+                # enable_peft_hotswap was not called, need to explicitly pass hotswap=True\n+                model.load_adapter(path_2, adapter_name=\"adapter_1\", hotswap=True)\n+\n+            with torch.inference_mode():\n+                lora_2_output_loaded = model(input).logits\n+            self.assertTrue(torch.allclose(lora_2_output, lora_2_output_loaded, atol=1e-6, rtol=1e-6))\n+\n+    def test_hotswap_wrong_peft_type_raises(self):\n+        # only LoRA is supported for now\n+        from peft import IA3Config\n+\n+        model_id = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n+        peft_id = \"peft-internal-testing/tiny-OPTForCausalLM-lora\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n+        peft_config = IA3Config(feedforward_modules=[])\n+        model.add_adapter(peft_config, adapter_name=\"ia3\")\n+\n+        msg = \"Hotswapping is currently only supported for LoRA\"\n+        with self.assertRaisesRegex(ValueError, msg):\n+            model.load_adapter(peft_id, adapter_name=\"ia3\", hotswap=True)\n+\n+    def test_hotswap_without_existing_adapter_raises(self):\n+        # we can only hotswap if there is already an adapter with the same name\n+        model_id = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n+        peft_id = \"peft-internal-testing/tiny-OPTForCausalLM-lora\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n+\n+        msg = \"To hotswap an adapter, there must already be an existing adapter with the same adapter name\"\n+        with self.assertRaisesRegex(ValueError, msg):\n+            model.load_adapter(peft_id, adapter_name=\"adapter_1\", hotswap=True)\n+\n+    def test_hotswap_different_adapter_name_raises(self):\n+        # we can only hotswap if there is already an adapter with the same name\n+        model_id = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n+        peft_id = \"peft-internal-testing/tiny-OPTForCausalLM-lora\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n+        model.load_adapter(peft_id, adapter_name=\"adapter_1\")\n+\n+        other_name = \"does_not_exist_yet\"\n+        msg = \"To hotswap an adapter, there must already be an existing adapter with the same adapter name\"\n+        with self.assertRaisesRegex(ValueError, msg):\n+            model.load_adapter(peft_id, adapter_name=other_name, hotswap=True)\n+\n+    def test_enable_peft_hotswap_called_after_adapter_added_raises(self):\n+        # ensure that when enable_peft_hotswap is called *after* loading the first adapter, an error is raised\n+        from peft import LoraConfig\n+\n+        model_id = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n+        lora_config = LoraConfig()\n+        model.add_adapter(lora_config)\n+        msg = re.escape(\"Call `enable_peft_hotswap` before loading the first adapter.\")\n+\n+        with self.assertRaisesRegex(RuntimeError, msg):\n+            model.enable_peft_hotswap(target_rank=32)\n+\n+    def test_enable_peft_hotswap_called_after_adapter_added_warns(self):\n+        # ensure that when enable_peft_hotswap is called *after* loading the first adapter, there is a warning if\n+        # check_compiled=\"warn\"\n+        from peft import LoraConfig\n+\n+        logger = logging.get_logger(\"transformers.integrations.peft\")\n+        model_id = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n+        lora_config = LoraConfig()\n+        model.add_adapter(lora_config)\n+        msg = \"It is recommended to call `enable_peft_hotswap` before loading the first adapter to avoid recompilation\"\n+\n+        with self.assertLogs(logger=logger, level=\"WARNING\") as cm:\n+            model.enable_peft_hotswap(target_rank=32, check_compiled=\"warn\")\n+            assert any(msg in log for log in cm.output)\n+\n+    def test_enable_peft_hotswap_called_after_adapter_added_ignored(self):\n+        # Ensure that when enable_peft_hotswap is called *after* loading the first adapter, there is no error or\n+        # warning if check_compiled=\"ignore\". Note that assertNoLogs only works with Python 3.10+.\n+        from peft import LoraConfig\n+\n+        logger = logging.get_logger(\"transformers.integrations.peft\")\n+        model_id = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n+        model = AutoModelForCausalLM.from_pretrained(model_id).to(torch_device)\n+        lora_config = LoraConfig()\n+        model.add_adapter(lora_config)\n+\n+        with self.assertNoLogs(logger, level=\"WARNING\"):\n+            model.enable_peft_hotswap(target_rank=32, check_compiled=\"ignore\")\n+\n+    def test_hotswap_without_compile_and_same_ranks_works(self):\n+        self._check_model_hotswap(rank1=8, rank2=8, do_compile=False)\n+\n+    def test_hotswap_without_compile_and_with_lower_rank_works(self):\n+        self._check_model_hotswap(rank1=13, rank2=7, do_compile=False)\n+\n+    def test_hotswap_without_compile_and_with_higher_rank_works(self):\n+        self._check_model_hotswap(rank1=7, rank2=13, do_compile=False)\n+\n+    def test_hotswap_with_compile_and_same_ranks_works(self):\n+        # It's important to add this context to raise an error on recompilation\n+        with (\n+            torch._dynamo.config.patch(error_on_recompile=True),\n+            torch._inductor.utils.fresh_inductor_cache(),\n+        ):\n+            self._check_model_hotswap(rank1=8, rank2=8, do_compile=True)\n+\n+    def test_hotswap_with_compile_and_lower_rank_works(self):\n+        # It's important to add this context to raise an error on recompilation\n+        with (\n+            torch._dynamo.config.patch(error_on_recompile=True),\n+            torch._inductor.utils.fresh_inductor_cache(),\n+        ):\n+            self._check_model_hotswap(rank1=13, rank2=7, do_compile=True)\n+\n+    def test_hotswap_with_compile_and_higher_rank_works(self):\n+        # It's important to add this context to raise an error on recompilation\n+        with (\n+            torch._dynamo.config.patch(error_on_recompile=True),\n+            torch._inductor.utils.fresh_inductor_cache(),\n+        ):\n+            self._check_model_hotswap(rank1=7, rank2=13, do_compile=True)"
        }
    ],
    "stats": {
        "total": 390,
        "additions": 382,
        "deletions": 8
    }
}