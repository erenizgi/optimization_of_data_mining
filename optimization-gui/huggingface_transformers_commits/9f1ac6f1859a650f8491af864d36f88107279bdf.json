{
    "author": "ArthurZucker",
    "message": "Remove something that should have never been there (#38254)\n\n* what the hell\n\n* update\n\n* style\n\n* style\n\n* typing\n\n* fix init issue\n\n* fix granite moe hybrid as well",
    "sha": "9f1ac6f1859a650f8491af864d36f88107279bdf",
    "files": [
        {
            "sha": "fa5dda36cd45731e06afe7088c1055bc8a0e9973",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 49,
            "deletions": 6,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f1ac6f1859a650f8491af864d36f88107279bdf/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f1ac6f1859a650f8491af864d36f88107279bdf/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=9f1ac6f1859a650f8491af864d36f88107279bdf",
            "patch": "@@ -24,15 +24,14 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Callable, Optional, TypedDict, Union\n+from typing import Any, Callable, Optional, TypedDict, Union\n \n import torch\n from torch import nn\n \n-import transformers.models.jamba.modeling_jamba as modeling_jamba\n from transformers.activations import ACT2FN\n \n-from ...cache_utils import Cache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -86,8 +85,7 @@ class BambaFlashAttentionKwargs(TypedDict, total=False):\n     seq_idx: torch.IntTensor\n \n \n-# Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n-class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynamicCache):\n+class HybridMambaAttentionDynamicCache(DynamicCache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -102,7 +100,7 @@ class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynami\n     \"\"\"\n \n     def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=None):\n-        super().__init__(config, batch_size, dtype, device)\n+        super().__init__()\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         conv_kernel_size = config.mamba_d_conv\n@@ -140,6 +138,51 @@ def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=\n         self.key_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n         self.value_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n \n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        # Update the cache\n+        if self.key_cache[layer_idx].shape[-1] == 0:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            device = self.key_cache[layer_idx].device\n+            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+            device = self.value_cache[layer_idx].device\n+            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            device = self.conv_states[layer_idx].device\n+            self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n+            device = self.ssm_states[layer_idx].device\n+            self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n+        if len(self.key_cache) <= layer_idx:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n+        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n+\n+    @classmethod\n+    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n+        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n+\n \n class BambaRotaryEmbedding(nn.Module):\n     def __init__(self, config: BambaConfig, device=None):"
        },
        {
            "sha": "1671007b0048c86c6543f14c50a4af62cd1b3e27",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f1ac6f1859a650f8491af864d36f88107279bdf/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f1ac6f1859a650f8491af864d36f88107279bdf/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=9f1ac6f1859a650f8491af864d36f88107279bdf",
            "patch": "@@ -25,9 +25,8 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-import transformers.models.jamba.modeling_jamba as modeling_jamba\n from transformers.activations import ACT2FN\n-from transformers.models.jamba.modeling_jamba import JambaAttentionDecoderLayer\n+from transformers.models.jamba.modeling_jamba import HybridMambaAttentionDynamicCache, JambaAttentionDecoderLayer\n from transformers.models.llama.modeling_llama import (\n     LlamaAttention,\n     LlamaForCausalLM,\n@@ -43,6 +42,7 @@\n     segment_sum,\n )\n \n+from ...cache_utils import Cache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -99,7 +99,7 @@ class BambaFlashAttentionKwargs(TypedDict, total=False):\n \n \n # Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n-class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynamicCache):\n+class HybridMambaAttentionDynamicCache(HybridMambaAttentionDynamicCache, Cache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -114,7 +114,7 @@ class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynami\n     \"\"\"\n \n     def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=None):\n-        super().__init__(config, batch_size, dtype, device)\n+        Cache.__init__()\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         conv_kernel_size = config.mamba_d_conv"
        },
        {
            "sha": "df823c30bd34b4b192d236e3d337ebdc8e52e856",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 49,
            "deletions": 6,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f1ac6f1859a650f8491af864d36f88107279bdf/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f1ac6f1859a650f8491af864d36f88107279bdf/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=9f1ac6f1859a650f8491af864d36f88107279bdf",
            "patch": "@@ -19,16 +19,15 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Callable, Optional, Union\n+from typing import Any, Callable, Optional, Union\n \n import torch\n import torch.nn.functional as F\n from torch import nn\n \n-import transformers.models.jamba.modeling_jamba as modeling_jamba\n from transformers.activations import ACT2FN\n \n-from ...cache_utils import Cache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -222,8 +221,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-# Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n-class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynamicCache):\n+class HybridMambaAttentionDynamicCache(DynamicCache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -238,7 +236,7 @@ class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynami\n     \"\"\"\n \n     def __init__(self, config: GraniteMoeHybridConfig, batch_size, dtype=torch.float16, device=None):\n-        super().__init__(config, batch_size, dtype, device)\n+        super().__init__()\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n         conv_kernel_size = config.mamba_d_conv\n@@ -276,6 +274,51 @@ def __init__(self, config: GraniteMoeHybridConfig, batch_size, dtype=torch.float\n         self.key_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n         self.value_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n \n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        # Update the cache\n+        if self.key_cache[layer_idx].shape[-1] == 0:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        for layer_idx in range(len(self.key_cache)):\n+            device = self.key_cache[layer_idx].device\n+            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+            device = self.value_cache[layer_idx].device\n+            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+            device = self.conv_states[layer_idx].device\n+            self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n+            device = self.ssm_states[layer_idx].device\n+            self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n+        if len(self.key_cache) <= layer_idx:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor], tuple[torch.Tensor]]:\n+        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n+\n+    @classmethod\n+    def from_legacy_cache(cls, past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n+        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n+\n \n # Helper methods for segment sum computation\n "
        }
    ],
    "stats": {
        "total": 118,
        "additions": 102,
        "deletions": 16
    }
}