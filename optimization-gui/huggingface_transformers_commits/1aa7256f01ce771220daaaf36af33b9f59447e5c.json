{
    "author": "manueldeprada",
    "message": "Refactor `MambaCache` to `modeling_mamba.py` (#38086)\n\n* Refactor MambaCache to modeling_mamba.py (parity with Zamba)\n\n* ruff\n\n* fix dummies\n\n* update\n\n* update\n\n* remove mamba ref in cache tests\n\n* remove cache_implementation from tests\n\n* update\n\n* ruff\n\n* ruff\n\n* sneaky regression\n\n* model consistency\n\n* fix test_multi_gpu_data_parallel_forward\n\n* fix falcon slow tests\n\n* ruff\n\n* ruff\n\n* add sample false\n\n* try to fix slow tests\n\n* Revert \"fix test_multi_gpu_data_parallel_forward\"\n\nThis reverts commit 66b7162c7c5c5ce8a73ccf48cffc8a96343ebb33.\n\n* fix tests on nvidia t4, remove dataparallel tests from mamba\n\n* ruff\n\n* remove DDP tests from mamba and falcon_mamba\n\n* add explicit error for MambaCache\n\n* mamba2 also needs to init cache in prepare_inputs_for_generation\n\n* ruff\n\n* ruff\n\n* move MambaCache to its own file\n\n* ruff\n\n* unprotected import fix\n\n* another attempt to fix unprotected imports\n\n* Revert \"another attempt to fix unprotected imports\"\n\nThis reverts commit 2338354fcab630de5899321f5daced5fb312c2a2.\n\n* fixing unprotected import, attempt 3\n\n* Update src/transformers/cache_utils.py\n\n* ruff's fault\n\n* fix arthur review\n\n* modular falcon mamba\n\n* found a hack\n\n* fix config docs\n\n* fix docs\n\n* add export info\n\n* merge modular falcon branch\n\n* oopsie\n\n* fix fast path failing\n\n* new approach\n\n* oopsie\n\n* fix types\n\n* Revert new pragma in modular\n\nThis reverts commit 80b1cf160ee251536f07c40b8a0857d499e70db6.\n\n* trying another modular workaround\n\n* review & fix ci\n\n* oopsie\n\n* clear prepare_inputs on mamba/mamba2/falcon_mamba",
    "sha": "1aa7256f01ce771220daaaf36af33b9f59447e5c",
    "files": [
        {
            "sha": "0b797c7c7829bd0d7ce6f88a1ff74e382990fa76",
            "filename": "docs/source/en/model_doc/falcon_mamba.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -110,6 +110,13 @@ outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n+## FalconMambaCache\n+\n+[[autodoc]] FalconMambaCache\n+    - update_conv_state\n+    - update_ssm_state\n+    - reset\n+\n ## FalconMambaConfig\n \n [[autodoc]] FalconMambaConfig"
        },
        {
            "sha": "06efa759717f0db838bf8b46df75702884ac7328",
            "filename": "docs/source/en/model_doc/mamba.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -116,6 +116,13 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n   trainer.train()\n    ```\n \n+## MambaCache\n+\n+[[autodoc]] MambaCache\n+    - update_conv_state\n+    - update_ssm_state\n+    - reset\n+\n ## MambaConfig\n \n [[autodoc]] MambaConfig"
        },
        {
            "sha": "3d1566580af3c7d27f6fb9d04b703efb253b4f72",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -371,7 +371,6 @@\n         \"EncoderDecoderCache\",\n         \"HQQQuantizedCache\",\n         \"HybridCache\",\n-        \"MambaCache\",\n         \"OffloadedCache\",\n         \"OffloadedStaticCache\",\n         \"QuantizedCache\","
        },
        {
            "sha": "eecf0c7c0e80e5e061d61adbda944ab0b3c0ed3d",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 121,
            "deletions": 100,
            "changes": 221,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -2,6 +2,7 @@\n import importlib.metadata\n import json\n import os\n+import warnings\n from collections.abc import Iterable\n from dataclasses import dataclass\n from typing import Any, Optional, Union\n@@ -18,6 +19,7 @@\n if is_hqq_available():\n     from hqq.core.quantize import Quantizer as HQQQuantizer\n \n+\n logger = logging.get_logger(__name__)\n \n \n@@ -2091,106 +2093,6 @@ def _prefetch_layer_in_context(self, layer_idx: int) -> None:\n             self.device_value_cache[self.active_device_layer].fill_(0.0)\n \n \n-class MambaCache:\n-    \"\"\"\n-    Cache for mamba model which does not have attention mechanism and key value states.\n-\n-    Arguments:\n-        config (`PretrainedConfig):\n-            The configuration file defining the shape-related attributes required to initialize the static cache.\n-        max_batch_size (`int`):\n-            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a smaller batch size is used.\n-        dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n-            The default `dtype` to use when initializing the layer.\n-        device (`torch.device` or `str`, *optional*):\n-            The device on which the cache should be initialized. Should be the same as the layer.\n-\n-    Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, MambaForCausalLM, MambaCache\n-\n-        >>> model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n-\n-        >>> inputs = tokenizer(text=\"My name is Mamba\", return_tensors=\"pt\")\n-\n-        >>> # Prepare a cache class and pass it to model's forward\n-        >>> past_key_values = MambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values\n-        MambaCache()\n-        ```\n-    \"\"\"\n-\n-    is_compileable = True\n-\n-    # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n-    def __init__(\n-        self,\n-        config: PretrainedConfig,\n-        max_batch_size: int,\n-        dtype: torch.dtype = torch.float16,\n-        device: Union[torch.device, str, None] = None,\n-    ):\n-        self.max_batch_size = max_batch_size\n-        self._dtype = dtype\n-        self.intermediate_size = config.intermediate_size\n-        self.ssm_state_size = config.state_size\n-        self.conv_kernel_size = config.conv_kernel\n-\n-        self.conv_states: list[torch.Tensor] = []\n-        self.ssm_states: list[torch.Tensor] = []\n-        device = torch.device(device) if device is not None else None\n-        for _ in range(config.num_hidden_layers):\n-            conv_state: torch.Tensor = torch.zeros(\n-                self.max_batch_size,\n-                self.intermediate_size,\n-                self.conv_kernel_size,\n-                device=device,\n-                dtype=self._dtype,\n-            )\n-            ssm_state: torch.Tensor = torch.zeros(\n-                self.max_batch_size,\n-                self.intermediate_size,\n-                self.ssm_state_size,\n-                device=device,\n-                dtype=self._dtype,\n-            )\n-\n-            torch._dynamo.mark_static_address(conv_state)\n-            torch._dynamo.mark_static_address(ssm_state)\n-            self.conv_states.append(conv_state)\n-            self.ssm_states.append(ssm_state)\n-\n-    def update_conv_state(\n-        self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor\n-    ) -> torch.Tensor:\n-        # This `if` blocks is only reached in multigpu and if `layer_device_map` is not passed. It is used\n-        # when the cache is initialized in the forward pass (e.g. Mamba)\n-        if self.conv_states[layer_idx].device != new_conv_state.device:\n-            self.conv_states[layer_idx] = self.conv_states[layer_idx].to(new_conv_state.device)\n-\n-        conv_state = self.conv_states[layer_idx]\n-        cache_position = cache_position.clamp(0, self.conv_kernel_size - 1)\n-\n-        conv_state = conv_state.roll(shifts=-1, dims=-1)\n-        conv_state[:, :, cache_position] = new_conv_state.to(device=conv_state.device, dtype=conv_state.dtype)\n-        self.conv_states[layer_idx].zero_()\n-        self.conv_states[layer_idx] += conv_state\n-        return self.conv_states[layer_idx]\n-\n-    def update_ssm_state(self, layer_idx: int, new_ssm_state: torch.Tensor):\n-        self.ssm_states[layer_idx] = new_ssm_state.to(self.ssm_states[layer_idx].device)\n-        return self.ssm_states[layer_idx]\n-\n-    def reset(self):\n-        for layer_idx in range(len(self.conv_states)):\n-            # In-place ops prevent breaking the static address\n-            self.conv_states[layer_idx].zero_()\n-            self.ssm_states[layer_idx].zero_()\n-\n-\n class OffloadedStaticCache(StaticCache):\n     \"\"\"\n     Static cache class to be used with `torch.compile(model)` that offloads to the CPU or\n@@ -2461,3 +2363,122 @@ def _prefetch_layer_in_context(self, layer_idx: int) -> None:\n \n         self._device_key_cache[layer_idx & 1].copy_(self.key_cache[layer_idx], non_blocking=True)\n         self._device_value_cache[layer_idx & 1].copy_(self.value_cache[layer_idx], non_blocking=True)\n+\n+\n+# TODO (manuel, joao): remove this class, it is here only for backwards compatibility\n+# PEP 562: Lazy loading for deprecated location of MambaCache\n+def __getattr__(name: str) -> Any:\n+    if name == \"MambaCache\":\n+        warnings.warn(\n+            (\n+                \"Importing `MambaCache` from `transformers.cache_utils` is deprecated and will be removed \"\n+                \"in a future version. Please import it from `transformers` or `transformers.models.mamba.cache_mamba` instead.\"\n+            ),\n+            FutureWarning,\n+            stacklevel=2,\n+        )\n+\n+        class MambaCache:\n+            \"\"\"\n+            Importing `MambaCache` from `transformers.cache_utils` is deprecated and will be removed\n+            in a future version. Please import it from `transformers` or `transformers.models.mamba.cache_mamba` instead.\n+\n+            Cache for mamba model which does not have attention mechanism and key value states.\n+\n+            Arguments:\n+                config (`PretrainedConfig):\n+                    The configuration file defining the shape-related attributes required to initialize the static cache.\n+                max_batch_size (`int`):\n+                    The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a smaller batch size is used.\n+                dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n+                    The default `dtype` to use when initializing the layer.\n+                device (`torch.device` or `str`, *optional*):\n+                    The device on which the cache should be initialized. Should be the same as the layer.\n+\n+            Example:\n+\n+                ```python\n+                >>> from transformers import AutoTokenizer, MambaForCausalLM, MambaCache\n+\n+                >>> model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n+                >>> tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n+\n+                >>> inputs = tokenizer(text=\"My name is Mamba\", return_tensors=\"pt\")\n+\n+                >>> # Prepare a cache class and pass it to model's forward\n+                >>> past_key_values = MambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n+                >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+                >>> outputs.past_key_values\n+                MambaCache()\n+                ```\n+            \"\"\"\n+\n+            is_compileable = True\n+\n+            # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n+            def __init__(\n+                self,\n+                config,\n+                max_batch_size: int,\n+                dtype: torch.dtype = torch.float16,\n+                device: Union[torch.device, str, None] = None,\n+            ):\n+                self.max_batch_size = max_batch_size\n+                self._dtype = dtype\n+                self.intermediate_size = config.intermediate_size\n+                self.ssm_state_size = config.state_size\n+                self.conv_kernel_size = config.conv_kernel\n+\n+                self.conv_states: list[torch.Tensor] = []\n+                self.ssm_states: list[torch.Tensor] = []\n+                device = torch.device(device) if device is not None else None\n+                for _ in range(config.num_hidden_layers):\n+                    conv_state: torch.Tensor = torch.zeros(\n+                        self.max_batch_size,\n+                        self.intermediate_size,\n+                        self.conv_kernel_size,\n+                        device=device,\n+                        dtype=self._dtype,\n+                    )\n+                    ssm_state: torch.Tensor = torch.zeros(\n+                        self.max_batch_size,\n+                        self.intermediate_size,\n+                        self.ssm_state_size,\n+                        device=device,\n+                        dtype=self._dtype,\n+                    )\n+\n+                    torch._dynamo.mark_static_address(conv_state)\n+                    torch._dynamo.mark_static_address(ssm_state)\n+                    self.conv_states.append(conv_state)\n+                    self.ssm_states.append(ssm_state)\n+\n+            def update_conv_state(\n+                self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor\n+            ) -> torch.Tensor:\n+                # This `if` blocks is only reached in multigpu and if `layer_device_map` is not passed. It is used\n+                # when the cache is initialized in the forward pass (e.g. Mamba)\n+                if self.conv_states[layer_idx].device != new_conv_state.device:\n+                    self.conv_states[layer_idx] = self.conv_states[layer_idx].to(new_conv_state.device)\n+\n+                conv_state = self.conv_states[layer_idx]\n+                cache_position = cache_position.clamp(0, self.conv_kernel_size - 1)\n+\n+                conv_state = conv_state.roll(shifts=-1, dims=-1)\n+                conv_state[:, :, cache_position] = new_conv_state.to(device=conv_state.device, dtype=conv_state.dtype)\n+                self.conv_states[layer_idx].zero_()\n+                self.conv_states[layer_idx] += conv_state\n+                return self.conv_states[layer_idx]\n+\n+            def update_ssm_state(self, layer_idx: int, new_ssm_state: torch.Tensor):\n+                self.ssm_states[layer_idx] = new_ssm_state.to(self.ssm_states[layer_idx].device)\n+                return self.ssm_states[layer_idx]\n+\n+            def reset(self):\n+                for layer_idx in range(len(self.conv_states)):\n+                    # In-place ops prevent breaking the static address\n+                    self.conv_states[layer_idx].zero_()\n+                    self.ssm_states[layer_idx].zero_()\n+\n+        return MambaCache\n+    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")"
        },
        {
            "sha": "165252927c1cd129a97eaf2a6268b9faa655cd3e",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -54,7 +54,6 @@\n         HQQQuantizedCache,\n         HybridCache,\n         HybridChunkedCache,\n-        MambaCache,\n         OffloadedHybridCache,\n         OffloadedStaticCache,\n         QuantizedCacheConfig,\n@@ -75,7 +74,6 @@\n         \"hybrid_chunked\": HybridChunkedCache,\n         \"offloaded_hybrid\": OffloadedHybridCache,\n         \"offloaded_hybrid_chunked\": OffloadedHybridCache,\n-        \"mamba\": MambaCache,\n     }\n     QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}\n     ALL_CACHE_IMPLEMENTATIONS = (\n@@ -186,7 +184,6 @@ class GenerationConfig(PushToHubMixin):\n             - `\"offloaded_static\"`: [`OffloadedStaticCache`]\n             - `\"sliding_window\"`: [`SlidingWindowCache`]\n             - `\"hybrid\"`: [`HybridCache`]\n-            - `\"mamba\"`: [`MambaCache`]\n             - `\"quantized\"`: [`QuantizedCache`]\n \n             If none is specified, we will use the default cache for the model (which is often [`DynamicCache`]). See"
        },
        {
            "sha": "76b3d7bd8a1dae97518205bdf1a2b8e132b5518b",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -1916,9 +1916,8 @@ def _get_cache(\n             or isinstance(\n                 cache_to_check, (HybridChunkedCache, OffloadedHybridCache)\n             )  # due to internal slicing, we always re-init\n+            or cache_to_check.max_cache_len < max_cache_len\n         )\n-        if cache_implementation != \"mamba\":\n-            need_new_cache = need_new_cache or cache_to_check.max_cache_len < max_cache_len\n \n         if requires_cross_attention_cache and hasattr(self, \"_cache\"):\n             need_new_cache = (\n@@ -1957,9 +1956,9 @@ def _get_cache(\n     def _supports_default_dynamic_cache(cls) -> bool:\n         \"\"\"\n         Return `True` if current model can use a `DynamicCache` instance when initializing the `past_key_values`.\n-        This adds exception for some models like `Jamba` model which uses its own `HybridMambaAttentionDynamicCache`\n+        This adds exception for some models like `Mamba` models which use their own caches\n         and do not need to initialize the Cache in advance in order to save memory (because no back and forth\n-        `to_legacy_cache` and `from_legacy_cache` will be performed for `HybridMambaAttentionDynamicCache`).\n+        `to_legacy_cache` and `from_legacy_cache` will be performed for mamba-based models).\n         \"\"\"\n         # NOTE: remove xlnet/reformer when the models are deprecated, non-standard model architecture/cache name\n         return not cls._is_stateful and all(\n@@ -2016,7 +2015,7 @@ def _prepare_cache_for_generation(\n         if generation_config.use_cache is False:\n             return\n \n-        # Quick escape route 3: model that only supports legacy caches = nothing to prepare\n+        # Quick escape route 3: model that only supports legacy caches or models that supply it in `prepare_inputs_for_generation` (mamba, zamba, ...)\n         if not self._supports_default_dynamic_cache():\n             if generation_config.cache_implementation is not None:\n                 warnings.warn("
        },
        {
            "sha": "86a0e9ad22d89836904867c4b751bbe65958a455",
            "filename": "src/transformers/models/falcon_mamba/configuration_falcon_mamba.py",
            "status": "modified",
            "additions": 15,
            "deletions": 12,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -1,5 +1,11 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/falcon_mamba/modular_falcon_mamba.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_falcon_mamba.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team.\n+# Copyright 2024 Tri Dao, Albert Gu, Technological Innovation Institute and HuggingFace Inc. team.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -12,15 +18,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"FALCONMAMBA configuration\"\"\"\n \n import math\n \n from ...configuration_utils import PretrainedConfig\n-from ...utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n \n \n class FalconMambaConfig(PretrainedConfig):\n@@ -79,10 +80,13 @@ class FalconMambaConfig(PretrainedConfig):\n             Whether or not to rescale `out_proj` weights when initializing.\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the cache should be used.\n-        use_mambapy (`bool`, *optional*, defaults to `False`):\n-            Determines the fallback strategy during training if the CUDA-based official implementation of FalconMamba is not available. If `True`, the falcon_mamba.py implementation is used. If `False`, the naive and slower implementation is used. Consider switching to the naive version if memory is limited.\n+        use_falcon_mambapy (`bool`, *optional*, defaults to `False`):\n+            This argument corresponds to `use_mambapy` in MambaConfig.\n+            Determines the fallback strategy during training if the CUDA-based official implementation of Mamba is not available. If `True`, the mamba.py implementation is used. If `False`, the naive and slower implementation is used. Consider switching to the naive version if memory is limited.\n         mixer_rms_eps (`float`, *optional*, defaults to 1e-06):\n             The RMS norm epsilon value that is used in the Mixer RMS norm for B, C and dt states.\n+\n+\n     Example:\n \n     ```python\n@@ -125,10 +129,11 @@ def __init__(\n         time_step_floor=1e-4,\n         rescale_prenorm_residual=False,\n         use_cache=True,\n-        use_mambapy=False,\n+        use_falcon_mambapy=False,\n         mixer_rms_eps=1e-6,\n         **kwargs,\n     ):\n+        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, pad_token_id=pad_token_id, **kwargs)\n         self.vocab_size = vocab_size\n         self.hidden_size = hidden_size\n         self.state_size = state_size\n@@ -153,10 +158,8 @@ def __init__(\n         self.rescale_prenorm_residual = rescale_prenorm_residual\n         self.residual_in_fp32 = residual_in_fp32\n         self.use_cache = use_cache\n-        self.use_mambapy = use_mambapy\n+        self.use_falcon_mambapy = use_falcon_mambapy\n         self.mixer_rms_eps = mixer_rms_eps\n \n-        super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, pad_token_id=pad_token_id, **kwargs)\n-\n \n __all__ = [\"FalconMambaConfig\"]"
        },
        {
            "sha": "56a5770ba7b62c41e209c7a67903a8135641950a",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 167,
            "deletions": 65,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/falcon_mamba/modular_falcon_mamba.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_falcon_mamba.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 Tri Dao, Albert Gu, Technological Innovation Institute and HuggingFace Inc. team.\n #\n@@ -12,29 +18,29 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch FALCONMAMBA model.\"\"\"\n \n import math\n from dataclasses import dataclass\n from typing import Any, Optional, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import MambaCache\n+from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available, is_mambapy_available\n+from ...utils.import_utils import (\n+    is_causal_conv1d_available,\n+    is_mamba_ssm_available,\n+    is_mambapy_available,\n+)\n from .configuration_falcon_mamba import FalconMambaConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n if is_mambapy_available():\n     from mambapy.pscan import pscan\n else:\n@@ -53,9 +59,109 @@\n else:\n     causal_conv1d_update, causal_conv1d_fn = None, None\n \n-is_fast_path_available = all(\n-    (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n-)\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class FalconMambaCache:\n+    \"\"\"\n+    Cache for falcon_mamba model which does not have attention mechanism and key value states.\n+\n+    Arguments:\n+        config (`PretrainedConfig):\n+            The configuration file defining the shape-related attributes required to initialize the static cache.\n+        max_batch_size (`int`):\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a smaller batch size is used.\n+        dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n+            The default `dtype` to use when initializing the layer.\n+        device (`torch.device` or `str`, *optional*):\n+            The device on which the cache should be initialized. Should be the same as the layer.\n+\n+    Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, FalconMambaForCausalLM, FalconMambaCache\n+\n+        >>> model = FalconMambaForCausalLM.from_pretrained(\"state-spaces/falcon_mamba-130m-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/falcon_mamba-130m-hf\")\n+\n+        >>> inputs = tokenizer(text=\"My name is FalconMamba\", return_tensors=\"pt\")\n+\n+        >>> # Prepare a cache class and pass it to model's forward\n+        >>> past_key_values = FalconMambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n+        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+        >>> outputs.past_key_values\n+        FalconMambaCache()\n+        ```\n+    \"\"\"\n+\n+    is_compileable = True\n+\n+    # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        max_batch_size: int,\n+        dtype: torch.dtype = torch.float16,\n+        device: Union[torch.device, str, None] = None,\n+    ):\n+        self.max_batch_size = max_batch_size\n+        self._dtype = dtype\n+        self.intermediate_size = config.intermediate_size\n+        self.ssm_state_size = config.state_size\n+        self.conv_kernel_size = config.conv_kernel\n+\n+        self.conv_states: list[torch.Tensor] = []\n+        self.ssm_states: list[torch.Tensor] = []\n+        device = torch.device(device) if device is not None else None\n+        for _ in range(config.num_hidden_layers):\n+            conv_state: torch.Tensor = torch.zeros(\n+                self.max_batch_size,\n+                self.intermediate_size,\n+                self.conv_kernel_size,\n+                device=device,\n+                dtype=self._dtype,\n+            )\n+            ssm_state: torch.Tensor = torch.zeros(\n+                self.max_batch_size,\n+                self.intermediate_size,\n+                self.ssm_state_size,\n+                device=device,\n+                dtype=self._dtype,\n+            )\n+\n+            torch._dynamo.mark_static_address(conv_state)\n+            torch._dynamo.mark_static_address(ssm_state)\n+            self.conv_states.append(conv_state)\n+            self.ssm_states.append(ssm_state)\n+\n+    def update_conv_state(\n+        self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor\n+    ) -> torch.Tensor:\n+        # This `if` blocks is only reached in multigpu and if `layer_device_map` is not passed. It is used\n+        # when the cache is initialized in the forward pass (e.g. FalconMamba)\n+        if self.conv_states[layer_idx].device != new_conv_state.device:\n+            self.conv_states[layer_idx] = self.conv_states[layer_idx].to(new_conv_state.device)\n+\n+        conv_state = self.conv_states[layer_idx]\n+        cache_position = cache_position.clamp(0, self.conv_kernel_size - 1)\n+\n+        conv_state = conv_state.roll(shifts=-1, dims=-1)\n+        conv_state[:, :, cache_position] = new_conv_state.to(device=conv_state.device, dtype=conv_state.dtype)\n+        self.conv_states[layer_idx].zero_()\n+        self.conv_states[layer_idx] += conv_state\n+        return self.conv_states[layer_idx]\n+\n+    def update_ssm_state(self, layer_idx: int, new_ssm_state: torch.Tensor):\n+        self.ssm_states[layer_idx].zero_()\n+        self.ssm_states[layer_idx] += new_ssm_state.to(self.ssm_states[layer_idx].device)\n+        return self.ssm_states[layer_idx]\n+\n+    def reset(self):\n+        for layer_idx in range(len(self.conv_states)):\n+            # In-place ops prevent breaking the static address\n+            self.conv_states[layer_idx].zero_()\n+            self.ssm_states[layer_idx].zero_()\n \n \n def rms_forward(hidden_states, variance_epsilon=1e-6):\n@@ -107,7 +213,7 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.activation = config.hidden_act\n         self.act = ACT2FN[config.hidden_act]\n \n-        self.use_mambapy = config.use_mambapy\n+        self.use_falcon_mambapy = config.use_falcon_mambapy\n \n         # projection of the input hidden states\n         self.in_proj = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=config.use_bias)\n@@ -126,6 +232,7 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n         self.use_bias = config.use_bias\n \n+        self.warn_slow_implementation()\n         # Triton expects to pass RMS weights even if they are non learnable, thus we need to create these weights here\n         self.register_buffer(\n             \"b_c_rms\", torch.nn.Parameter(torch.ones(self.ssm_state_size), requires_grad=False), persistent=False\n@@ -135,8 +242,12 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         )\n         self.rms_eps = config.mixer_rms_eps\n \n+    def warn_slow_implementation(self):\n+        is_fast_path_available = all(\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+        )\n         if not is_fast_path_available:\n-            if self.use_mambapy:\n+            if self.use_falcon_mambapy:\n                 if is_mambapy_available():\n                     logger.warning_once(\n                         \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n@@ -157,7 +268,7 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n     def cuda_kernels_forward(\n         self,\n         hidden_states: torch.Tensor,\n-        cache_params: Optional[MambaCache] = None,\n+        cache_params: Optional[FalconMambaCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n@@ -269,10 +380,10 @@ def cuda_kernels_forward(\n             contextualized_states = self.out_proj(scan_outputs.transpose(1, 2))\n         return contextualized_states\n \n-    def slow_forward(\n-        self,\n+    # fmt: off\n+    def slow_forward(self,\n         input_states,\n-        cache_params: Optional[MambaCache] = None,\n+        cache_params: Optional[FalconMambaCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n@@ -344,7 +455,7 @@ def slow_forward(\n         deltaB_u = discrete_B * hidden_states[:, :, :, None].float()\n \n         # 3.c perform the recurrence y â† SSM(A, B, C)(x)\n-        if self.use_mambapy and self.training and cache_params is None:\n+        if self.use_falcon_mambapy and self.training and cache_params is None:\n             hs = pscan(\n                 discrete_A.transpose(1, 2), deltaB_u.transpose(1, 2)\n             )  # [batch, seq_len, intermediate_size, ssm_state_size]\n@@ -371,21 +482,23 @@ def slow_forward(\n         # 4. Final linear projection\n         contextualized_states = self.out_proj(scan_output.transpose(1, 2))  # [batch, seq_len, hidden_size]\n         return contextualized_states\n+    # fmt: on\n \n-    # Copied from transformers.models.mamba.modeling_mamba.MambaMixer.forward\n     def forward(\n         self,\n         hidden_states,\n-        cache_params: Optional[MambaCache] = None,\n+        cache_params: Optional[FalconMambaCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n+        is_fast_path_available = all(\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+        )\n         if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not torch._dynamo.is_compiling():\n             return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n         return self.slow_forward(hidden_states, cache_params, cache_position, attention_mask)\n \n \n-# Copied from transformers.models.mamba.modeling_mamba.MambaRMSNorm with Mamba->FalconMamba\n class FalconMambaRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -395,17 +508,15 @@ def __init__(self, hidden_size, eps=1e-6):\n         self.weight = nn.Parameter(torch.ones(hidden_size))\n         self.variance_epsilon = eps\n \n-    def extra_repr(self):\n-        return f\"{self.weight.shape[0]}, eps={self.variance_epsilon}\"\n-\n-    # Ignore copy\n     def forward(self, hidden_states):\n         return self.weight.to(hidden_states.device) * rms_forward(\n             hidden_states, variance_epsilon=self.variance_epsilon\n         )\n \n+    def extra_repr(self):\n+        return f\"{self.weight.shape[0]}, eps={self.variance_epsilon}\"\n+\n \n-# Copied from transformers.models.mamba.modeling_mamba.MambaBlock with Mamba->FalconMamba,FalconMambaCache->MambaCache\n class FalconMambaBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n@@ -418,7 +529,7 @@ def __init__(self, config, layer_idx):\n     def forward(\n         self,\n         hidden_states,\n-        cache_params: Optional[MambaCache] = None,\n+        cache_params: Optional[FalconMambaCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n@@ -435,7 +546,6 @@ def forward(\n \n \n @auto_docstring\n-# Copied from transformers.models.mamba.modeling_mamba.MambaPreTrainedModel with Mamba->FalconMamba\n class FalconMambaPreTrainedModel(PreTrainedModel):\n     config: FalconMambaConfig\n     base_model_prefix = \"backbone\"\n@@ -507,21 +617,20 @@ def _init_weights(self, module):\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n-    Class for the FALCONMAMBA model outputs.\n+    Class for the FALCON_MAMBA model outputs.\n     \"\"\"\n )\n-# Copied from transformers.models.mamba.modeling_mamba.MambaOutput with MAMBA->FALCONMAMBA,Mamba->FalconMamba,FalconMambaCache->MambaCache\n class FalconMambaOutput(ModelOutput):\n     r\"\"\"\n-    cache_params (`MambaCache`):\n+    cache_params (`FalconMambaCache`):\n         The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n         avoid providing the old `input_ids`.\n \n         Includes both the State space model state matrices after the selective scan, and the Convolutional states\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n-    cache_params: Optional[MambaCache] = None\n+    cache_params: Optional[FalconMambaCache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n \n \n@@ -531,14 +640,13 @@ class FalconMambaOutput(ModelOutput):\n     Base class for causal language model (or autoregressive) outputs.\n     \"\"\"\n )\n-# Copied from transformers.models.mamba.modeling_mamba.MambaCausalLMOutput with Mamba->FalconMamba,FalconMambaCache->MambaCache\n class FalconMambaCausalLMOutput(ModelOutput):\n     r\"\"\"\n     loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    cache_params (`MambaCache`):\n+    cache_params (`FalconMambaCache`):\n         The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n         avoid providing the old `input_ids`.\n \n@@ -547,7 +655,7 @@ class FalconMambaCausalLMOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     logits: Optional[torch.FloatTensor] = None\n-    cache_params: Optional[MambaCache] = None\n+    cache_params: Optional[FalconMambaCache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n \n \n@@ -577,15 +685,15 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.LongTensor] = None,\n-        cache_params: Optional[MambaCache] = None,\n+        cache_params: Optional[FalconMambaCache] = None,\n         use_cache: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, FalconMambaOutput]:\n         r\"\"\"\n-        cache_params (`MambaCache`, *optional*):\n+        cache_params (`FalconMambaCache`, *optional*):\n             If passed along, the model uses the previous state in all the blocks (which will give the output for the\n             `input_ids` provided as if the model add `state_input_ids + input_ids` as context).\n         use_cache (`bool`, *optional*):\n@@ -608,7 +716,7 @@ def forward(\n \n         if use_cache:\n             if cache_params is None:\n-                cache_params = MambaCache(\n+                cache_params = FalconMambaCache(\n                     self.config, inputs_embeds.size(0), device=inputs_embeds.device, dtype=inputs_embeds.dtype\n                 )\n                 cache_position = torch.arange(0, self.config.conv_kernel, device=inputs_embeds.device)\n@@ -623,6 +731,7 @@ def forward(\n                 )\n         else:\n             cache_params = None\n+\n         hidden_states = inputs_embeds\n         all_hidden_states = () if output_hidden_states else None\n         for mixer_block in self.layers:\n@@ -653,11 +762,10 @@ def forward(\n \n @auto_docstring(\n     custom_intro=\"\"\"\n-    The FALCONMAMBA Model transformer with a language modeling head on top (linear layer with weights tied to the input\n+    The FALCON_MAMBA Model transformer with a language modeling head on top (linear layer with weights tied to the input\n     embeddings).\n     \"\"\"\n )\n-# Copied from transformers.models.mamba.modeling_mamba.MambaForCausalLM with MAMBA->FALCONMAMBA,Mamba->FalconMamba,mamba->falcon_mamba,FalconMambaCache->MambaCache\n class FalconMambaForCausalLM(FalconMambaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n@@ -704,38 +812,32 @@ def prepare_inputs_for_generation(\n         input_ids,\n         inputs_embeds=None,\n         use_cache=None,\n-        cache_params: Optional[MambaCache] = None,\n+        cache_params: Optional[FalconMambaCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ):\n         # Overwritten -- uses `cache_params` as opposed to `past_key_values`\n-\n-        if use_cache:\n-            # `cache_position` should have been initialized in `generate`\n-            if cache_position is None:\n-                raise ValueError(\n-                    \"`cache_position` should not be None as it should have been initialized in \"\n-                    \"`model.generate`, you are responsible for passing in a valid `cache_position` if \"\n-                    \"you are calling `prepare_inputs_for_generation` directly with `use_cache=True`\"\n-                )\n-            if cache_position[0] > 0:\n-                input_ids = input_ids[:, -1].unsqueeze(-1)\n-\n-                if attention_mask is not None:\n-                    attention_mask = None\n-\n+        model_inputs = {\"input_ids\": input_ids.contiguous()}\n+        if use_cache and cache_params is None:\n+            # we initialize the `cache_position` to full size of `conv_states` at prefill stage\n+            # considering padding will be applied when input length is shorter, and truncation\n+            # will be applied when it is longer, so it will be equivalent to always have it match\n+            # the length of `cache_params.conv_states`, which is `config.conv_kernel`\n+            cache_position = torch.arange(0, self.backbone.config.conv_kernel, device=input_ids.device)\n+            if inputs_embeds is not None:\n+                model_inputs = {\"inputs_embeds\": inputs_embeds}\n+                max_batch_size = inputs_embeds.size(0)\n             else:\n-                # we initialize the `cache_position` to full size of `conv_states` at prefill stage\n-                # considering padding will be applied when input length is shorter, and truncation\n-                # will be applied when it is longer, so it will be equivalent to always have it match\n-                # the length of `cache_params.conv_states`, which is `config.conv_kernel`\n-                cache_position = torch.arange(0, self.config.conv_kernel, device=input_ids.device)\n+                max_batch_size = input_ids.size(0)\n+            cache_params = FalconMambaCache(self.backbone.config, max_batch_size, device=self.device, dtype=self.dtype)\n \n-        if inputs_embeds is not None and cache_params is None:\n+        if use_cache and cache_position[0] > 0:\n+            model_inputs[\"input_ids\"] = input_ids[:, -1].unsqueeze(-1).contiguous()\n+            attention_mask = None\n+\n+        if not use_cache and inputs_embeds is not None:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids.contiguous()}\n \n         model_inputs.update(\n             {\n@@ -753,7 +855,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        cache_params: Optional[MambaCache] = None,\n+        cache_params: Optional[FalconMambaCache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -762,7 +864,7 @@ def forward(\n         **kwargs,  # for now we need this for generation\n     ) -> Union[tuple, FalconMambaCausalLMOutput]:\n         r\"\"\"\n-        cache_params (`MambaCache`, *optional*):\n+        cache_params (`FalconMambaCache`, *optional*):\n             If passed along, the model uses the previous state in all the blocks (which will give the output for the\n             `input_ids` provided as if the model add `state_input_ids + input_ids` as context).\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -811,4 +913,4 @@ def forward(\n         )\n \n \n-__all__ = [\"FalconMambaForCausalLM\", \"FalconMambaModel\", \"FalconMambaPreTrainedModel\"]\n+__all__ = [\"FalconMambaForCausalLM\", \"FalconMambaModel\", \"FalconMambaPreTrainedModel\", \"FalconMambaCache\"]"
        },
        {
            "sha": "fdd1da3e2f4c8be3546df059b2a8e6db30bf81e9",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "added",
            "additions": 540,
            "deletions": 0,
            "changes": 540,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -0,0 +1,540 @@\n+# coding=utf-8\n+# Copyright 2024 Tri Dao, Albert Gu, Technological Innovation Institute and HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch FALCONMAMBA model.\"\"\"\n+\n+from typing import Optional\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...utils import auto_docstring, logging\n+from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available, is_mambapy_available\n+from ..mamba.configuration_mamba import MambaConfig\n+from ..mamba.modeling_mamba import (\n+    MambaBlock,\n+    MambaCache,\n+    MambaCausalLMOutput,\n+    MambaForCausalLM,\n+    MambaMixer,\n+    MambaModel,\n+    MambaOutput,\n+    MambaPreTrainedModel,\n+    MambaRMSNorm,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_mambapy_available():\n+    from mambapy.pscan import pscan\n+else:\n+    pscan = None\n+\n+if is_mamba_ssm_available():\n+    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn\n+    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n+\n+    from ...kernels.falcon_mamba import mamba_inner_fn\n+else:\n+    selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_update, causal_conv1d_fn = None, None\n+\n+\n+class FalconMambaConfig(MambaConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`FalconMambaModel`]. It is used to instantiate a FALCON_MAMBA\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the FALCON_MAMBA\n+    [tiiuae/falcon-mamba-7b](https://huggingface.co/tiiuae/falcon-mamba-7b) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 50280):\n+            Vocabulary size of the FALCON_MAMBA model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`FalconMambaModel`].\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the embeddings and hidden states.\n+        state_size (`int`, *optional*, defaults to 16): shape of the state space latents.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the model.\n+        layer_norm_epsilon (`float`, *optional*, defaults to 1e-05):\n+            The epsilon to use in the layer normalization layers.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 0):\n+            The id of the beginning of sentence token in the vocabulary.\n+        eos_token_id (`int`, *optional*, defaults to 0):\n+            The id of the end of sentence token in the vocabulary.\n+        expand (`int`, *optional*, defaults to 2): Expanding factor used to determine the intermediate size.\n+        conv_kernel (`int`, *optional*, defaults to 4): Size of the convolution kernel.\n+        use_bias (`bool`, *optional*, defaults to `False`):\n+            Whether or not to use bias in [\"in_proj\", \"out_proj\"] of the mixer block\n+        use_conv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether or not to use bias in the convolution layer of the mixer block.\n+        hidden_act (`str`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        initializer_range (`float`, *optional*, defaults to 0.1):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        residual_in_fp32 (`bool`, *optional*, defaults to `True`):\n+            Whether or not residuals should be in `float32`. If set to `False` residuals will keep the same `dtype` as the rest of the model\n+        time_step_rank (`Union[int,str]`, *optional*, defaults to `\"auto\"`):\n+            Rank of the discretization projection matrix. `\"auto\"` means that it will default to `math.ceil(self.hidden_size / 16)`\n+        time_step_scale (`float`, *optional*, defaults to 1.0):\n+            Scale used used to scale `dt_proj.bias`.\n+        time_step_min (`float`, *optional*, defaults to 0.001):\n+            Minimum `time_step` used to bound `dt_proj.bias`.\n+        time_step_max (`float`, *optional*, defaults to 0.1):\n+            Maximum `time_step` used to bound `dt_proj.bias`.\n+        time_step_init_scheme (`float`, *optional*, defaults to `\"random\"`):\n+            Init scheme used for `dt_proj.weight`. Should be one of `[\"random\",\"uniform\"]`\n+        time_step_floor (`float`, *optional*, defaults to 0.0001):\n+            Minimum clamping value of the `dt_proj.bias` layer initialization.\n+        rescale_prenorm_residual (`bool`, *optional*, defaults to `False`):\n+            Whether or not to rescale `out_proj` weights when initializing.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the cache should be used.\n+        use_falcon_mambapy (`bool`, *optional*, defaults to `False`):\n+            This argument corresponds to `use_mambapy` in MambaConfig.\n+            Determines the fallback strategy during training if the CUDA-based official implementation of Mamba is not available. If `True`, the mamba.py implementation is used. If `False`, the naive and slower implementation is used. Consider switching to the naive version if memory is limited.\n+        mixer_rms_eps (`float`, *optional*, defaults to 1e-06):\n+            The RMS norm epsilon value that is used in the Mixer RMS norm for B, C and dt states.\n+\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import FalconMambaConfig, FalconMambaModel\n+\n+    >>> # Initializing a FalconMamba configuration\n+    >>> configuration = FalconMambaConfig()\n+\n+    >>> # Initializing a model (with random weights) from the configuration\n+    >>> model = FalconMambaModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=50280,\n+        hidden_size=768,\n+        state_size=16,\n+        num_hidden_layers=32,\n+        layer_norm_epsilon=1e-5,\n+        pad_token_id=0,\n+        bos_token_id=0,\n+        eos_token_id=0,\n+        expand=2,\n+        conv_kernel=4,\n+        use_bias=False,\n+        use_conv_bias=True,\n+        hidden_act=\"silu\",\n+        initializer_range=0.1,\n+        residual_in_fp32=True,\n+        time_step_rank=\"auto\",\n+        time_step_scale=1.0,\n+        time_step_min=0.001,\n+        time_step_max=0.1,\n+        time_step_init_scheme=\"random\",\n+        time_step_floor=1e-4,\n+        rescale_prenorm_residual=False,\n+        use_cache=True,\n+        use_falcon_mambapy=False,\n+        mixer_rms_eps=1e-6,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            hidden_size=hidden_size,\n+            state_size=state_size,\n+            num_hidden_layers=num_hidden_layers,\n+            layer_norm_epsilon=layer_norm_epsilon,\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            expand=expand,\n+            conv_kernel=conv_kernel,\n+            use_bias=use_bias,\n+            use_conv_bias=use_conv_bias,\n+            hidden_act=hidden_act,\n+            initializer_range=initializer_range,\n+            residual_in_fp32=residual_in_fp32,\n+            time_step_rank=time_step_rank,\n+            time_step_scale=time_step_scale,\n+            time_step_min=time_step_min,\n+            time_step_max=time_step_max,\n+            time_step_init_scheme=time_step_init_scheme,\n+            time_step_floor=time_step_floor,\n+            rescale_prenorm_residual=rescale_prenorm_residual,\n+            use_cache=use_cache,\n+            use_falcon_mambapy=use_falcon_mambapy,\n+            **kwargs,\n+        )\n+        self.mixer_rms_eps = mixer_rms_eps\n+\n+\n+class FalconMambaCache(MambaCache):\n+    pass\n+\n+\n+def rms_forward(hidden_states, variance_epsilon=1e-6):\n+    \"\"\"\n+    Calculates simple RMSNorm with no learnable weights. `MambaRMSNorm` will\n+    leverage this in order to multiply the final result with the RMSNorm weight\n+\n+    Args:\n+        hidden_states (`torch.Tensor`):\n+            Hidden states to normalize\n+        variance_epsilon (`float`):\n+            The eps value to add in the square root scaling factor\n+    \"\"\"\n+    input_dtype = hidden_states.dtype\n+    hidden_states = hidden_states.to(torch.float32)\n+\n+    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+    hidden_states = hidden_states * torch.rsqrt(variance + variance_epsilon)\n+    return hidden_states.to(input_dtype)\n+\n+\n+class FalconMambaMixer(MambaMixer):\n+    def warn_slow_implementation(self):\n+        is_fast_path_available = all(\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+        )\n+        if not is_fast_path_available:\n+            if self.use_falcon_mambapy:\n+                if is_mambapy_available():\n+                    logger.warning_once(\n+                        \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n+                        \" is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation and\"\n+                        \" https://github.com/Dao-AILab/causal-conv1d\"\n+                    )\n+                else:\n+                    raise ImportError(\n+                        \"use_mambapy is set to True but the mambapy package is not installed. To install it follow https://github.com/alxndrTL/mamba.py.\"\n+                    )\n+            else:\n+                logger.warning_once(\n+                    \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n+                    \" is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and\"\n+                    \" https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\"\n+                )\n+\n+    def __init__(self, config: FalconMambaConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        # Triton expects to pass RMS weights even if they are non learnable, thus we need to create these weights here\n+        self.register_buffer(\n+            \"b_c_rms\", torch.nn.Parameter(torch.ones(self.ssm_state_size), requires_grad=False), persistent=False\n+        )\n+        self.register_buffer(\n+            \"dt_rms\", torch.nn.Parameter(torch.ones(self.intermediate_size), requires_grad=False), persistent=False\n+        )\n+        self.rms_eps = config.mixer_rms_eps\n+\n+    def cuda_kernels_forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cache_params: Optional[FalconMambaCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+    ):\n+        # 1. Gated MLP's linear projection\n+        projected_states = self.in_proj(hidden_states).transpose(1, 2)\n+\n+        if self.training and cache_params is None:  # Doesn't support outputting the states -> used for training\n+            contextualized_states = mamba_inner_fn(\n+                projected_states,\n+                self.conv1d.weight,\n+                self.conv1d.bias if self.use_conv_bias else None,\n+                self.x_proj.weight,\n+                self.dt_proj.weight,\n+                self.out_proj.weight,\n+                self.out_proj.bias.float() if self.use_bias else None,\n+                -torch.exp(self.A_log.float()),\n+                None,  # input-dependent B\n+                None,  # input-dependent C\n+                self.D.float(),\n+                delta_bias=self.dt_proj.bias.float(),\n+                delta_softplus=True,\n+                b_rms_weight=self.b_c_rms,\n+                c_rms_weight=self.b_c_rms,\n+                dt_rms_weight=self.dt_rms,\n+                b_c_dt_rms_eps=self.rms_eps,\n+            )\n+\n+        else:\n+            hidden_states, gate = projected_states.chunk(2, dim=1)\n+\n+            if attention_mask is not None:\n+                hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n+            # 2. Convolution sequence transformation\n+            conv_weights = self.conv1d.weight.view(self.conv1d.weight.size(0), self.conv1d.weight.size(2))\n+            if cache_params is not None and cache_position[0] > 0:\n+                hidden_states = causal_conv1d_update(\n+                    hidden_states.squeeze(-1),\n+                    cache_params.conv_states[self.layer_idx],\n+                    conv_weights,\n+                    self.conv1d.bias,\n+                    self.activation,\n+                )\n+                hidden_states = hidden_states.unsqueeze(-1)\n+            else:\n+                if cache_params is not None:\n+                    conv_states = nn.functional.pad(\n+                        hidden_states, (self.conv_kernel_size - hidden_states.shape[-1], 0)\n+                    )\n+                    cache_params.update_conv_state(self.layer_idx, conv_states, cache_position)\n+                hidden_states = causal_conv1d_fn(\n+                    hidden_states, conv_weights, self.conv1d.bias, activation=self.activation\n+                )\n+\n+            if attention_mask is not None:\n+                hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n+            # 3. State Space Model sequence transformation\n+            # 3.a. input varying initialization of time_step, B and C\n+            ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n+            time_step, B, C = torch.split(\n+                ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n+            )\n+\n+            B = rms_forward(B, variance_epsilon=self.rms_eps)\n+            C = rms_forward(C, variance_epsilon=self.rms_eps)\n+            time_step = rms_forward(time_step, variance_epsilon=self.rms_eps)\n+\n+            # In case the model has been quantized, we need a hack to properly call the `nn.Linear` module\n+            # at the price of a small overhead.\n+            if hasattr(self.config, \"_pre_quantization_dtype\"):\n+                discrete_time_step = (self.dt_proj(time_step) - self.dt_proj.bias).transpose(1, 2)\n+            else:\n+                discrete_time_step = self.dt_proj.weight @ time_step.transpose(1, 2)\n+\n+            A = -torch.exp(self.A_log.float())\n+            # 3.c perform the recurrence y â† SSM(A, B, C)(x)\n+            time_proj_bias = self.dt_proj.bias.float() if hasattr(self.dt_proj, \"bias\") else None\n+            if cache_params is not None and cache_position[0] > 0:\n+                scan_outputs = selective_state_update(\n+                    cache_params.ssm_states[self.layer_idx],\n+                    hidden_states[..., 0],\n+                    discrete_time_step[..., 0],\n+                    A,\n+                    B[:, 0],\n+                    C[:, 0],\n+                    self.D,\n+                    gate[..., 0],\n+                    time_proj_bias,\n+                    dt_softplus=True,\n+                ).unsqueeze(-1)\n+            else:\n+                scan_outputs, ssm_state = selective_scan_fn(\n+                    hidden_states,\n+                    discrete_time_step,\n+                    A,\n+                    B.transpose(1, 2),\n+                    C.transpose(1, 2),\n+                    self.D.float(),\n+                    gate,\n+                    time_proj_bias,\n+                    delta_softplus=True,\n+                    return_last_state=True,\n+                )\n+                if ssm_state is not None and cache_params is not None:\n+                    cache_params.update_ssm_state(self.layer_idx, ssm_state)\n+\n+            # 4. Final linear projection\n+            contextualized_states = self.out_proj(scan_outputs.transpose(1, 2))\n+        return contextualized_states\n+\n+    def slow_forward(\n+        self,\n+        input_states,\n+        cache_params: Optional[FalconMambaCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+    ):\n+        batch_size, seq_len, _ = input_states.shape\n+        dtype = input_states.dtype\n+        # 1. Gated MLP's linear projection\n+        projected_states = self.in_proj(input_states).transpose(1, 2)  # [batch, 2 * intermediate_size, seq_len]\n+        hidden_states, gate = projected_states.chunk(2, dim=1)\n+\n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n+        # 2. Convolution sequence transformation\n+        if cache_params is not None:\n+            ssm_state = cache_params.ssm_states[self.layer_idx].clone()\n+            ssm_state = ssm_state.to(hidden_states.device)\n+            # use `cache_position.shape[0]` to check whether we are in prefill\n+            # stage, it's equivalent to check `cache_position[0] == 0`, which\n+            # breaks dynamo fullgraph constraints\n+            if cache_position is not None and cache_position.shape[0] == self.conv_kernel_size:\n+                conv_state = nn.functional.pad(hidden_states, (self.conv_kernel_size - hidden_states.shape[-1], 0))\n+\n+                cache_params.update_conv_state(self.layer_idx, conv_state, cache_position)\n+                hidden_states = self.act(\n+                    self.conv1d(hidden_states)[..., :seq_len]\n+                )  # [batch, intermediate_size, seq_len]\n+            else:\n+                conv_state = cache_params.update_conv_state(self.layer_idx, hidden_states, cache_position)\n+                conv_state = conv_state.to(self.conv1d.weight.device)\n+                hidden_states = torch.sum(conv_state * self.conv1d.weight[:, 0, :], dim=-1)\n+                if self.use_conv_bias:\n+                    hidden_states += self.conv1d.bias\n+                hidden_states = (\n+                    self.act(hidden_states).to(dtype).unsqueeze(-1)\n+                )  # [batch, intermediate_size, 1] : decoding\n+        else:\n+            ssm_state = torch.zeros(\n+                (batch_size, self.intermediate_size, self.ssm_state_size), device=hidden_states.device, dtype=dtype\n+            )\n+            hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])  # [batch, intermediate_size, seq_len]\n+\n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n+        # 3. State Space Model sequence transformation\n+        # 3.a. Selection:  [batch, seq_len, self.time_step_rank + self.ssm_state_size * 2]\n+        ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n+        time_step, B, C = torch.split(\n+            ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n+        )\n+\n+        B = rms_forward(B, variance_epsilon=self.rms_eps)\n+        C = rms_forward(C, variance_epsilon=self.rms_eps)\n+        time_step = rms_forward(time_step, variance_epsilon=self.rms_eps)\n+\n+        discrete_time_step = self.dt_proj(time_step)  # [batch, seq_len, intermediate_size]\n+        discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(\n+            1, 2\n+        )  # [batch, intermediate_size, seq_len]\n+\n+        # 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)\n+        A = -torch.exp(self.A_log.float())  # [intermediate_size, ssm_state_size]\n+        discrete_A = torch.exp(\n+            A[None, :, None, :] * discrete_time_step[:, :, :, None]\n+        )  # [batch, intermediate_size, seq_len, ssm_state_size]\n+        discrete_B = (\n+            discrete_time_step[:, :, :, None] * B[:, None, :, :].float()\n+        )  # [batch, intermediate_size, seq_len, ssm_state_size]\n+        deltaB_u = discrete_B * hidden_states[:, :, :, None].float()\n+\n+        # 3.c perform the recurrence y â† SSM(A, B, C)(x)\n+        if self.use_falcon_mambapy and self.training and cache_params is None:\n+            hs = pscan(\n+                discrete_A.transpose(1, 2), deltaB_u.transpose(1, 2)\n+            )  # [batch, seq_len, intermediate_size, ssm_state_size]\n+            scan_output = (hs @ C.unsqueeze(-1)).squeeze(3).transpose(1, 2)  # [batch, intermediate_size, seq_len]\n+            scan_output = scan_output + hidden_states * self.D[None, :, None]\n+            scan_output = scan_output * self.act(gate)\n+        else:\n+            scan_outputs = []\n+            for i in range(seq_len):\n+                ssm_state = (\n+                    discrete_A[:, :, i, :] * ssm_state + deltaB_u[:, :, i, :]\n+                )  # [batch, intermediate_size, ssm_state]\n+                scan_output = torch.matmul(\n+                    ssm_state.to(dtype), C[:, i, :].unsqueeze(-1)\n+                )  # [batch, intermediate_size, 1]\n+                scan_outputs.append(scan_output[:, :, 0])\n+            scan_output = torch.stack(scan_outputs, dim=-1)  # [batch, intermediate_size, seq_len]\n+            scan_output = scan_output + (hidden_states * self.D[None, :, None])\n+            scan_output = scan_output * self.act(gate)\n+\n+            if cache_params is not None:\n+                cache_params.update_ssm_state(self.layer_idx, ssm_state)\n+\n+        # 4. Final linear projection\n+        contextualized_states = self.out_proj(scan_output.transpose(1, 2))  # [batch, seq_len, hidden_size]\n+        return contextualized_states\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        cache_params: Optional[FalconMambaCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+    ):\n+        is_fast_path_available = all(\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+        )\n+        if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not torch._dynamo.is_compiling():\n+            return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n+        return self.slow_forward(hidden_states, cache_params, cache_position, attention_mask)\n+\n+\n+class FalconMambaRMSNorm(MambaRMSNorm):\n+    def forward(self, hidden_states):\n+        return self.weight.to(hidden_states.device) * rms_forward(\n+            hidden_states, variance_epsilon=self.variance_epsilon\n+        )\n+\n+\n+class FalconMambaBlock(MambaBlock):\n+    pass\n+\n+\n+@auto_docstring\n+class FalconMambaPreTrainedModel(MambaPreTrainedModel):\n+    pass\n+\n+\n+class FalconMambaOutput(MambaOutput):\n+    pass\n+\n+\n+class FalconMambaCausalLMOutput(MambaCausalLMOutput):\n+    pass\n+\n+\n+class FalconMambaModel(MambaModel, FalconMambaPreTrainedModel):\n+    def __init__(self, config):\n+        FalconMambaPreTrainedModel.__init__(config)\n+\n+        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n+        self.layers = nn.ModuleList(\n+            [FalconMambaBlock(config, layer_idx=idx) for idx in range(config.num_hidden_layers)]\n+        )\n+\n+        self.gradient_checkpointing = False\n+        self.norm_f = FalconMambaRMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def load_hook(self, state_dict, prefix, *args):\n+        raise AttributeError(\"Not needed for FalconMamba\")\n+\n+\n+class FalconMambaForCausalLM(MambaForCausalLM):\n+    pass\n+\n+\n+__all__ = [\n+    \"FalconMambaForCausalLM\",\n+    \"FalconMambaModel\",\n+    \"FalconMambaPreTrainedModel\",\n+    \"FalconMambaCache\",\n+    \"FalconMambaConfig\",\n+]"
        },
        {
            "sha": "06d87b4d5c6105039817962868e5249c5dac6265",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 128,
            "deletions": 28,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import MambaCache\n+from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n@@ -55,9 +55,106 @@\n else:\n     causal_conv1d_update, causal_conv1d_fn = None, None\n \n-is_fast_path_available = all(\n-    (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n-)\n+\n+class MambaCache:\n+    \"\"\"\n+    Cache for mamba model which does not have attention mechanism and key value states.\n+\n+    Arguments:\n+        config (`PretrainedConfig):\n+            The configuration file defining the shape-related attributes required to initialize the static cache.\n+        max_batch_size (`int`):\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a smaller batch size is used.\n+        dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n+            The default `dtype` to use when initializing the layer.\n+        device (`torch.device` or `str`, *optional*):\n+            The device on which the cache should be initialized. Should be the same as the layer.\n+\n+    Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, MambaForCausalLM, MambaCache\n+\n+        >>> model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n+\n+        >>> inputs = tokenizer(text=\"My name is Mamba\", return_tensors=\"pt\")\n+\n+        >>> # Prepare a cache class and pass it to model's forward\n+        >>> past_key_values = MambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n+        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n+        >>> outputs.past_key_values\n+        MambaCache()\n+        ```\n+    \"\"\"\n+\n+    is_compileable = True\n+\n+    # TODO (joao): add layer_device_map arg and update code in `generate` accordingly\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        max_batch_size: int,\n+        dtype: torch.dtype = torch.float16,\n+        device: Union[torch.device, str, None] = None,\n+    ):\n+        self.max_batch_size = max_batch_size\n+        self._dtype = dtype\n+        self.intermediate_size = config.intermediate_size\n+        self.ssm_state_size = config.state_size\n+        self.conv_kernel_size = config.conv_kernel\n+\n+        self.conv_states: list[torch.Tensor] = []\n+        self.ssm_states: list[torch.Tensor] = []\n+        device = torch.device(device) if device is not None else None\n+        for _ in range(config.num_hidden_layers):\n+            conv_state: torch.Tensor = torch.zeros(\n+                self.max_batch_size,\n+                self.intermediate_size,\n+                self.conv_kernel_size,\n+                device=device,\n+                dtype=self._dtype,\n+            )\n+            ssm_state: torch.Tensor = torch.zeros(\n+                self.max_batch_size,\n+                self.intermediate_size,\n+                self.ssm_state_size,\n+                device=device,\n+                dtype=self._dtype,\n+            )\n+\n+            torch._dynamo.mark_static_address(conv_state)\n+            torch._dynamo.mark_static_address(ssm_state)\n+            self.conv_states.append(conv_state)\n+            self.ssm_states.append(ssm_state)\n+\n+    def update_conv_state(\n+        self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor\n+    ) -> torch.Tensor:\n+        # This `if` blocks is only reached in multigpu and if `layer_device_map` is not passed. It is used\n+        # when the cache is initialized in the forward pass (e.g. Mamba)\n+        if self.conv_states[layer_idx].device != new_conv_state.device:\n+            self.conv_states[layer_idx] = self.conv_states[layer_idx].to(new_conv_state.device)\n+\n+        conv_state = self.conv_states[layer_idx]\n+        cache_position = cache_position.clamp(0, self.conv_kernel_size - 1)\n+\n+        conv_state = conv_state.roll(shifts=-1, dims=-1)\n+        conv_state[:, :, cache_position] = new_conv_state.to(device=conv_state.device, dtype=conv_state.dtype)\n+        self.conv_states[layer_idx].zero_()\n+        self.conv_states[layer_idx] += conv_state\n+        return self.conv_states[layer_idx]\n+\n+    def update_ssm_state(self, layer_idx: int, new_ssm_state: torch.Tensor):\n+        self.ssm_states[layer_idx].zero_()\n+        self.ssm_states[layer_idx] += new_ssm_state.to(self.ssm_states[layer_idx].device)\n+        return self.ssm_states[layer_idx]\n+\n+    def reset(self):\n+        for layer_idx in range(len(self.conv_states)):\n+            # In-place ops prevent breaking the static address\n+            self.conv_states[layer_idx].zero_()\n+            self.ssm_states[layer_idx].zero_()\n \n \n class MambaMixer(nn.Module):\n@@ -109,6 +206,12 @@ def __init__(self, config: MambaConfig, layer_idx: int):\n         self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n         self.use_bias = config.use_bias\n \n+        self.warn_slow_implementation()\n+\n+    def warn_slow_implementation(self):\n+        is_fast_path_available = all(\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+        )\n         if not is_fast_path_available:\n             if self.use_mambapy:\n                 if is_mambapy_available():\n@@ -319,6 +422,9 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n+        is_fast_path_available = all(\n+            (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+        )\n         if is_fast_path_available and \"cuda\" in self.x_proj.weight.device.type and not torch._dynamo.is_compiling():\n             return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n         return self.slow_forward(hidden_states, cache_params, cache_position, attention_mask)\n@@ -650,32 +756,26 @@ def prepare_inputs_for_generation(\n         **kwargs,\n     ):\n         # Overwritten -- uses `cache_params` as opposed to `past_key_values`\n-\n-        if use_cache:\n-            # `cache_position` should have been initialized in `generate`\n-            if cache_position is None:\n-                raise ValueError(\n-                    \"`cache_position` should not be None as it should have been initialized in \"\n-                    \"`model.generate`, you are responsible for passing in a valid `cache_position` if \"\n-                    \"you are calling `prepare_inputs_for_generation` directly with `use_cache=True`\"\n-                )\n-            if cache_position[0] > 0:\n-                input_ids = input_ids[:, -1].unsqueeze(-1)\n-\n-                if attention_mask is not None:\n-                    attention_mask = None\n-\n+        model_inputs = {\"input_ids\": input_ids.contiguous()}\n+        if use_cache and cache_params is None:\n+            # we initialize the `cache_position` to full size of `conv_states` at prefill stage\n+            # considering padding will be applied when input length is shorter, and truncation\n+            # will be applied when it is longer, so it will be equivalent to always have it match\n+            # the length of `cache_params.conv_states`, which is `config.conv_kernel`\n+            cache_position = torch.arange(0, self.backbone.config.conv_kernel, device=input_ids.device)\n+            if inputs_embeds is not None:\n+                model_inputs = {\"inputs_embeds\": inputs_embeds}\n+                max_batch_size = inputs_embeds.size(0)\n             else:\n-                # we initialize the `cache_position` to full size of `conv_states` at prefill stage\n-                # considering padding will be applied when input length is shorter, and truncation\n-                # will be applied when it is longer, so it will be equivalent to always have it match\n-                # the length of `cache_params.conv_states`, which is `config.conv_kernel`\n-                cache_position = torch.arange(0, self.config.conv_kernel, device=input_ids.device)\n+                max_batch_size = input_ids.size(0)\n+            cache_params = MambaCache(self.backbone.config, max_batch_size, device=self.device, dtype=self.dtype)\n+\n+        if use_cache and cache_position[0] > 0:\n+            model_inputs[\"input_ids\"] = input_ids[:, -1].unsqueeze(-1).contiguous()\n+            attention_mask = None\n \n-        if inputs_embeds is not None and cache_params is None:\n+        if not use_cache and inputs_embeds is not None:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids.contiguous()}\n \n         model_inputs.update(\n             {\n@@ -751,4 +851,4 @@ def forward(\n         )\n \n \n-__all__ = [\"MambaForCausalLM\", \"MambaModel\", \"MambaPreTrainedModel\"]\n+__all__ = [\"MambaForCausalLM\", \"MambaModel\", \"MambaPreTrainedModel\", \"MambaCache\"]"
        },
        {
            "sha": "5a83186fb074956e09c3e037bc656d4c8ab49d0b",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 23,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -970,38 +970,33 @@ def prepare_inputs_for_generation(\n         **kwargs,\n     ):\n         # Overwritten -- uses `cache_params` as opposed to `past_key_values`\n-\n-        if use_cache:\n-            # `cache_position` should have been initialized in `generate`\n-            if cache_position is None:\n-                raise ValueError(\n-                    \"`cache_position` should not be None as it should have been initialized in \"\n-                    \"`model.generate`, you are responsible for passing in a valid `cache_position` if \"\n-                    \"you are calling `prepare_inputs_for_generation` directly with `use_cache=True`\"\n-                )\n-            if cache_position[0] > 0:\n-                input_ids = input_ids[:, -1][..., None]\n-\n-                if attention_mask is not None:\n-                    attention_mask = None\n+        model_inputs = {\"input_ids\": input_ids.contiguous()}\n+        if use_cache and cache_params is None:\n+            # we initialize the `cache_position` to full size of `conv_states` at prefill stage\n+            # considering padding will be applied when input length is shorter, and truncation\n+            # will be applied when it is longer, so it will be equivalent to always have it match\n+            # the length of `cache_params.conv_states`, which is `config.conv_kernel`\n+            cache_position = torch.arange(0, self.backbone.config.conv_kernel, device=input_ids.device)\n+            if inputs_embeds is not None:\n+                model_inputs = {\"inputs_embeds\": inputs_embeds}\n+                max_batch_size = inputs_embeds.size(0)\n             else:\n-                # we initialize the `cache_position` to full size of `conv_states` at prefill stage\n-                # considering padding will be applied when input length is shorter, and truncation\n-                # will be applied when it is longer, so it will be equivalent to always have it match\n-                # the length of `cache_params.conv_states`, which is `config.conv_kernel`\n-                cache_position = torch.arange(0, self.config.conv_kernel, device=input_ids.device)\n+                max_batch_size = input_ids.size(0)\n+            cache_params = Mamba2Cache(self.backbone.config, max_batch_size, device=self.device, dtype=self.dtype)\n \n-        if inputs_embeds is not None and cache_params is None:\n+        if use_cache and cache_position[0] > 0:\n+            model_inputs[\"input_ids\"] = input_ids[:, -1].unsqueeze(-1).contiguous()\n+            attention_mask = None\n+\n+        if not use_cache and inputs_embeds is not None:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n \n         model_inputs.update(\n             {\n-                \"attention_mask\": attention_mask,\n                 \"cache_params\": cache_params,\n                 \"use_cache\": use_cache,\n                 \"cache_position\": cache_position,\n+                \"attention_mask\": attention_mask,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "7026bf1697c84d75cf4621602bd2483b5b924f4c",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -44,13 +44,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class MambaCache(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class OffloadedCache(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "855d0f2103688ae512b233500be3ac8aac3f5a32",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 12,
            "deletions": 31,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -26,7 +26,6 @@\n     require_torch_accelerator,\n     require_torch_large_accelerator,\n     require_torch_multi_accelerator,\n-    require_torch_multi_gpu,\n     slow,\n     torch_device,\n )\n@@ -41,10 +40,10 @@\n     import torch\n \n     from transformers import (\n+        FalconMambaCache,\n         FalconMambaForCausalLM,\n         FalconMambaModel,\n     )\n-    from transformers.cache_utils import MambaCache\n \n \n # Copied from transformers.tests.models.mamba.MambaModelTester with Mamba->FalconMamba,mamba->falcon_mamba\n@@ -312,31 +311,6 @@ def assertInterval(self, member, container, msg=None):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    @require_torch_multi_gpu\n-    def test_multi_gpu_data_parallel_forward(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        # some params shouldn't be scattered by nn.DataParallel\n-        # so just remove them if they are present.\n-        blacklist_non_batched_params = [\"cache_params\"]\n-        for k in blacklist_non_batched_params:\n-            inputs_dict.pop(k, None)\n-\n-        # move input tensors to cuda:O\n-        for k, v in inputs_dict.items():\n-            if torch.is_tensor(v):\n-                inputs_dict[k] = v.to(0)\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=config)\n-            model.to(0)\n-            model.eval()\n-\n-            # Wrap model in nn.DataParallel\n-            model = torch.nn.DataParallel(model)\n-            with torch.no_grad():\n-                _ = model(**self._prepare_for_class(inputs_dict, model_class))\n-\n     def test_falcon_mamba_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_falcon_mamba_model(*config_and_inputs)\n@@ -411,7 +385,7 @@ def check_equivalence(model, tuple_inputs, dict_inputs, additional_kwargs={}):\n                 dict_output = model(**dict_inputs, return_dict=True, **additional_kwargs).to_tuple()\n \n                 def recursive_check(tuple_object, dict_object):\n-                    if isinstance(tuple_object, MambaCache):  # MODIFIED PART START\n+                    if isinstance(tuple_object, FalconMambaCache):  # MODIFIED PART START\n                         recursive_check(tuple_object.conv_states, dict_object.conv_states)\n                         recursive_check(tuple_object.ssm_states, dict_object.ssm_states)\n                     elif isinstance(tuple_object, (list, tuple)):  # MODIFIED PART END\n@@ -458,6 +432,10 @@ def recursive_check(tuple_object, dict_object):\n             dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n             check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n \n+    @unittest.skip(\"Mamba models do not support DDP.\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n \n @require_torch\n @require_torch_accelerator\n@@ -497,7 +475,9 @@ def test_generation_fp16(self):\n     @require_bitsandbytes\n     def test_generation_4bit(self):\n         quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, quantization_config=quantization_config)\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, quantization_config=quantization_config).to(\n+            torch_device\n+        )\n \n         inputs = self.tokenizer(self.text, return_tensors=\"pt\").to(torch_device)\n         out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n@@ -513,6 +493,7 @@ def test_generation_torch_compile(self):\n \n         inputs = self.tokenizer(self.text, return_tensors=\"pt\").to(torch_device)\n         out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        print(self.tokenizer.batch_decode(out, skip_special_tokens=False)[0])\n \n         self.assertEqual(\n             self.tokenizer.batch_decode(out, skip_special_tokens=False)[0],\n@@ -543,7 +524,7 @@ def test_batched_generation(self):\n         inputs = tok(texts, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(torch_device)\n         model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0, torch_dtype=torch.float16)\n \n-        out = model.generate(**inputs, max_new_tokens=20)\n+        out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         out = tok.batch_decode(out, skip_special_tokens=True)\n \n         self.assertListEqual(out, EXPECTED_OUTPUT)\n@@ -553,7 +534,7 @@ def test_batched_generation(self):\n             inputs_embeds = model.get_input_embeddings()(inputs.pop(\"input_ids\"))\n \n         inputs[\"inputs_embeds\"] = inputs_embeds\n-        out = model.generate(**inputs, max_new_tokens=20)\n+        out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         out = tok.batch_decode(out, skip_special_tokens=True)\n \n         EXPECTED_OUTPUTS = Expectations("
        },
        {
            "sha": "e99c8b1e5795013451639f22e17123368e3a2597",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 8,
            "deletions": 29,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -20,7 +20,7 @@\n from parameterized import parameterized\n \n from transformers import AutoTokenizer, MambaConfig, is_torch_available\n-from transformers.testing_utils import require_torch, require_torch_multi_gpu, slow, torch_device\n+from transformers.testing_utils import require_torch, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -32,10 +32,10 @@\n     import torch\n \n     from transformers import (\n+        MambaCache,\n         MambaForCausalLM,\n         MambaModel,\n     )\n-    from transformers.models.mamba.modeling_mamba import MambaCache\n \n \n class MambaModelTester:\n@@ -279,31 +279,6 @@ def assertInterval(self, member, container, msg=None):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    @require_torch_multi_gpu\n-    def test_multi_gpu_data_parallel_forward(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        # some params shouldn't be scattered by nn.DataParallel\n-        # so just remove them if they are present.\n-        blacklist_non_batched_params = [\"cache_params\"]\n-        for k in blacklist_non_batched_params:\n-            inputs_dict.pop(k, None)\n-\n-        # move input tensors to cuda:O\n-        for k, v in inputs_dict.items():\n-            if torch.is_tensor(v):\n-                inputs_dict[k] = v.to(0)\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=config)\n-            model.to(0)\n-            model.eval()\n-\n-            # Wrap model in nn.DataParallel\n-            model = torch.nn.DataParallel(model)\n-            with torch.no_grad():\n-                _ = model(**self._prepare_for_class(inputs_dict, model_class))\n-\n     def test_mamba_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_mamba_model(*config_and_inputs)\n@@ -452,6 +427,10 @@ def test_dtype_mismatch_handled_in_cache(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.hidden_size),\n         )\n \n+    @unittest.skip(\"Mamba models do not support DDP.\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n \n @require_torch\n class MambaIntegrationTests(unittest.TestCase):\n@@ -547,11 +526,11 @@ def test_compile_mamba_cache(self):\n             torch_device\n         )\n \n-        output = model.generate(input_ids, max_new_tokens=20, cache_implementation=\"mamba\")\n+        output = model.generate(input_ids, max_new_tokens=20)\n         output_sentence = self.tokenizer.decode(output[0].tolist())\n         self.assertEqual(output_sentence, expected_output)\n \n         model.forward = torch.compile(model.forward, fullgraph=True, mode=\"reduce-overhead\")\n-        output = model.generate(input_ids, max_new_tokens=20, cache_implementation=\"mamba\")\n+        output = model.generate(input_ids, max_new_tokens=20)\n         output_sentence = self.tokenizer.decode(output[0].tolist())\n         self.assertEqual(output_sentence, expected_output)"
        },
        {
            "sha": "b1998b7cfeded5a82d32acb31d3eb9c28401c62b",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -62,8 +62,6 @@\n TEST_CACHE_IMPLEMENTATIONS = [\n     cache_name\n     for cache_name in ALL_CACHE_IMPLEMENTATIONS\n-    # TODO (joao): Mamba is not compatible with most models, remove from `ALL_CACHE_IMPLEMENTATIONS`?\n-    if cache_name != \"mamba\"\n     # TODO (joao): offloaded_hybrid == offloaded_hybrid_chunked, deprecate one of them\n     if cache_name != \"offloaded_hybrid\"\n ]"
        },
        {
            "sha": "3b447dc7bffb981d3c875413799955fd4550727f",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aa7256f01ce771220daaaf36af33b9f59447e5c/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aa7256f01ce771220daaaf36af33b9f59447e5c/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=1aa7256f01ce771220daaaf36af33b9f59447e5c",
            "patch": "@@ -141,7 +141,12 @@ def leave_Name(self, original_node, updated_node):\n         return updated_node\n \n     def leave_ImportFrom(self, original_node, updated_node):\n-        \"\"\"The imports from other file types (configuration, processing etc) should use original model name.\"\"\"\n+        \"\"\"\n+        The imports from other file types (configuration, processing etc) should use original model name.\n+        Also, no replaces on absolute imports (e.g. `from mamba_ssm import ...`)\n+        \"\"\"\n+        if len(original_node.relative) == 0:  # no replaces on absolute imports\n+            return original_node\n         if self.original_new_model_name != self.new_name and m.matches(updated_node.module, m.Name()):\n             patterns = \"|\".join(ALL_FILE_TYPES)\n             regex = rf\"({patterns})_{self.new_name}\""
        }
    ],
    "stats": {
        "total": 1340,
        "additions": 1033,
        "deletions": 307
    }
}