{
    "author": "eginhard",
    "message": "refactor: remove custom BarkLayerNorm (#39003)\n\n`nn.LayerNorm` supports `bias=False` since Pytorch 2.1",
    "sha": "3233e9b7c3745705c7047a823a19a6ac889239aa",
    "files": [
        {
            "sha": "775114e3bbf59370b38c22fb7a89dd4a3940d94e",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 5,
            "deletions": 18,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/3233e9b7c3745705c7047a823a19a6ac889239aa/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3233e9b7c3745705c7047a823a19a6ac889239aa/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=3233e9b7c3745705c7047a823a19a6ac889239aa",
            "patch": "@@ -282,18 +282,6 @@ def forward(\n }\n \n \n-class BarkLayerNorm(nn.Module):\n-    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False.\"\"\"\n-\n-    def __init__(self, hidden_size, bias=True):\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.bias = nn.Parameter(torch.zeros(hidden_size)) if bias else None\n-\n-    def forward(self, input):\n-        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, eps=1e-5)\n-\n-\n class BarkMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -315,11 +303,10 @@ def __init__(self, config, is_causal=False):\n         super().__init__()\n \n         if is_causal:\n-            # if causal, uses handmade LayerNorm, so that the layerNorm bias is optional\n-            # this handmade layerNorm is used to stick with Bark choice of leaving optional bias in\n-            # AutoRegressive models (corresponding to the \"Text\" and the \"Coarse\" modules)\n-            self.layernorm_1 = BarkLayerNorm(config.hidden_size, bias=config.bias)\n-            self.layernorm_2 = BarkLayerNorm(config.hidden_size, bias=config.bias)\n+            # if causal, the layerNorm bias is optional to stick with Bark choice of leaving optional bias\n+            # in AutoRegressive models (corresponding to the \"Text\" and the \"Coarse\" modules)\n+            self.layernorm_1 = nn.LayerNorm(config.hidden_size, bias=config.bias)\n+            self.layernorm_2 = nn.LayerNorm(config.hidden_size, bias=config.bias)\n         else:\n             self.layernorm_1 = nn.LayerNorm(config.hidden_size)\n             self.layernorm_2 = nn.LayerNorm(config.hidden_size)\n@@ -427,7 +414,7 @@ def __init__(self, config):\n         self.layers = nn.ModuleList([BarkBlock(config, is_causal=True) for _ in range(config.num_layers)])\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n-        self.layernorm_final = BarkLayerNorm(config.hidden_size, bias=config.bias)\n+        self.layernorm_final = nn.LayerNorm(config.hidden_size, bias=config.bias)\n \n         self.lm_head = nn.Linear(config.hidden_size, config.output_vocab_size, bias=False)\n         self.gradient_checkpointing = False"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 5,
        "deletions": 18
    }
}