{
    "author": "LysandreJik",
    "message": "Responses API in `transformers serve` (#39155)\n\n* Scaffolding\n\n* Explicit content\n\n* Naïve Responses API streaming implementation\n\n* Cleanup\n\n* Responses API (to be merged into #39155) (#39338)\n\n* Scaffolding\n\n* Explicit content\n\n* Naïve Responses API streaming implementation\n\n* Cleanup\n\n* use openai\n\n* validate request, including detecting unused fields\n\n* dict indexing\n\n* dict var access\n\n* tmp commit (tests failing)\n\n* add slow\n\n* use oai output type in completions\n\n* (little rebase errors)\n\n* working spec?\n\n* guard type hint\n\n* type hints. fix state (CB can now load different models)\n\n* type hints; fn names; error type\n\n* add docstrings\n\n* responses + kv cache\n\n* metadata support; fix kv cache; error event\n\n* add output_index and content_index\n\n* docstrings\n\n* add test_build_response_event\n\n* docs/comments\n\n* gate test requirements; terminate cb manager on model switch\n\n* nasty type hints\n\n* more type hints\n\n* disable validation by default; enable force models\n\n* todo\n\n---------\n\nCo-authored-by: Lysandre <hi@lysand.re>\n\n* Slight bugfixes\n\n* PR comments from #39338\n\n* make fixup\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\nCo-authored-by: Joao Gante <joao@huggingface.co>",
    "sha": "de5ca373acefc3c5cfc99aa697e7a073f7a2de23",
    "files": [
        {
            "sha": "5f73f7e13633db91b1c820bd95f633acdfbd98dd",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=de5ca373acefc3c5cfc99aa697e7a073f7a2de23",
            "patch": "@@ -71,7 +71,7 @@ vllm serve Qwen/Qwen2.5-1.5B-Instruct \\\n > This section is experimental and subject to change in future versions\n \n <!-- TODO: LLMs -> models, after we add audio/image input/output support -->\n-You can serve LLMs supported by `transformers` with the `transformers serve` CLI. It spawns a local server that offers a chat Completions API compatible with the OpenAI SDK, which is the _de facto_ standard for LLM conversations. This way, you can use the server from many third party applications, or test it using the `transformers chat` CLI ([docs](conversations.md#chat-cli)).\n+You can serve LLMs supported by `transformers` with the `transformers serve` CLI. It spawns a local server that offers a Chat Completion API or a Response API compatible with the OpenAI SDK, which are the _de facto_ standard for LLM conversations. This way, you can use the server from many third party applications, or test it using the `transformers chat` CLI ([docs](conversations.md#chat-cli)).\n \n To launch a server, simply use the `transformers serve` CLI command:\n "
        },
        {
            "sha": "75e25e45be7f5f9b6a7a7356eca53ec5d325ae49",
            "filename": "setup.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=de5ca373acefc3c5cfc99aa697e7a073f7a2de23",
            "patch": "@@ -137,6 +137,7 @@\n     \"onnxconverter-common\",\n     \"onnxruntime-tools>=1.4.2\",\n     \"onnxruntime>=1.4.0\",\n+    \"openai\",\n     \"opencv-python\",\n     \"optimum-benchmark>=0.3.0\",\n     \"optuna\",\n@@ -314,7 +315,7 @@ def run(self):\n \n extras[\"integrations\"] = extras[\"hub-kernels\"] + extras[\"optuna\"] + extras[\"ray\"] + extras[\"sigopt\"]\n \n-extras[\"serving\"] = deps_list(\"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\") + extras[\"torch\"]\n+extras[\"serving\"] = deps_list(\"openai\", \"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\") + extras[\"torch\"]\n extras[\"audio\"] = deps_list(\n     \"librosa\",\n     \"pyctcdecode\","
        },
        {
            "sha": "81a01932a04f9fde997f6bc0c78855ecdd09eea8",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=de5ca373acefc3c5cfc99aa697e7a073f7a2de23",
            "patch": "@@ -471,7 +471,7 @@ def get_generation_parameterization(\n             # This is a chat session, so we have a few non-standard defaults\n             # !!!!!!!!!\n             generation_config = copy.deepcopy(model_generation_config)\n-            generation_config.update({\"do_sample\": True, \"max_new_tokens\": 256})\n+            generation_config.update(**{\"do_sample\": True, \"max_new_tokens\": 256})\n \n         # Finally: parse and apply `generate_flags`\n         parsed_generate_flags = self.parse_generate_flags(args.generate_flags)"
        },
        {
            "sha": "bf762b2214cb84028870c4a28400eddcb7cd8053",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 824,
            "deletions": 353,
            "changes": 1177,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=de5ca373acefc3c5cfc99aa697e7a073f7a2de23",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n import copy\n import functools\n import json\n@@ -19,11 +20,16 @@\n from argparse import ArgumentParser, Namespace\n from dataclasses import dataclass, field\n from threading import Thread\n-from typing import Any, Optional\n+from typing import Generator, Optional\n \n from huggingface_hub import ModelInfo, model_info\n \n-from transformers.utils.import_utils import is_fastapi_available, is_pydantic_available, is_uvicorn_available\n+from transformers.utils.import_utils import (\n+    is_fastapi_available,\n+    is_openai_available,\n+    is_pydantic_available,\n+    is_uvicorn_available,\n+)\n \n from .. import LogitsProcessorList, PreTrainedTokenizerFast, TextIteratorStreamer\n from ..generation.continuous_batching import ContinuousBatchingManager, RequestStatus\n@@ -42,53 +48,108 @@\n         PreTrainedModel,\n     )\n \n-\n-if is_pydantic_available() and is_fastapi_available() and is_uvicorn_available():\n+serve_dependencies_available = (\n+    is_pydantic_available() and is_fastapi_available() and is_uvicorn_available() and is_openai_available()\n+)\n+if serve_dependencies_available:\n     import uvicorn\n-    from fastapi import FastAPI\n+    from fastapi import FastAPI, HTTPException\n     from fastapi.middleware.cors import CORSMiddleware\n     from fastapi.responses import JSONResponse, StreamingResponse\n-    from pydantic import BaseModel\n-\n-    class Message(BaseModel):\n-        role: str\n-        content: str\n-\n-    class ChatCompletionInput(BaseModel):\n-        messages: list[Message]\n-\n-        stream: Optional[bool] = False\n-        model: Optional[str] = None\n-        request_id: Optional[str] = None\n-        extra_body: Optional[dict] = None\n-        frequency_penalty: Optional[float] = None\n-        logit_bias: Optional[list[float]] = None\n-        max_tokens: Optional[int] = None\n-        stop: Optional[list[str]] = None\n-        temperature: Optional[float] = None\n-        top_p: Optional[float] = None\n-        seed: Optional[int] = None\n-\n-        # Additional options supported by the HFH InferenceClient\n-        # that aren't yet supported here.\n-\n-        # logprobs: Optional[bool] = None\n-        tools: Any = None\n-        # n: Optional[int] = None\n-        # presence_penalty: Optional[float] = None\n-        # response_format: Optional[ChatCompletionInputGrammarType] = None\n-        # stream_options: Optional[ChatCompletionInputStreamOptions] = None\n-        # tool_choice: Optional[Union[ChatCompletionInputToolChoiceClass, \"ChatCompletionInputToolChoiceEnum\"]] = None\n-        # tool_prompt: Optional[str] = None\n-        # top_logprobs: Optional[int] = None\n-\n-        # transformers-specific request fields\n-        generation_config: Optional[str] = None\n+    from openai.types.chat.chat_completion_chunk import (\n+        ChatCompletionChunk,\n+        Choice,\n+        ChoiceDelta,\n+        ChoiceDeltaToolCall,\n+        ChoiceDeltaToolCallFunction,\n+    )\n+    from openai.types.chat.completion_create_params import CompletionCreateParamsStreaming\n+    from openai.types.responses import (\n+        Response,\n+        ResponseCompletedEvent,\n+        ResponseContentPartAddedEvent,\n+        ResponseContentPartDoneEvent,\n+        ResponseCreatedEvent,\n+        ResponseError,\n+        ResponseErrorEvent,\n+        ResponseFailedEvent,\n+        ResponseInProgressEvent,\n+        ResponseOutputItemAddedEvent,\n+        ResponseOutputItemDoneEvent,\n+        ResponseOutputMessage,\n+        ResponseOutputText,\n+        ResponseTextDeltaEvent,\n+        ResponseTextDoneEvent,\n+    )\n+    from openai.types.responses.response_create_params import ResponseCreateParamsStreaming\n+    from pydantic import BaseModel, TypeAdapter, ValidationError\n \n+    # Expand OpenAI's request input types with an optional `generation_config` field\n+    class TransformersResponseCreateParamsStreaming(ResponseCreateParamsStreaming, total=False):\n+        \"\"\"\n+        OpenAI's ResponseCreateParamsStreaming with an additional field for the generation config (as a json string).\n+        \"\"\"\n \n-logger = logging.get_logger(__name__)\n+        generation_config: Optional[str]\n+\n+    class TransformersCompletionCreateParamsStreaming(CompletionCreateParamsStreaming, total=False):\n+        \"\"\"\n+        OpenAI's CompletionCreateParamsStreaming with additional fields for the generation config (as a json string)\n+        and the request ID to re-use the previous KV cache.\n+        \"\"\"\n+\n+        generation_config: Optional[str]\n+        request_id: Optional[str]\n+\n+    # Contrarily to OpenAI's output types, input types are `TypedDict`, which don't have validation\n+    response_validator = TypeAdapter(TransformersResponseCreateParamsStreaming)\n+    completion_validator = TypeAdapter(TransformersCompletionCreateParamsStreaming)\n+\n+    # Define request fields that are not yet used in `transformers serve`. Receiving these fields will raise an\n+    # HTTPException.\n+    UNUSED_RESPONSE_FIELDS = {\n+        \"background\",\n+        \"include\",\n+        \"max_tool_calls\",\n+        \"previous_response_id\",\n+        \"prompt\",\n+        \"reasoning\",\n+        \"service_tier\",\n+        \"store\",\n+        \"text\",\n+        \"tool_choice\",\n+        \"top_logprobs\",\n+        \"truncation\",\n+        \"user\",\n+    }\n+\n+    UNUSED_CHAT_COMPLETION_FIELDS = {\n+        \"audio\",\n+        \"function_call\",\n+        \"functions\",\n+        \"logprobs\",\n+        \"max_completion_tokens\",\n+        \"metadata\",\n+        \"modalities\",\n+        \"n\",\n+        \"parallel_tool_calls\",\n+        \"prediction\",\n+        \"presence_penalty\",\n+        \"reasoning_effort\",\n+        \"response_format\",\n+        \"service_tier\",\n+        \"stop\",\n+        \"store\",\n+        \"stream_options\",\n+        \"tool_choice\",\n+        \"top_logprobs\",\n+        \"user\",\n+        \"web_search_options\",\n+    }\n \n \n+logger = logging.get_logger(__name__)\n+\n # Possible tokens that indicate the start/end of a tool call\n # TODO (joao, matt): streamline tool token detection logic\n _TOOL_CALL_TOKENS = {\n@@ -110,26 +171,30 @@ def serve_command_factory(args: Namespace):\n \n \n def create_generation_config_from_req(\n-    req: \"ChatCompletionInput\", model_generation_config: \"GenerationConfig\", **kwargs\n+    req: dict,\n+    model_generation_config: \"GenerationConfig\",\n+    **kwargs,\n ) -> \"GenerationConfig\":\n     \"\"\"\n     Creates a generation config from the parameters of the request. If a generation config is passed in the request,\n     it will be used as a baseline for parameterization. Otherwise, we will use the model's default generation config.\n     Other parameters in the request will be applied on top of the baseline.\n \n     Args:\n-        req (`ChatCompletionInput`):\n+        req (`dict`):\n             The request which may optionally contain generation parameters.\n         model_generation_config (`GenerationConfig`):\n             The model's default generation config.\n+        kwargs (`dict`):\n+            Additional parameters to set in the generation config.\n \n     Returns:\n         The prepared `GenerationConfig` object.\n     \"\"\"\n     # If there is a generation config in the request, it is a json string serialization from a `GenerationConfig`\n     # object. For simplicity, flags set here take precedence over all other flags.\n-    if req.generation_config is not None:\n-        generation_config = GenerationConfig(**json.loads(req.generation_config))\n+    if req.get(\"generation_config\") is not None:\n+        generation_config = GenerationConfig(**json.loads(req[\"generation_config\"]))\n     else:\n         generation_config = copy.deepcopy(model_generation_config)\n \n@@ -139,20 +204,31 @@ def create_generation_config_from_req(\n         if v is not None:\n             setattr(generation_config, k, v)\n \n-    if req.frequency_penalty is not None:\n-        generation_config.repetition_penalty = float(req.frequency_penalty)\n-    if req.logit_bias is not None:\n-        generation_config.sequence_bias = req.logit_bias\n-    if req.stop is not None:\n-        generation_config.stop_strings = req.stop\n-    if req.temperature is not None:\n-        generation_config.temperature = float(req.temperature)\n-        if float(req.temperature) == 0.0:\n+    # Response-specific parameters\n+    if req.get(\"max_output_tokens\") is not None:\n+        generation_config.max_new_tokens = int(req[\"max_output_tokens\"])\n+\n+    # Completion-specific parameters\n+    if req.get(\"max_tokens\") is not None:\n+        generation_config.max_new_tokens = int(req[\"max_tokens\"])\n+    if req.get(\"frequency_penalty\") is not None:\n+        generation_config.repetition_penalty = float(req[\"frequency_penalty\"])\n+    if req.get(\"logit_bias\") is not None:\n+        generation_config.sequence_bias = req[\"logit_bias\"]\n+    if req.get(\"stop\") is not None:\n+        generation_config.stop_strings = req[\"stop\"]\n+    if req.get(\"temperature\") is not None:\n+        generation_config.temperature = float(req[\"temperature\"])\n+        if float(req[\"temperature\"]) == 0.0:\n             generation_config.do_sample = False\n-    if req.top_p is not None:\n-        generation_config.top_p = float(req.top_p)\n-    if req.seed is not None:\n-        torch.manual_seed(req.seed)\n+    if req.get(\"top_p\") is not None:\n+        generation_config.top_p = float(req[\"top_p\"])\n+    if req.get(\"seed\") is not None:\n+        torch.manual_seed(req[\"seed\"])\n+\n+    # Sets server-specific defaults, if unset\n+    if generation_config.max_new_tokens is None:\n+        generation_config.max_new_tokens = 1024\n \n     return generation_config\n \n@@ -228,14 +304,28 @@ class ServeArguments:\n         },\n     )\n \n+    # TODO\n+    # Testing\n+    # As of 2025-07-11, testing on https://github.com/openai/openai-responses-starter-app/, validation on the\n+    # Response input is failing. The app works well without validation. Enable at some point in the future.\n+    input_validation: bool = field(\n+        default=False,\n+        metadata={\n+            \"help\": (\"Whether to turn on strict input validation.\"),\n+        },\n+    )\n+    force_model: Optional[str] = field(\n+        default=None,\n+        metadata={\n+            \"help\": (\n+                \"Name of the model to be forced on all requests. This is useful for testing Apps that don't allow \"\n+                \"changing models in the request.\"\n+            ),\n+        },\n+    )\n \n-class ServeCommand(BaseTransformersCLICommand):\n-    loaded_model: Optional[str] = None\n-    running_continuous_batching_manager: Optional[ContinuousBatchingManager] = None\n-\n-    model: PreTrainedModel\n-    tokenizer: PreTrainedTokenizerFast\n \n+class ServeCommand(BaseTransformersCLICommand):\n     @staticmethod\n     def register_subcommand(parser: ArgumentParser):\n         \"\"\"\n@@ -249,39 +339,113 @@ def register_subcommand(parser: ArgumentParser):\n         serve_parser.set_defaults(func=serve_command_factory)\n \n     def __init__(self, args: ServeArguments):\n-        if not is_pydantic_available() or not is_fastapi_available() or not is_uvicorn_available():\n+        if not serve_dependencies_available:\n             raise ImportError(\n                 \"Missing dependencies for the serving CLI. Please install with `pip install transformers[serving]`\"\n             )\n \n+        # Store and process input arguments\n         self.args = args\n         self.use_continuous_batching = self.args.attn_implementation == \"sdpa_paged\"\n         self.enable_cors = self.args.enable_cors\n \n-        # State: preserves information about the last call and last KV cache, to determine whether we can reuse the KV\n-        # cache and avoid re-running prefil\n-        self.last_messages = None\n-        self.last_kv_cache = None\n-\n+        # Set up logging\n         transformers_logger = logging.get_logger(\"transformers\")\n         transformers_logger.setLevel(logging.log_levels[self.args.log_level.lower()])\n \n         cb_logger = logging.get_logger(\"transformers.generation.continuous_batching\")\n         cb_logger.setLevel(logging.log_levels[self.args.log_level.lower()])\n \n-    def build_chunk(\n+        # Internal state:\n+        # 1. Tracks the most recently used model, to prevent reloading the model unnecessarily\n+        self.loaded_model: Optional[str] = None\n+        self.running_continuous_batching_manager: Optional[ContinuousBatchingManager] = None\n+        self.model: PreTrainedModel\n+        self.tokenizer: PreTrainedTokenizerFast\n+\n+        # 2. preserves information about the last call and last KV cache, to determine whether we can reuse the KV\n+        # cache and avoid re-running prefil\n+        self.last_messages = None\n+        self.last_kv_cache = None\n+\n+    def _validate_request(\n         self,\n-        request_id: str,\n+        request: dict,\n+        schema: \"_TypedDictMeta\",  # noqa: F821\n+        validator: \"TypeAdapter\",\n+        unused_fields: set,\n+    ):\n+        \"\"\"\n+        Validates the request against the schema, and checks for unexpected keys.\n+\n+        Args:\n+            request (`dict`):\n+                The request to validate.\n+            schema (`_TypedDictMeta`):\n+                The schema of the request to validate. It is a `TypedDict` definition.\n+            validator (`TypeAdapter`):\n+                The validator to use to validate the request. Built from `schema`.\n+            unused_fields (`set`):\n+                Fields accepted by `schema`, but not used in `transformers serve`.\n+\n+        Raises:\n+            HTTPException: If the request is invalid or contains unexpected or unused fields.\n+        \"\"\"\n+        logger.debug(f\"Validating request: {request}\")\n+\n+        # Validate unexpected keys -- Pydantic doesn't validate extra keys in the request.\n+        input_keys = set(request.keys())\n+        possible_keys = schema.__mutable_keys__\n+        unexpected_keys = input_keys - possible_keys\n+        if unexpected_keys:\n+            logger.error(f\"Unexpected keys in the request: {unexpected_keys}\")\n+            raise HTTPException(status_code=422, detail=f\"Unexpected keys in the request: {unexpected_keys}\")\n+\n+        if self.args.input_validation:\n+            # Validate expected keys\n+            try:\n+                validator.validate_python(request)\n+            except ValidationError as e:\n+                logger.error(f\"Validation error: {e.errors()}\")\n+                raise HTTPException(status_code=422, detail=e.errors())\n+\n+            # Validate unused fields\n+            unused_fields_in_request = input_keys & unused_fields\n+            if unused_fields_in_request:\n+                logger.error(f\"Unused fields in the request: {unused_fields_in_request}\")\n+                raise HTTPException(\n+                    status_code=422, detail=f\"Unused fields in the request: {unused_fields_in_request}\"\n+                )\n+\n+    def validate_response_request(self, request: dict):\n+        self._validate_request(\n+            request=request,\n+            schema=TransformersResponseCreateParamsStreaming,\n+            validator=response_validator,\n+            unused_fields=UNUSED_RESPONSE_FIELDS,\n+        )\n+\n+    def validate_chat_completion_request(self, request: dict):\n+        self._validate_request(\n+            request=request,\n+            schema=TransformersCompletionCreateParamsStreaming,\n+            validator=completion_validator,\n+            unused_fields=UNUSED_CHAT_COMPLETION_FIELDS,\n+        )\n+\n+    def build_chat_completion_chunk(\n+        self,\n+        request_id: Optional[str] = \"\",\n         content: Optional[str] = None,\n         role: Optional[str] = None,\n         finish_reason: Optional[str] = None,\n-        tool_calls: Optional[list[dict]] = None,\n+        tool_calls: Optional[list[\"ChoiceDeltaToolCall\"]] = None,\n     ) -> str:\n         \"\"\"\n-        Builds a chunk of a streaming response.\n+        Builds a chunk of a streaming OpenAI Chat Completion response.\n \n-        IMPORTANT: The built chunk won't contain empty fields (fields with `None`). Some downstream apps, like Cursor,\n-        assume that when the field exists, it has data.\n+        IMPORTANT: The serialized chunk won't contain empty fields (fields with `None`). Some downstream apps,\n+        like Cursor, assume that when the field exists, it has data.\n \n         Args:\n             request_id (`str`):\n@@ -292,30 +456,47 @@ def build_chunk(\n                 The role of the next content, until a new role is defined.\n             finish_reason (`str`, *optional*):\n                 The reason the generation by the model has finished.\n-            tool_calls (`list[dict]`, *optional*):\n+            tool_calls (`list[ChoiceDeltaToolCall]`, *optional*):\n                 Data about the tool calls, when they are triggered.\n \n         Returns:\n             `str`: The built chunk, a string containing a JSON string with the payload.\n         \"\"\"\n-        payload = {\n-            \"object\": \"chat.completion.chunk\",\n-            \"id\": request_id,\n-            \"created\": int(time.time()),\n-            \"model\": self.loaded_model,\n-            \"choices\": [{\"delta\": {}, \"index\": 0}],\n-            \"system_fingerprint\": \"\",\n-        }\n-        if content is not None:\n-            payload[\"choices\"][0][\"delta\"][\"content\"] = content\n-        if role is not None:\n-            payload[\"choices\"][0][\"delta\"][\"role\"] = role\n-        if tool_calls is not None:\n-            payload[\"choices\"][0][\"delta\"][\"tool_calls\"] = tool_calls\n-        if finish_reason is not None:\n-            payload[\"choices\"][0][\"finish_reason\"] = finish_reason\n+        chunk = ChatCompletionChunk(\n+            id=request_id,\n+            created=int(time.time()),\n+            model=self.loaded_model,\n+            choices=[\n+                Choice(\n+                    delta=ChoiceDelta(\n+                        content=content,\n+                        role=role,\n+                        tool_calls=tool_calls,\n+                    ),\n+                    index=0,\n+                    finish_reason=finish_reason,\n+                )\n+            ],\n+            system_fingerprint=\"\",\n+            object=\"chat.completion.chunk\",\n+        )\n+        return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n+\n+    def build_response_event(self, response: \"BaseModel\") -> str:\n+        \"\"\"\n+        Builds a event of a streaming OpenAI Response response.\n+\n+        IMPORTANT: The serialized chunk won't contain empty fields (fields with `None`). Some downstream apps,\n+        like Cursor, assume that when the field exists, it has data.\n \n-        return f\"data: {json.dumps(payload)}\\n\\n\"\n+        Args:\n+            response (`BaseModel`):\n+                The response to build an event from. One of the multiple OpenAI Response output types\n+\n+        Returns:\n+            `str`: The built chunk, a string containing a JSON string with the payload.\n+        \"\"\"\n+        return f\"data: {response.model_dump_json(exclude_none=True)}\\n\\n\"\n \n     def run(self):\n         app = FastAPI()\n@@ -331,31 +512,22 @@ def run(self):\n                 allow_headers=[\"*\"],\n             )\n \n-        if self.use_continuous_batching:\n-            self.continuous_batching(app)\n-        else:\n-            self.generate(app)\n-\n-        @functools.lru_cache(maxsize=None)\n-        def get_text_gen_models() -> list[ModelInfo]:\n-            \"\"\"\n-            This is by no means a limit to which models may be instantiated with `transformers serve`: any chat-based\n-            model working with generate can work.\n-\n-            This is a limited list of models to ensure we have a discoverable /v1/models endpoint for third-party\n-            integrations.\n-            \"\"\"\n-            return [\n-                model_info(\"Menlo/Jan-nano\"),\n-                model_info(\"Menlo/Jan-nano-128k\"),\n-                model_info(\"Qwen/Qwen2.5-0.5B-Instruct\"),\n-                model_info(\"Qwen/Qwen2.5-3B-Instruct\"),\n-                model_info(\"Qwen/Qwen2.5-7B-Instruct\"),\n-                model_info(\"Qwen/Qwen2.5-14B-Instruct\"),\n-                model_info(\"meta-llama/Llama-3.1-8B-Instruct\"),\n-                model_info(\"meta-llama/Llama-3.2-1B-Instruct\"),\n-                model_info(\"meta-llama/Llama-3.3-70B-Instruct\"),\n-            ]\n+        @app.post(\"/v1/chat/completions\")\n+        def chat_completion(request: dict):\n+            self.validate_chat_completion_request(request=request)\n+\n+            if self.use_continuous_batching:\n+                output = self.continuous_batching_chat_completion(request)\n+            else:\n+                output = self.generate_chat_completion(request)\n+            return StreamingResponse(output, media_type=\"text/event-stream\")\n+\n+        @app.post(\"/v1/responses\")\n+        def responses(request: dict):\n+            self.validate_response_request(request=request)\n+\n+            output = self.generate_response(request)\n+            return StreamingResponse(output, media_type=\"text/event-stream\")\n \n         @app.get(\"/v1/models\")\n         def get_all_models():\n@@ -369,284 +541,565 @@ def get_all_models():\n                             \"created\": model.created_at.timestamp(),\n                             \"owned_by\": model.author,\n                         }\n-                        for model in get_text_gen_models()\n+                        for model in self.get_text_gen_models()\n                     ],\n                 }\n             )\n \n         uvicorn.run(app, host=self.args.host, port=self.args.port, log_level=self.args.log_level)\n \n-    def continuous_batching(self, app):\n-        @app.post(\"/v1/chat/completions\")\n-        def _serve(req: \"ChatCompletionInput\"):\n-            if not req.stream:\n-                return {\"error\": \"Only streaming mode is supported.\"}\n-\n-            update_model = self.canonicalized_model_name(req.model) != self.loaded_model\n-            if update_model:\n-                self.model, self.tokenizer = self.load_model_and_tokenizer(req.model, self.args)\n-\n-            generation_config = create_generation_config_from_req(\n-                req,\n-                model_generation_config=self.model.generation_config,\n-                eos_token_id=self.tokenizer.eos_token_id,\n-                pad_token_id=self.tokenizer.pad_token_id,\n-                use_cache=False,\n-                num_blocks=1,\n-                block_size=1024,\n-                do_sample=False,\n-                max_batch_tokens=10,\n-                scheduler=\"fifo\",\n+    @functools.lru_cache(maxsize=None)\n+    def get_text_gen_models(self) -> list[ModelInfo]:\n+        \"\"\"\n+        This is by no means a limit to which models may be instantiated with `transformers serve`: any chat-based\n+        model working with generate can work.\n+\n+        This is a limited list of models to ensure we have a discoverable /v1/models endpoint for third-party\n+        integrations.\n+        \"\"\"\n+        return [\n+            model_info(\"Menlo/Jan-nano\"),\n+            model_info(\"Menlo/Jan-nano-128k\"),\n+            model_info(\"Qwen/Qwen2.5-0.5B-Instruct\"),\n+            model_info(\"Qwen/Qwen2.5-3B-Instruct\"),\n+            model_info(\"Qwen/Qwen2.5-7B-Instruct\"),\n+            model_info(\"Qwen/Qwen2.5-14B-Instruct\"),\n+            model_info(\"meta-llama/Llama-3.1-8B-Instruct\"),\n+            model_info(\"meta-llama/Llama-3.2-1B-Instruct\"),\n+            model_info(\"meta-llama/Llama-3.3-70B-Instruct\"),\n+        ]\n+\n+    def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None, None]:\n+        \"\"\"\n+        Generates an OpenAI Chat Completion using continuous batching.\n+\n+        Args:\n+            req (`dict`): The request to generate an OpenAI Chat Completion for.\n+\n+        Returns:\n+            `Generator[str, None, None]`: A generator that yields the OpenAI Chat Completion chunks.\n+        \"\"\"\n+        if self.args.force_model is not None:\n+            req[\"model\"] = self.args.force_model\n+\n+        update_model = self.canonicalized_model_name(req[\"model\"]) != self.loaded_model\n+        if update_model:\n+            # When switching models, terminate a continuous batching manager if it is running.\n+            if self.running_continuous_batching_manager is not None:\n+                self.running_continuous_batching_manager.stop(block=True, timeout=2)\n+                self.running_continuous_batching_manager = None\n+            self.load_model_and_tokenizer(req[\"model\"], self.args)\n+\n+        generation_config = create_generation_config_from_req(\n+            req,\n+            model_generation_config=self.model.generation_config,\n+            eos_token_id=self.tokenizer.eos_token_id,\n+            pad_token_id=self.tokenizer.pad_token_id,\n+            use_cache=False,\n+            num_blocks=1,\n+            block_size=1024,\n+            do_sample=False,\n+            max_batch_tokens=10,\n+            scheduler=\"fifo\",\n+        )\n+\n+        if self.running_continuous_batching_manager is None:\n+            self.running_continuous_batching_manager = self.model.init_continuous_batching(\n+                generation_config=generation_config, streaming=True\n             )\n \n-            if self.running_continuous_batching_manager is None or update_model:\n-                self.running_continuous_batching_manager = self.model.init_continuous_batching(\n-                    generation_config=generation_config, streaming=True\n+            # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching\n+            # and correctly applied in non-cb\n+            self.running_continuous_batching_manager.logit_processor = LogitsProcessorList()\n+            self.running_continuous_batching_manager.start()\n+\n+        # TODO (Joao, Lysandre): this should also work with tool support\n+        inputs = self.tokenizer.apply_chat_template(\n+            req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True\n+        ).to(self.model.device)\n+\n+        def stream_chat_completion(_inputs):\n+            try:\n+                request_id = self.running_continuous_batching_manager.add_request(\n+                    _inputs, request_id=req.get(\"request_id\"), max_new_tokens=generation_config.max_new_tokens\n                 )\n \n-                # TODO (Joao, Lysandre): the logits processors should be fixed in continuous batching\n-                # and correctly applied in non-cb\n-                self.running_continuous_batching_manager.logit_processor = LogitsProcessorList()\n-                self.running_continuous_batching_manager.start()\n-\n-            # TODO (Joao, Lysandre): this should also work with tool support\n-            inputs = self.tokenizer.apply_chat_template(\n-                req.messages, return_tensors=\"pt\", add_generation_prompt=True\n-            ).to(self.model.device)\n-\n-            def stream_response(_inputs):\n-                try:\n-                    max_new_tokens = req.max_tokens or generation_config.max_new_tokens or 1024\n-                    request_id = self.running_continuous_batching_manager.add_request(\n-                        _inputs, request_id=req.request_id, max_new_tokens=max_new_tokens\n-                    )\n-                    queue_is_flushed = False\n+                queue_is_flushed = False\n+\n+                # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n+                # they come from the assistant.\n+                yield self.build_chat_completion_chunk(request_id, role=\"assistant\")\n+\n+                for result in self.running_continuous_batching_manager:\n+                    if result.request_id != request_id:\n+                        continue\n+                    if req.get(\"request_id\") is not None and not queue_is_flushed:\n+                        if result.status == RequestStatus.FINISHED:\n+                            continue\n+                        else:\n+                            queue_is_flushed = True\n+\n+                    finish_reason = \"stop\" if result.status == RequestStatus.FINISHED else None\n+                    if result.status == RequestStatus.FINISHED:\n+                        yield self.build_chat_completion_chunk(request_id, finish_reason=finish_reason)\n+                        break\n+                    else:\n+                        yield self.build_chat_completion_chunk(request_id=request_id, content=result.next_token)\n+\n+            except Exception as e:\n+                logger.error(str(e))\n+                yield f'data: {{\"error\": \"{str(e)}\"}}'\n+\n+        return stream_chat_completion(inputs[0])\n+\n+    def generate_chat_completion(self, req: dict) -> Generator[str, None, None]:\n+        \"\"\"\n+        Generates an OpenAI Chat Completion using `generate`.\n+\n+        Args:\n+            req (`dict`): The request to generate an OpenAI Chat Completion for.\n+\n+        Returns:\n+            `Generator[str, None, None]`: A generator that yields the OpenAI Chat Completion chunks.\n+        \"\"\"\n+        if self.args.force_model is not None:\n+            req[\"model\"] = self.args.force_model\n+\n+        update_model = self.canonicalized_model_name(req[\"model\"]) != self.loaded_model\n+        if update_model:\n+            self.load_model_and_tokenizer(req[\"model\"], self.args)\n+\n+        # HACK for tiny-agents: it sends a request after the assistant message (???). Let's assume we can't have a\n+        # request whose last message is from the assistant.\n+        if req[\"messages\"][-1][\"role\"] == \"assistant\":\n+            return\n+\n+        # ====== TOOL PREPROCESSING LOGIC ======\n+        tool_model_family = None\n+        for supported_model_families in _MODELS_WITH_TOOL_SUPPORT:\n+            if supported_model_families in self.model.config.architectures[0].lower():\n+                tool_model_family = supported_model_families\n+                break\n+        # TODO: trigger 2 constrained generations after the tool call start token is emitted:\n+        # 1. force generation to pick from the tool names\n+        # 2. force generation to pick from that tool's arguments\n+        # ====== END OF TOOL PREPROCESSING LOGIC ======\n+\n+        if tool_model_family is not None:\n+            text = self.tokenizer.apply_chat_template(\n+                req[\"messages\"], add_generation_prompt=True, tokenize=False, tools=req.get(\"tools\")\n+            )\n+        else:\n+            text = self.tokenizer.apply_chat_template(req[\"messages\"], add_generation_prompt=True, tokenize=False)\n+        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)[\"input_ids\"]\n+        request_id = req.get(\"request_id\", \"req_0\")\n+\n+        generation_streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True, skip_prompt=True)\n+        generation_config = create_generation_config_from_req(\n+            req, model_generation_config=self.model.generation_config\n+        )\n+\n+        last_kv_cache = None\n+        if self.is_continuation(req) and not update_model:\n+            last_kv_cache = self.last_kv_cache\n+\n+        generation_kwargs = {\n+            \"inputs\": inputs,\n+            \"attention_mask\": torch.ones_like(inputs),\n+            \"streamer\": generation_streamer,\n+            \"generation_config\": generation_config,\n+            \"return_dict_in_generate\": True,\n+            \"past_key_values\": last_kv_cache,\n+        }\n+\n+        def stream_chat_completion(streamer, _request_id):\n+            # Thin wrapper to save the KV cache after generation\n+            def generate_with_cache(**kwargs):\n+                generate_output = self.model.generate(**kwargs)\n+                self.last_kv_cache = generate_output.past_key_values\n+\n+            thread = Thread(target=generate_with_cache, kwargs=generation_kwargs)\n \n-                    # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n-                    # they come from the assistant.\n-                    yield self.build_chunk(request_id, role=\"assistant\")\n+            try:\n+                thread.start()\n+                tool_state = ToolState()\n \n-                    for result in self.running_continuous_batching_manager:\n-                        if result.request_id != request_id:\n+                # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n+                # they come from the assistant.\n+                yield self.build_chat_completion_chunk(request_id, role=\"assistant\")\n+\n+                for result in streamer:\n+                    # ====== TOOL CALL LOGIC ======\n+                    if tool_model_family is not None:\n+                        # Start of a tool call: reset state variables, set `inside_tool_call`\n+                        if result.strip() == _TOOL_CALL_TOKENS[tool_model_family][\"start\"]:\n+                            tool_state.inside_tool_call = True\n+                            continue\n+\n+                        # End of tool call: reset `inside_tool_call`, emit a `finish_reason`\n+                        if result.strip() == _TOOL_CALL_TOKENS[tool_model_family][\"end\"]:\n+                            tool_state.reset()\n+                            yield self.build_chat_completion_chunk(\n+                                request_id=_request_id, role=None, finish_reason=\"tool_calls\"\n+                            )\n                             continue\n \n-                        if req.request_id is not None and not queue_is_flushed:\n-                            if result.status == RequestStatus.FINISHED:\n-                                continue\n+                        # Inside a tool call\n+                        if tool_state.inside_tool_call:\n+                            tool_state.buffer += result\n+\n+                            # First step: extract the tool name (may need several tokens, and we can't emit a delta\n+                            # until we have the full name)\n+                            if not tool_state.has_tool_name_defined:\n+                                tool_name = re.search(r\"\\\"name\\\": \\\"(.*?)\\\"\", tool_state.buffer)\n+                                if tool_name is None:\n+                                    continue\n+                                else:\n+                                    tool_name = tool_name.group(1)\n+                                tool_state.has_tool_name_defined = True\n+                                tool = ChoiceDeltaToolCall(\n+                                    function=ChoiceDeltaToolCallFunction(name=tool_name),\n+                                    index=0,\n+                                    type=\"function\",\n+                                    id=_request_id + \"_tool_call\",  # Only the first tool call delta has an id\n+                                )\n+\n+                            # Second step: extract tool arguments. The tool arguments can be seen as a json string\n+                            # within the tool json string. We emit a delta for the arguments.\n                             else:\n-                                queue_is_flushed = True\n+                                # Empty text: skip\n+                                if result == \"\":\n+                                    continue\n+                                # Until we see the `\"arguments\": {` in the buffer, we skip\n+                                # TODO: other models will likely need more elaborate processing here\n+                                if '\"arguments\": {' not in tool_state.buffer:\n+                                    continue\n+\n+                                # Handle nesting. We want to exclude the last } from the emitted arguments (it's\n+                                # closing the outermost nesting level, outside the arguments block)\n+                                tool_state.arg_nesting_level += result.count(\"{\")\n+                                tool_state.arg_nesting_level -= result.count(\"}\")\n+                                if tool_state.arg_nesting_level < 0:\n+                                    result = \"\".join(result.split(\"}\")[:-2]) + \"}\"  # e.g. \"4}}\\n\" -> \"4}\"\n+\n+                                tool = ChoiceDeltaToolCall(\n+                                    function=ChoiceDeltaToolCallFunction(arguments=result),\n+                                    index=0,\n+                                    type=\"function\",\n+                                )\n+\n+                            yield self.build_chat_completion_chunk(\n+                                request_id=_request_id, role=None, tool_calls=[tool]\n+                            )\n+                            continue\n+                    # ====== END OF TOOL CALL LOGIC ======\n \n-                        finish_reason = \"stop\" if result.status == RequestStatus.FINISHED else None\n-                        if result.status == RequestStatus.FINISHED:\n-                            yield self.build_chunk(request_id, finish_reason=finish_reason)\n-                            break\n-                        else:\n-                            yield self.build_chunk(request_id=request_id, content=result.next_token)\n+                    # All non-tool related tokens are emitted as assistant messages. Empty text is skipped.\n+                    if result != \"\":\n+                        yield self.build_chat_completion_chunk(_request_id, content=result)\n+                yield self.build_chat_completion_chunk(_request_id, finish_reason=\"stop\")\n+\n+                thread.join()\n+            except Exception as e:\n+                logger.error(str(e))\n+                yield f'data: {{\"error\": \"{str(e)}\"}}'\n+\n+            finally:\n+                thread.join()\n \n-                except Exception as e:\n-                    logger.error(str(e))\n-                    yield f'data: {{\"error\": \"{str(e)}\"}}'\n+        return stream_chat_completion(generation_streamer, request_id)\n \n-            return StreamingResponse(stream_response(inputs[0]), media_type=\"text/event-stream\")\n+    def generate_response(self, req: dict) -> Generator[str, None, None]:\n+        \"\"\"\n+        Generates an OpenAI Response using `generate`.\n+\n+        Args:\n+            req (`dict`): The request to generate an OpenAI Response for.\n \n-    def is_continuation(self, req: \"ChatCompletionInput\") -> bool:\n+        Returns:\n+            `Generator[str, None, None]`: A generator that yields the OpenAI Response events.\n+        \"\"\"\n+        # TODO -- Implement non-streaming mode\n+        if self.args.force_model is not None:\n+            req[\"model\"] = self.args.force_model\n+\n+        update_model = self.canonicalized_model_name(req[\"model\"]) != self.loaded_model\n+        if update_model:\n+            self.load_model_and_tokenizer(req[\"model\"], self.args)\n+\n+        text = self.tokenizer.apply_chat_template(req[\"input\"], add_generation_prompt=True, tokenize=False)\n+        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)[\"input_ids\"]\n+        request_id = req.get(\"previous_response_id\", \"req_0\")\n+\n+        generation_streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True, skip_prompt=True)\n+        generation_config = create_generation_config_from_req(\n+            req, model_generation_config=self.model.generation_config\n+        )\n+\n+        last_kv_cache = None\n+        if self.is_continuation(req) and not update_model:\n+            last_kv_cache = self.last_kv_cache\n+\n+        generation_kwargs = {\n+            \"inputs\": inputs,\n+            \"attention_mask\": torch.ones_like(inputs),\n+            \"streamer\": generation_streamer,\n+            \"generation_config\": generation_config,\n+            \"return_dict_in_generate\": True,\n+            \"past_key_values\": last_kv_cache,\n+        }\n+\n+        def stream_response(streamer, _request_id):\n+            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n+            sequence_number = 0\n+            output_index = 0\n+            content_index = 0\n+\n+            try:\n+                thread.start()\n+                created_at = time.time()  # the spec expects a unix timestamp in seconds\n+\n+                # We start by acknowledging the request (the request has `status=\"queued\"`), and then by moving it to\n+                # in progress (`status=\"in_progress\"`)\n+                response_created = ResponseCreatedEvent(\n+                    type=\"response.created\",\n+                    sequence_number=sequence_number,\n+                    response=Response(\n+                        id=f\"resp_{request_id}\",\n+                        created_at=created_at,\n+                        status=\"queued\",\n+                        model=self.loaded_model,\n+                        instructions=req.get(\"instructions\"),\n+                        text={\"format\": {\"type\": \"text\"}},\n+                        object=\"response\",\n+                        tools=[],\n+                        output=[],\n+                        parallel_tool_calls=req.get(\"parallel_tool_calls\", False),\n+                        tool_choice=\"auto\",\n+                        metadata=req.get(\"metadata\"),\n+                    ),\n+                )\n+                sequence_number += 1\n+                yield self.build_response_event(response_created)\n+\n+                response_in_progress = ResponseInProgressEvent(\n+                    type=\"response.in_progress\",\n+                    sequence_number=sequence_number,\n+                    response=Response(\n+                        id=f\"resp_{request_id}\",\n+                        created_at=created_at,\n+                        status=\"in_progress\",\n+                        model=self.loaded_model,\n+                        instructions=req.get(\"instructions\"),\n+                        text={\"format\": {\"type\": \"text\"}},\n+                        object=\"response\",\n+                        tools=[],\n+                        output=[],\n+                        parallel_tool_calls=req.get(\"parallel_tool_calls\", False),\n+                        tool_choice=\"auto\",\n+                        metadata=req.get(\"metadata\"),\n+                    ),\n+                )\n+                sequence_number += 1\n+                yield self.build_response_event(response_in_progress)\n+\n+                # Start the output item. Emit the assistant role to start the stream. Other chunks won't have a role,\n+                # as it is implicit\n+                response_output_item_added = ResponseOutputItemAddedEvent(\n+                    type=\"response.output_item.added\",\n+                    sequence_number=sequence_number,\n+                    output_index=output_index,\n+                    item=ResponseOutputMessage(\n+                        id=f\"msg_{request_id}\", type=\"message\", status=\"in_progress\", role=\"assistant\", content=[]\n+                    ),\n+                )\n+                sequence_number += 1\n+                yield self.build_response_event(response_output_item_added)\n+\n+                # Start the content part of the event\n+                response_content_part_added = ResponseContentPartAddedEvent(\n+                    type=\"response.content_part.added\",\n+                    item_id=f\"msg_{request_id}\",\n+                    sequence_number=sequence_number,\n+                    output_index=output_index,\n+                    content_index=content_index,\n+                    part=ResponseOutputText(type=\"output_text\", text=\"\", annotations=[]),\n+                )\n+                sequence_number += 1\n+                yield self.build_response_event(response_content_part_added)\n+\n+                # Stream the actual generated text\n+                results = \"\"\n+                for result in streamer:\n+                    results += result\n+                    response_output_text_delta = ResponseTextDeltaEvent(\n+                        type=\"response.output_text.delta\",\n+                        item_id=f\"msg_{request_id}\",\n+                        sequence_number=sequence_number,\n+                        output_index=output_index,\n+                        content_index=content_index,\n+                        delta=result,\n+                    )\n+                    sequence_number += 1\n+                    yield self.build_response_event(response_output_text_delta)\n+\n+                # Signal the end of the text generation\n+                response_output_text_done = ResponseTextDoneEvent(\n+                    type=\"response.output_text.done\",\n+                    item_id=f\"msg_{request_id}\",\n+                    sequence_number=sequence_number,\n+                    output_index=output_index,\n+                    content_index=0,\n+                    text=results,\n+                )\n+                sequence_number += 1\n+                yield self.build_response_event(response_output_text_done)\n+\n+                # Complete the content part\n+                response_content_part_done = ResponseContentPartDoneEvent(\n+                    type=\"response.content_part.done\",\n+                    item_id=f\"msg_{request_id}\",\n+                    sequence_number=sequence_number,\n+                    output_index=output_index,\n+                    content_index=content_index,\n+                    part=ResponseOutputText(type=\"output_text\", text=response_output_text_done.text, annotations=[]),\n+                )\n+                sequence_number += 1\n+                content_index += 1\n+                yield self.build_response_event(response_content_part_done)\n+\n+                # Complete the output item\n+                response_output_item_done = ResponseOutputItemDoneEvent(\n+                    type=\"response.output_item.done\",\n+                    sequence_number=sequence_number,\n+                    output_index=output_index,\n+                    item=ResponseOutputMessage(\n+                        id=f\"msg_{request_id}\",\n+                        type=\"message\",\n+                        status=\"completed\",\n+                        role=\"assistant\",\n+                        content=[response_content_part_done.part],\n+                        annotations=[],\n+                    ),\n+                )\n+                sequence_number += 1\n+                output_index += 1\n+                yield self.build_response_event(response_output_item_done)\n+\n+                # Finally, Complete the event\n+                response_completed = ResponseCompletedEvent(\n+                    type=\"response.completed\",\n+                    sequence_number=sequence_number,\n+                    response=Response(\n+                        id=f\"resp_{request_id}\",\n+                        created_at=created_at,\n+                        status=\"completed\",\n+                        model=self.loaded_model,\n+                        instructions=req.get(\"instructions\"),\n+                        text={\"format\": {\"type\": \"text\"}},\n+                        output=[response_output_item_done.item],\n+                        object=\"response\",\n+                        tools=[],\n+                        parallel_tool_calls=req.get(\"parallel_tool_calls\", False),\n+                        tool_choice=\"auto\",\n+                        metadata=req.get(\"metadata\"),\n+                    ),\n+                )\n+                sequence_number += 1\n+                yield self.build_response_event(response_completed)\n+\n+                thread.join()\n+            except Exception as e:\n+                logger.error(f\"Exception in response generation: {str(e)}\")\n+                error_event = ResponseErrorEvent(\n+                    type=\"error\",\n+                    sequence_number=sequence_number,\n+                    message=str(e),\n+                )\n+                sequence_number += 1\n+                yield self.build_response_event(error_event)\n+\n+                response_failed = ResponseFailedEvent(\n+                    type=\"response.failed\",\n+                    sequence_number=sequence_number,\n+                    response=Response(\n+                        id=f\"resp_{request_id}\",\n+                        created_at=created_at,\n+                        status=\"failed\",\n+                        model=self.loaded_model,\n+                        instructions=req.get(\"instructions\"),\n+                        text={\"format\": {\"type\": \"text\"}},\n+                        output=[],\n+                        object=\"response\",\n+                        tools=[],\n+                        parallel_tool_calls=False,\n+                        tool_choice=\"auto\",\n+                        metadata=req.get(\"metadata\"),\n+                        error=ResponseError(\n+                            code=\"server_error\",\n+                            message=str(e),\n+                        ),\n+                    ),\n+                )\n+                sequence_number += 1\n+                yield self.build_response_event(response_failed)\n+\n+            finally:\n+                thread.join()\n+\n+        return stream_response(generation_streamer, request_id)\n+\n+    def is_continuation(self, req: dict) -> bool:\n         \"\"\"\n         Determines whether the current request is a continuation of the last request. In other words, if it is the\n         same chat session.\n \n         Args:\n-            req (`ChatCompletionInput`): The request to check.\n+            req (`dict`): The request to check.\n \n         Returns:\n             `True` if the request is a continuation of the last request, `False` otherwise.\n         \"\"\"\n+        messages = req.get(\"messages\") or req.get(\"input\")  # ChatCompletion and Response have different fields\n         req_continues_last_messages = True\n \n         # No cached messages: this is a new request\n         if self.last_messages is None:\n             req_continues_last_messages = False\n         # The new request has no new rounds of conversation: this is a new request\n-        elif len(self.last_messages) >= len(req.messages):\n+        elif len(self.last_messages) >= len(messages):\n             req_continues_last_messages = False\n         # Otherwise, check that the last messages are a subset of the new request\n         else:\n             for i in range(len(self.last_messages)):\n-                if self.last_messages[i] != req.messages[i]:\n+                if self.last_messages[i] != messages[i]:\n                     req_continues_last_messages = False\n                     break\n \n-        self.last_messages = req.messages\n+        self.last_messages = messages\n         return req_continues_last_messages\n \n-    def generate(self, app):\n-        @app.post(\"/v1/chat/completions\")\n-        def _serve(req: \"ChatCompletionInput\"):\n-            logger.debug(f\"Received request: {req}\")\n-            update_model = self.canonicalized_model_name(req.model) != self.loaded_model\n-\n-            if update_model:\n-                self.model, self.tokenizer = self.load_model_and_tokenizer(req.model, self.args)\n-\n-            if not req.stream:\n-                return {\"error\": \"Only streaming mode is supported.\"}\n-\n-            # HACK for tiny-agents: it sends a request after the assistant message (???). Let's assume we can't have a\n-            # request whose last message is from the assistant.\n-            if req.messages[-1].role == \"assistant\":\n-                return\n-\n-            # ====== TOOL PREPROCESSING LOGIC ======\n-            tool_model_family = None\n-            for supported_model_families in _MODELS_WITH_TOOL_SUPPORT:\n-                if supported_model_families in self.model.config.architectures[0].lower():\n-                    tool_model_family = supported_model_families\n-                    break\n-            # TODO: trigger 2 constrained generations after the tool call start token is emitted:\n-            # 1. force generation to pick from the tool names\n-            # 2. force generation to pick from that tool's arguments\n-            # ====== END OF TOOL PREPROCESSING LOGIC ======\n-\n-            if tool_model_family is not None:\n-                text = self.tokenizer.apply_chat_template(\n-                    req.messages, add_generation_prompt=True, tokenize=False, tools=req.tools\n-                )\n-            else:\n-                text = self.tokenizer.apply_chat_template(req.messages, add_generation_prompt=True, tokenize=False)\n-\n-            inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)[\"input_ids\"]\n-            request_id = req.request_id if req.request_id is not None else \"req_0\"\n-\n-            generation_streamer = TextIteratorStreamer(self.tokenizer, skip_special_tokens=True, skip_prompt=True)\n+    @staticmethod\n+    def get_quantization_config(args: ServeArguments) -> Optional[\"BitsAndBytesConfig\"]:\n+        \"\"\"\n+        Returns the quantization config for the given CLI arguments.\n \n-            generation_config = create_generation_config_from_req(\n-                req,\n-                model_generation_config=self.model.generation_config,\n-            )\n-            max_new_tokens = req.max_tokens or generation_config.max_new_tokens or 1024\n-            generation_config.max_new_tokens = max_new_tokens\n-\n-            last_kv_cache = None\n-            if self.is_continuation(req) and not update_model:\n-                last_kv_cache = self.last_kv_cache\n-\n-            generation_kwargs = {\n-                \"inputs\": inputs,\n-                \"attention_mask\": torch.ones_like(inputs),\n-                \"streamer\": generation_streamer,\n-                \"generation_config\": generation_config,\n-                \"return_dict_in_generate\": True,\n-                \"past_key_values\": last_kv_cache,\n-            }\n-\n-            def stream_response(streamer, _request_id):\n-                # Thin wrapper to save the KV cache after generation\n-                def generate_with_cache(**kwargs):\n-                    generate_output = self.model.generate(**kwargs)\n-                    self.last_kv_cache = generate_output.past_key_values\n-\n-                thread = Thread(target=generate_with_cache, kwargs=generation_kwargs)\n-\n-                try:\n-                    thread.start()\n-                    tool_state = ToolState()\n-\n-                    # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n-                    # they come from the assistant.\n-                    logger.debug(\"Starting model output\")\n-                    yield self.build_chunk(_request_id, role=\"assistant\")\n-\n-                    for result in streamer:\n-                        logger.debug(f\"Model output: {result}\")\n-\n-                        # ====== TOOL CALL LOGIC ======\n-                        if tool_model_family is not None:\n-                            # Start of a tool call: reset state variables, set `inside_tool_call`\n-                            if result.strip() == _TOOL_CALL_TOKENS[tool_model_family][\"start\"]:\n-                                tool_state.inside_tool_call = True\n-                                continue\n-\n-                            # End of tool call: reset `inside_tool_call`, emit a `finish_reason`\n-                            if result.strip() == _TOOL_CALL_TOKENS[tool_model_family][\"end\"]:\n-                                tool_state.reset()\n-                                yield self.build_chunk(_request_id, finish_reason=\"tool_calls\")\n-                                continue\n-\n-                            # Inside a tool call\n-                            if tool_state.inside_tool_call:\n-                                tool_state.buffer += result\n-\n-                                # First step: extract the tool name (may need several tokens, and we can't emit a delta\n-                                # until we have the full name)\n-                                if not tool_state.has_tool_name_defined:\n-                                    tool_name = re.search(r\"\\\"name\\\": \\\"(.*?)\\\"\", tool_state.buffer)\n-                                    if tool_name is None:\n-                                        continue\n-                                    else:\n-                                        tool_name = tool_name.group(1)\n-                                    tool_state.has_tool_name_defined = True\n-                                    tool = {\n-                                        \"function\": {\"name\": tool_name},\n-                                        \"index\": 0,\n-                                        \"type\": \"function\",\n-                                        \"id\": _request_id + \"_tool_call\",  # Only the first tool call delta has an id\n-                                    }\n-\n-                                # Second step: extract tool arguments. The tool arguments can be seen as a json string\n-                                # within the tool json string. We emit a delta for the arguments.\n-                                else:\n-                                    # Empty text: skip\n-                                    if result == \"\":\n-                                        continue\n-                                    # Until we see the `\"arguments\": {` in the buffer, we skip\n-                                    # TODO: other models will likely need more elaborate processing here\n-                                    if '\"arguments\": {' not in tool_state.buffer:\n-                                        continue\n-\n-                                    # Handle nesting. We want to exclude the last } from the emitted arguments (it's\n-                                    # closing the outermost nesting level, outside the arguments block)\n-                                    tool_state.arg_nesting_level += result.count(\"{\")\n-                                    tool_state.arg_nesting_level -= result.count(\"}\")\n-                                    if tool_state.arg_nesting_level < 0:\n-                                        result = \"\".join(result.split(\"}\")[:-2]) + \"}\"  # e.g. \"4}}\\n\" -> \"4}\"\n-\n-                                    tool = {\n-                                        \"function\": {\"arguments\": result},\n-                                        \"index\": 0,\n-                                        \"type\": \"function\",\n-                                    }\n-\n-                                yield self.build_chunk(_request_id, tool_calls=[tool])\n-                                continue\n-                        # ====== END OF TOOL CALL LOGIC ======\n-\n-                        # All non-tool related tokens are emitted as assistant messages. Empty text is skipped.\n-                        if result != \"\":\n-                            yield self.build_chunk(_request_id, content=result)\n-                    yield self.build_chunk(_request_id, finish_reason=\"stop\")\n-\n-                    thread.join()\n-                except Exception as e:\n-                    logger.error(str(e))\n-                    raise\n-                    yield f'data: {{\"error\": \"{str(e)}\"}}'\n-\n-                finally:\n-                    thread.join()\n-\n-            return StreamingResponse(stream_response(generation_streamer, request_id), media_type=\"text/event-stream\")\n+        Args:\n+            args (`ServeArguments`): The serve arguments. May contain quantization settings, device, etc.\n \n-    @staticmethod\n-    def get_quantization_config(model_args: ServeArguments) -> Optional[\"BitsAndBytesConfig\"]:\n-        if model_args.load_in_4bit:\n+        Returns:\n+            `Optional[BitsAndBytesConfig]`: The quantization config.\n+        \"\"\"\n+        if args.load_in_4bit:\n             quantization_config = BitsAndBytesConfig(\n                 load_in_4bit=True,\n                 # For consistency with model weights, we use the same value as `torch_dtype`\n-                bnb_4bit_compute_dtype=model_args.torch_dtype,\n-                bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,\n-                bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,\n-                bnb_4bit_quant_storage=model_args.torch_dtype,\n+                bnb_4bit_compute_dtype=args.torch_dtype,\n+                bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n+                bnb_4bit_use_double_quant=args.use_bnb_nested_quant,\n+                bnb_4bit_quant_storage=args.torch_dtype,\n             )\n-        elif model_args.load_in_8bit:\n+        elif args.load_in_8bit:\n             quantization_config = BitsAndBytesConfig(\n                 load_in_8bit=True,\n             )\n@@ -656,13 +1109,30 @@ def get_quantization_config(model_args: ServeArguments) -> Optional[\"BitsAndByte\n         return quantization_config\n \n     def canonicalized_model_name(self, model_id: str) -> str:\n+        \"\"\"\n+        Canonicalizes the model name to the format \"model_id@revision\". If the model_id DOESN'T contain an @, it\n+        defaults to \"model_id@main\".\n+\n+        Args:\n+            model_id (`str`): The model ID.\n+\n+        Returns:\n+            `str`: The canonicalized model name.\n+        \"\"\"\n         if \"@\" in model_id:\n             return model_id\n         return f\"{model_id}@main\"\n \n-    def load_model_and_tokenizer(\n-        self, model_id_and_revision: str, args: ServeArguments\n-    ) -> tuple[PreTrainedModel, PreTrainedTokenizerFast]:\n+    def load_model_and_tokenizer(self, model_id_and_revision: str, args: ServeArguments):\n+        \"\"\"\n+        Loads the model and tokenizer from the given model ID and revision into the ServeCommand instance.\n+\n+        Args:\n+            model_id_and_revision (`str`):\n+                The model ID and revision to load.\n+            args (`ServeArguments`):\n+                The serve arguments. May contain quantization settings, device, etc.\n+        \"\"\"\n         logger.warning(f\"Loading {model_id_and_revision}\")\n \n         if \"@\" in model_id_and_revision:\n@@ -699,7 +1169,8 @@ def load_model_and_tokenizer(\n         self.loaded_model = f\"{model_id}@{revision}\"\n \n         logger.warning(f\"Loaded model {self.loaded_model}\")\n-        return model, tokenizer\n+        self.model = model\n+        self.tokenizer = tokenizer\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "1f071c6bb650bde548c4da63c6218bcc7b2f7a07",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=de5ca373acefc3c5cfc99aa697e7a073f7a2de23",
            "patch": "@@ -43,6 +43,7 @@\n     \"onnxconverter-common\": \"onnxconverter-common\",\n     \"onnxruntime-tools\": \"onnxruntime-tools>=1.4.2\",\n     \"onnxruntime\": \"onnxruntime>=1.4.0\",\n+    \"openai\": \"openai\",\n     \"opencv-python\": \"opencv-python\",\n     \"optimum-benchmark\": \"optimum-benchmark>=0.3.0\",\n     \"optuna\": \"optuna\","
        },
        {
            "sha": "1df380b6fd706766d0c5792ba2460b6e3c7fdd40",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=de5ca373acefc3c5cfc99aa697e7a073f7a2de23",
            "patch": "@@ -112,6 +112,7 @@\n     is_natten_available,\n     is_nltk_available,\n     is_onnx_available,\n+    is_openai_available,\n     is_optimum_available,\n     is_optimum_quanto_available,\n     is_pandas_available,\n@@ -1536,6 +1537,13 @@ def require_speech(test_case):\n     return unittest.skipUnless(is_speech_available(), \"test requires torchaudio\")(test_case)\n \n \n+def require_openai(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires openai\n+    \"\"\"\n+    return unittest.skipUnless(is_openai_available(), \"test requires openai\")(test_case)\n+\n+\n def require_mistral_common(test_case):\n     \"\"\"\n     Decorator marking a test that requires mistral-common. These tests are skipped when mistral-common isn't available."
        },
        {
            "sha": "251c2309edfd7bc4e7704b62b34fd7b8d3e2abcc",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=de5ca373acefc3c5cfc99aa697e7a073f7a2de23",
            "patch": "@@ -515,6 +515,10 @@ def is_uvicorn_available():\n     return _uvicorn_available\n \n \n+def is_openai_available():\n+    return _openai_available\n+\n+\n def is_pretty_midi_available():\n     return _pretty_midi_available\n \n@@ -730,10 +734,6 @@ def is_onnx_available():\n     return _onnx_available\n \n \n-def is_openai_available():\n-    return _openai_available\n-\n-\n def is_flax_available():\n     return _flax_available\n \n@@ -1916,6 +1916,12 @@ def check_torch_load_is_safe():\n `pip install uvicorn`. Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n+# docstyle-ignore\n+OPENAI_IMPORT_ERROR = \"\"\"\n+{0} requires the openai library but it was not found in your environment. You can install it with pip:\n+`pip install openai`. Please note that you may need to restart your runtime after installation.\n+\"\"\"\n+\n # docstyle-ignore\n PYTESSERACT_IMPORT_ERROR = \"\"\"\n {0} requires the PyTesseract library but it was not found in your environment. You can install it with pip:\n@@ -2046,6 +2052,7 @@ def check_torch_load_is_safe():\n         (\"pydantic\", (is_pydantic_available, PYDANTIC_IMPORT_ERROR)),\n         (\"fastapi\", (is_fastapi_available, FASTAPI_IMPORT_ERROR)),\n         (\"uvicorn\", (is_uvicorn_available, UVICORN_IMPORT_ERROR)),\n+        (\"openai\", (is_openai_available, OPENAI_IMPORT_ERROR)),\n         (\"mistral-common\", (is_mistral_common_available, MISTRAL_COMMON_IMPORT_ERROR)),\n     ]\n )"
        },
        {
            "sha": "403f08ae7eacef5bbba5a34bab752b4ac4ae283b",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 100,
            "deletions": 31,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5ca373acefc3c5cfc99aa697e7a073f7a2de23/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=de5ca373acefc3c5cfc99aa697e7a073f7a2de23",
            "patch": "@@ -24,9 +24,16 @@\n import transformers.commands.transformers_cli as cli\n from transformers import GenerationConfig\n from transformers.commands.serving import ServeArguments, ServeCommand\n-from transformers.testing_utils import CaptureStd, slow\n+from transformers.testing_utils import CaptureStd, require_openai, slow\n+from transformers.utils.import_utils import is_openai_available\n \n \n+if is_openai_available():\n+    from openai.types.chat.chat_completion_chunk import ChoiceDeltaToolCall, ChoiceDeltaToolCallFunction\n+    from openai.types.responses import Response, ResponseCreatedEvent\n+\n+\n+@require_openai\n class ServeCLITest(unittest.TestCase):\n     def test_help(self):\n         \"\"\"Minimal test: we can invoke the help command.\"\"\"\n@@ -49,36 +56,94 @@ def test_parsed_args(self):\n         self.assertEqual(parsed_args.host, \"0.0.0.0\")\n         self.assertEqual(parsed_args.port, 9000)\n \n-    def test_completions_build_chunk(self):\n-        \"\"\"Tests that the chunks are correctly built for the Completions API.\"\"\"\n+    def test_build_chat_completion_chunk(self):\n+        \"\"\"\n+        Tests that the chunks are correctly built for the Chat Completion API. The `choices` checks implictly\n+        confirm that empty fields are not emitted.\n+        \"\"\"\n         dummy = ServeCommand.__new__(ServeCommand)\n         dummy.args = type(\"Args\", (), {})()\n+        dummy.loaded_model = \"dummy_model@main\"\n+\n+        # The keys for these fields must be present in every chunk\n+        MANDATORY_FIELDS = [\"data\", \"id\", \"choices\", \"created\", \"model\", \"object\", \"system_fingerprint\"]\n \n         # Case 1: most fields are provided\n-        chunk = ServeCommand.build_chunk(dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\")\n-        self.assertIn(\"chat.completion.chunk\", chunk)\n-        self.assertIn(\"data:\", chunk)\n+        chunk = ServeCommand.build_chat_completion_chunk(\n+            dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\"\n+        )\n+        for field in MANDATORY_FIELDS:\n+            self.assertIn(field, chunk)\n         self.assertIn(\n-            '\"choices\": [{\"delta\": {\"content\": \"hello\", \"role\": \"user\"}, \"index\": 0, \"finish_reason\": \"stop\"}]', chunk\n+            '\"choices\":[{\"delta\":{\"content\":\"hello\",\"role\":\"user\"},\"finish_reason\":\"stop\",\"index\":0}]', chunk\n         )\n \n         # Case 2: only the role is provided -- other fields in 'choices' are omitted\n-        chunk = ServeCommand.build_chunk(dummy, request_id=\"req0\", role=\"user\")\n-        self.assertIn(\"chat.completion.chunk\", chunk)\n-        self.assertIn(\"data:\", chunk)\n-        self.assertIn('\"choices\": [{\"delta\": {\"role\": \"user\"}, \"index\": 0}]', chunk)\n+        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", role=\"user\")\n+        for field in MANDATORY_FIELDS:\n+            self.assertIn(field, chunk)\n+        self.assertIn('\"choices\":[{\"delta\":{\"role\":\"user\"},\"index\":0}]', chunk)\n \n         # Case 3: only the content is provided -- other fields in 'choices' are omitted\n-        chunk = ServeCommand.build_chunk(dummy, request_id=\"req0\", content=\"hello\")\n-        self.assertIn(\"chat.completion.chunk\", chunk)\n-        self.assertIn(\"data:\", chunk)\n-        self.assertIn('\"choices\": [{\"delta\": {\"content\": \"hello\"}, \"index\": 0}]', chunk)\n+        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", content=\"hello\")\n+        for field in MANDATORY_FIELDS:\n+            self.assertIn(field, chunk)\n+        self.assertIn('\"choices\":[{\"delta\":{\"content\":\"hello\"},\"index\":0}]', chunk)\n+\n+        # Case 4: tool calls support a list of ChoiceDeltaToolCall objects\n+        tool_call = ChoiceDeltaToolCall(\n+            index=0,\n+            function=ChoiceDeltaToolCallFunction(name=\"foo_bar\", arguments='{\"foo1\": \"bar1\", \"foo2\": \"bar2\"}'),\n+            type=\"function\",\n+        )\n+        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", tool_calls=[tool_call])\n+        for field in MANDATORY_FIELDS:\n+            self.assertIn(field, chunk)\n+        expected_choices_content = (\n+            'choices\":[{\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"arguments\":\"{\\\\\"foo1\\\\\": \\\\\"bar1\\\\\", '\n+            '\\\\\"foo2\\\\\": \\\\\"bar2\\\\\"}\",\"name\":\"foo_bar\"},\"type\":\"function\"}]},\"index\":0}]'\n+        )\n+        self.assertIn(expected_choices_content, chunk)\n \n-        # Case 4: tool calls support a list of nested dictionaries\n-        chunk = ServeCommand.build_chunk(dummy, request_id=\"req0\", tool_calls=[{\"foo1\": \"bar1\", \"foo2\": \"bar2\"}])\n-        self.assertIn(\"chat.completion.chunk\", chunk)\n-        self.assertIn(\"data:\", chunk)\n-        self.assertIn('\"choices\": [{\"delta\": {\"tool_calls\": [{\"foo1\": \"bar1\", \"foo2\": \"bar2\"}]}, \"index\": 0}]', chunk)\n+    def test_build_response_event(self):\n+        \"\"\"\n+        Tests that the events are correctly built for the Response API.\n+\n+        Contrarily to the Chat Completion API, the Response API has a wide set of possible output objects. This test\n+        only checks a few basic assumptions -- we rely on OpenAI's pydantic models to enforce the correct schema.\n+        \"\"\"\n+        dummy = ServeCommand.__new__(ServeCommand)\n+        dummy.args = type(\"Args\", (), {})()\n+\n+        response_created = ResponseCreatedEvent(\n+            type=\"response.created\",\n+            sequence_number=0,\n+            response=Response(\n+                id=\"resp_0\",\n+                created_at=time.time(),\n+                status=\"queued\",\n+                model=\"dummy_model@main\",\n+                instructions=None,  # <--- is set to None = should NOT be in the output.\n+                text={\"format\": {\"type\": \"text\"}},\n+                object=\"response\",\n+                tools=[],  # <--- empty lists should be in the output (they are often mandatory fields)\n+                output=[],\n+                parallel_tool_calls=False,\n+                tool_choice=\"auto\",\n+                metadata=None,\n+            ),\n+        )\n+\n+        event = dummy.build_response_event(response_created)\n+        self.assertTrue(event.startswith(\"data: \"))  # Sanity check: event formatting\n+        self.assertIn('\"model\":\"dummy_model@main\"', event)  # Sanity check: set field\n+        self.assertIn('\"status\":\"queued\"', event)\n+        self.assertIn(\"tools\", event)  # empty lists should be in the output\n+        self.assertIn(\"output\", event)\n+        self.assertNotIn(\"instructions\", event)  # None fields should NOT be in the output\n+        self.assertNotIn(\"metadata\", event)\n+        self.assertNotIn(\"error\", event)  # Unset optional fields should NOT be in the output\n+        self.assertNotIn(\"top_p\", event)\n \n \n def async_retry(fn, max_attempts=5, delay=2):\n@@ -105,7 +170,7 @@ class ServeCompletionsMixin:\n \n     @async_retry\n     async def run_server(self, request):\n-        client = AsyncInferenceClient(\"http://localhost:8000\")\n+        client = AsyncInferenceClient(f\"http://localhost:{self.port}\")\n         stream = client.chat_completion(**request)\n \n         all_payloads = []\n@@ -119,8 +184,7 @@ async def run_server(self, request):\n         [\n             (\"default_request\", {}),\n             (\"one_token\", {\"max_tokens\": 1}),\n-            #  TODO: CB fails next case, seems like it is unable to switch models. fix me\n-            # (\"different_model\", {\"model\": \"HuggingFaceTB/SmolLM2-135M-Instruct\"}),\n+            (\"different_model\", {\"model\": \"HuggingFaceTB/SmolLM2-135M-Instruct\"}),\n             (\n                 \"tool_call\",\n                 {\n@@ -191,20 +255,20 @@ def test_generation_config_in_request(self):\n         # sets `do_sample=True`\n         self.assertEqual(output_text, '<think>\\nOkay, the user just asked, \"')\n \n-    # TODO: implement API-compliant error handling, and then test it\n-    # See https://platform.openai.com/docs/guides/error-codes,\n     # TODO: one test for each request flag, to confirm it is working as expected\n     # TODO: speed-based test to confirm that KV cache is working across requests\n \n \n-@slow  # TODO (joao): this shouldn't be needed\n-class ServeCompletionsGenerateTest(ServeCompletionsMixin, unittest.TestCase):\n+@slow  # server startup time is slow on our push CI\n+@require_openai\n+class ServeCompletionsGenerateIntegrationTest(ServeCompletionsMixin, unittest.TestCase):\n     \"\"\"Tests the `generate` version of the Completions API.\"\"\"\n \n     @classmethod\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n-        args = ServeArguments()\n+        cls.port = 8001\n+        args = ServeArguments(port=cls.port)\n         serve_command = ServeCommand(args)\n         thread = Thread(target=serve_command.run)\n         thread.daemon = True\n@@ -287,15 +351,20 @@ def test_tool_call(self):\n         self.assertTrue(all(reason is None for reason in finish_reasons[:-1]))\n \n \n-@slow  # TODO (joao): this shouldn't be needed\n-class ServeCompletionsContinuousBatchingTest(ServeCompletionsMixin, unittest.TestCase):\n+@slow  # server startup time is slow on our push CI\n+@require_openai\n+class ServeCompletionsContinuousBatchingIntegrationTest(ServeCompletionsMixin, unittest.TestCase):\n     \"\"\"Tests the `continuous_batching` version of the Completions API.\"\"\"\n \n     @classmethod\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n-        args = ServeArguments(attn_implementation=\"sdpa_paged\")  # important: toggle continuous batching\n+        cls.port = 8002\n+        args = ServeArguments(port=cls.port, attn_implementation=\"sdpa_paged\")  # important: toggle continuous batching\n         serve_command = ServeCommand(args)\n         thread = Thread(target=serve_command.run)\n         thread.daemon = True\n         thread.start()\n+\n+\n+# TODO: Response integration tests"
        }
    ],
    "stats": {
        "total": 1339,
        "additions": 948,
        "deletions": 391
    }
}