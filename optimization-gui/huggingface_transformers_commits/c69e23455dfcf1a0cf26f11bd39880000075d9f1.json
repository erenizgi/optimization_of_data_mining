{
    "author": "Isotr0py",
    "message": "Support loading Gemma3 QAT GGUF models (#37649)\n\n* fix gemma3 qat gguf support\n\nSigned-off-by: isotr0py <2037008807@qq.com>\n\n* update test\n\nSigned-off-by: isotr0py <2037008807@qq.com>\n\n* make ruff happy\n\nSigned-off-by: isotr0py <2037008807@qq.com>\n\n---------\n\nSigned-off-by: isotr0py <2037008807@qq.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "c69e23455dfcf1a0cf26f11bd39880000075d9f1",
    "files": [
        {
            "sha": "3ce50f8fec2c9c044f076684e2934a9829be352c",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c69e23455dfcf1a0cf26f11bd39880000075d9f1/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c69e23455dfcf1a0cf26f11bd39880000075d9f1/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=c69e23455dfcf1a0cf26f11bd39880000075d9f1",
            "patch": "@@ -258,6 +258,8 @@ def process(self, weights, name, **kwargs):\n \n \n def read_field(reader, field):\n+    if field not in reader.fields:\n+        return []\n     value = reader.fields[field]\n     return [_gguf_parse_value(value.parts[_data_index], value.types) for _data_index in value.data]\n \n@@ -369,6 +371,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n     parsed_parameters = {k: {} for k in GGUF_TO_TRANSFORMERS_MAPPING}\n \n     architecture = read_field(reader, \"general.architecture\")[0]\n+    # NOTE: Some GGUF checkpoints may miss `general.name` field in metadata\n     model_name = read_field(reader, \"general.name\")\n \n     updated_architecture = None"
        },
        {
            "sha": "52c700b16f37d1bd400503efae9175658d89d952",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c69e23455dfcf1a0cf26f11bd39880000075d9f1/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c69e23455dfcf1a0cf26f11bd39880000075d9f1/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=c69e23455dfcf1a0cf26f11bd39880000075d9f1",
            "patch": "@@ -298,6 +298,7 @@ class GgufModelTests(unittest.TestCase):\n     gemma2_model_id = \"bartowski/gemma-2-2b-it-GGUF\"\n     original_gemma3_text_model_id = \"google/gemma-3-1b-it\"\n     original_gemma3_vision_model_id = \"google/gemma-3-4b-it\"\n+    gemma3_qat_model_id = \"google/gemma-3-1b-it-qat-q4_0-gguf\"\n     gemma3_text_model_id = \"unsloth/gemma-3-1b-it-GGUF\"\n     gemma3_vision_model_id = \"unsloth/gemma-3-4b-it-GGUF\"\n \n@@ -329,7 +330,7 @@ class GgufModelTests(unittest.TestCase):\n     q3_k_gemma2_model_id = \"gemma-2-2b-it-Q3_K_L.gguf\"\n     q8_0_gemma2_model_id = \"gemma-2-2b-it-Q8_0.gguf\"\n     fp32_gemma2_model_id = \"gemma-2-2b-it-f32.gguf\"\n-    q2_k_gemma3_text_model_id = \"gemma-3-1b-it-Q2_K.gguf\"\n+    q4_0_gemma3_qat_model_id = \"gemma-3-1b-it-q4_0.gguf\"\n     bf16_gemma3_text_model_id = \"gemma-3-1b-it-BF16.gguf\"\n     bf16_gemma3_vision_model_id = \"gemma-3-4b-it-BF16.gguf\"\n \n@@ -889,19 +890,20 @@ def test_gemma2_weights_conversion_fp32(self):\n             else:\n                 raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n \n+    @require_read_token\n     @unittest.skipUnless(is_gguf_available(\"0.16.0\"), \"test requires gguf version >= 0.16.0\")\n-    def test_gemma3_text_q2_k(self):\n+    def test_gemma3_qat_q4_0(self):\n         model = AutoModelForCausalLM.from_pretrained(\n-            self.gemma3_text_model_id,\n-            gguf_file=self.q2_k_gemma3_text_model_id,\n+            self.gemma3_qat_model_id,\n+            gguf_file=self.q4_0_gemma3_qat_model_id,\n             torch_dtype=torch.float16,\n         )\n \n-        tokenizer = AutoTokenizer.from_pretrained(self.gemma3_text_model_id, gguf_file=self.q2_k_gemma3_text_model_id)\n+        tokenizer = AutoTokenizer.from_pretrained(self.gemma3_qat_model_id, gguf_file=self.q4_0_gemma3_qat_model_id)\n         text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"]\n         out = model.generate(text, max_new_tokens=10)\n \n-        EXPECTED_TEXT = \"Hello,\\n\\nI'm looking for a small,\"\n+        EXPECTED_TEXT = 'Hello with the prompt, \"What is the best way'\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n     @require_read_token"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 11,
        "deletions": 6
    }
}