{
    "author": "LysandreJik",
    "message": "`transformers chat` launched without base_url has a direct tie to localhost:8000 (#42463)\n\n* Chat detect serve\n\n* Update some docs\n\n* Cleanups\n\n* Apply suggestions from code review\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n---------\n\nCo-authored-by: Lucain <lucainp@gmail.com>",
    "sha": "a099b27ddb2fe4a0f690ba6007ee38f109ccbf31",
    "files": [
        {
            "sha": "6e8e4bf3b3690c603745af2d8f5fdde1aef12788",
            "filename": "README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/README.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/README.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/README.md?ref=a099b27ddb2fe4a0f690ba6007ee38f109ccbf31",
            "patch": "@@ -134,7 +134,7 @@ pipeline(\"the secret to baking a really good cake is \")\n To chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n \n > [!TIP]\n-> You can also chat with a model directly from the command line.\n+> You can also chat with a model directly from the command line, as long as [`transformers serve` is running](https://huggingface.co/docs/transformers/main/en/serving).\n > ```shell\n > transformers chat Qwen/Qwen2.5-0.5B-Instruct\n > ```"
        },
        {
            "sha": "2c808ab4d30a513903893fbedbe6cc75f5e99b78",
            "filename": "docs/source/en/conversations.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/docs%2Fsource%2Fen%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/docs%2Fsource%2Fen%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fconversations.md?ref=a099b27ddb2fe4a0f690ba6007ee38f109ccbf31",
            "patch": "@@ -26,6 +26,8 @@ This guide shows you how to quickly load chat models in Transformers from the co\n \n After you've [installed Transformers](./installation), you can chat with a model directly from the command line. The command below launches an interactive session with a model, with a few base commands listed at the start of the session.\n \n+> For the following commands, please make sure [`transformers serve` is running](https://huggingface.co/docs/transformers/main/en/serving).\n+\n ```bash\n transformers chat Qwen/Qwen2.5-0.5B-Instruct\n ```"
        },
        {
            "sha": "9b9ed8c448ce1c10e8ff7fe6bf0e115545d1f141",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=a099b27ddb2fe4a0f690ba6007ee38f109ccbf31",
            "patch": "@@ -23,7 +23,7 @@ Text generation is the most popular application for large language models (LLMs)\n In Transformers, the [`~GenerationMixin.generate`] API handles text generation, and it is available for all models with generative capabilities. This guide will show you the basics of text generation with [`~GenerationMixin.generate`] and some common pitfalls to avoid.\n \n > [!TIP]\n-> You can also chat with a model directly from the command line. ([reference](./conversations.md#transformers))\n+> For the following commands, please make sure [`transformers serve` is running](https://huggingface.co/docs/transformers/main/en/serving).\n >\n > ```shell\n > transformers chat Qwen/Qwen2.5-0.5B-Instruct"
        },
        {
            "sha": "81989d7cc3184282f54486c6cace56f9844ee5cc",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=a099b27ddb2fe4a0f690ba6007ee38f109ccbf31",
            "patch": "@@ -44,6 +44,12 @@ The server supports the following REST APIs:\n - `/v1/audio/transcriptions`\n - `/v1/models`\n \n+Please make sure to have the correct dependencies installed for the instructions below:\n+\n+```shell\n+pip install transformers[serving]\n+```\n+\n To launch a server, simply use the `transformers serve` CLI command:\n \n ```shell\n@@ -53,7 +59,7 @@ transformers serve\n The simplest way to interact with the server is through our `transformers chat` CLI\n \n ```shell\n-transformers chat localhost:8000 --model-name-or-path Qwen/Qwen3-4B\n+transformers chat Qwen/Qwen3-4B\n ```\n \n or by sending an HTTP request, like we'll see below."
        },
        {
            "sha": "7eaad9289e6842e429c9e64e1137441c1e3bec39",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=a099b27ddb2fe4a0f690ba6007ee38f109ccbf31",
            "patch": "@@ -276,7 +276,7 @@ def run(self):\n \n extras[\"integrations\"] = extras[\"hub-kernels\"] + extras[\"optuna\"] + extras[\"ray\"]\n \n-extras[\"serving\"] = deps_list(\"openai\", \"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\") + extras[\"torch\"]\n+extras[\"serving\"] = deps_list(\"openai\", \"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\", \"rich\") + extras[\"torch\"]\n extras[\"audio\"] = deps_list(\n     \"librosa\",\n     \"pyctcdecode\","
        },
        {
            "sha": "0ae735ec856251943fa4bb6fb36e0df8831f14cb",
            "filename": "src/transformers/cli/chat.py",
            "status": "modified",
            "additions": 28,
            "deletions": 30,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/src%2Ftransformers%2Fcli%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/src%2Ftransformers%2Fcli%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fchat.py?ref=a099b27ddb2fe4a0f690ba6007ee38f109ccbf31",
            "patch": "@@ -20,8 +20,9 @@\n import time\n from collections.abc import AsyncIterator\n from typing import Annotated, Any, Optional\n+from urllib.parse import urljoin, urlparse\n \n-import click\n+import httpx\n import typer\n import yaml\n from huggingface_hub import AsyncInferenceClient, ChatCompletionStreamOutput\n@@ -44,6 +45,7 @@\n     from rich.live import Live\n     from rich.markdown import Markdown\n \n+DEFAULT_HTTP_ENDPOINT = {\"hostname\": \"localhost\", \"port\": 8000}\n ALLOWED_KEY_CHARS = set(string.ascii_letters + string.whitespace)\n ALLOWED_VALUE_CHARS = set(\n     string.ascii_letters + string.digits + string.whitespace + r\".!\\\"#$%&'()*+,\\-/:<=>?@[]^_`{|}~\"\n@@ -195,38 +197,17 @@ def print_status(self, config: GenerationConfig):\n         self._console.print()\n \n \n-class ChatCommand(typer.core.TyperCommand):\n-    \"\"\"Custom Click command to override missing parameter error message.\n-\n-    Transformers v5 introduced a breaking change in the `transformers chat` command: the `model_id` parameter\n-    is now required, and the command can no longer starts a server. This class overrides the default error message\n-    to provide a more helpful message to users who may be used to the old behavior.\n-    \"\"\"\n-\n-    def parse_args(self, ctx, args):\n-        try:\n-            return super().parse_args(ctx, args)\n-        except click.MissingParameter as e:\n-            if e.param and e.param.name == \"model_id\":\n-                typer.echo(\"Error: Missing argument 'MODEL_ID'.\\n\")\n-                typer.echo(\n-                    \"Launching a server directly from the `transformers chat` command is no longer supported. \"\n-                    \"Please use `transformers serve` to launch a server. \"\n-                    \"Use --help for more information.\",\n-                )\n-                ctx.exit(1)\n-            raise\n-\n-\n class Chat:\n     \"\"\"Chat with a model from the command line.\"\"\"\n \n     # Defining a class to help with internal state but in practice it's just a method to call\n     # TODO: refactor into a proper module with helpers + 1 main method\n     def __init__(\n         self,\n-        base_url: Annotated[str, typer.Argument(help=\"Base url to connect to (e.g. http://localhost:8000/v1).\")],\n         model_id: Annotated[str, typer.Argument(help=\"ID of the model to use (e.g. 'HuggingFaceTB/SmolLM3-3B').\")],\n+        base_url: Annotated[\n+            Optional[str], typer.Argument(help=\"Base url to connect to (e.g. http://localhost:8000/v1).\")\n+        ] = f\"http://{DEFAULT_HTTP_ENDPOINT['hostname']}:{DEFAULT_HTTP_ENDPOINT['port']}\",\n         generate_flags: Annotated[\n             list[str] | None,\n             typer.Argument(\n@@ -257,6 +238,11 @@ def __init__(\n     ) -> None:\n         \"\"\"Chat with a model from the command line.\"\"\"\n         self.base_url = base_url\n+\n+        parsed = urlparse(self.base_url)\n+        if parsed.hostname == DEFAULT_HTTP_ENDPOINT[\"hostname\"] and parsed.port == DEFAULT_HTTP_ENDPOINT[\"port\"]:\n+            self.check_health(self.base_url)\n+\n         self.model_id = model_id\n         self.system_prompt = system_prompt\n         self.save_folder = save_folder\n@@ -286,8 +272,23 @@ def __init__(\n         # Run chat session\n         asyncio.run(self._inner_run())\n \n-    # -----------------------------------------------------------------------------------------------------------------\n-    # User commands\n+    @staticmethod\n+    def check_health(url):\n+        health_url = urljoin(url + \"/\", \"health\")\n+        try:\n+            output = httpx.get(health_url)\n+            if output.status_code != 200:\n+                raise ValueError(\n+                    f\"The server running on {url} returned status code {output.status_code} on health check (/health).\"\n+                )\n+        except httpx.ConnectError:\n+            raise ValueError(\n+                f\"No server currently running on {url}. To run a local server, please run `transformers serve` in a\"\n+                f\"separate shell. Find more information here: https://huggingface.co/docs/transformers/serving\"\n+            )\n+\n+        return True\n+\n     def handle_non_exit_user_commands(\n         self,\n         user_input: str,\n@@ -362,9 +363,6 @@ def handle_non_exit_user_commands(\n \n         return chat, valid_command, config\n \n-    # -----------------------------------------------------------------------------------------------------------------\n-    # Main logic\n-\n     async def _inner_run(self):\n         interface = RichInterface(model_id=self.model_id, user_id=self.user)\n         interface.clear()"
        },
        {
            "sha": "0fe67becd24ea8505af784fde3c693f7543b2571",
            "filename": "src/transformers/cli/transformers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/src%2Ftransformers%2Fcli%2Ftransformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a099b27ddb2fe4a0f690ba6007ee38f109ccbf31/src%2Ftransformers%2Fcli%2Ftransformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Ftransformers.py?ref=a099b27ddb2fe4a0f690ba6007ee38f109ccbf31",
            "patch": "@@ -17,7 +17,7 @@\n \n from transformers.cli.add_fast_image_processor import add_fast_image_processor\n from transformers.cli.add_new_model_like import add_new_model_like\n-from transformers.cli.chat import Chat, ChatCommand\n+from transformers.cli.chat import Chat\n from transformers.cli.download import download\n from transformers.cli.serve import Serve\n from transformers.cli.system import env, version\n@@ -27,7 +27,7 @@\n \n app.command()(add_fast_image_processor)\n app.command()(add_new_model_like)\n-app.command(name=\"chat\", cls=ChatCommand)(Chat)\n+app.command(name=\"chat\")(Chat)\n app.command()(download)\n app.command()(env)\n app.command(name=\"serve\")(Serve)"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 42,
        "deletions": 36
    }
}