{
    "author": "lambertwjh",
    "message": "fix_image_processing_fast_for_glm4v (#40483)\n\n* fix_image_processing_fast_for_glm4v\n\n* fix(format): auto-ruff format\n\n* add test image processing glm4v\n\n* fix quality\n\n---------\n\nCo-authored-by: Your Name <you@example.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "dae1ccfb980657b596b6adf1d800cf1dbf1967eb",
    "files": [
        {
            "sha": "061654519d21dd674b6600b1a8ed21f6a0d6bd8a",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v_fast.py",
            "status": "modified",
            "additions": 56,
            "deletions": 39,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/dae1ccfb980657b596b6adf1d800cf1dbf1967eb/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dae1ccfb980657b596b6adf1d800cf1dbf1967eb/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py?ref=dae1ccfb980657b596b6adf1d800cf1dbf1967eb",
            "patch": "@@ -22,6 +22,8 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n )\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n@@ -128,46 +130,54 @@ def _preprocess(\n         Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n         \"\"\"\n \n-        processed_images = []\n-        processed_grids = []\n-\n-        all_target_sizes = []\n-        for image in images:\n-            height, width = image.shape[-2:]\n-            resized_height, resized_width = smart_resize(\n-                num_frames=temporal_patch_size,\n-                height=height,\n-                width=width,\n-                temporal_factor=temporal_patch_size,\n-                factor=patch_size * merge_size,\n-                min_pixels=size.shortest_edge,\n-                max_pixels=size.longest_edge,\n-            )\n-            all_target_sizes.append((resized_height, resized_width))\n-\n-        target_height = max([s[0] for s in all_target_sizes])\n-        target_width = max([s[1] for s in all_target_sizes])\n-\n-        for image in images:\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            height, width = stacked_images.shape[-2:]\n             if do_resize:\n-                image = self.resize(\n-                    image,\n-                    size=SizeDict(height=target_height, width=target_width),\n+                resized_height, resized_width = smart_resize(\n+                    num_frames=temporal_patch_size,\n+                    height=height,\n+                    width=width,\n+                    temporal_factor=temporal_patch_size,\n+                    factor=patch_size * merge_size,\n+                    min_pixels=size.shortest_edge,\n+                    max_pixels=size.longest_edge,\n+                )\n+                stacked_images = self.resize(\n+                    stacked_images,\n+                    size=SizeDict(height=resized_height, width=resized_width),\n                     interpolation=interpolation,\n                 )\n+            resized_images_grouped[shape] = stacked_images\n+\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        processed_grids = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            resized_height, resized_width = stacked_images.shape[-2:]\n+\n+            patches = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            if patches.ndim == 4:  # (B, C, H, W)\n+                patches = patches.unsqueeze(1)  # (B, T=1, C, H, W)\n+\n+            if patches.shape[1] % temporal_patch_size != 0:\n+                repeats = patches[:, -1:].repeat(\n+                    1, temporal_patch_size - (patches.shape[1] % temporal_patch_size), 1, 1, 1\n+                )\n+                patches = torch.cat([patches, repeats], dim=1)\n+\n+            batch_size, t_len, channel = patches.shape[:3]\n+            grid_t = t_len // temporal_patch_size\n+            grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n \n-            image = self.rescale_and_normalize(\n-                image.unsqueeze(0), do_rescale, rescale_factor, do_normalize, image_mean, image_std\n-            ).squeeze(0)\n-\n-            patches = image.unsqueeze(0)\n-            if patches.shape[0] % temporal_patch_size != 0:\n-                repeats = patches[-1:].repeat(temporal_patch_size - (patches.shape[0] % temporal_patch_size), 1, 1, 1)\n-                patches = torch.cat([patches, repeats], dim=0)\n-            channel = patches.shape[1]\n-            grid_t = patches.shape[0] // temporal_patch_size\n-            grid_h, grid_w = target_height // patch_size, target_width // patch_size\n             patches = patches.view(\n+                batch_size,\n                 grid_t,\n                 temporal_patch_size,\n                 channel,\n@@ -178,15 +188,22 @@ def _preprocess(\n                 merge_size,\n                 patch_size,\n             )\n-            patches = patches.permute(0, 3, 6, 4, 7, 2, 1, 5, 8)\n+            # (B, grid_t, gh, gw, mh, mw, C, tp, ph, pw)\n+            patches = patches.permute(0, 1, 4, 7, 5, 8, 3, 2, 6, 9)\n+\n             flatten_patches = patches.reshape(\n+                batch_size,\n                 grid_t * grid_h * grid_w,\n                 channel * temporal_patch_size * patch_size * patch_size,\n             )\n-            processed_images.append(flatten_patches)\n-            processed_grids.append([grid_t, grid_h, grid_w])\n \n-        pixel_values = torch.stack(processed_images, dim=0)\n+            processed_images_grouped[shape] = flatten_patches\n+            processed_grids[shape] = [[grid_t, grid_h, grid_w]] * batch_size\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_grids = reorder_images(processed_grids, grouped_images_index)\n+\n+        pixel_values = torch.cat(processed_images, dim=0)\n         image_grid_thw = torch.tensor(processed_grids)\n \n         return BatchFeature("
        },
        {
            "sha": "cb5af4b275d2abdd4cf281fd80a7b7e780f4cedf",
            "filename": "tests/models/glm4v/test_image_processing_glm4v.py",
            "status": "added",
            "additions": 254,
            "deletions": 0,
            "changes": 254,
            "blob_url": "https://github.com/huggingface/transformers/blob/dae1ccfb980657b596b6adf1d800cf1dbf1967eb/tests%2Fmodels%2Fglm4v%2Ftest_image_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dae1ccfb980657b596b6adf1d800cf1dbf1967eb/tests%2Fmodels%2Fglm4v%2Ftest_image_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_image_processing_glm4v.py?ref=dae1ccfb980657b596b6adf1d800cf1dbf1967eb",
            "patch": "@@ -0,0 +1,254 @@\n+# Copyright 2021 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import Glm4vImageProcessor\n+    from transformers.models.glm4v.image_processing_glm4v import smart_resize\n+\n+    if is_torchvision_available():\n+        from transformers import Glm4vImageProcessorFast\n+\n+\n+class Glm4vImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        min_resolution=30,\n+        max_resolution=80,\n+        do_resize=True,\n+        size=None,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        temporal_patch_size=2,\n+        patch_size=14,\n+        merge_size=2,\n+    ):\n+        size = size if size is not None else {\"longest_edge\": 20, \"shortest_edge\": 10}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.temporal_patch_size = temporal_patch_size\n+        self.patch_size = patch_size\n+        self.merge_size = merge_size\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_normalize\": self.do_normalize,\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"temporal_patch_size\": self.temporal_patch_size,\n+            \"patch_size\": self.patch_size,\n+            \"merge_size\": self.merge_size,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        grid_t = 1\n+        hidden_dim = self.num_channels * self.temporal_patch_size * self.patch_size * self.patch_size\n+        seq_len = 0\n+        for image in images:\n+            if isinstance(image, list) and isinstance(image[0], Image.Image):\n+                image = np.stack([np.array(frame) for frame in image])\n+            elif hasattr(image, \"shape\"):\n+                pass\n+            else:\n+                image = np.array(image)\n+            if hasattr(image, \"shape\") and len(image.shape) >= 3:\n+                if isinstance(image, np.ndarray):\n+                    if len(image.shape) == 4:\n+                        height, width = image.shape[1:3]\n+                    elif len(image.shape) == 3:\n+                        height, width = image.shape[:2]\n+                    else:\n+                        height, width = self.min_resolution, self.min_resolution\n+                else:\n+                    height, width = image.shape[-2:]\n+            else:\n+                height, width = self.min_resolution, self.min_resolution\n+\n+            resized_height, resized_width = smart_resize(\n+                self.temporal_patch_size,\n+                height,\n+                width,\n+                factor=self.patch_size * self.merge_size,\n+                min_pixels=self.size[\"shortest_edge\"],\n+                max_pixels=self.size[\"longest_edge\"],\n+            )\n+            grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n+            seq_len += grid_t * grid_h * grid_w\n+        return (seq_len, hidden_dim)\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class ViTImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = Glm4vImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = Glm4vImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = Glm4vImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 10, \"longest_edge\": 20})\n+\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size={\"shortest_edge\": 42, \"longest_edge\": 42}\n+            )\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 42})\n+\n+    # batch size is flattened\n+    def test_call_pil(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_numpy(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_pytorch(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_numpy_4_channels(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Test that can process images which have an arbitrary number of channels\n+            # Initialize image_processing\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            # create random numpy tensors\n+            self.image_processor_tester.num_channels = 4\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+\n+            # Test not batched input\n+            encoded_images = image_processor(\n+                image_inputs[0],\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=0,\n+                image_std=1,\n+            ).pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processor(\n+                image_inputs,\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=0,\n+                image_std=1,\n+            ).pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)"
        }
    ],
    "stats": {
        "total": 349,
        "additions": 310,
        "deletions": 39
    }
}