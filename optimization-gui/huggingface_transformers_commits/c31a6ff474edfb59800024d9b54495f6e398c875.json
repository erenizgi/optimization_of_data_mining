{
    "author": "alex-bene",
    "message": "Add post_process_depth_estimation to image processors and support ZoeDepth's inference intricacies (#32550)\n\n* add colorize_depth and matplotlib availability check\r\n\r\n* add post_process_depth_estimation for zoedepth + tests\r\n\r\n* add post_process_depth_estimation for DPT + tests\r\n\r\n* add post_process_depth_estimation in DepthEstimationPipeline & special case for zoedepth\r\n\r\n* run `make fixup`\r\n\r\n* fix import related error on tests\r\n\r\n* fix more import related errors on test\r\n\r\n* forgot some `torch` calls in declerations\r\n\r\n* remove `torch` call in zoedepth tests that caused error\r\n\r\n* updated docs for depth estimation\r\n\r\n* small fix for `colorize` input/output types\r\n\r\n* remove `colorize_depth`, fix various names, remove matplotlib dependency\r\n\r\n* fix formatting\r\n\r\n* run fixup\r\n\r\n* different images for test\r\n\r\n* update examples in `forward` functions\r\n\r\n* fixed broken links\r\n\r\n* fix output types for docs\r\n\r\n* possible format fix inside `<Tip>`\r\n\r\n* Readability related updates\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Readability related update\r\n\r\n* cleanup after merge\r\n\r\n* refactor `post_process_depth_estimation` to return dict; simplify ZoeDepth's `post_process_depth_estimation`\r\n\r\n* rewrite dict merging to support python 3.8\r\n\r\n---------\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "c31a6ff474edfb59800024d9b54495f6e398c875",
    "files": [
        {
            "sha": "7cdf72de5c8474b4c91fbc510fecb7569ca3b486",
            "filename": "docs/source/en/model_doc/depth_anything.md",
            "status": "modified",
            "additions": 10,
            "deletions": 13,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything.md?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -84,27 +84,24 @@ If you want to do the pre- and postprocessing yourself, here's how to do that:\n \n >>> with torch.no_grad():\n ...     outputs = model(**inputs)\n-...     predicted_depth = outputs.predicted_depth\n-\n->>> # interpolate to original size\n->>> prediction = torch.nn.functional.interpolate(\n-...     predicted_depth.unsqueeze(1),\n-...     size=image.size[::-1],\n-...     mode=\"bicubic\",\n-...     align_corners=False,\n+\n+>>> # interpolate to original size and visualize the prediction\n+>>> post_processed_output = image_processor.post_process_depth_estimation(\n+...     outputs,\n+...     target_sizes=[(image.height, image.width)],\n ... )\n \n->>> # visualize the prediction\n->>> output = prediction.squeeze().cpu().numpy()\n->>> formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n->>> depth = Image.fromarray(formatted)\n+>>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n+>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n+>>> depth = depth.detach().cpu().numpy() * 255\n+>>> depth = Image.fromarray(depth.astype(\"uint8\"))\n ```\n \n ## Resources\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Depth Anything.\n \n-- [Monocular depth estimation task guide](../tasks/depth_estimation)\n+- [Monocular depth estimation task guide](../tasks/monocular_depth_estimation)\n - A notebook showcasing inference with [`DepthAnythingForDepthEstimation`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Depth%20Anything/Predicting_depth_in_an_image_with_Depth_Anything.ipynb). ðŸŒŽ\n \n If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."
        },
        {
            "sha": "c98017d2bbc510afe68b46c8021659958d835692",
            "filename": "docs/source/en/model_doc/depth_anything_v2.md",
            "status": "modified",
            "additions": 10,
            "deletions": 13,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything_v2.md?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -78,27 +78,24 @@ If you want to do the pre- and post-processing yourself, here's how to do that:\n \n >>> with torch.no_grad():\n ...     outputs = model(**inputs)\n-...     predicted_depth = outputs.predicted_depth\n-\n->>> # interpolate to original size\n->>> prediction = torch.nn.functional.interpolate(\n-...     predicted_depth.unsqueeze(1),\n-...     size=image.size[::-1],\n-...     mode=\"bicubic\",\n-...     align_corners=False,\n+\n+>>> # interpolate to original size and visualize the prediction\n+>>> post_processed_output = image_processor.post_process_depth_estimation(\n+...     outputs,\n+...     target_sizes=[(image.height, image.width)],\n ... )\n \n->>> # visualize the prediction\n->>> output = prediction.squeeze().cpu().numpy()\n->>> formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n->>> depth = Image.fromarray(formatted)\n+>>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n+>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n+>>> depth = depth.detach().cpu().numpy() * 255\n+>>> depth = Image.fromarray(depth.astype(\"uint8\"))\n ```\n \n ## Resources\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Depth Anything.\n \n-- [Monocular depth estimation task guide](../tasks/depth_estimation)\n+- [Monocular depth estimation task guide](../tasks/monocular_depth_estimation)\n - [Depth Anything V2 demo](https://huggingface.co/spaces/depth-anything/Depth-Anything-V2).\n - A notebook showcasing inference with [`DepthAnythingForDepthEstimation`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Depth%20Anything/Predicting_depth_in_an_image_with_Depth_Anything.ipynb). ðŸŒŽ\n - [Core ML conversion of the `small` variant for use on Apple Silicon](https://huggingface.co/apple/coreml-depth-anything-v2-small)."
        },
        {
            "sha": "74e25f3c3f6ea52fa23292b065d0f6d6cc3ab53b",
            "filename": "docs/source/en/model_doc/zoedepth.md",
            "status": "modified",
            "additions": 51,
            "deletions": 39,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -39,54 +39,66 @@ The original code can be found [here](https://github.com/isl-org/ZoeDepth).\n The easiest to perform inference with ZoeDepth is by leveraging the [pipeline API](../main_classes/pipelines.md):\n \n ```python\n-from transformers import pipeline\n-from PIL import Image\n-import requests\n+>>> from transformers import pipeline\n+>>> from PIL import Image\n+>>> import requests\n \n-url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-image = Image.open(requests.get(url, stream=True).raw)\n+>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n \n-pipe = pipeline(task=\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n-result = pipe(image)\n-depth = result[\"depth\"]\n+>>> pipe = pipeline(task=\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n+>>> result = pipe(image)\n+>>> depth = result[\"depth\"]\n ```\n \n Alternatively, one can also perform inference using the classes:\n \n ```python\n-from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n-import torch\n-import numpy as np\n-from PIL import Image\n-import requests\n-\n-url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-image = Image.open(requests.get(url, stream=True).raw)\n-\n-image_processor = AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\")\n-model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\")\n-\n-# prepare image for the model\n-inputs = image_processor(images=image, return_tensors=\"pt\")\n-\n-with torch.no_grad():\n-    outputs = model(**inputs)\n-    predicted_depth = outputs.predicted_depth\n-\n-# interpolate to original size\n-prediction = torch.nn.functional.interpolate(\n-    predicted_depth.unsqueeze(1),\n-    size=image.size[::-1],\n-    mode=\"bicubic\",\n-    align_corners=False,\n-)\n-\n-# visualize the prediction\n-output = prediction.squeeze().cpu().numpy()\n-formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n-depth = Image.fromarray(formatted)\n+>>> from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n+>>> import torch\n+>>> import numpy as np\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> image_processor = AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\")\n+>>> model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\")\n+\n+>>> # prepare image for the model\n+>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+>>> with torch.no_grad():   \n+...     outputs = model(pixel_values)\n+\n+>>> # interpolate to original size and visualize the prediction\n+>>> ## ZoeDepth dynamically pads the input image. Thus we pass the original image size as argument\n+>>> ## to `post_process_depth_estimation` to remove the padding and resize to original dimensions.\n+>>> post_processed_output = image_processor.post_process_depth_estimation(\n+...     outputs,\n+...     source_sizes=[(image.height, image.width)],\n+... )\n+\n+>>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n+>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n+>>> depth = depth.detach().cpu().numpy() * 255\n+>>> depth = Image.fromarray(depth.astype(\"uint8\"))\n ```\n \n+<Tip>\n+<p>In the <a href=\"https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131\">original implementation</a> ZoeDepth model performs inference on both the original and flipped images and averages out the results. The <code>post_process_depth_estimation</code> function can handle this for us by passing the flipped outputs to the optional <code>outputs_flipped</code> argument:</p>\n+<pre><code class=\"language-Python\">&gt;&gt;&gt; with torch.no_grad():   \n+...     outputs = model(pixel_values)\n+...     outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))\n+&gt;&gt;&gt; post_processed_output = image_processor.post_process_depth_estimation(\n+...     outputs,\n+...     source_sizes=[(image.height, image.width)],\n+...     outputs_flipped=outputs_flipped,\n+... )\n+</code></pre>\n+</Tip>\n+\n ## Resources\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with ZoeDepth."
        },
        {
            "sha": "3ded3179154aaee0128cf78ec71ebde73dcfe279",
            "filename": "docs/source/en/tasks/monocular_depth_estimation.md",
            "status": "modified",
            "additions": 24,
            "deletions": 87,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -126,97 +126,34 @@ Pass the prepared inputs through the model:\n ...     outputs = model(pixel_values)\n ```\n \n-Let's post-process and visualize the results. \n-\n-We need to pad and then resize the outputs so that predicted depth map has the same dimension as the original image. After resizing we will remove the padded regions from the depth. \n+Let's post-process the results to remove any padding and resize the depth map to match the original image size. The `post_process_depth_estimation` outputs a list of dicts containing the `\"predicted_depth\"`.\n \n ```py\n->>> import numpy as np\n->>> import torch.nn.functional as F\n-\n->>> predicted_depth = outputs.predicted_depth.unsqueeze(dim=1)\n->>> height, width = pixel_values.shape[2:]\n-\n->>> height_padding_factor = width_padding_factor = 3\n->>> pad_h = int(np.sqrt(height/2) * height_padding_factor)\n->>> pad_w = int(np.sqrt(width/2) * width_padding_factor)\n-\n->>> if predicted_depth.shape[-2:] != pixel_values.shape[-2:]:\n->>>    predicted_depth = F.interpolate(predicted_depth, size= (height, width), mode='bicubic', align_corners=False)\n-\n->>> if pad_h > 0:\n-     predicted_depth = predicted_depth[:, :, pad_h:-pad_h,:]\n->>> if pad_w > 0:\n-     predicted_depth = predicted_depth[:, :, :, pad_w:-pad_w]\n+>>> # ZoeDepth dynamically pads the input image. Thus we pass the original image size as argument\n+>>> # to `post_process_depth_estimation` to remove the padding and resize to original dimensions.\n+>>> post_processed_output = image_processor.post_process_depth_estimation(\n+...     outputs,\n+...     source_sizes=[(image.height, image.width)],\n+... )\n+\n+>>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n+>>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n+>>> depth = depth.detach().cpu().numpy() * 255\n+>>> depth = Image.fromarray(depth.astype(\"uint8\"))\n ```\n \n-We can now visualize the results (the function below is taken from the [GaussianObject](https://github.com/GaussianObject/GaussianObject/blob/ad6629efadb57902d5f8bc0fa562258029a4bdf1/pred_monodepth.py#L11) framework).\n-\n-```py\n-import matplotlib\n-\n-def colorize(value, vmin=None, vmax=None, cmap='gray_r', invalid_val=-99, invalid_mask=None, background_color=(128, 128, 128, 255), gamma_corrected=False, value_transform=None):\n-    \"\"\"Converts a depth map to a color image.\n-\n-    Args:\n-        value (torch.Tensor, numpy.ndarray): Input depth map. Shape: (H, W) or (1, H, W) or (1, 1, H, W). All singular dimensions are squeezed\n-        vmin (float, optional): vmin-valued entries are mapped to start color of cmap. If None, value.min() is used. Defaults to None.\n-        vmax (float, optional):  vmax-valued entries are mapped to end color of cmap. If None, value.max() is used. Defaults to None.\n-        cmap (str, optional): matplotlib colormap to use. Defaults to 'magma_r'.\n-        invalid_val (int, optional): Specifies value of invalid pixels that should be colored as 'background_color'. Defaults to -99.\n-        invalid_mask (numpy.ndarray, optional): Boolean mask for invalid regions. Defaults to None.\n-        background_color (tuple[int], optional): 4-tuple RGB color to give to invalid pixels. Defaults to (128, 128, 128, 255).\n-        gamma_corrected (bool, optional): Apply gamma correction to colored image. Defaults to False.\n-        value_transform (Callable, optional): Apply transform function to valid pixels before coloring. Defaults to None.\n-\n-    Returns:\n-        numpy.ndarray, dtype - uint8: Colored depth map. Shape: (H, W, 4)\n-    \"\"\"\n-    if isinstance(value, torch.Tensor):\n-        value = value.detach().cpu().numpy()\n-\n-    value = value.squeeze()\n-    if invalid_mask is None:\n-        invalid_mask = value == invalid_val\n-    mask = np.logical_not(invalid_mask)\n-\n-    # normalize\n-    vmin = np.percentile(value[mask],2) if vmin is None else vmin\n-    vmax = np.percentile(value[mask],85) if vmax is None else vmax\n-    if vmin != vmax:\n-        value = (value - vmin) / (vmax - vmin)  # vmin..vmax\n-    else:\n-        # Avoid 0-division\n-        value = value * 0.\n-\n-    # squeeze last dim if it exists\n-    # grey out the invalid values\n-\n-    value[invalid_mask] = np.nan\n-    cmapper = matplotlib.colormaps.get_cmap(cmap)\n-    if value_transform:\n-        value = value_transform(value)\n-        # value = value / value.max()\n-    value = cmapper(value, bytes=True)  # (nxmx4)\n-\n-    # img = value[:, :, :]\n-    img = value[...]\n-    img[invalid_mask] = background_color\n-\n-    #     return img.transpose((2, 0, 1))\n-    if gamma_corrected:\n-        # gamma correction\n-        img = img / 255\n-        img = np.power(img, 2.2)\n-        img = img * 255\n-        img = img.astype(np.uint8)\n-    return img\n-\n->>> result = colorize(predicted_depth.cpu().squeeze().numpy())\n->>> Image.fromarray(result)\n-```\n-\n-\n+<Tip>\n+<p>In the <a href=\"https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131\">original implementation</a> ZoeDepth model performs inference on both the original and flipped images and averages out the results. The <code>post_process_depth_estimation</code> function can handle this for us by passing the flipped outputs to the optional <code>outputs_flipped</code> argument:</p>\n+<pre><code class=\"language-Python\">&gt;&gt;&gt; with torch.no_grad():   \n+...     outputs = model(pixel_values)\n+...     outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))\n+&gt;&gt;&gt; post_processed_output = image_processor.post_process_depth_estimation(\n+...     outputs,\n+...     source_sizes=[(image.height, image.width)],\n+...     outputs_flipped=outputs_flipped,\n+... )\n+</code></pre>\n+</Tip>\n \n <div class=\"flex justify-center\">\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-visualization-zoe.png\" alt=\"Depth estimation visualization\"/>"
        },
        {
            "sha": "59c628786328e6eb0f9510f69901d4ef76be7b82",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -413,20 +413,18 @@ def forward(\n \n         >>> with torch.no_grad():\n         ...     outputs = model(**inputs)\n-        ...     predicted_depth = outputs.predicted_depth\n \n         >>> # interpolate to original size\n-        >>> prediction = torch.nn.functional.interpolate(\n-        ...     predicted_depth.unsqueeze(1),\n-        ...     size=image.size[::-1],\n-        ...     mode=\"bicubic\",\n-        ...     align_corners=False,\n+        >>> post_processed_output = image_processor.post_process_depth_estimation(\n+        ...     outputs,\n+        ...     target_sizes=[(image.height, image.width)],\n         ... )\n \n         >>> # visualize the prediction\n-        >>> output = prediction.squeeze().cpu().numpy()\n-        >>> formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n-        >>> depth = Image.fromarray(formatted)\n+        >>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n+        >>> depth = predicted_depth * 255 / predicted_depth.max()\n+        >>> depth = depth.detach().cpu().numpy()\n+        >>> depth = Image.fromarray(depth.astype(\"uint8\"))\n         ```\"\"\"\n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "20024e5fefa19833dc8d08e00e523199de35381d",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 53,
            "deletions": 2,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -15,7 +15,11 @@\n \"\"\"Image processor class for DPT.\"\"\"\n \n import math\n-from typing import Dict, Iterable, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Tuple, Union\n+\n+\n+if TYPE_CHECKING:\n+    from ...modeling_outputs import DepthEstimatorOutput\n \n import numpy as np\n \n@@ -37,7 +41,13 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...utils import (\n+    TensorType,\n+    filter_out_non_signature_kwargs,\n+    is_vision_available,\n+    logging,\n+    requires_backends,\n+)\n \n \n if is_torch_available():\n@@ -461,3 +471,44 @@ def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple]\n             semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n \n         return semantic_segmentation\n+\n+    def post_process_depth_estimation(\n+        self,\n+        outputs: \"DepthEstimatorOutput\",\n+        target_sizes: Optional[Union[TensorType, List[Tuple[int, int]], None]] = None,\n+    ) -> List[Dict[str, TensorType]]:\n+        \"\"\"\n+        Converts the raw output of [`DepthEstimatorOutput`] into final depth predictions and depth PIL images.\n+        Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DepthEstimatorOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`TensorType` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+\n+        Returns:\n+            `List[Dict[str, TensorType]]`: A list of dictionaries of tensors representing the processed depth\n+            predictions.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+\n+        predicted_depth = outputs.predicted_depth\n+\n+        if (target_sizes is not None) and (len(predicted_depth) != len(target_sizes)):\n+            raise ValueError(\n+                \"Make sure that you pass in as many target sizes as the batch dimension of the predicted depth\"\n+            )\n+\n+        results = []\n+        target_sizes = [None] * len(predicted_depth) if target_sizes is None else target_sizes\n+        for depth, target_size in zip(predicted_depth, target_sizes):\n+            if target_size is not None:\n+                depth = torch.nn.functional.interpolate(\n+                    depth.unsqueeze(0).unsqueeze(1), size=target_size, mode=\"bicubic\", align_corners=False\n+                ).squeeze()\n+\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results"
        },
        {
            "sha": "2d4654a234c2c6326c11ab10627d1b890343490c",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -1121,20 +1121,18 @@ def forward(\n \n         >>> with torch.no_grad():\n         ...     outputs = model(**inputs)\n-        ...     predicted_depth = outputs.predicted_depth\n \n         >>> # interpolate to original size\n-        >>> prediction = torch.nn.functional.interpolate(\n-        ...     predicted_depth.unsqueeze(1),\n-        ...     size=image.size[::-1],\n-        ...     mode=\"bicubic\",\n-        ...     align_corners=False,\n+        >>> post_processed_output = image_processor.post_process_depth_estimation(\n+        ...     outputs,\n+        ...     target_sizes=[(image.height, image.width)],\n         ... )\n \n         >>> # visualize the prediction\n-        >>> output = prediction.squeeze().cpu().numpy()\n-        >>> formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n-        >>> depth = Image.fromarray(formatted)\n+        >>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n+        >>> depth = predicted_depth * 255 / predicted_depth.max()\n+        >>> depth = depth.detach().cpu().numpy()\n+        >>> depth = Image.fromarray(depth.astype(\"uint8\"))\n         ```\"\"\"\n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "2211ab07c09d4c3331ac47d013b6b1ebbc27796e",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 129,
            "deletions": 15,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -15,10 +15,14 @@\n \"\"\"Image processor class for ZoeDepth.\"\"\"\n \n import math\n-from typing import Dict, Iterable, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n \n+\n+if TYPE_CHECKING:\n+    from .modeling_zoedepth import ZoeDepthDepthEstimatorOutput\n+\n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import PaddingMode, pad, to_channel_dimension_format\n from ...image_utils import (\n@@ -126,10 +130,10 @@ class ZoeDepthImageProcessor(BaseImageProcessor):\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n             Defines the resampling filter to use if resizing the image. Can be overidden by `resample` in `preprocess`.\n         keep_aspect_ratio (`bool`, *optional*, defaults to `True`):\n-            If `True`, the image is resized by choosing the smaller of the height and width scaling factors and using it for\n-            both dimensions. This ensures that the image is scaled down as little as possible while still fitting within the\n-            desired output size. In case `ensure_multiple_of` is also set, the image is further resized to a size that is a\n-            multiple of this value by flooring the height and width to the nearest multiple of this value.\n+            If `True`, the image is resized by choosing the smaller of the height and width scaling factors and using it\n+            for both dimensions. This ensures that the image is scaled down as little as possible while still fitting\n+            within the desired output size. In case `ensure_multiple_of` is also set, the image is further resized to a\n+            size that is a multiple of this value by flooring the height and width to the nearest multiple of this value.\n             Can be overidden by `keep_aspect_ratio` in `preprocess`.\n         ensure_multiple_of (`int`, *optional*, defaults to 32):\n             If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Works by flooring\n@@ -331,19 +335,21 @@ def preprocess(\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing. If `keep_aspect_ratio` is `True`, he image is resized by choosing the smaller of\n-                the height and width scaling factors and using it for both dimensions. If `ensure_multiple_of` is also set,\n-                the image is further resized to a size that is a multiple of this value.\n+                Size of the image after resizing. If `keep_aspect_ratio` is `True`, he image is resized by choosing the\n+                smaller of the height and width scaling factors and using it for both dimensions. If `ensure_multiple_of`\n+                is also set, the image is further resized to a size that is a multiple of this value.\n             keep_aspect_ratio (`bool`, *optional*, defaults to `self.keep_aspect_ratio`):\n-                If `True` and `do_resize=True`, the image is resized by choosing the smaller of the height and width scaling factors and using it for\n-                both dimensions. This ensures that the image is scaled down as little as possible while still fitting within the\n-                desired output size. In case `ensure_multiple_of` is also set, the image is further resized to a size that is a\n-                multiple of this value by flooring the height and width to the nearest multiple of this value.\n+                If `True` and `do_resize=True`, the image is resized by choosing the smaller of the height and width\n+                scaling factors and using it for both dimensions. This ensures that the image is scaled down as little\n+                as possible while still fitting within the desired output size. In case `ensure_multiple_of` is also\n+                set, the image is further resized to a size that is a multiple of this value by flooring the height and\n+                width to the nearest multiple of this value.\n             ensure_multiple_of (`int`, *optional*, defaults to `self.ensure_multiple_of`):\n-                If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Works by flooring\n-                the height and width to the nearest multiple of this value.\n+                If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Works by\n+                flooring the height and width to the nearest multiple of this value.\n \n-                Works both with and without `keep_aspect_ratio` being set to `True`. Can be overidden by `ensure_multiple_of` in `preprocess`.\n+                Works both with and without `keep_aspect_ratio` being set to `True`. Can be overidden by\n+                `ensure_multiple_of` in `preprocess`.\n             resample (`int`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`, Only\n                 has an effect if `do_resize` is set to `True`.\n@@ -442,3 +448,111 @@ def preprocess(\n \n         data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def post_process_depth_estimation(\n+        self,\n+        outputs: \"ZoeDepthDepthEstimatorOutput\",\n+        source_sizes: Optional[Union[TensorType, List[Tuple[int, int]], None]] = None,\n+        target_sizes: Optional[Union[TensorType, List[Tuple[int, int]], None]] = None,\n+        outputs_flipped: Optional[Union[\"ZoeDepthDepthEstimatorOutput\", None]] = None,\n+        do_remove_padding: Optional[Union[bool, None]] = None,\n+    ) -> List[Dict[str, TensorType]]:\n+        \"\"\"\n+        Converts the raw output of [`ZoeDepthDepthEstimatorOutput`] into final depth predictions and depth PIL images.\n+        Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`ZoeDepthDepthEstimatorOutput`]):\n+                Raw outputs of the model.\n+            source_sizes (`TensorType` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the source size\n+                (height, width) of each image in the batch before preprocessing. This argument should be dealt as\n+                \"required\" unless the user passes `do_remove_padding=False` as input to this function.\n+            target_sizes (`TensorType` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+            outputs_flipped ([`ZoeDepthDepthEstimatorOutput`], *optional*):\n+                Raw outputs of the model from flipped input (averaged out in the end).\n+            do_remove_padding (`bool`, *optional*):\n+                By default ZoeDepth addes padding equal to `int(âˆš(height / 2) * 3)` (and similarly for width) to fix the\n+                boundary artifacts in the output depth map, so we need remove this padding during post_processing. The\n+                parameter exists here in case the user changed the image preprocessing to not include padding.\n+\n+        Returns:\n+            `List[Dict[str, TensorType]]`: A list of dictionaries of tensors representing the processed depth\n+            predictions.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+\n+        predicted_depth = outputs.predicted_depth\n+\n+        if (outputs_flipped is not None) and (predicted_depth.shape != outputs_flipped.predicted_depth.shape):\n+            raise ValueError(\"Make sure that `outputs` and `outputs_flipped` have the same shape\")\n+\n+        if (target_sizes is not None) and (len(predicted_depth) != len(target_sizes)):\n+            raise ValueError(\n+                \"Make sure that you pass in as many target sizes as the batch dimension of the predicted depth\"\n+            )\n+\n+        if do_remove_padding is None:\n+            do_remove_padding = self.do_pad\n+\n+        if source_sizes is None and do_remove_padding:\n+            raise ValueError(\n+                \"Either `source_sizes` should be passed in, or `do_remove_padding` should be set to False\"\n+            )\n+\n+        if (source_sizes is not None) and (len(predicted_depth) != len(source_sizes)):\n+            raise ValueError(\n+                \"Make sure that you pass in as many source image sizes as the batch dimension of the logits\"\n+            )\n+\n+        if outputs_flipped is not None:\n+            predicted_depth = (predicted_depth + torch.flip(outputs_flipped.predicted_depth, dims=[-1])) / 2\n+\n+        predicted_depth = predicted_depth.unsqueeze(1)\n+\n+        # Zoe Depth model adds padding around the images to fix the boundary artifacts in the output depth map\n+        # The padding length is `int(np.sqrt(img_h/2) * fh)` for the height and similar for the width\n+        # fh (and fw respectively) are equal to '3' by default\n+        # Check [here](https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L57)\n+        # for the original implementation.\n+        # In this section, we remove this padding to get the final depth image and depth prediction\n+        padding_factor_h = padding_factor_w = 3\n+\n+        results = []\n+        target_sizes = [None] * len(predicted_depth) if target_sizes is None else target_sizes\n+        source_sizes = [None] * len(predicted_depth) if source_sizes is None else source_sizes\n+        for depth, target_size, source_size in zip(predicted_depth, target_sizes, source_sizes):\n+            # depth.shape = [1, H, W]\n+            if source_size is not None:\n+                pad_h = pad_w = 0\n+\n+                if do_remove_padding:\n+                    pad_h = int(np.sqrt(source_size[0] / 2) * padding_factor_h)\n+                    pad_w = int(np.sqrt(source_size[1] / 2) * padding_factor_w)\n+\n+                depth = nn.functional.interpolate(\n+                    depth.unsqueeze(1),\n+                    size=[source_size[0] + 2 * pad_h, source_size[1] + 2 * pad_w],\n+                    mode=\"bicubic\",\n+                    align_corners=False,\n+                )\n+\n+                if pad_h > 0:\n+                    depth = depth[:, :, pad_h:-pad_h, :]\n+                if pad_w > 0:\n+                    depth = depth[:, :, :, pad_w:-pad_w]\n+\n+                depth = depth.squeeze(1)\n+            # depth.shape = [1, H, W]\n+            if target_size is not None:\n+                target_size = [target_size[0], target_size[1]]\n+                depth = nn.functional.interpolate(\n+                    depth.unsqueeze(1), size=target_size, mode=\"bicubic\", align_corners=False\n+                )\n+            depth = depth.squeeze()\n+            # depth.shape = [H, W]\n+            results.append({\"predicted_depth\": depth})\n+\n+        return results"
        },
        {
            "sha": "979b78aba678a514f64a34d7022d539ce10feddf",
            "filename": "src/transformers/models/zoedepth/modeling_zoedepth.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -1338,20 +1338,18 @@ def forward(\n \n         >>> with torch.no_grad():\n         ...     outputs = model(**inputs)\n-        ...     predicted_depth = outputs.predicted_depth\n \n         >>> # interpolate to original size\n-        >>> prediction = torch.nn.functional.interpolate(\n-        ...     predicted_depth.unsqueeze(1),\n-        ...     size=image.size[::-1],\n-        ...     mode=\"bicubic\",\n-        ...     align_corners=False,\n+        >>> post_processed_output = image_processor.post_process_depth_estimation(\n+        ...     outputs,\n+        ...     source_sizes=[(image.height, image.width)],\n         ... )\n \n         >>> # visualize the prediction\n-        >>> output = prediction.squeeze().cpu().numpy()\n-        >>> formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n-        >>> depth = Image.fromarray(formatted)\n+        >>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n+        >>> depth = predicted_depth * 255 / predicted_depth.max()\n+        >>> depth = depth.detach().cpu().numpy()\n+        >>> depth = Image.fromarray(depth.astype(\"uint8\"))\n         ```\"\"\"\n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "ae86c552a720afd643415ba19a696f7faf4dc53a",
            "filename": "src/transformers/pipelines/depth_estimation.py",
            "status": "modified",
            "additions": 22,
            "deletions": 15,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fpipelines%2Fdepth_estimation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/src%2Ftransformers%2Fpipelines%2Fdepth_estimation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fdepth_estimation.py?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -1,9 +1,13 @@\n import warnings\n from typing import List, Union\n \n-import numpy as np\n-\n-from ..utils import add_end_docstrings, is_torch_available, is_vision_available, logging, requires_backends\n+from ..utils import (\n+    add_end_docstrings,\n+    is_torch_available,\n+    is_vision_available,\n+    logging,\n+    requires_backends,\n+)\n from .base import Pipeline, build_pipeline_init_args\n \n \n@@ -13,8 +17,6 @@\n     from ..image_utils import load_image\n \n if is_torch_available():\n-    import torch\n-\n     from ..models.auto.modeling_auto import MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES\n \n logger = logging.get_logger(__name__)\n@@ -114,14 +116,19 @@ def _forward(self, model_inputs):\n         return model_outputs\n \n     def postprocess(self, model_outputs):\n-        predicted_depth = model_outputs.predicted_depth\n-        prediction = torch.nn.functional.interpolate(\n-            predicted_depth.unsqueeze(1), size=model_outputs[\"target_size\"], mode=\"bicubic\", align_corners=False\n+        outputs = self.image_processor.post_process_depth_estimation(\n+            model_outputs,\n+            # this acts as `source_sizes` for ZoeDepth and as `target_sizes` for the rest of the models so do *not*\n+            # replace with `target_sizes = [model_outputs[\"target_size\"]]`\n+            [model_outputs[\"target_size\"]],\n         )\n-        output = prediction.squeeze().cpu().numpy()\n-        formatted = (output * 255 / np.max(output)).astype(\"uint8\")\n-        depth = Image.fromarray(formatted)\n-        output_dict = {}\n-        output_dict[\"predicted_depth\"] = predicted_depth\n-        output_dict[\"depth\"] = depth\n-        return output_dict\n+\n+        formatted_outputs = []\n+        for output in outputs:\n+            depth = output[\"predicted_depth\"].detach().cpu().numpy()\n+            depth = (depth - depth.min()) / (depth.max() - depth.min())\n+            depth = Image.fromarray((depth * 255).astype(\"uint8\"))\n+\n+            formatted_outputs.append({\"predicted_depth\": output[\"predicted_depth\"], \"depth\": depth})\n+\n+        return formatted_outputs[0] if len(outputs) == 1 else formatted_outputs"
        },
        {
            "sha": "376ea8b310080d97aa2bb21edd24e7ad69f20c3c",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -384,3 +384,29 @@ def test_post_processing_semantic_segmentation(self):\n         segmentation = image_processor.post_process_semantic_segmentation(outputs=outputs)\n         expected_shape = torch.Size((480, 480))\n         self.assertEqual(segmentation[0].shape, expected_shape)\n+\n+    def test_post_processing_depth_estimation(self):\n+        image_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n+        model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n+\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        predicted_depth = image_processor.post_process_depth_estimation(outputs=outputs)[0][\"predicted_depth\"]\n+        expected_shape = torch.Size((384, 384))\n+        self.assertTrue(predicted_depth.shape == expected_shape)\n+\n+        predicted_depth_l = image_processor.post_process_depth_estimation(outputs=outputs, target_sizes=[(500, 500)])\n+        predicted_depth_l = predicted_depth_l[0][\"predicted_depth\"]\n+        expected_shape = torch.Size((500, 500))\n+        self.assertTrue(predicted_depth_l.shape == expected_shape)\n+\n+        output_enlarged = torch.nn.functional.interpolate(\n+            predicted_depth.unsqueeze(0).unsqueeze(1), size=(500, 500), mode=\"bicubic\", align_corners=False\n+        ).squeeze()\n+        self.assertTrue(output_enlarged.shape == expected_shape)\n+        self.assertTrue(torch.allclose(predicted_depth_l, output_enlarged, rtol=1e-3))"
        },
        {
            "sha": "a9c1ffb149d8a575d097ae5cf59d3c6541722b81",
            "filename": "tests/models/zoedepth/test_modeling_zoedepth.py",
            "status": "modified",
            "additions": 99,
            "deletions": 0,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -16,6 +16,8 @@\n \n import unittest\n \n+import numpy as np\n+\n from transformers import Dinov2Config, ZoeDepthConfig\n from transformers.file_utils import is_torch_available, is_vision_available\n from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n@@ -212,6 +214,25 @@ def prepare_img():\n @require_vision\n @slow\n class ZoeDepthModelIntegrationTest(unittest.TestCase):\n+    expected_slice_post_processing = {\n+        (False, False): [\n+            [[1.1348238, 1.1193453, 1.130562], [1.1754476, 1.1613507, 1.1701596], [1.2287744, 1.2101802, 1.2148322]],\n+            [[2.7170, 2.6550, 2.6839], [2.9827, 2.9438, 2.9587], [3.2340, 3.1817, 3.1602]],\n+        ],\n+        (False, True): [\n+            [[1.0610938, 1.1042216, 1.1429265], [1.1099341, 1.148696, 1.1817775], [1.1656011, 1.1988826, 1.2268101]],\n+            [[2.5848, 2.7391, 2.8694], [2.7882, 2.9872, 3.1244], [2.9436, 3.1812, 3.3188]],\n+        ],\n+        (True, False): [\n+            [[1.8382794, 1.8380532, 1.8375976], [1.848761, 1.8485023, 1.8479986], [1.8571457, 1.8568444, 1.8562847]],\n+            [[6.2030, 6.1902, 6.1777], [6.2303, 6.2176, 6.2053], [6.2561, 6.2436, 6.2312]],\n+        ],\n+        (True, True): [\n+            [[1.8306141, 1.8305621, 1.8303483], [1.8410318, 1.8409299, 1.8406585], [1.8492792, 1.8491366, 1.8488203]],\n+            [[6.2616, 6.2520, 6.2435], [6.2845, 6.2751, 6.2667], [6.3065, 6.2972, 6.2887]],\n+        ],\n+    }  # (pad, flip)\n+\n     def test_inference_depth_estimation(self):\n         image_processor = ZoeDepthImageProcessor.from_pretrained(\"Intel/zoedepth-nyu\")\n         model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu\").to(torch_device)\n@@ -255,3 +276,81 @@ def test_inference_depth_estimation_multiple_heads(self):\n         ).to(torch_device)\n \n         self.assertTrue(torch.allclose(outputs.predicted_depth[0, :3, :3], expected_slice, atol=1e-4))\n+\n+    def check_target_size(\n+        self,\n+        image_processor,\n+        pad_input,\n+        images,\n+        outputs,\n+        raw_outputs,\n+        raw_outputs_flipped=None,\n+    ):\n+        outputs_large = image_processor.post_process_depth_estimation(\n+            raw_outputs,\n+            [img.size[::-1] for img in images],\n+            outputs_flipped=raw_outputs_flipped,\n+            target_sizes=[tuple(np.array(img.size[::-1]) * 2) for img in images],\n+            do_remove_padding=pad_input,\n+        )\n+\n+        for img, out, out_l in zip(images, outputs, outputs_large):\n+            out = out[\"predicted_depth\"]\n+            out_l = out_l[\"predicted_depth\"]\n+            out_l_reduced = torch.nn.functional.interpolate(\n+                out_l.unsqueeze(0).unsqueeze(1), size=img.size[::-1], mode=\"bicubic\", align_corners=False\n+            )\n+            self.assertTrue((np.array(out_l.shape)[::-1] == np.array(img.size) * 2).all())\n+            self.assertTrue(torch.allclose(out, out_l_reduced, rtol=2e-2))\n+\n+    def check_post_processing_test(self, image_processor, images, model, pad_input=True, flip_aug=True):\n+        inputs = image_processor(images=images, return_tensors=\"pt\", do_pad=pad_input).to(torch_device)\n+\n+        with torch.no_grad():\n+            raw_outputs = model(**inputs)\n+            raw_outputs_flipped = None\n+            if flip_aug:\n+                raw_outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))\n+\n+        outputs = image_processor.post_process_depth_estimation(\n+            raw_outputs,\n+            [img.size[::-1] for img in images],\n+            outputs_flipped=raw_outputs_flipped,\n+            do_remove_padding=pad_input,\n+        )\n+\n+        expected_slices = torch.tensor(self.expected_slice_post_processing[pad_input, flip_aug]).to(torch_device)\n+        for img, out, expected_slice in zip(images, outputs, expected_slices):\n+            out = out[\"predicted_depth\"]\n+            self.assertTrue(img.size == out.shape[::-1])\n+            self.assertTrue(torch.allclose(expected_slice, out[:3, :3], rtol=1e-3))\n+\n+        self.check_target_size(image_processor, pad_input, images, outputs, raw_outputs, raw_outputs_flipped)\n+\n+    def test_post_processing_depth_estimation_post_processing_nopad_noflip(self):\n+        images = [prepare_img(), Image.open(\"./tests/fixtures/tests_samples/COCO/000000004016.png\")]\n+        image_processor = ZoeDepthImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\", keep_aspect_ratio=False)\n+        model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\").to(torch_device)\n+\n+        self.check_post_processing_test(image_processor, images, model, pad_input=False, flip_aug=False)\n+\n+    def test_inference_depth_estimation_post_processing_nopad_flip(self):\n+        images = [prepare_img(), Image.open(\"./tests/fixtures/tests_samples/COCO/000000004016.png\")]\n+        image_processor = ZoeDepthImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\", keep_aspect_ratio=False)\n+        model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\").to(torch_device)\n+\n+        self.check_post_processing_test(image_processor, images, model, pad_input=False, flip_aug=True)\n+\n+    def test_inference_depth_estimation_post_processing_pad_noflip(self):\n+        images = [prepare_img(), Image.open(\"./tests/fixtures/tests_samples/COCO/000000004016.png\")]\n+        image_processor = ZoeDepthImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\", keep_aspect_ratio=False)\n+        model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\").to(torch_device)\n+\n+        self.check_post_processing_test(image_processor, images, model, pad_input=True, flip_aug=False)\n+\n+    def test_inference_depth_estimation_post_processing_pad_flip(self):\n+        images = [prepare_img(), Image.open(\"./tests/fixtures/tests_samples/COCO/000000004016.png\")]\n+        image_processor = ZoeDepthImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\", keep_aspect_ratio=False)\n+        model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\").to(torch_device)\n+\n+        self.check_post_processing_test(image_processor, images, model, pad_input=True, flip_aug=True)"
        },
        {
            "sha": "a905aa8169ba2e5f06fc134f44f389f818de70d1",
            "filename": "tests/pipelines/test_pipelines_depth_estimation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c31a6ff474edfb59800024d9b54495f6e398c875/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c31a6ff474edfb59800024d9b54495f6e398c875/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py?ref=c31a6ff474edfb59800024d9b54495f6e398c875",
            "patch": "@@ -129,7 +129,7 @@ def test_large_model_pt(self):\n \n         # This seems flaky.\n         # self.assertEqual(outputs[\"depth\"], \"1a39394e282e9f3b0741a90b9f108977\")\n-        self.assertEqual(nested_simplify(outputs[\"predicted_depth\"].max().item()), 29.304)\n+        self.assertEqual(nested_simplify(outputs[\"predicted_depth\"].max().item()), 29.306)\n         self.assertEqual(nested_simplify(outputs[\"predicted_depth\"].min().item()), 2.662)\n \n     @require_torch"
        }
    ],
    "stats": {
        "total": 658,
        "additions": 446,
        "deletions": 212
    }
}