{
    "author": "Rocketknight1",
    "message": "Every model forward() should have **kwargs (#42603)\n\n* Add **kwargs into every Model.forward()\n\n* Add the test back in\n\n* And the others I missed\n\n* Fix udop test\n\n* Fix fast2speech2conformer test\n\n* make fixup",
    "sha": "9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
    "files": [
        {
            "sha": "ffd0663f031e9259467494c9e4f6f2ab3659d8c3",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1004,6 +1004,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndNoAttention]:\n         r\"\"\"\n         Examples:\n@@ -1169,6 +1170,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, AlignOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):"
        },
        {
            "sha": "f04c2d55ae690cc42e8441288e09711730ba09f1",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -891,6 +891,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -970,6 +971,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1061,6 +1063,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndProjection]:\n         r\"\"\"\n         Examples:\n@@ -1236,6 +1239,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, AltCLIPOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):"
        },
        {
            "sha": "91cdf260e71e7a22cf976a5a9a4e78daf75add0f",
            "filename": "src/transformers/models/audioflamingo3/modeling_audioflamingo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -323,6 +323,7 @@ def forward(\n         self,\n         input_features: torch.Tensor,\n         input_features_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:"
        },
        {
            "sha": "c2157fe9860640f275b15e589188ec6c841b44d4",
            "filename": "src/transformers/models/audioflamingo3/modular_audioflamingo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -60,6 +60,7 @@ def forward(\n         self,\n         input_features: torch.Tensor,\n         input_features_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:"
        },
        {
            "sha": "e1aa340b97431899e310b010403a2fa5dd6f059e",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -903,6 +903,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -1024,6 +1025,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, AutoFormerDecoderOutput]:\n         r\"\"\"\n         Args:\n@@ -1360,6 +1362,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[AutoformerModelOutput, tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1610,6 +1613,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqTSPredictionOutput, tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "f81a096c85de10f08449406837c466f9797a80e1",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -426,6 +426,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n         input_embeds (`torch.FloatTensor` of shape `(batch_size, input_sequence_length, hidden_size)`, *optional*):\n@@ -1028,6 +1029,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         codebook_idx (`int`):"
        },
        {
            "sha": "e07a2c967acfe17cd1260a6845d009f3b125569b",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -547,6 +547,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -694,6 +695,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -921,6 +923,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1067,6 +1070,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1228,6 +1232,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1360,6 +1365,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1505,6 +1511,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "4373d5365b35a7cedc5817dfb4e5ea8e8da36e28",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -726,6 +726,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BeitModelOutputWithPooling]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n@@ -818,6 +819,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`):\n@@ -911,6 +913,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1244,6 +1247,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SemanticSegmenterOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n@@ -1371,6 +1375,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "06015951cebc8fa5c7b9647098923a94719e451e",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1918,6 +1918,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[BigBirdForPreTrainingOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2028,6 +2029,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[MaskedLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2277,6 +2279,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[SequenceClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -2394,6 +2397,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[MultipleChoiceModelOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -2500,6 +2504,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[TokenClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2591,6 +2596,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[BigBirdForQuestionAnsweringModelOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         question_lengths (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*):"
        },
        {
            "sha": "f575cfa77a2539d1029ed66ff8bf1bc491fdf2cd",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1595,6 +1595,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1868,6 +1869,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -2097,6 +2099,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2235,6 +2238,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2369,6 +2373,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2490,6 +2495,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2616,6 +2622,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "29972c4dee4faf4c951785f61f524dc2527259a1",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -620,6 +620,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -711,6 +712,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "6fd30113d8e3b2795d3cbb7cf29782452ad84665",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -442,6 +442,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -533,6 +534,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "ca4b775b53a0ccaf01bc33136e19564c03bb70ad",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -666,7 +666,11 @@ def __init__(self, config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BaseModelOutputWithPoolingAndNoAttention:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -721,6 +725,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> ImageClassifierOutputWithNoAttention:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -767,7 +772,11 @@ def __init__(self, config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "71f0ec5383424ed2e9b741df7e06adc449104c08",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -493,6 +493,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -643,6 +644,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -885,6 +887,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1039,6 +1042,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1196,6 +1200,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "8f9511b0a325dfddf5ecd512187e3084b8c744a9",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -484,6 +484,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -630,6 +631,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -858,6 +860,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -999,6 +1002,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1156,6 +1160,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "bb079b7724d1c74ce6d21ef6853d2e8176524e2f",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -609,6 +609,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         is_decoder: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor`, *optional*):\n@@ -771,6 +772,7 @@ def forward(\n         reduction: Optional[str] = \"mean\",\n         cache_position: Optional[torch.Tensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states (`torch.FloatTensor`, *optional*): Sequence of"
        },
        {
            "sha": "c0ba30e38d24b65ace3c6741d76b43f55ffc5cfc",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1948,6 +1948,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Blip2ImageTextMatchingModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "ba0089ab3c63060bc5fa87f6577a515c175cf5c4",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -465,6 +465,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -883,6 +884,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -1006,6 +1008,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -1084,6 +1087,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):"
        },
        {
            "sha": "27dab651b8c5b3c36efffba0c6772424fefd5985",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -960,7 +960,7 @@ def __init__(self, config):\n     def dtype(self):\n         return self.visual.embeddings.patch_embedding.weight.dtype\n \n-    def forward(self, image, image_mask=None, interpolate_pos_encoding=False):\n+    def forward(self, image, image_mask=None, interpolate_pos_encoding=False, **kwargs):\n         return self.visual(image.type(self.dtype), image_mask, interpolate_pos_encoding)\n \n \n@@ -1223,6 +1223,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BridgeTowerModelOutput]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n@@ -1530,6 +1531,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[MaskedLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n@@ -1630,6 +1632,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[SequenceClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n@@ -1742,6 +1745,7 @@ def forward(\n         output_hidden_states: Optional[bool] = True,\n         return_dict: Optional[bool] = None,\n         return_loss: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[BridgeTowerContrastiveOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):"
        },
        {
            "sha": "de786db3d60c5f0a5f7f98cdae5cb9278a1b2a83",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -563,6 +563,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n@@ -701,6 +702,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n@@ -821,6 +823,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BrosSpadeOutput]:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n@@ -957,6 +960,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):"
        },
        {
            "sha": "ad221016295fa12951b14546ee819cf4b34fb9a9",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -836,6 +836,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, CanineModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1006,6 +1007,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1089,6 +1091,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1192,6 +1195,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1287,6 +1291,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "1de5f8f20000b4084d6d3d5bf2decacfb648b89c",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -839,6 +839,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -926,6 +927,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -1091,6 +1093,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ChineseCLIPOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):"
        },
        {
            "sha": "c5252dfc629a10682e76ff0b1e0a300326403bf3",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1356,6 +1356,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n@@ -1446,6 +1447,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1627,6 +1629,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ClapOutput]:\n         r\"\"\"\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n@@ -1740,6 +1743,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ClapTextModelOutput]:\n         r\"\"\"\n         Examples:\n@@ -1803,6 +1807,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ClapAudioModelOutput]:\n         r\"\"\"\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):"
        },
        {
            "sha": "da76e9812c958c01e15a91ee1bbb13bbf1d2f7b0",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -676,6 +676,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -776,6 +777,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = True,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -933,6 +935,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = True,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, CLIPSegOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n@@ -1125,6 +1128,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = True,\n+        **kwargs,\n     ):\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -1239,6 +1243,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = True,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, CLIPSegOutput]:\n         r\"\"\"\n         conditional_pixel_values (`torch.FloatTensor`, *optional*):"
        },
        {
            "sha": "6c4259404697e25c68234c21e9d1a586e8b1f20c",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -861,6 +861,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -1020,6 +1021,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1170,6 +1172,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1339,6 +1342,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1635,6 +1639,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, ClvpOutput]:\n         r\"\"\"\n         conditioning_encoder_inputs_embeds (`torch.FloatTensor`, *optional*):"
        },
        {
            "sha": "b611797a9346f39cb5366e1da6dca0b52357840d",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -141,6 +141,7 @@ def forward(\n         pixel_values: Optional[torch.Tensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> ColQwen2ForRetrievalOutput:\n         r\"\"\"\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):"
        },
        {
            "sha": "6f98205bf376174d7063add9e72c643fb8b3db61",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -322,6 +322,7 @@ def forward(\n         pixel_values: Optional[torch.Tensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> ColQwen2ForRetrievalOutput:\n         r\"\"\"\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):"
        },
        {
            "sha": "ceaa000b4d893eb44ad36779c6c0945259a32d3d",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1032,6 +1032,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1156,6 +1157,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1344,6 +1346,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], ConditionalDetrModelOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n@@ -1529,6 +1532,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], ConditionalDetrObjectDetectionOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n@@ -1693,6 +1697,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], ConditionalDetrSegmentationOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):"
        },
        {
            "sha": "b6864bb740785016ea26363047ed01e1c0930f7d",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -629,6 +629,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -729,6 +730,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -824,6 +826,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -906,6 +909,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1013,6 +1017,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1078,6 +1083,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "c16face60634ec29c58f6721f46fb2953bac9eff",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -268,7 +268,7 @@ def __init__(self, config):\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n-        self, pixel_values: Optional[torch.FloatTensor] = None, output_hidden_states: Optional[bool] = None\n+        self, pixel_values: Optional[torch.FloatTensor] = None, output_hidden_states: Optional[bool] = None, **kwargs\n     ) -> BaseModelOutputWithPoolingAndNoAttention:\n         if output_hidden_states is None:\n             output_hidden_states = self.config.output_hidden_states\n@@ -370,9 +370,7 @@ def __init__(self, config):\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n-        self,\n-        pixel_values: torch.Tensor,\n-        output_hidden_states: Optional[bool] = None,\n+        self, pixel_values: torch.Tensor, output_hidden_states: Optional[bool] = None, **kwargs\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "7c62c3230cbaf7f2f9353459149e8c3782e0cc7c",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -289,7 +289,7 @@ def __init__(self, config):\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n-        self, pixel_values: Optional[torch.FloatTensor] = None, output_hidden_states: Optional[bool] = None\n+        self, pixel_values: Optional[torch.FloatTensor] = None, output_hidden_states: Optional[bool] = None, **kwargs\n     ) -> BaseModelOutputWithPoolingAndNoAttention:\n         if output_hidden_states is None:\n             output_hidden_states = self.config.output_hidden_states\n@@ -393,9 +393,7 @@ def __init__(self, config):\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n-        self,\n-        pixel_values: torch.Tensor,\n-        output_hidden_states: Optional[bool] = None,\n+        self, pixel_values: torch.Tensor, output_hidden_states: Optional[bool] = None, **kwargs\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "057928228bcf88bb52aae246f9fb12b251c07933",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -534,6 +534,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "d9cc28ea467c448577e4b72bbb1b65ada16fbd53",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -523,6 +523,7 @@ def forward(\n         pixel_values: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithCLSToken]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -577,6 +578,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutputWithNoAttention]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "ab4765211c0c611bb36064d91bb976e110560cf5",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -681,6 +681,7 @@ def forward(\n         memory_mask=None,\n         output_attentions=None,\n         return_dict=None,\n+        **kwargs,\n     ) -> DFineDecoderOutput:\n         r\"\"\"\n         Args:\n@@ -1247,6 +1248,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DFineModelOutput]:\n         r\"\"\"\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):"
        },
        {
            "sha": "88df00124822be66f363be1c1573b05d6883edfa",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -726,6 +726,7 @@ def forward(\n         memory_mask=None,\n         output_attentions=None,\n         return_dict=None,\n+        **kwargs,\n     ) -> DFineDecoderOutput:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "e55a7cdf63cf607e2bef37742fcf9e51d49b4dc2",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -886,6 +886,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1016,6 +1017,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1222,6 +1224,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DabDetrModelOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n@@ -1469,6 +1472,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DabDetrObjectDetectionOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):"
        },
        {
            "sha": "c7baf14969f6781709808231fab4a97ad446f765",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -754,6 +754,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Data2VecAudioBaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -856,6 +857,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -967,6 +969,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1070,6 +1073,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1241,6 +1245,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, XVectorOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "14cd8b089ab65209bb8e7f9c7b28df669aac1163",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -741,6 +741,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Data2VecVisionModelOutputWithPooling]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n@@ -828,6 +829,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1173,6 +1175,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SemanticSegmenterOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "cfd90efc1d6fcf6d259820ad158696e9c43d4ad6",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -655,6 +655,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -860,6 +861,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -969,6 +971,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1063,6 +1066,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1125,6 +1129,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "a93f652c645b3c94d0b5ff6b1f8215d7e399e1db",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -732,6 +732,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -936,6 +937,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1047,6 +1049,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1142,6 +1145,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1205,6 +1209,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1293,6 +1298,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "11fa8a88deb30eeb0b3f0a8f473b72f4dc5df243",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -431,6 +431,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -656,6 +657,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DecisionTransformerOutput]:\n         r\"\"\"\n         states (`torch.FloatTensor` of shape `(batch_size, episode_length, state_dim)`):"
        },
        {
            "sha": "8a77709b5920ccea021bb04f66da1fbd6c18f470",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1036,6 +1036,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1151,6 +1152,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1468,6 +1470,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DeformableDetrModelOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n@@ -1745,6 +1748,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DeformableDetrObjectDetectionOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):"
        },
        {
            "sha": "1055074538f2a1eec953ca72b39646c40620269a",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -337,6 +337,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], DepthEstimatorOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "47dec136bd35c87bafb30a1acf226112294028e1",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -645,6 +645,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, DepthProOutput]:\n         r\"\"\"\n         Examples:\n@@ -1027,6 +1028,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], DepthProDepthEstimatorOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "f87318f244ee58cb6be222fc9b970a78b9d87619",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -788,6 +788,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -905,6 +906,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1078,6 +1080,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DetrModelOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n@@ -1258,6 +1261,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DetrObjectDetectionOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n@@ -1404,6 +1408,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], DetrSegmentationOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):"
        },
        {
            "sha": "c5d7e7ad7cf8c31d6a27efc6accddfd5735f1a11",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -596,6 +596,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, DinatModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -668,6 +669,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, DinatImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -740,6 +742,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "75c75fc23ede556a802015b7435159a6725b9f84",
            "filename": "src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -214,7 +214,7 @@ def __init__(self, config: DINOv3ConvNextConfig):\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n-        self, pixel_values: torch.FloatTensor, output_hidden_states: Optional[bool] = None\n+        self, pixel_values: torch.FloatTensor, output_hidden_states: Optional[bool] = None, **kwargs\n     ) -> BaseModelOutputWithPoolingAndNoAttention:\n         hidden_states = pixel_values\n "
        },
        {
            "sha": "3d93423c2311a26b8af441c12312f5f2e15be9ee",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -837,6 +837,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, DonutSwinModelOutput]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`):\n@@ -923,6 +924,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, DonutSwinImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "4fbb3b10869866bc914ed4d522189256419c6bf5",
            "filename": "src/transformers/models/dpr/modeling_dpr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -129,6 +129,7 @@ def forward(\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = False,\n+        **kwargs,\n     ) -> Union[BaseModelOutputWithPooling, tuple[Tensor, ...]]:\n         outputs = self.bert_model(\n             input_ids=input_ids,\n@@ -181,6 +182,7 @@ def forward(\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = False,\n+        **kwargs,\n     ) -> Union[DPRReaderOutput, tuple[Tensor, ...]]:\n         # notations: N - number of questions in a batch, M - number of passages per questions, L - sequence length\n         n_passages, sequence_length = input_ids.size() if input_ids is not None else inputs_embeds.size()[:2]\n@@ -282,6 +284,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[DPRContextEncoderOutput, tuple[Tensor, ...]]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -387,6 +390,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[DPRQuestionEncoderOutput, tuple[Tensor, ...]]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -492,6 +496,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[DPRReaderOutput, tuple[Tensor, ...]]:\n         r\"\"\"\n         input_ids (`tuple[torch.LongTensor]` of shapes `(n_passages, sequence_length)`):"
        },
        {
            "sha": "af9169cdc5917b4771e91347ef19e9ad34fcbe38",
            "filename": "src/transformers/models/edgetam_video/modeling_edgetam_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -2117,6 +2117,7 @@ def forward(\n         frame_idx: Optional[int] = None,\n         frame: Optional[torch.Tensor] = None,\n         reverse: bool = False,\n+        **kwargs,\n     ) -> EdgeTamVideoSegmentationOutput:\n         r\"\"\"\n         inference_session (`EdgeTamVideoInferenceSession`):"
        },
        {
            "sha": "1e9e0faf897999775cbfbeed6c4ff73482a1b8d3",
            "filename": "src/transformers/models/edgetam_video/modular_edgetam_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodular_edgetam_video.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1256,6 +1256,7 @@ def forward(\n         frame_idx: Optional[int] = None,\n         frame: Optional[torch.Tensor] = None,\n         reverse: bool = False,\n+        **kwargs,\n     ) -> EdgeTamVideoSegmentationOutput:\n         r\"\"\"\n         inference_session (`EdgeTamVideoInferenceSession`):"
        },
        {
            "sha": "167e5253c9f47ee8e91f95f029ecd433fe61c06d",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -471,6 +471,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndNoAttention]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -529,6 +530,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutputWithNoAttention]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "3d14f1d40df749d4c332d34cffc132a5e5b17fa3",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -739,6 +739,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -1119,6 +1120,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -1243,6 +1245,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -1320,6 +1323,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):"
        },
        {
            "sha": "f549b198a00c0c57ba466434d07a80ba21441278",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -703,6 +703,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, FalconMambaOutput]:\n         r\"\"\"\n         cache_params (`FalconMambaCache`, *optional*):"
        },
        {
            "sha": "08b2f0265e7988229e401e684e5780b1c3abcfa5",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1118,6 +1118,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, FastSpeech2ConformerModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1433,7 +1434,7 @@ def remove_weight_norm(self):\n         waveform.\n         \"\"\"\n     )\n-    def forward(self, spectrogram: torch.FloatTensor) -> torch.FloatTensor:\n+    def forward(self, spectrogram: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n         r\"\"\"\n         spectrogram (`torch.FloatTensor`):\n             Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,\n@@ -1509,6 +1510,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, FastSpeech2ConformerModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "e3b190533d94922450105003ef2985d711e94a76",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -792,6 +792,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1002,6 +1003,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1090,6 +1092,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1195,6 +1198,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1286,6 +1290,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1423,6 +1428,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, FlaubertForQuestionAnsweringOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1538,6 +1544,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):"
        },
        {
            "sha": "9521329461edb4fbc76db1ecf9288b0d6e12fa9a",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -725,6 +725,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, image_num_patches)`):\n@@ -804,6 +805,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, text_seq_length)`):\n@@ -896,6 +898,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         hidden_states (`torch.FloatTensor` of shape `(batch_size, image_num_patches + text_seq_len, hidden_size)`):\n@@ -1103,6 +1106,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: bool = True,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, FlavaOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, image_num_patches + text_seq_len)`):\n@@ -1380,7 +1384,7 @@ def get_codebook_probs(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         z_logits = self.blocks(pixel_values)\n         return nn.Softmax(dim=1)(z_logits)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n+    def forward(self, pixel_values: torch.FloatTensor, **kwargs) -> torch.Tensor:\n         f\"\"\"\n         Args:\n             pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n@@ -1575,6 +1579,7 @@ def forward(\n         output_hidden_states: bool = True,\n         return_dict: Optional[bool] = None,\n         return_loss: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], FlavaForPreTrainingOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, text_seq_len)`):"
        },
        {
            "sha": "afa01e8255d1d0040c640f296b42faaf9f6fff0c",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -541,7 +541,7 @@ def __init__(self, config: Florence2VisionConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def forward(self, hidden_states: torch.Tensor):\n+    def forward(self, hidden_states: torch.Tensor, **kwargs):\n         for conv, block in zip(self.convs, self.blocks):\n             hidden_states = conv(hidden_states)\n             for layer in block:\n@@ -708,6 +708,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Florence2Seq2SeqModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "ac809ac0a6b38ecd699def01c48e5193a1bedd7a",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1422,7 +1422,7 @@ def __init__(self, config: Florence2VisionConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def forward(self, hidden_states: torch.Tensor):\n+    def forward(self, hidden_states: torch.Tensor, **kwargs):\n         for conv, block in zip(self.convs, self.blocks):\n             hidden_states = conv(hidden_states)\n             for layer in block:\n@@ -1551,6 +1551,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Florence2Seq2SeqModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "0550423851b34b8acaec9c8e6486c2438e8d2c15",
            "filename": "src/transformers/models/fnet/modeling_fnet.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -439,6 +439,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -540,6 +541,7 @@ def forward(\n         next_sentence_label: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, FNetForPreTrainingOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -632,6 +634,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -783,6 +786,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -856,6 +860,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -950,6 +955,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1008,6 +1014,7 @@ def forward(\n         end_positions: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "7112cbff02a79fb68ad35c1325aabcd49a34c6f8",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -628,6 +628,7 @@ def forward(\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, FocalNetModelOutput]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`):\n@@ -710,6 +711,7 @@ def forward(\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, FocalNetMaskedImageModelingOutput]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`):\n@@ -812,6 +814,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, FocalNetImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -871,6 +874,7 @@ def forward(\n         pixel_values: torch.Tensor,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "0b51cf3014248dd7f17586065eb6276ea227587f",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -843,6 +843,7 @@ def forward(\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -980,6 +981,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):"
        },
        {
            "sha": "918e117090ffc389e1b25c90e791e0ec9bebd18b",
            "filename": "src/transformers/models/funnel/modeling_funnel.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -766,6 +766,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -832,6 +833,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -923,6 +925,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, FunnelForPreTrainingOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1012,6 +1015,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1079,6 +1083,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1158,6 +1163,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1233,6 +1239,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1295,6 +1302,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "3c791a38868923b3674acf377c90cfbdb96e2734",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -920,7 +920,7 @@ def __init__(self, config: Gemma3nAudioConfig):\n         )\n \n     def forward(\n-        self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor\n+        self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor, **kwargs\n     ) -> tuple[torch.Tensor, torch.BoolTensor]:\n         \"\"\"Encodes a batch of MELs.\n "
        },
        {
            "sha": "f43881344ef7e2d918f696ae9b84a700eb56d380",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1474,7 +1474,7 @@ def __init__(self, config: Gemma3nAudioConfig):\n         )\n \n     def forward(\n-        self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor\n+        self, audio_mel: torch.Tensor, audio_mel_mask: torch.BoolTensor, **kwargs\n     ) -> tuple[torch.Tensor, torch.BoolTensor]:\n         \"\"\"Encodes a batch of MELs.\n "
        },
        {
            "sha": "2583e92409e9b8559a1ed8f5caa31ec5956c30a6",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -827,6 +827,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Examples:\n@@ -972,6 +973,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "58c0ca50d17e24731436cb3b8d2c25d95d4ceb75",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -768,7 +768,7 @@ def rot_pos_emb(self, grid_thw):\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb, pos_ids\n \n-    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):"
        },
        {
            "sha": "2cd6c5d0fd062c34a3b5a0f772f4c03a55b8b6ef",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -786,7 +786,7 @@ def rot_pos_emb(self, grid_thw):\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb, pos_ids\n \n-    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):"
        },
        {
            "sha": "46c7bd2edd47dba9b50355497aadb0d889f7b7bc",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -975,7 +975,7 @@ def rot_pos_emb(self, grid_thw):\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb, pos_ids\n \n-    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs) -> torch.Tensor:\n         \"\"\"\n         Args:\n             hidden_states (`torch.Tensor` of shape `(seq_len, hidden_size)`):"
        },
        {
            "sha": "05c218e5c3aee141c3cade981d51008fccdd0ad0",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -411,6 +411,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -597,6 +598,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], DepthEstimatorOutput]:\n         r\"\"\"\n         labels (`torch.FloatTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "16a0d987c2f93ea73265a3bd56fd36357bd57f25",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1021,6 +1021,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -1148,6 +1149,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -1228,6 +1230,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):"
        },
        {
            "sha": "da73f2e6beffddfb3a1c92879e2d781d5cdf4247",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -826,6 +826,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.Tensor` of shape `(batch_size, input_ids_length)`):"
        },
        {
            "sha": "841ea59d8687212aefe8b5ac7fbc1eab08d805d4",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -419,6 +419,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -773,6 +774,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -894,6 +896,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -974,6 +977,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):"
        },
        {
            "sha": "7490e019e18d3e77440d5d11043e10e04207bcc8",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -645,6 +645,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -724,6 +725,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -783,6 +785,7 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> QuestionAnsweringModelOutput:\n         outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,"
        },
        {
            "sha": "4b2bbb1c7946281a715483c0b8a6b1b39422ccb2",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -518,6 +518,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> SequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -597,6 +598,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> TokenClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -656,6 +658,7 @@ def forward(\n         end_positions: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> QuestionAnsweringModelOutput:\n         outputs: BaseModelOutputWithPast = self.gpt_neox(\n             input_ids,"
        },
        {
            "sha": "811e628cc3a04c9057e97a8753ce81a18bf5e3ae",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -431,6 +431,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         Example:"
        },
        {
            "sha": "e29c7f986e706fdda26cef413b74550646cee2a1",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -482,6 +482,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`, *optional*):\n@@ -819,6 +820,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`, *optional*):\n@@ -930,6 +932,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`, *optional*):"
        },
        {
            "sha": "0d1619e7d5ccb5848da6398e30b395bf43d46cb6",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1510,6 +1510,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1664,6 +1665,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -2056,6 +2058,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, text_sequence_length)`):\n@@ -2460,6 +2463,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[list[dict[str, Union[torch.LongTensor, torch.FloatTensor]]]] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, text_sequence_length)`):"
        },
        {
            "sha": "7e81f28091afdfd65bb35d397bc36768dc062be1",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1045,6 +1045,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -1145,6 +1146,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -1297,6 +1299,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_segmentation: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, GroupViTModelOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):"
        },
        {
            "sha": "a6aafffcf2cca55094ffb36de15ad407bda7c608",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -347,7 +347,11 @@ def __init__(self, config: HGNetV2Config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:\n@@ -426,6 +430,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> ImageClassifierOutputWithNoAttention:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "cf5b0482fc2212e293e2685848f3285094884763",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -470,7 +470,11 @@ def __init__(self, config: HGNetV2Config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:\n@@ -549,6 +553,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> ImageClassifierOutputWithNoAttention:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "b7c88713cf3cc73964042b664daefdec2d6360b7",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -848,6 +848,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         noise (`torch.FloatTensor` of shape `(batch_size, num_mask_units)`, *optional*):\n@@ -1132,6 +1133,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, HieraForPreTrainingOutput]:\n         r\"\"\"\n         noise (`torch.FloatTensor` of shape `(batch_size, num_mask_units)`, *optional*):\n@@ -1249,6 +1251,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, HieraForImageClassificationOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1325,6 +1328,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         \"\"\"\n         Returns:"
        },
        {
            "sha": "1db4112acdb84a8dbcc50c5bc5e8f855efefd158",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -892,6 +892,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1038,6 +1039,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -1149,6 +1151,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "239385e729110a6995c4f9d0a9acff2a8cf2e699",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -226,6 +226,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "cb9679940723c1972283265f35b1b5a90025c58c",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -653,6 +653,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, tuple[torch.FloatTensor]]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -746,6 +747,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[MaskedLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -836,6 +838,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[SequenceClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -916,6 +919,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[MultipleChoiceModelOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1018,6 +1022,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[TokenClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1102,6 +1107,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[QuestionAnsweringModelOutput, tuple[torch.FloatTensor]]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "cfef2ca2ee833502b092d74b225f949f178f07c8",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -879,6 +879,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -998,6 +999,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -1296,6 +1298,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqTSModelOutput, tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n@@ -1573,6 +1576,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqTSModelOutput, tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):"
        },
        {
            "sha": "ae5d7ff6ed1b18f11b6b3b74b99fe4328557d2fa",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -415,6 +415,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:"
        },
        {
            "sha": "0d28f1fdac2ef7beb9e7d5e7e9525cda2e82488b",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -449,9 +449,7 @@ def get_input_embeddings(self):\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n-        self,\n-        pixel_values: torch.Tensor,\n-        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+        self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.BoolTensor] = None, **kwargs\n     ) -> Union[tuple, InternVLVisionModelOutputWithPooling]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):"
        },
        {
            "sha": "8fecfb89cd3189f52946a96185c2725f65b9ac26",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -406,9 +406,7 @@ def get_input_embeddings(self):\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n-        self,\n-        pixel_values: torch.Tensor,\n-        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+        self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.BoolTensor] = None, **kwargs\n     ) -> Union[tuple, InternVLVisionModelOutputWithPooling]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):"
        },
        {
            "sha": "26a9031054af16c923f17ec2f3feaac309a6c1b4",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1007,6 +1007,7 @@ def decode(self, image_tokens: torch.LongTensor) -> torch.FloatTensor:\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n+        **kwargs,\n     ) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n         batch_size = pixel_values.shape[0]\n         quant, embedding_loss, indices = self.encode(pixel_values)"
        },
        {
            "sha": "4368c3ee08346a08fc9d8d5e8af746e195cb385d",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -823,6 +823,7 @@ def decode(self, image_tokens: torch.LongTensor) -> torch.FloatTensor:\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n+        **kwargs,\n     ) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n         batch_size = pixel_values.shape[0]\n         quant, embedding_loss, indices = self.encode(pixel_values)"
        },
        {
            "sha": "1f966d83aa6b81d27ff56e1b47cd358f9ee1b942",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1199,6 +1199,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         return self.model(\n             pixel_values=pixel_values,"
        },
        {
            "sha": "e43c548d57b9afe5a03f37282658afee9cdcbcf5",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -837,6 +837,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "9185ebcda3debbd54ba76990588c2d77b6b2bd57",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -465,6 +465,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n@@ -600,6 +601,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n@@ -716,6 +718,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n@@ -850,6 +853,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n@@ -963,6 +967,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):"
        },
        {
            "sha": "669571bb9f36da59a7a30c446deeec6525615299",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -701,6 +701,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `((batch_size, sequence_length), 4)`, *optional*):\n@@ -858,6 +859,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `batch_size, sequence_length`):\n@@ -1061,6 +1063,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `batch_size, sequence_length`):\n@@ -1212,6 +1215,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `batch_size, sequence_length`):"
        },
        {
            "sha": "32d3e71c1c1a7deed228f7a0b44e9b97c4713caf",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -657,6 +657,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, token_sequence_length)`):\n@@ -897,6 +898,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         pixel_values: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n@@ -997,6 +999,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         bbox: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n@@ -1115,6 +1118,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         bbox: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):"
        },
        {
            "sha": "bde43338bba8ec1f4d7bc26f0a8cc2fb69f815ee",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1379,6 +1379,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1573,6 +1574,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1788,6 +1790,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], LEDSeq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1938,6 +1941,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], LEDSeq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2120,6 +2124,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], LEDSeq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -2258,6 +2263,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], LEDSeq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):"
        },
        {
            "sha": "3bc98d4eb8a61e9f6277ddc835e1cd49d45e12d1",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -489,6 +489,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndNoAttention]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -550,6 +551,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutputWithNoAttention]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -616,6 +618,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LevitForImageClassificationWithTeacherOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "252a81e9aa0efefe7cc22e8b0ed84eee3dc1e1cf",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -870,6 +870,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, \"LightGlueKeypointMatchingOutput\"]:\n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "d461a5ec1627306c6e37554ac4b230e43176d64a",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -927,6 +927,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, \"LightGlueKeypointMatchingOutput\"]:\n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "efb2c520a8af24d956d0bbd0ec1b5474cfbbe090",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -538,6 +538,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPooling]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n@@ -665,6 +666,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n@@ -780,6 +782,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):\n@@ -897,6 +900,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `(batch_size, sequence_length, 4)`, *optional*):"
        },
        {
            "sha": "657b8e003dc28798b0e26b411266effa1c1b16a3",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1072,6 +1072,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[BaseModelOutput, tuple[torch.Tensor, ...]]:\n         r\"\"\"\n "
        },
        {
            "sha": "1e48f90db17e5e2d9e61ec6ab1ef5a3e34d822fb",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1414,6 +1414,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LongformerBaseModelOutputWithPooling]:\n         r\"\"\"\n         global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1567,6 +1568,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LongformerMaskedLMOutput]:\n         r\"\"\"\n         global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1678,6 +1680,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LongformerSequenceClassifierOutput]:\n         r\"\"\"\n         global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1800,6 +1803,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LongformerQuestionAnsweringModelOutput]:\n         r\"\"\"\n         global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1928,6 +1932,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LongformerTokenClassifierOutput]:\n         r\"\"\"\n         global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2007,6 +2012,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LongformerMultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):"
        },
        {
            "sha": "39c5078220eae398492d1ac4a03527f94359ed8a",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1283,6 +1283,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1618,6 +1619,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1783,6 +1785,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1946,6 +1949,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], BaseModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "2f634d95407c9449ddd9c60f52803d65e3fed5a0",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -837,6 +837,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseLukeModelOutputWithPooling]:\n         r\"\"\"\n         entity_ids (`torch.LongTensor` of shape `(batch_size, entity_length)`):\n@@ -1087,6 +1088,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LukeMaskedLMOutput]:\n         r\"\"\"\n         entity_ids (`torch.LongTensor` of shape `(batch_size, entity_length)`):\n@@ -1220,6 +1222,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, EntityClassificationOutput]:\n         r\"\"\"\n         entity_ids (`torch.LongTensor` of shape `(batch_size, entity_length)`):\n@@ -1348,6 +1351,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, EntityPairClassificationOutput]:\n         r\"\"\"\n         entity_ids (`torch.LongTensor` of shape `(batch_size, entity_length)`):\n@@ -1483,6 +1487,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, EntitySpanClassificationOutput]:\n         r\"\"\"\n         entity_ids (`torch.LongTensor` of shape `(batch_size, entity_length)`):\n@@ -1638,6 +1643,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LukeSequenceClassifierOutput]:\n         r\"\"\"\n         entity_ids (`torch.LongTensor` of shape `(batch_size, entity_length)`):\n@@ -1764,6 +1770,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LukeTokenClassifierOutput]:\n         r\"\"\"\n         entity_ids (`torch.LongTensor` of shape `(batch_size, entity_length)`):\n@@ -1865,6 +1872,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LukeQuestionAnsweringModelOutput]:\n         r\"\"\"\n         entity_ids (`torch.LongTensor` of shape `(batch_size, entity_length)`):\n@@ -1982,6 +1990,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, LukeMultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):"
        },
        {
            "sha": "c89033b4fcc2ad7cc1498d38e73529243a6f1f24",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -711,6 +711,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[LxmertModelOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         visual_feats (`torch.FloatTensor` of shape `(batch_size, num_visual_features, visual_feat_dim)`):\n@@ -1244,6 +1245,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[LxmertForQuestionAnsweringOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         visual_feats (`torch.FloatTensor` of shape `(batch_size, num_visual_features, visual_feat_dim)`):"
        },
        {
            "sha": "011d5180cc9c708a5ea3da20ca0f4ef7fd250ccb",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -561,6 +561,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -713,6 +714,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -941,6 +943,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1046,6 +1049,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):"
        },
        {
            "sha": "c967caf245d74e9e0264d37d9f9eede9e2cca312",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -640,6 +640,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, MambaOutput]:\n         r\"\"\"\n         cache_params (`MambaCache`, *optional*):"
        },
        {
            "sha": "5ba1ebd860a1e778288d22dd3568d5b8b67ba776",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -504,6 +504,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -645,6 +646,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -925,6 +927,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Seq2SeqModelOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1140,6 +1143,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Seq2SeqLMOutput:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1288,6 +1292,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "3086829894d61a45b939c51151589e09f57b0878",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -562,6 +562,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         xpath_tags_seq (`torch.LongTensor` of shape `(batch_size, sequence_length, config.max_depth)`, *optional*):\n@@ -669,6 +670,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         r\"\"\"\n         xpath_tags_seq (`torch.LongTensor` of shape `(batch_size, sequence_length, config.max_depth)`, *optional*):\n@@ -784,6 +786,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         xpath_tags_seq (`torch.LongTensor` of shape `(batch_size, sequence_length, config.max_depth)`, *optional*):\n@@ -886,6 +889,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         xpath_tags_seq (`torch.LongTensor` of shape `(batch_size, sequence_length, config.max_depth)`, *optional*):"
        },
        {
            "sha": "5f7cbc8710b40f317477e3b6d88f18ed4166facb",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -2184,6 +2184,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Mask2FormerModelOutput:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -2305,6 +2306,7 @@ def forward(\n         output_auxiliary_logits: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Mask2FormerForUniversalSegmentationOutput:\n         r\"\"\"\n         mask_labels (`list[torch.Tensor]`, *optional*):"
        },
        {
            "sha": "b16a480dd7c75f61bf194072ff4b1a4332e97552",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1496,6 +1496,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> MaskFormerModelOutput:\n         r\"\"\"\n         Examples:\n@@ -1667,6 +1668,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> MaskFormerForInstanceSegmentationOutput:\n         r\"\"\"\n         mask_labels (`list[torch.Tensor]`, *optional*):"
        },
        {
            "sha": "a9ab377a00d8e0c494ed1f718532dd9d74213bd4",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -738,6 +738,7 @@ def forward(\n         output_hidden_states=None,\n         interpolate_pos_encoding=False,\n         return_dict=None,\n+        **kwargs,\n     ):\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -815,6 +816,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         output_hidden_states = ("
        },
        {
            "sha": "c01c8d7c8337bb6e0be9fc304b779f2b9fa56201",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -540,6 +540,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -691,6 +692,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -919,6 +921,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqModelOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1052,6 +1055,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1205,6 +1209,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1338,6 +1343,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1480,6 +1486,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "b70646d70ab110bfd55fb95e640cfa1cfccf462a",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -608,6 +608,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -735,6 +736,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MegatronBertForPreTrainingOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -955,6 +957,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1140,6 +1143,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1223,6 +1227,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1326,6 +1331,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1391,6 +1397,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "7a67c2c251546fc1c18784378e2e8d3f40cf76f0",
            "filename": "src/transformers/models/mgp_str/modeling_mgp_str.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -322,6 +322,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -385,6 +386,7 @@ def forward(\n         output_a3_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], MgpstrModelOutput]:\n         r\"\"\"\n         output_a3_attentions (`bool`, *optional*):"
        },
        {
            "sha": "2f7626f73bb5d8febc80cd312bf7db1453e6ea2d",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1685,6 +1685,7 @@ def forward(\n         encoder_past_key_values: Optional[Cache] = None,\n         decoder_past_key_values: Optional[Cache] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor, torch.Tensor], MimiOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`, *optional*):"
        },
        {
            "sha": "91f3b63949dcba56e15619c111446c274e71acaf",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1180,6 +1180,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1476,6 +1477,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1951,6 +1953,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, text_sequence_length)`):\n@@ -2431,6 +2434,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[list[dict[str, Union[torch.LongTensor, torch.FloatTensor]]]] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, text_sequence_length)`):"
        },
        {
            "sha": "f545bdb3b032923fa98c11609f95108d6012218f",
            "filename": "src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -195,6 +195,7 @@ def forward(\n         pixel_values: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndNoAttention]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -260,6 +261,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutputWithNoAttention]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "8397979d4efa73878c5428d2e8b99fd247fa1766",
            "filename": "src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -331,6 +331,7 @@ def forward(\n         pixel_values: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndNoAttention]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -396,6 +397,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutputWithNoAttention]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -524,6 +526,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SemanticSegmenterOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "dcacef0bda40b42f4451622a4fbe73747effac49",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -659,6 +659,7 @@ def forward(\n         pixel_values: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndNoAttention]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -725,6 +726,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutputWithNoAttention]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -889,6 +891,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SemanticSegmenterOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "35b30bd26a741bf4276960161b20d63ed46fd054",
            "filename": "src/transformers/models/mobilevitv2/modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -623,6 +623,7 @@ def forward(\n         pixel_values: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndNoAttention]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -691,6 +692,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutputWithNoAttention]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -858,6 +860,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SemanticSegmenterOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "8740d3ed4b5ee55d6043e5a41310fd54bc4ad9cb",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -852,6 +852,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutput]:\n         r\"\"\"\n         sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1345,6 +1346,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "b1efa80d89d90ae9f56f55953c2d4b043f304135",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -975,6 +975,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor, ...], BaseModelOutput]:\n         r\"\"\"\n         sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1468,6 +1469,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "771d3f34132777f31ddb08a3c4a5a597f32a89ec",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -882,6 +882,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         \"\"\"\n         Args:\n@@ -1228,6 +1229,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "1c284110e99ccdac42d88cea2f3347c4b42e5f07",
            "filename": "src/transformers/models/mpnet/modeling_mpnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -488,6 +488,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -577,6 +578,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -656,6 +658,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -748,6 +751,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -831,6 +835,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "e1b20084475f5698871c4064b75b2e1a85d397d8",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -498,6 +498,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n@@ -700,6 +701,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):"
        },
        {
            "sha": "f01d61a83a6511b945dba7c61703cc38aa775d9a",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -826,6 +826,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithCrossAttentions]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -919,6 +920,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1007,6 +1009,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1086,6 +1089,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1189,6 +1193,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1263,6 +1268,7 @@ def forward(\n         end_positions: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "e936b18ebed80481b42662e8e740bb15cb759821",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -671,6 +671,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -898,6 +899,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1081,6 +1083,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1268,6 +1271,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], BaseModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1340,6 +1344,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1480,6 +1485,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1587,6 +1593,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "1c6214addd41c52ecfb6f59442044083bcc621c8",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -482,6 +482,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):\n@@ -716,6 +717,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):"
        },
        {
            "sha": "3b5afbf962ea3ffef7e638959087e760a7ad2b50",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -455,6 +455,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):\n@@ -670,6 +671,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):\n@@ -785,6 +787,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, MusicgenMelodyOutputWithPast]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, sequence_length)`):"
        },
        {
            "sha": "387be9b920b5184850fdbe435efdad7e7f53d852",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -534,6 +534,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -698,6 +699,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -917,6 +919,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1065,6 +1068,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1213,6 +1217,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1372,6 +1377,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1548,6 +1554,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "7b4e94599d9674da746f807301755d269a6f735e",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -657,6 +657,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "ffd7e8076b84bcff07fec639dbfdcce7719a457d",
            "filename": "src/transformers/models/nystromformer/modeling_nystromformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -443,6 +443,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -539,6 +540,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -628,6 +630,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -709,6 +712,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -814,6 +818,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -881,6 +886,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "766e7d517d05f2aadf7462ec2b622c62d18a731c",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1316,6 +1316,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         \"\"\"\n         Args:\n@@ -1505,6 +1506,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], OmDetTurboObjectDetectionOutput]:\n         r\"\"\"\n         classes_input_ids (`torch.LongTensor` of shape `(total_classes (>= batch_size), sequence_length)`):"
        },
        {
            "sha": "2d16b470838a9f6ededddf4004a264acd574e0a8",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -2872,6 +2872,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> OneFormerModelOutput:\n         r\"\"\"\n         task_inputs (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -3058,6 +3059,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> OneFormerForUniversalSegmentationOutput:\n         r\"\"\"\n         task_inputs (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "d65f038e08e723a16f00e1bb5551488b8b9004f7",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -317,6 +317,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -514,6 +515,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], OpenAIGPTDoubleHeadsModelOutput]:\n         r\"\"\"\n         mc_token_ids (`torch.LongTensor` of shape `(batch_size, num_choices)`, *optional*, default to index of the last token of the input):\n@@ -624,6 +626,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "eb19970bd41ab3c94d98f4d43323290c9c384a9c",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -836,6 +836,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -947,6 +948,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         Example:"
        },
        {
            "sha": "3ad43aafb67a919efe7cca2be69014487d664731",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -793,6 +793,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`):\n@@ -903,6 +904,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -1052,6 +1054,7 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n         return_base_image_embeds: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Owlv2Output]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n@@ -1602,6 +1605,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Owlv2ObjectDetectionOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`, *optional*):"
        },
        {
            "sha": "8aa171da21aa6384110ce69303876ab469af57e4",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -777,6 +777,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`):\n@@ -885,6 +886,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -1033,6 +1035,7 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n         return_base_image_embeds: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, OwlViTOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n@@ -1543,6 +1546,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> OwlViTObjectDetectionOutput:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size * num_max_text_queries, sequence_length)`, *optional*):"
        },
        {
            "sha": "49e7f02f09d2c09daba2c3c9621269b8ba47da93",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1141,6 +1141,7 @@ def forward(\n         past_values: torch.Tensor,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, PatchTSMixerEncoderOutput]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`):\n@@ -1251,6 +1252,7 @@ def forward(\n         observed_mask: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> PatchTSMixerModelOutput:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`):\n@@ -1362,6 +1364,7 @@ def forward(\n         output_hidden_states: Optional[bool] = False,\n         return_loss: bool = True,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> PatchTSMixerForPreTrainingOutput:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`):\n@@ -1574,6 +1577,7 @@ def forward(\n         output_hidden_states: Optional[bool] = False,\n         return_loss: bool = True,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> PatchTSMixerForPredictionOutput:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`):\n@@ -1797,6 +1801,7 @@ def forward(\n         output_hidden_states: Optional[bool] = False,\n         return_loss: bool = True,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> PatchTSMixerForTimeSeriesClassificationOutput:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`):\n@@ -1987,6 +1992,7 @@ def forward(\n         output_hidden_states: Optional[bool] = False,\n         return_loss: bool = True,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> PatchTSMixerForRegressionOutput:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, seq_length, num_input_channels)`):"
        },
        {
            "sha": "4fe0647dea31682f979474c2ef82a978fcdd2f4b",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -716,6 +716,7 @@ def forward(\n         patch_input: torch.Tensor,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n+        **kwargs,\n     ) -> BaseModelOutput:\n         \"\"\"\n         Parameters:\n@@ -1104,6 +1105,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, PatchTSTModelOutput]:\n         r\"\"\"\n         Parameters:\n@@ -1240,6 +1242,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, PatchTSTForPretrainingOutput]:\n         r\"\"\"\n         Parameters:\n@@ -1399,6 +1402,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, PatchTSTForClassificationOutput]:\n         r\"\"\"\n         past_values (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`, *required*):\n@@ -1606,6 +1610,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, PatchTSTForPredictionOutput]:\n         r\"\"\"\n         Parameters:\n@@ -1852,6 +1857,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, PatchTSTForRegressionOutput]:\n         r\"\"\"\n         past_values (`torch.Tensor` of shape `(bs, sequence_length, num_input_channels)`, *required*):"
        },
        {
            "sha": "e1f4036690203cc7a9ec76a57c8045a5a46e5c91",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -518,6 +518,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -695,6 +696,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -946,6 +948,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1111,6 +1114,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1283,6 +1287,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "f3fea33d179db491c7c1fdbb281d7453a9be12a7",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -821,6 +821,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -989,6 +990,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1241,6 +1243,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1388,6 +1391,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):"
        },
        {
            "sha": "1d9e1e7599969710f9aa0e1902a14ac659a94d18",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -615,6 +615,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, PerceiverModelOutput]:\n         r\"\"\"\n         inputs (`torch.FloatTensor`):\n@@ -850,6 +851,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n         input_ids: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, PerceiverMaskedLMOutput]:\n         r\"\"\"\n         inputs (`torch.FloatTensor`):\n@@ -975,6 +977,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n         input_ids: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, PerceiverClassifierOutput]:\n         r\"\"\"\n         inputs (`torch.FloatTensor`):\n@@ -1107,6 +1110,7 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n         pixel_values: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, PerceiverClassifierOutput]:\n         r\"\"\"\n         inputs (`torch.FloatTensor`):\n@@ -1229,6 +1233,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n         pixel_values: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, PerceiverClassifierOutput]:\n         r\"\"\"\n         inputs (`torch.FloatTensor`):\n@@ -1350,6 +1355,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n         pixel_values: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, PerceiverClassifierOutput]:\n         r\"\"\"\n         inputs (`torch.FloatTensor`):\n@@ -1487,6 +1493,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, PerceiverClassifierOutput]:\n         r\"\"\"\n         inputs (`torch.FloatTensor`):\n@@ -1695,6 +1702,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, PerceiverClassifierOutput]:\n         r\"\"\"\n         inputs (`torch.FloatTensor`):"
        },
        {
            "sha": "27b312a7e389fb3ded81d78840226d93171572c3",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1014,7 +1014,7 @@ def calculate_hs_mask(self, hidden_states, device, mask):\n         pad_mask = pad_mask & enc_streaming_mask\n         return pad_mask\n \n-    def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor]):\n+    def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor], **kwargs):\n         hidden_states = self.encoder_embedding(hidden_states)\n         hidden_states, hs_mask, mask = self.forward_embeddings(hidden_states, mask)\n "
        },
        {
            "sha": "2d1fd160e2ee38e1e34a4cf2654f8979a1f514e6",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1205,7 +1205,7 @@ def calculate_hs_mask(self, hidden_states, device, mask):\n         pad_mask = pad_mask & enc_streaming_mask\n         return pad_mask\n \n-    def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor]):\n+    def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor], **kwargs):\n         hidden_states = self.encoder_embedding(hidden_states)\n         hidden_states, hs_mask, mask = self.forward_embeddings(hidden_states, mask)\n "
        },
        {
            "sha": "0c1630485267c7e7c2404e67ca431f80fca62d4e",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -481,6 +481,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         flattened_patches (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_channels x patch_height x patch_width)`):\n@@ -1359,6 +1360,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         flattened_patches (`torch.FloatTensor` of shape `(batch_size, seq_length, hidden_size)`):"
        },
        {
            "sha": "808c22dd19ca4fdc7aa68c4f084e74211be24325",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -366,6 +366,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -621,6 +622,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -867,6 +869,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1002,6 +1005,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1159,6 +1163,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1316,6 +1321,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "d9975799bb86c9863d9276aef5940ac28cea13ab",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -108,6 +108,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -243,6 +244,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):"
        },
        {
            "sha": "73749df140040315dff5b59c24c13d31be97948c",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -276,6 +276,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithNoAttention]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -339,6 +340,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutputWithNoAttention]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "8bb3d244ce009efd9b9d45322efa40a78c163301",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -644,6 +644,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1051,6 +1052,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "ad5006fbfc49cc255de61e28d68de3bf4943a8ed",
            "filename": "src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -393,6 +393,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], DepthEstimatorOutput]:\n         r\"\"\"\n         prompt_depth (`torch.FloatTensor` of shape `(batch_size, 1, height, width)`, *optional*):"
        },
        {
            "sha": "1305c5e0d2f2ad47dea82227009afe09033e677c",
            "filename": "src/transformers/models/prompt_depth_anything/modular_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -236,6 +236,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], DepthEstimatorOutput]:\n         r\"\"\"\n         prompt_depth (`torch.FloatTensor` of shape `(batch_size, 1, height, width)`, *optional*):"
        },
        {
            "sha": "e562cfeb95985a9d374f943521f4fd208ab2665d",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -993,6 +993,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Example:\n@@ -1113,6 +1114,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, ProphetNetDecoderModelOutput]:\n         r\"\"\"\n         Example:\n@@ -1416,6 +1418,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, ProphetNetSeq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):"
        },
        {
            "sha": "c106440346e50f5c0d6f2636544dda00c32d5833",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -458,6 +458,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -512,6 +513,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "87ab03ae89f770243da30a7245860090abd37d66",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -406,6 +406,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -460,6 +461,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -523,6 +525,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "17124a5b4b6667f6644f6e94919f98077b09e716",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -2314,6 +2314,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Qwen2_5OmniTalkerCausalLMOutputWithPast]:\n         r\"\"\"\n         thinker_reply_part (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -3453,7 +3454,7 @@ def process_mel_spectrogram(self, mel_spectrogram):\n         decibel_spectrum = self.amplitude_to_db(amplitude_spectrum, -115) - 20\n         return self.normalize_spectrogram(decibel_spectrum, 1, -115)\n \n-    def forward(self, mel_spectrogram):\n+    def forward(self, mel_spectrogram, **kwargs):\n         processed_spectrogram = self.process_mel_spectrogram(mel_spectrogram)\n         hidden_representation = self.conv_pre(processed_spectrogram)\n \n@@ -3586,6 +3587,7 @@ def forward(\n         drop_audio_conditioning=False,\n         drop_code=False,\n         apply_cfg=True,\n+        **kwargs,\n     ):\n         batch_size = hidden_states.shape[0]\n         if time_step.ndim == 0:"
        },
        {
            "sha": "3f0b621026445aa867df57d9fc5a6321f9364c52",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -2515,6 +2515,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Qwen2_5OmniTalkerCausalLMOutputWithPast]:\n         r\"\"\"\n         thinker_reply_part (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -3611,7 +3612,7 @@ def process_mel_spectrogram(self, mel_spectrogram):\n         decibel_spectrum = self.amplitude_to_db(amplitude_spectrum, -115) - 20\n         return self.normalize_spectrogram(decibel_spectrum, 1, -115)\n \n-    def forward(self, mel_spectrogram):\n+    def forward(self, mel_spectrogram, **kwargs):\n         processed_spectrogram = self.process_mel_spectrogram(mel_spectrogram)\n         hidden_representation = self.conv_pre(processed_spectrogram)\n \n@@ -3744,6 +3745,7 @@ def forward(\n         drop_audio_conditioning=False,\n         drop_code=False,\n         apply_cfg=True,\n+        **kwargs,\n     ):\n         batch_size = hidden_states.shape[0]\n         if time_step.ndim == 0:"
        },
        {
            "sha": "21d049d823612c2d3751aab4390ed73f3de44c08",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -323,6 +323,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -685,6 +686,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Qwen2AudioCausalLMOutputWithPast]:\n         r\"\"\"\n         feature_attention_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):"
        },
        {
            "sha": "bcb90986ddb6caf550f58264002780cd3ef20e5e",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -716,6 +716,7 @@ def forward(\n         input_features,\n         feature_lens=None,\n         aftercnn_lens=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         feature_lens (`torch.LongTensor` of shape `(batch_size,)`):\n@@ -3715,7 +3716,7 @@ def __init__(self, config: Qwen3OmniMoeCode2WavConfig, layer_idx):\n \n         self.block = nn.ModuleList(block)\n \n-    def forward(self, hidden):\n+    def forward(self, hidden, **kwargs):\n         for block in self.block:\n             hidden = block(hidden)\n         return hidden\n@@ -3757,7 +3758,7 @@ def __init__(self, config: Qwen3OmniMoeCode2WavConfig):\n \n         self.post_init()\n \n-    def forward(self, codes):\n+    def forward(self, codes, **kwargs):\n         if codes.shape[1] != self.config.num_quantizers:\n             raise ValueError(f\"Expected {self.config.num_quantizers} layer of codes, got {codes.shape[1]}\")\n         hidden = self.code_embedding(codes + self.code_offset).mean(1)"
        },
        {
            "sha": "8b19907769064c63b376a2a67a33384940080002",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1205,6 +1205,7 @@ def forward(\n         input_features,\n         feature_lens=None,\n         aftercnn_lens=None,\n+        **kwargs,\n     ):\n         aftercnn_lens = _get_feat_extract_output_lengths(feature_lens)\n         chunk_num = torch.ceil(feature_lens / (self.n_window * 2)).long()\n@@ -2336,7 +2337,7 @@ def __init__(self, config: Qwen3OmniMoeCode2WavConfig, layer_idx):\n \n         self.block = nn.ModuleList(block)\n \n-    def forward(self, hidden):\n+    def forward(self, hidden, **kwargs):\n         for block in self.block:\n             hidden = block(hidden)\n         return hidden\n@@ -2378,7 +2379,7 @@ def __init__(self, config: Qwen3OmniMoeCode2WavConfig):\n \n         self.post_init()\n \n-    def forward(self, codes):\n+    def forward(self, codes, **kwargs):\n         if codes.shape[1] != self.config.num_quantizers:\n             raise ValueError(f\"Expected {self.config.num_quantizers} layer of codes, got {codes.shape[1]}\")\n         hidden = self.code_embedding(codes + self.code_offset).mean(1)"
        },
        {
            "sha": "7cbb78283d348546af3994d6bceab32bbd29e458",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -439,6 +439,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_retrieved: Optional[bool] = None,\n         n_docs: Optional[int] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], RetrievAugLMOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "c8cc28bc626f08909cadff0e58f5deabb77b69ec",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -643,6 +643,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithNoAttention]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "5a8deb2574a7da230f1e0d01b7250f7be017ccb7",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1946,6 +1946,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ReformerModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -2297,6 +2298,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -2428,6 +2430,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -2577,6 +2580,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "8695c25964d9222d6464b2783432c190e6af2053",
            "filename": "src/transformers/models/regnet/modeling_regnet.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -294,7 +294,11 @@ def __init__(self, config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BaseModelOutputWithPoolingAndNoAttention:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -348,6 +352,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> ImageClassifierOutputWithNoAttention:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "be6ea8319ddfdee6d0842a566b36235818b9e571",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -540,6 +540,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -659,6 +660,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -857,6 +859,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -940,6 +943,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1043,6 +1047,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1109,6 +1114,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "24e893d027869c5ed3c4296e1d86f5c49e2ae123",
            "filename": "src/transformers/models/resnet/modeling_resnet.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -280,7 +280,11 @@ def __init__(self, config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BaseModelOutputWithPoolingAndNoAttention:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -333,6 +337,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> ImageClassifierOutputWithNoAttention:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -380,7 +385,11 @@ def __init__(self, config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "d3012c3cd6aeaaa90e2f2290a32989a8351534cb",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -693,6 +693,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[BaseModelOutputWithPastAndCrossAttentions, tuple[torch.Tensor]]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -821,6 +822,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[MaskedLMOutput, tuple[torch.Tensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1035,6 +1037,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[SequenceClassifierOutput, tuple[torch.Tensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1114,6 +1117,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[MultipleChoiceModelOutput, tuple[torch.Tensor]]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1210,6 +1214,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[TokenClassifierOutput, tuple[torch.Tensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1275,6 +1280,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[QuestionAnsweringModelOutput, tuple[torch.Tensor]]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "9bb365873e343429502d9a21e1b27d12636af203",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1311,6 +1311,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1592,6 +1593,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], RTDetrModelOutput]:\n         r\"\"\"\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):"
        },
        {
            "sha": "54d6197d9b3b8e5f7a0a907fca515923afb29b97",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr_resnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -342,7 +342,11 @@ def __init__(self, config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n                         Examples:"
        },
        {
            "sha": "9d52217b3542d66defb46c85be3efad9756144a1",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -591,6 +591,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1488,6 +1489,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], RTDetrV2ModelOutput]:\n         r\"\"\"\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):"
        },
        {
            "sha": "92b009b90bb8a5db96d9255a95a82d00d08cd11d",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -520,6 +520,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, RwkvOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):"
        },
        {
            "sha": "d107016ccfc2b396a00e58657fe1d6ae33eecb16",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1700,6 +1700,7 @@ def forward(\n         frame: Optional[torch.Tensor] = None,\n         reverse: bool = False,\n         run_mem_encoder: bool = True,\n+        **kwargs,\n     ) -> Sam2VideoSegmentationOutput:\n         r\"\"\"\n         inference_session (`Sam2VideoInferenceSession`):"
        },
        {
            "sha": "e876ee5bda5b19e6d3fb04595186bf211081b67d",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -2337,6 +2337,7 @@ def forward(\n         frame: Optional[torch.Tensor] = None,\n         reverse: bool = False,\n         run_mem_encoder: bool = True,\n+        **kwargs,\n     ) -> Sam2VideoSegmentationOutput:\n         r\"\"\"\n         inference_session (`Sam2VideoInferenceSession`):"
        },
        {
            "sha": "9662eb56d6194f992f433ccbad08f8e72a1c90d3",
            "filename": "src/transformers/models/sam3_tracker_video/modeling_sam3_tracker_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1719,6 +1719,7 @@ def forward(\n         frame: Optional[torch.Tensor] = None,\n         reverse: bool = False,\n         run_mem_encoder: bool = True,\n+        **kwargs,\n     ) -> Sam3TrackerVideoSegmentationOutput:\n         r\"\"\"\n         inference_session (`Sam3TrackerVideoInferenceSession`):"
        },
        {
            "sha": "ddb2d0cce23da1e7f07a7782820b71b43bce9f4d",
            "filename": "src/transformers/models/sam3_video/modeling_sam3_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fmodeling_sam3_video.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1697,6 +1697,7 @@ def forward(\n         frame_idx: Optional[int] = None,\n         frame: Optional[torch.Tensor] = None,\n         reverse: bool = False,\n+        **kwargs,\n     ):\n         r\"\"\"\n         inference_session (`Sam3VideoInferenceSession`):"
        },
        {
            "sha": "f82f9fd6f263d86d818fd391142049210914a7ba",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1770,6 +1770,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1914,6 +1915,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -2035,6 +2037,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -2354,7 +2357,7 @@ def _transpose_conv_out_length(input_length, kernel_size, stride, pad, dilation=\n         return input_lengths\n \n     def forward(\n-        self, input_ids: torch.LongTensor, spkr_id: torch.Tensor, lang_id: torch.Tensor\n+        self, input_ids: torch.LongTensor, spkr_id: torch.Tensor, lang_id: torch.Tensor, **kwargs\n     ) -> tuple[torch.Tensor]:\n         \"\"\"\n         Args:\n@@ -2996,6 +2999,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "25a1707b955e3cf034c7032dc6ebb45759b8243e",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1812,6 +1812,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1995,6 +1996,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SeamlessM4Tv2TextToUnitDecoderOutput]:\n         r\"\"\"\n         Args:\n@@ -2122,6 +2124,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -2556,7 +2559,7 @@ def _transpose_conv_out_length(input_length, kernel_size, stride, pad, dilation=\n \n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.forward with SeamlessM4T->SeamlessM4Tv2, spkr_id->speaker_id\n     def forward(\n-        self, input_ids: torch.LongTensor, speaker_id: torch.Tensor, lang_id: torch.Tensor\n+        self, input_ids: torch.LongTensor, speaker_id: torch.Tensor, lang_id: torch.Tensor, **kwargs\n     ) -> tuple[torch.Tensor]:\n         \"\"\"\n         Args:\n@@ -3214,6 +3217,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "b9c025fd956fcd4eeff905238db946f60577f488",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -434,6 +434,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -486,6 +487,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SegFormerImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -572,7 +574,7 @@ def __init__(self, config):\n \n         self.config = config\n \n-    def forward(self, encoder_hidden_states: torch.FloatTensor) -> torch.Tensor:\n+    def forward(self, encoder_hidden_states: torch.FloatTensor, **kwargs) -> torch.Tensor:\n         batch_size = encoder_hidden_states[-1].shape[0]\n \n         all_hidden_states = ()\n@@ -627,6 +629,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SemanticSegmenterOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "10a988773e79df75632376dd5a2c82cf812e8115",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -647,6 +647,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SegGptEncoderOutput]:\n         r\"\"\"\n         prompt_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n@@ -843,6 +844,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SegGptImageSegmentationOutput]:\n         r\"\"\"\n         prompt_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):"
        },
        {
            "sha": "7d2a8872e1e8d22794a3e71ecd4fb0c6afb0a3ab",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -773,6 +773,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -902,6 +903,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -1013,6 +1015,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "08a81a7b3b6971c0edc98f43c310c0861bf7b17f",
            "filename": "src/transformers/models/sew/modular_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -392,6 +392,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "dc6aa9245384fd9d18e628fad14ab525edcaa441",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1318,6 +1318,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1445,6 +1446,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -1557,6 +1559,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "46f59a9240b28547424d929d214dfd293d9b37c0",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -510,6 +510,7 @@ def forward(\n         spatial_shapes: torch.LongTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n@@ -760,6 +761,7 @@ def forward(\n         spatial_shapes: torch.LongTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n@@ -927,6 +929,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> Siglip2Output:\n         r\"\"\"\n         pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n@@ -1058,6 +1061,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> ImageClassifierOutput:\n         r\"\"\"\n         pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):"
        },
        {
            "sha": "9b653df8403df62c27e724a3524650ee66947e33",
            "filename": "src/transformers/models/siglip2/modular_siglip2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -247,6 +247,7 @@ def forward(\n         spatial_shapes: torch.LongTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         spatial_shapes (`torch.LongTensor` of shape `(batch_size, 2)`):\n@@ -324,6 +325,7 @@ def forward(\n         spatial_shapes: torch.LongTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n@@ -419,6 +421,7 @@ def forward(\n         return_loss: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> Siglip2Output:\n         r\"\"\"\n         pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):\n@@ -522,6 +525,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> ImageClassifierOutput:\n         r\"\"\"\n         pixel_attention_mask (`torch.Tensor` of shape `(batch_size, image_size, image_size)`, *optional*):"
        },
        {
            "sha": "6692e2f564bd40928ae4d67e93e96aca9ceb0716",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -567,6 +567,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -707,6 +708,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -899,6 +901,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1035,6 +1038,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):"
        },
        {
            "sha": "279a803254544a124052ec5bc555c1477b9382ff",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1239,6 +1239,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         \"\"\"\n         Args:\n@@ -1342,6 +1343,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         hidden_states, attention_mask = self.prenet(input_values, attention_mask)\n \n@@ -1382,6 +1384,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         hidden_states = self.prenet(input_values)\n \n@@ -1416,6 +1419,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         return self.wrapped_encoder(\n             hidden_states=input_values,\n@@ -1454,6 +1458,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -1613,6 +1618,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         decoder_hidden_states = self.prenet(input_values, speaker_embeddings)\n \n@@ -1663,6 +1669,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         decoder_hidden_states, attention_mask = self.prenet(input_values, attention_mask, past_key_values)\n \n@@ -1707,6 +1714,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         outputs = self.wrapped_decoder(\n             hidden_states=input_values,\n@@ -1905,6 +1913,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         input_values (`torch.Tensor` of shape `(batch_size, sequence_length)`):\n@@ -2046,6 +2055,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -2356,6 +2366,7 @@ def forward(\n         labels: Optional[torch.FloatTensor] = None,\n         stop_labels: Optional[torch.Tensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSpectrogramOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -2694,6 +2705,7 @@ def forward(\n         labels: Optional[torch.FloatTensor] = None,\n         stop_labels: Optional[torch.Tensor] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSpectrogramOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -3023,7 +3035,7 @@ def remove_weight_norm(self):\n         waveform.\n         \"\"\"\n     )\n-    def forward(self, spectrogram: torch.FloatTensor) -> torch.FloatTensor:\n+    def forward(self, spectrogram: torch.FloatTensor, **kwargs) -> torch.FloatTensor:\n         r\"\"\"\n         spectrogram (`torch.FloatTensor`):\n             Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,"
        },
        {
            "sha": "bd16970470f3e569b1d6c6659c35c7cf90cbfef4",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -368,6 +368,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `batch_size, sequence_length`, *optional*):\n@@ -516,6 +517,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         question_positions: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `batch_size, sequence_length`, *optional*):\n@@ -658,6 +660,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         question_positions: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SplinterForPreTrainingOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_questions, sequence_length)`):"
        },
        {
            "sha": "1d11c0e3d33ca7be4762194c4d76f8448ff59fe2",
            "filename": "src/transformers/models/squeezebert/modeling_squeezebert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -443,6 +443,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -528,6 +529,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -599,6 +601,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -683,6 +686,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -786,6 +790,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -851,6 +856,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "5412dd600c82e6a4cf7c0b57491ee2eed0a21789",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -492,6 +492,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "7770ca9ee0b344a4cf22a725b278e6168f15bae4",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -670,6 +670,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SuperGlueKeypointMatchingOutput]:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "b82b9b30b716418e45df2123d1ade7dc769e81a0",
            "filename": "src/transformers/models/superpoint/modeling_superpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -378,6 +378,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SuperPointKeypointDescriptionOutput]:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "78e9de9ca46d2eb54f35cc828b5e6f0d72804d7d",
            "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -428,6 +428,7 @@ def forward(\n         pixel_values: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithNoAttention]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -478,6 +479,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutputWithNoAttention]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "ce75887959c882c54eb13ab4d520e6a99a7da66c",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -860,6 +860,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SwinModelOutput]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n@@ -946,6 +947,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SwinMaskedImageModelingOutput]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`):\n@@ -1059,6 +1061,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SwinImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1129,6 +1132,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         \"\"\"\n         Returns:"
        },
        {
            "sha": "0b5be51c5cc08fc2096241d770283c6a9f86513d",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -754,6 +754,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -972,6 +973,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageSuperResolutionOutput]:\n         r\"\"\"\n         Example:"
        },
        {
            "sha": "c2193a27371bf04923a8c7d1456844c9d177ea0e",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -942,6 +942,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Swinv2ModelOutput]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n@@ -1030,6 +1031,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Swinv2MaskedImageModelingOutput]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`):\n@@ -1144,6 +1146,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Swinv2ImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1209,6 +1212,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "bc772b443cbe276afcfe4cb6cb0b3bdab5223b0d",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -673,6 +673,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -879,6 +880,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1044,6 +1046,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1209,6 +1212,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], BaseModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1279,6 +1283,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1417,6 +1422,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1520,6 +1526,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "f5523d920c0a24fa171a319735627d4fcd6058dc",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -749,6 +749,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -869,6 +870,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -1043,6 +1045,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], TableTransformerModelOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n@@ -1202,6 +1205,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], TableTransformerObjectDetectionOutput]:\n         r\"\"\"\n         decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):"
        },
        {
            "sha": "1dda79ef6fb51dbf5c4a5e7233a81c818870150f",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -563,6 +563,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, 7)`, *optional*):\n@@ -843,6 +844,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TableQuestionAnsweringOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, 7)`, *optional*):\n@@ -1164,6 +1166,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, 7)`, *optional*):"
        },
        {
            "sha": "a95d6ee55b602ac5ddc3e74476792f0bd10b34d0",
            "filename": "src/transformers/models/textnet/modeling_textnet.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftextnet%2Fmodeling_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftextnet%2Fmodeling_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fmodeling_textnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -233,7 +233,11 @@ def __init__(self, config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[Any, list[Any]], tuple[Any], BaseModelOutputWithPoolingAndNoAttention]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         output_hidden_states = (\n@@ -288,6 +292,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> ImageClassifierOutputWithNoAttention:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -353,7 +358,11 @@ def __init__(self, config):\n \n     @auto_docstring\n     def forward(\n-        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+        self,\n+        pixel_values: Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[tuple], BackboneOutput]:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "a093cae53d599ecaaaca5ae96714139c7092a9c2",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -658,6 +658,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -777,6 +778,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         Args:\n@@ -1075,6 +1077,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqTSModelOutput, tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):\n@@ -1320,6 +1323,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[Seq2SeqTSModelOutput, tuple]:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`):"
        },
        {
            "sha": "8010de9843135fe009f920759a6464f9020bb54b",
            "filename": "src/transformers/models/timesfm/modeling_timesfm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -361,6 +361,7 @@ def forward(\n         freq: torch.Tensor,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n+        **kwargs,\n     ) -> TimesFmOutput:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -668,6 +669,7 @@ def forward(\n         truncate_negative: bool = False,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> TimesFmOutputForPrediction:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "fd5b3cc1bc77c9a1bb88bef88830cc955d77fd29",
            "filename": "src/transformers/models/timesfm/modular_timesfm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -317,6 +317,7 @@ def forward(\n         freq: torch.Tensor,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n+        **kwargs,\n     ) -> TimesFmOutput:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -624,6 +625,7 @@ def forward(\n         truncate_negative: bool = False,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> TimesFmOutputForPrediction:\n         r\"\"\"\n         past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "25a7c12c0da58bfd50bc4bd57729d25d85605849",
            "filename": "src/transformers/models/timesformer/modeling_timesformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -494,6 +494,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], BaseModelOutput]:\n         r\"\"\"\n         Examples:\n@@ -624,6 +625,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "ed3f19d9e4100ab7e1f7c45aad4e3e5ed69aae98",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -459,6 +459,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -686,6 +687,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "b74e71123c9c8dbcb267407c39ab8d3e5c713ed5",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -721,6 +721,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Examples:\n@@ -822,6 +823,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs,\n     ):\n         r\"\"\"\n         labels (`torch.FloatTensor` of shape `(batch_size, 3)`, *optional*):"
        },
        {
            "sha": "476b88c87608fa884b3e3fe0a565b5b19b543ea1",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1105,6 +1105,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1474,6 +1475,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> tuple[Tensor, ...]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `({0}, 4)`, *optional*):\n@@ -1652,6 +1654,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         labels: Optional[Tensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> tuple[Tensor, ...]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `({0}, 4)`, *optional*):\n@@ -1821,6 +1824,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], BaseModelOutputWithAttentionMask]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "aadeb60844d151e4bb53d608d8db3a13bd21f945",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -621,6 +621,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -966,6 +967,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1147,6 +1149,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1332,6 +1335,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], BaseModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1403,6 +1407,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1545,6 +1550,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1649,6 +1655,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "c74382148b61ee493d5a10067fcccb333ece9ede",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1001,6 +1001,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, UniSpeechBaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1108,6 +1109,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, UniSpeechForPreTrainingOutput]:\n         r\"\"\"\n         Example:\n@@ -1255,6 +1257,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -1366,6 +1369,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "3f91944028c35c6ee34b0b794fdd2396059e9e7c",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -244,6 +244,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, UniSpeechBaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -351,6 +352,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, UniSpeechForPreTrainingOutput]:\n         r\"\"\"\n         Example:"
        },
        {
            "sha": "d1de2cd2bcd33cda69f42114ca6b49cf6435e299",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1006,6 +1006,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, UniSpeechSatBaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1120,6 +1121,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, UniSpeechSatForPreTrainingOutput]:\n         r\"\"\"\n         Example:\n@@ -1251,6 +1253,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -1362,6 +1365,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1465,6 +1469,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1636,6 +1641,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, XVectorOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "55c3d6e4751cebb21d1e4d3a65e6e88097b6b737",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -255,6 +255,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, UniSpeechSatBaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -369,6 +370,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, UniSpeechSatForPreTrainingOutput]:\n         r\"\"\"\n         Example:"
        },
        {
            "sha": "a3c6c8b36675ccb150f7c8453a0e80d49f68f803",
            "filename": "src/transformers/models/univnet/modeling_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -476,6 +476,7 @@ def forward(\n         padding_mask: Optional[torch.FloatTensor] = None,\n         generator: Optional[torch.Generator] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], UnivNetModelOutput]:\n         r\"\"\"\n         noise_sequence (`torch.FloatTensor`, *optional*):"
        },
        {
            "sha": "847f4247b73763015d9d0c2b234141abe7b4589d",
            "filename": "src/transformers/models/upernet/modeling_upernet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -301,6 +301,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SemanticSegmenterOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "41d13d4f91125f2ab1c76f9669eee3b608c36419",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -556,6 +556,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[BaseModelOutputWithPooling, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n@@ -708,6 +709,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[MaskedLMOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n@@ -875,6 +877,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[SequenceClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n@@ -979,6 +982,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[SequenceClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n@@ -1082,6 +1086,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[ViltForImagesAndTextClassificationOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n@@ -1210,6 +1215,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[TokenClassifierOutput, tuple[torch.FloatTensor]]:\n         r\"\"\"\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):"
        },
        {
            "sha": "6ef8cd0cf662a674e05ebbccd27b1c738edc4452",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -184,6 +184,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], CLIPOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):"
        },
        {
            "sha": "c823f575f95919339e0f94c96f008a3a6be7e794",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -550,6 +550,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPooling]:\n         r\"\"\"\n         visual_embeds (`torch.FloatTensor` of shape `(batch_size, visual_seq_length, visual_embedding_dim)`, *optional*):\n@@ -735,6 +736,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n         sentence_image_labels: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], VisualBertForPreTrainingOutput]:\n         r\"\"\"\n         visual_embeds (`torch.FloatTensor` of shape `(batch_size, visual_seq_length, visual_embedding_dim)`, *optional*):\n@@ -877,6 +879,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1063,6 +1066,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         visual_embeds (`torch.FloatTensor` of shape `(batch_size, visual_seq_length, visual_embedding_dim)`, *optional*):\n@@ -1199,6 +1203,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         visual_embeds (`torch.FloatTensor` of shape `(batch_size, visual_seq_length, visual_embedding_dim)`, *optional*):\n@@ -1372,6 +1377,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         region_to_phrase_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         visual_embeds (`torch.FloatTensor` of shape `(batch_size, visual_seq_length, visual_embedding_dim)`, *optional*):"
        },
        {
            "sha": "c8171fdf1f75e1051fee1c2f10df0e5a0f06098b",
            "filename": "src/transformers/models/vitdet/modeling_vitdet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -630,6 +630,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         Examples:\n@@ -706,6 +707,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> BackboneOutput:\n         r\"\"\"\n         Examples:"
        },
        {
            "sha": "9db0dd7a8041071645143699ab9446e3e9bc0a8d",
            "filename": "src/transformers/models/vitmatte/modeling_vitmatte.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -234,6 +234,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "312b76fa37463a04b03b9ea4b64a488e637a8db8",
            "filename": "src/transformers/models/vits/modeling_vits.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1275,6 +1275,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.FloatTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[Any], VitsModelOutput]:\n         r\"\"\"\n         speaker_id (`int`, *optional*):"
        },
        {
            "sha": "31eec7d9fec76468a02e8ad91d2be5ca1a5443d8",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1088,6 +1088,7 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ImageClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "dfd8362543ce0c08506b8d8f7b0114a552c85234",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1340,6 +1340,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Wav2Vec2BaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1453,6 +1454,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Wav2Vec2ForPreTrainingOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1628,6 +1630,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1729,6 +1732,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -1840,6 +1844,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1943,6 +1948,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -2114,6 +2120,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, XVectorOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "3baa80668f2ac61274b566406e7783fd752a2ffb",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -994,6 +994,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Wav2Vec2BertBaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1086,6 +1087,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -1192,6 +1194,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1282,6 +1285,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1440,6 +1444,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, XVectorOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "32ed8ecde0e327d1f1a32d36a35f1ccb83692faf",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -702,6 +702,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Wav2Vec2BertBaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -768,6 +769,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -856,6 +858,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -926,6 +929,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -987,6 +991,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, XVectorOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "ad548cdbddcbead8ff3ee210f47b2af1cc15aea9",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1142,6 +1142,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Wav2Vec2ConformerBaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1255,6 +1256,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, Wav2Vec2ConformerForPreTrainingOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1459,6 +1461,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -1570,6 +1573,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1673,6 +1677,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1844,6 +1849,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, XVectorOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "f6e2ddba6182700142115fb5a33cdccbeace189b",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1047,6 +1047,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, WavLMBaseModelOutput]:\n         r\"\"\"\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1180,6 +1181,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n@@ -1291,6 +1293,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1394,6 +1397,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1565,6 +1569,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, XVectorOutput]:\n         r\"\"\"\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "b2e13b71e5179616faf983827faf38e1e9657029",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -608,6 +608,7 @@ def forward(\n         output_attentions=None,\n         output_hidden_states=None,\n         return_dict=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -734,6 +735,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         cache_position=None,\n+        **kwargs,\n     ):\n         r\"\"\"\n         Args:\n@@ -982,6 +984,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1129,6 +1132,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1299,6 +1303,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         encoder_outputs (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1422,6 +1427,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "37f88d8ff81c042f30cc7612aee39c2575922277",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -737,6 +737,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -927,6 +928,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n@@ -1340,6 +1342,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, XCLIPOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):"
        },
        {
            "sha": "f9001d09a4af797ee638a835339ad0424891ea63",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -407,6 +407,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n         encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):"
        },
        {
            "sha": "3d6256869590789e49d841c07b8ff8a353f184f4",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1082,6 +1082,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1190,6 +1191,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1291,6 +1293,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, XLMForQuestionAnsweringOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1406,6 +1409,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1491,6 +1495,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):"
        },
        {
            "sha": "c5f370655a9b1e2e107dfc8d95282f2a79b37b9b",
            "filename": "src/transformers/models/yoso/modeling_yoso.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -642,6 +642,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -734,6 +735,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -823,6 +825,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -904,6 +907,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1009,6 +1013,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1085,6 +1090,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "fac9f62ee652d26320be55f4117834ecdfe48be9",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -870,6 +870,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1192,6 +1193,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "6079b846188fe6359bafc132ae3d03cc154de2c5",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1294,6 +1294,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1638,6 +1639,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):"
        },
        {
            "sha": "b97fdfab60260a4a126b6e2910f1ddf74d81f1ce",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -993,6 +993,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "a0d540081b4d40a1f896c2ccd484f32c8b5e7f4f",
            "filename": "src/transformers/models/zoedepth/modeling_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -1251,6 +1251,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple[torch.Tensor], DepthEstimatorOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):"
        },
        {
            "sha": "64324738253ebedb51e61cd94e72f687257a000b",
            "filename": "tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -222,6 +222,7 @@ def test_forward_signature(self):\n             \"return_dict\",\n             \"output_attentions\",\n             \"output_hidden_states\",\n+            \"kwargs\",\n         ]\n         self.assertListEqual(arg_names, expected_arg_names)\n \n@@ -640,6 +641,7 @@ def test_forward_signature(self):\n             \"return_dict\",\n             \"output_attentions\",\n             \"output_hidden_states\",\n+            \"kwargs\",\n         ]\n         self.assertListEqual(arg_names, expected_arg_names)\n "
        },
        {
            "sha": "1e6f6d2af8d5300f2e4d0680708b8152267701d6",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -353,6 +353,7 @@ def test_forward_signature(self):\n                 \"encoder_outputs\",\n                 \"input_ids\",\n                 \"inputs_embeds\",\n+                \"kwargs\",\n             ]\n             if model_class in self.all_generative_model_classes:\n                 expected_arg_names.append("
        },
        {
            "sha": "a00e3a3887000d51488ba40d32221614077dd08d",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9b74e4c43402049608fc3e74c9b675d9b37ef0f7/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=9b74e4c43402049608fc3e74c9b675d9b37ef0f7",
            "patch": "@@ -31,6 +31,7 @@\n It has no auto-fix mode.\n \"\"\"\n \n+import ast\n import os\n import re\n import types\n@@ -1199,6 +1200,68 @@ def check_deprecated_constant_is_up_to_date():\n         raise Exception(\"\\n\".join(message))\n \n \n+def check_models_have_kwargs():\n+    \"\"\"\n+    Checks that all model classes defined in modeling files accept **kwargs in their forward pass.\n+    Since we ast.parse() here, it might be a good idea to add other tests that inspect modeling code here rather than\n+    repeatedly ast.parsing() in each test!\n+    \"\"\"\n+    models_dir = Path(PATH_TO_TRANSFORMERS) / \"models\"\n+    failing_classes = []\n+    for model_dir in models_dir.iterdir():\n+        if model_dir.name == \"deprecated\":\n+            continue\n+        if model_dir.is_dir() and (modeling_file := list(model_dir.glob(\"modeling_*.py\"))):\n+            modeling_file = modeling_file[0]\n+\n+            with open(modeling_file, \"r\") as f:\n+                tree = ast.parse(f.read())\n+\n+            # Map all classes in the file to their base classes\n+            class_bases = {}\n+            all_class_nodes = {}\n+\n+            for node in ast.walk(tree):\n+                if isinstance(node, ast.ClassDef):\n+                    # We only care about base classes that are simple names\n+                    bases = [b.id for b in node.bases if isinstance(b, ast.Name)]\n+                    class_bases[node.name] = bases\n+                    all_class_nodes[node.name] = node\n+\n+            inherits_from_pretrained = {\"PreTrainedModel\"}\n+            # Loop over classes and mark the ones that inherit from PreTrainedModel, or from\n+            # previously marked classes (which indicates indirect inheritance from PreTrainedModel)\n+            # Keep going until you go through the whole list without discovering a new child class, then break\n+            while True:\n+                for class_name, bases in class_bases.items():\n+                    if class_name in inherits_from_pretrained:\n+                        continue\n+                    if inherits_from_pretrained.intersection(bases):\n+                        inherits_from_pretrained.add(class_name)\n+                        break\n+                else:\n+                    break\n+\n+            # 2. Iterate through classes and check conditions\n+            for class_name, class_def in all_class_nodes.items():\n+                if class_name not in inherits_from_pretrained:\n+                    continue\n+\n+                forward_method = next(\n+                    (n for n in class_def.body if isinstance(n, ast.FunctionDef) and n.name == \"forward\"), None\n+                )\n+                if forward_method:\n+                    # 3. Check for **kwargs (represented as .kwarg in AST)\n+                    if forward_method.args.kwarg is None:\n+                        failing_classes.append(class_name)\n+\n+    if failing_classes:\n+        raise Exception(\n+            \"The following model classes do not accept **kwargs in their forward() method: \\n\"\n+            f\"{', '.join(failing_classes)}.\"\n+        )\n+\n+\n def check_repo_quality():\n     \"\"\"Check all models are tested and documented.\"\"\"\n     print(\"Repository-wide checks:\")\n@@ -1221,6 +1284,8 @@ def check_repo_quality():\n     check_all_auto_mappings_importable()\n     print(\"    - checking the DEPRECATED_MODELS constant is up to date.\")\n     check_deprecated_constant_is_up_to_date()\n+    print(\"    - checking all models accept **kwargs in their call.\")\n+    check_models_have_kwargs()\n \n \n if __name__ == \"__main__\":"
        }
    ],
    "stats": {
        "total": 903,
        "additions": 856,
        "deletions": 47
    }
}