{
    "author": "gante",
    "message": "[speech2text] fix init of sinusoidal embeddings  (#37931)\n\n* fix init (meta device -> bad numbers)\n\n* fast test\n\n* dont init sinusoidal twice\n\n* make fixup",
    "sha": "af2866a8b19c621f67396db03031066a2f232c54",
    "files": [
        {
            "sha": "d38aeab3f847f3e307e30cb123cd0d413c278d84",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2866a8b19c621f67396db03031066a2f232c54/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2866a8b19c621f67396db03031066a2f232c54/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=af2866a8b19c621f67396db03031066a2f232c54",
            "patch": "@@ -125,9 +125,7 @@ def make_weights(self, num_embeddings: int, embedding_dim: int):\n             # in forward put the weights on the correct dtype and device of the param\n             emb_weights = emb_weights.to(dtype=self.weights.dtype, device=self.weights.device)\n \n-        self.weights = nn.Parameter(emb_weights)\n-        self.weights.requires_grad = False\n-        self.weights.detach_()\n+        self.register_buffer(\"weights\", emb_weights, persistent=False)\n \n     @staticmethod\n     def get_embedding(num_embeddings: int, embedding_dim: int):\n@@ -718,11 +716,6 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, MusicgenSinusoidalPositionalEmbedding):\n-            weights = module.get_embedding(*module.weights.shape)\n-            weights = nn.Parameter(weights, requires_grad=False)\n-            weights.detach_()\n-            module.weights = weights\n \n \n MUSICGEN_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "a35cdaa407f833190443ec639978ce38f8c5bc89",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2866a8b19c621f67396db03031066a2f232c54/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2866a8b19c621f67396db03031066a2f232c54/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=af2866a8b19c621f67396db03031066a2f232c54",
            "patch": "@@ -140,9 +140,7 @@ def make_weights(self, num_embeddings: int, embedding_dim: int):\n             # in forward put the weights on the correct dtype and device of the param\n             emb_weights = emb_weights.to(dtype=self.weights.dtype, device=self.weights.device)\n \n-        self.weights = nn.Parameter(emb_weights)\n-        self.weights.requires_grad = False\n-        self.weights.detach_()\n+        self.register_buffer(\"weights\", emb_weights, persistent=False)\n \n     @staticmethod\n     def get_embedding(num_embeddings: int, embedding_dim: int):\n@@ -677,11 +675,6 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, MusicgenMelodySinusoidalPositionalEmbedding):\n-            weights = module.get_embedding(*module.weights.shape)\n-            weights = nn.Parameter(weights, requires_grad=False)\n-            weights.detach_()\n-            module.weights = weights\n \n \n MUSICGEN_MELODY_START_DOCSTRING = r\"\"\""
        },
        {
            "sha": "397c4f3886cfe9f33e2f69bea4b56be6b5e2e093",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2866a8b19c621f67396db03031066a2f232c54/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2866a8b19c621f67396db03031066a2f232c54/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=af2866a8b19c621f67396db03031066a2f232c54",
            "patch": "@@ -113,9 +113,7 @@ def make_weights(self, num_embeddings: int, embedding_dim: int, padding_idx: Opt\n             # in forward put the weights on the correct dtype and device of the param\n             emb_weights = emb_weights.to(dtype=self.weights.dtype, device=self.weights.device)\n \n-        self.weights = nn.Parameter(emb_weights)\n-        self.weights.requires_grad = False\n-        self.weights.detach_()\n+        self.register_buffer(\"weights\", emb_weights, persistent=False)\n \n     @staticmethod\n     def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):"
        },
        {
            "sha": "8d784487ab8c5ca081b0f4648feb4110236a67ca",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2866a8b19c621f67396db03031066a2f232c54/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2866a8b19c621f67396db03031066a2f232c54/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=af2866a8b19c621f67396db03031066a2f232c54",
            "patch": "@@ -299,9 +299,7 @@ def make_weights(self, num_embeddings: int, embedding_dim: int, padding_idx: Opt\n             # in forward put the weights on the correct dtype and device of the param\n             emb_weights = emb_weights.to(dtype=self.weights.dtype, device=self.weights.device)\n \n-        self.weights = nn.Parameter(emb_weights)\n-        self.weights.requires_grad = False\n-        self.weights.detach_()\n+        self.register_buffer(\"weights\", emb_weights, persistent=False)\n \n     @staticmethod\n     def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):"
        },
        {
            "sha": "e80bb7948a141ea5cca0a18c202c72c351585ea7",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2866a8b19c621f67396db03031066a2f232c54/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2866a8b19c621f67396db03031066a2f232c54/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=af2866a8b19c621f67396db03031066a2f232c54",
            "patch": "@@ -1556,7 +1556,7 @@ def test_generate_text_audio_prompt(self):\n         self.assertTrue(\n             output_values.shape == (2, 1, 36480)\n         )  # input values take shape 32000 and we generate from there\n-        torch.testing.assert_close(output_values[0, 0, -16:].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output_values[0, 0, -16:].cpu(), EXPECTED_VALUES, rtol=2e-4, atol=2e-4)\n \n \n @require_torch\n@@ -1631,5 +1631,5 @@ def test_generate_text_audio_prompt(self):\n         # (bsz, channels, seq_len)\n         self.assertTrue(output_values.shape == (2, 2, 37760))\n         # input values take shape 32000 and we generate from there - we check the last (generated) values\n-        torch.testing.assert_close(output_values[0, 0, -16:].cpu(), EXPECTED_VALUES_LEFT, rtol=1e-4, atol=1e-4)\n-        torch.testing.assert_close(output_values[0, 1, -16:].cpu(), EXPECTED_VALUES_RIGHT, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output_values[0, 0, -16:].cpu(), EXPECTED_VALUES_LEFT, rtol=2e-4, atol=2e-4)\n+        torch.testing.assert_close(output_values[0, 1, -16:].cpu(), EXPECTED_VALUES_RIGHT, rtol=2e-4, atol=2e-4)"
        },
        {
            "sha": "922ca66e0892a0b5de6f7db6f197615b7bc0eaa7",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 18,
            "deletions": 32,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2866a8b19c621f67396db03031066a2f232c54/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2866a8b19c621f67396db03031066a2f232c54/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=af2866a8b19c621f67396db03031066a2f232c54",
            "patch": "@@ -19,6 +19,8 @@\n import tempfile\n import unittest\n \n+from datasets import load_dataset\n+\n from transformers import Speech2TextConfig\n from transformers.testing_utils import (\n     is_torch_available,\n@@ -27,10 +29,8 @@\n     require_torch,\n     require_torch_fp16,\n     require_torchaudio,\n-    slow,\n     torch_device,\n )\n-from transformers.utils import cached_property\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -701,52 +701,38 @@ def test_tf_from_pt_safetensors(self):\n @require_torchaudio\n @require_sentencepiece\n @require_tokenizers\n-@slow\n class Speech2TextModelIntegrationTests(unittest.TestCase):\n-    @cached_property\n-    def default_processor(self):\n-        return Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n-\n-    def _load_datasamples(self, num_samples):\n-        from datasets import load_dataset\n-\n+    @classmethod\n+    def setUpClass(cls):\n+        model_name = \"facebook/s2t-small-librispeech-asr\"\n+        cls.model = Speech2TextForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n+        cls.processor = Speech2TextProcessor.from_pretrained(model_name)\n+        # loads 4 samples\n         ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n-        # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n-\n-        return [x[\"array\"] for x in speech_samples]\n+        speech_samples = ds.sort(\"id\").select(range(4))[:4][\"audio\"]\n+        cls.dataset = [x[\"array\"] for x in speech_samples]\n \n     def test_generation_librispeech(self):\n-        model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n-        model.to(torch_device)\n-        processor = self.default_processor\n+        input_speech = [self.dataset[0]]\n+        input_features = self.processor(input_speech, return_tensors=\"pt\").input_features.to(torch_device)\n \n-        input_speech = self._load_datasamples(1)\n-\n-        input_features = processor(input_speech, return_tensors=\"pt\").input_features.to(torch_device)\n-\n-        generated_ids = model.generate(input_features)\n-        generated_transcript = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+        generated_ids = self.model.generate(input_features)\n+        generated_transcript = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n \n         EXPECTED_TRANSCRIPTIONS = [\n             \"mister quilter is the apostle of the middle classes and we are glad to welcome his gospel\"\n         ]\n         self.assertListEqual(generated_transcript, EXPECTED_TRANSCRIPTIONS)\n \n     def test_generation_librispeech_batched(self):\n-        model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n-        model.to(torch_device)\n-        processor = self.default_processor\n-\n-        input_speech = self._load_datasamples(4)\n-\n-        inputs = processor(input_speech, return_tensors=\"pt\", padding=True)\n+        input_speech = self.dataset\n+        inputs = self.processor(input_speech, return_tensors=\"pt\", padding=True)\n \n         input_features = inputs.input_features.to(torch_device)\n         attention_mask = inputs.attention_mask.to(torch_device)\n \n-        generated_ids = model.generate(input_features, attention_mask=attention_mask)\n-        generated_transcripts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+        generated_ids = self.model.generate(input_features, attention_mask=attention_mask)\n+        generated_transcripts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n \n         EXPECTED_TRANSCRIPTIONS = [\n             \"mister quilter is the apostle of the middle classes and we are glad to welcome his gospel\","
        }
    ],
    "stats": {
        "total": 82,
        "additions": 25,
        "deletions": 57
    }
}