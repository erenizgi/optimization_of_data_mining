{
    "author": "vasqu",
    "message": "[`FA`] Fix some model tests (#40350)\n\n* fix\n\n* cleanup, revert aimv2 fa changes\n\n* fix aria\n\n* i searched a long time but the cross dependency is for the recent models so...\n\n* this was something... evolla\n\n* fix modernbert decoder + make fa test more robust\n\n* nit",
    "sha": "cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
    "files": [
        {
            "sha": "472eccd0b5755ac53012c86af90ddf1d91804ff8",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -630,6 +630,7 @@ def _get_vector_norm(tensor: torch.Tensor) -> torch.Tensor:\n class Aimv2Model(Aimv2PreTrainedModel):\n     config: Aimv2Config\n     _no_split_modules = [\"Aimv2TextEmbeddings\", \"Aimv2EncoderLayer\", \"Aimv2VisionEmbeddings\"]\n+    _supports_flash_attn = True\n \n     def __init__(self, config: Aimv2Config):\n         super().__init__(config)"
        },
        {
            "sha": "9de5f9e03b8a5d33ef2487708ca7e2398fa1b27c",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -614,6 +614,8 @@ def forward(\n \n @auto_docstring\n class Aimv2Model(CLIPModel, nn.Module):\n+    _supports_flash_attn = True\n+\n     def __init__(self, config: Aimv2Config):\n         nn.Module().__init__(config)\n "
        },
        {
            "sha": "dbdc82718e4b2f9a63b973eb0f027edd2dab467f",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -632,7 +632,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn = False\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     _supports_attention_backend = True"
        },
        {
            "sha": "ce09f85c5db84e7b4e9cc7e3f07c53c70811f6c2",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -1283,7 +1283,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn = False\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     _supports_attention_backend = True"
        },
        {
            "sha": "e73294ff05c0043393ebac08916929afb1d2480f",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -673,6 +673,7 @@ class CLIPTextModel(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n \n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n+    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPTextConfig):\n         super().__init__(config)\n@@ -830,6 +831,7 @@ def forward(\n class CLIPModel(CLIPPreTrainedModel):\n     config: CLIPConfig\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\", \"CLIPVisionEmbeddings\"]\n+    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: CLIPConfig):\n         super().__init__(config)"
        },
        {
            "sha": "2902e3a5ef4f1ec36d81260fd4be1c1d0d451cd7",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -715,7 +715,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class EvollaSaProtPreTrainedModel(PreTrainedModel):\n     config: SaProtConfig\n     _no_split_modules = [\"EvollaSaProtLayer\"]\n-    _supports_flash_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -1511,9 +1510,9 @@ class EvollaPreTrainedModel(PreTrainedModel):\n         \"EvollaSequenceAlignerCrossAttention\",\n     ]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn = True\n+    _supports_flash_attn = False  # see dependency on `EvollaSaProtProteinEncoder`\n     _supports_sdpa = True\n-    _supports_flex_attn = True\n+    _supports_flex_attn = False  # see dependency on `EvollaSaProtProteinEncoder`\n \n     _can_compile_fullgraph = True\n     _supports_attention_backend = False"
        },
        {
            "sha": "da1024bd597953a16c0e407b8fb2e8b94d2ea47f",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -193,7 +193,6 @@ class EvollaSaProtPooler(EsmPooler):\n class EvollaSaProtPreTrainedModel(PreTrainedModel):\n     config: SaProtConfig\n     _no_split_modules = [\"EvollaSaProtLayer\"]\n-    _supports_flash_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -772,6 +771,8 @@ def forward(\n \n \n class EvollaPreTrainedModel(LlamaPreTrainedModel):\n+    _supports_flash_attn = False  # see dependency on `EvollaSaProtProteinEncoder`\n+    _supports_flex_attn = False  # see dependency on `EvollaSaProtProteinEncoder`\n     _supports_attention_backend = False\n     _no_split_modules = [\n         \"EvollaDecoderLayer\","
        },
        {
            "sha": "c8567916751e6718f3f6066f9b599f106045daf5",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -283,7 +283,7 @@ def forward(self, hidden_states: torch.Tensor):\n class GraniteSpeechPreTrainedModel(PreTrainedModel):\n     config: GraniteSpeechConfig\n \n-    _supports_flash_attn = True\n+    _supports_flash_attn = False  # `blip_2_qformer` dependency does not allow for this\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "68530bbea11f09dc135d7d62bb1da9796d981f0b",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -883,7 +883,7 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IdeficsDecoderLayer\", \"IdeficsGatedCrossAttentionLayer\"]\n     _supports_sdpa = True\n \n-    _supports_flash_attn = True\n+    _supports_flash_attn = False  # only eager/sdpa creation is supported\n     _can_compile_fullgraph = False  # IDEFICS cannot compile due to dynamic control flow when checking inputs\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "d322472180f1164602d43080f5f16587fd3be1f4",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -545,6 +545,7 @@ class MetaClip2TextModel(MetaClip2PreTrainedModel):\n     config: MetaClip2TextConfig\n \n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n+    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2TextConfig):\n         super().__init__(config)\n@@ -789,6 +790,7 @@ def _get_vector_norm(tensor: torch.Tensor) -> torch.Tensor:\n class MetaClip2Model(MetaClip2PreTrainedModel):\n     config: MetaClip2Config\n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\", \"MetaClip2VisionEmbeddings\"]\n+    _supports_flash_attn = False  # mask creation only accounts for sdpa/eager\n \n     def __init__(self, config: MetaClip2Config):\n         super().__init__(config)"
        },
        {
            "sha": "fd3dd9ea9820baf2124ebfa09704553c61cc6fc7",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -221,12 +221,8 @@ def forward(\n @auto_docstring\n class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n     config: ModernBertDecoderConfig\n-    base_model_prefix = \"model\"\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _no_split_modules = [\"ModernBertDecoderLayer\"]\n-    _supports_flash_attn = True\n-    _supports_sdpa = False\n-    _supports_gradient_checkpointing = True\n     _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n@@ -280,6 +276,20 @@ def init_weight(module: nn.Module, std: float):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n+    def _check_and_adjust_attn_implementation(\n+        self, attn_implementation: Optional[str], is_init_check: bool = False\n+    ) -> str:\n+        \"\"\"We overwrite this to make sdpa the first selection again if nothing was requested.\"\"\"\n+\n+        try:\n+            attn_implementation = (\n+                \"sdpa\" if attn_implementation is None and self._sdpa_can_dispatch() else attn_implementation\n+            )\n+        except (ValueError, ImportError):\n+            pass\n+\n+        return super()._check_and_adjust_attn_implementation(attn_implementation, is_init_check)\n+\n \n @auto_docstring\n class ModernBertDecoderModel(ModernBertDecoderPreTrainedModel):"
        },
        {
            "sha": "823248f6b40f52f93e7e7ca41b349e0cf40548a7",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -398,12 +398,8 @@ def forward(\n @auto_docstring\n class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n     config: ModernBertDecoderConfig\n-    base_model_prefix = \"model\"\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _no_split_modules = [\"ModernBertDecoderLayer\"]\n-    _supports_flash_attn = True\n-    _supports_sdpa = False\n-    _supports_gradient_checkpointing = True\n     _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n@@ -457,6 +453,20 @@ def init_weight(module: nn.Module, std: float):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n+    def _check_and_adjust_attn_implementation(\n+        self, attn_implementation: Optional[str], is_init_check: bool = False\n+    ) -> str:\n+        \"\"\"We overwrite this to make sdpa the first selection again if nothing was requested.\"\"\"\n+\n+        try:\n+            attn_implementation = (\n+                \"sdpa\" if attn_implementation is None and self._sdpa_can_dispatch() else attn_implementation\n+            )\n+        except (ValueError, ImportError):\n+            pass\n+\n+        return super()._check_and_adjust_attn_implementation(attn_implementation, is_init_check)\n+\n \n @auto_docstring\n class ModernBertDecoderModel(ModernBertDecoderPreTrainedModel):"
        },
        {
            "sha": "74e5d7fd5a6f160d6868bc513397ef50549b22d2",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -622,7 +622,6 @@ def __init__(self, config: SiglipTextConfig):\n         self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n         self.head = nn.Linear(embed_dim, config.projection_size)\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     @can_return_tuple\n     @auto_docstring\n@@ -649,7 +648,10 @@ def forward(\n \n         # note: SigLIP's text model does not use a causal mask, unlike the original CLIP model.\n         # expand attention_mask\n-        if attention_mask is not None and not self._use_flash_attention_2:\n+        uses_flash_attention = \"flash\" in self.config._attn_implementation\n+        if uses_flash_attention:\n+            attention_mask = None\n+        elif attention_mask is not None and not uses_flash_attention:\n             # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n "
        },
        {
            "sha": "40c03dc5980f32e7f3b407635c39afcc9e1f9536",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -647,7 +647,6 @@ def __init__(self, config: Siglip2TextConfig):\n         self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n         self.head = nn.Linear(embed_dim, config.projection_size)\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     @can_return_tuple\n     @auto_docstring\n@@ -674,7 +673,10 @@ def forward(\n \n         # note: Siglip2's text model does not use a causal mask, unlike the original CLIP model.\n         # expand attention_mask\n-        if attention_mask is not None and not self._use_flash_attention_2:\n+        uses_flash_attention = \"flash\" in self.config._attn_implementation\n+        if uses_flash_attention:\n+            attention_mask = None\n+        elif attention_mask is not None and not uses_flash_attention:\n             # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n             attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n "
        },
        {
            "sha": "c25f3052dc0b71e7bf5ee7f9fd626b80d8db0326",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb1df4d26a93cdade051710c7dfa3aaa0e99abd6/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=cb1df4d26a93cdade051710c7dfa3aaa0e99abd6",
            "patch": "@@ -3501,6 +3501,11 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n                 model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n                 model.to(torch_device)\n \n+                # Some models have support for FA but not SDPA - making sure we have a valid attention\n+                initial_attention_implementation = \"sdpa\"\n+                if model.config._attn_implementation != \"sdpa\":\n+                    initial_attention_implementation = \"eager\"\n+\n                 dummy_input = inputs_dict[model.main_input_name][:1]\n                 if dummy_input.dtype in [torch.float32, torch.float16]:\n                     dummy_input = dummy_input.to(torch.bfloat16)\n@@ -3526,7 +3531,7 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n                     model.set_attn_implementation(attn_implementation)\n                     outputs_fa = model(dummy_input, output_hidden_states=True)\n \n-                model.set_attn_implementation(\"sdpa\")\n+                model.set_attn_implementation(initial_attention_implementation)\n                 logits = (\n                     outputs.hidden_states[-1]\n                     if not model.config.is_encoder_decoder\n@@ -3563,7 +3568,7 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n                     model.set_attn_implementation(attn_implementation)\n                     outputs_fa = model(dummy_input, **other_inputs)\n \n-                model.set_attn_implementation(\"sdpa\")\n+                model.set_attn_implementation(initial_attention_implementation)\n                 logits = (\n                     outputs.hidden_states[-1]\n                     if not model.config.is_encoder_decoder"
        }
    ],
    "stats": {
        "total": 80,
        "additions": 58,
        "deletions": 22
    }
}