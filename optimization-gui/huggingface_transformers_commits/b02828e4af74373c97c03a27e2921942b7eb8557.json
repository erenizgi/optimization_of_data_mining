{
    "author": "muellerzr",
    "message": "Let `EarlyStoppingCallback` not require `load_best_model_at_end` (#35101)\n\n* Bookmark\r\n\r\n* Add warning",
    "sha": "b02828e4af74373c97c03a27e2921942b7eb8557",
    "files": [
        {
            "sha": "8f241a9db4a370aec6b98b2e6b50e33c0471646f",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b02828e4af74373c97c03a27e2921942b7eb8557/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b02828e4af74373c97c03a27e2921942b7eb8557/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=b02828e4af74373c97c03a27e2921942b7eb8557",
            "patch": "@@ -707,10 +707,14 @@ def check_metric_value(self, args, state, control, metric_value):\n             self.early_stopping_patience_counter += 1\n \n     def on_train_begin(self, args, state, control, **kwargs):\n-        assert args.load_best_model_at_end, \"EarlyStoppingCallback requires load_best_model_at_end = True\"\n+        if not args.load_best_model_at_end:\n+            logger.warning(\n+                \"Using EarlyStoppingCallback without load_best_model_at_end=True. \"\n+                \"Once training is finished, the best model will not be loaded automatically.\"\n+            )\n         assert (\n             args.metric_for_best_model is not None\n-        ), \"EarlyStoppingCallback requires metric_for_best_model is defined\"\n+        ), \"EarlyStoppingCallback requires metric_for_best_model to be defined\"\n         assert (\n             args.eval_strategy != IntervalStrategy.NO\n         ), \"EarlyStoppingCallback requires IntervalStrategy of steps or epoch\""
        },
        {
            "sha": "d89c4aa80302857fe8f2bfb0ff5672542e7b68a2",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b02828e4af74373c97c03a27e2921942b7eb8557/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b02828e4af74373c97c03a27e2921942b7eb8557/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=b02828e4af74373c97c03a27e2921942b7eb8557",
            "patch": "@@ -3484,6 +3484,23 @@ def test_early_stopping_callback(self):\n             except AssertionError:\n                 self.assertEqual(trainer.state.global_step, 0)\n \n+        # even if load_best_model_at_end is False, `best_model_checkpoint` should be set\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            trainer = get_regression_trainer(\n+                output_dir=tmp_dir,\n+                num_train_epochs=20,\n+                gradient_accumulation_steps=1,\n+                per_device_train_batch_size=16,\n+                load_best_model_at_end=False,\n+                eval_strategy=IntervalStrategy.EPOCH,\n+                save_strategy=IntervalStrategy.EPOCH,\n+                compute_metrics=AlmostAccuracy(),\n+                metric_for_best_model=\"accuracy\",\n+            )\n+            trainer.add_callback(EarlyStoppingCallback(1, 0.0001))\n+            train_output = trainer.train()\n+            self.assertIsNotNone(trainer.state.best_model_checkpoint)\n+\n     def test_flos_extraction(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             trainer = get_regression_trainer(learning_rate=0.1, output_dir=tmp_dir)"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 23,
        "deletions": 2
    }
}