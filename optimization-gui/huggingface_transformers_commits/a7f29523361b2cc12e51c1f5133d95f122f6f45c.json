{
    "author": "zRzRzRzRzRzRzR",
    "message": "GLM-ASR  Support (#42875)\n\n* draft only\n\n* update\n\n* rename\n\n* format config\n\n* update config\n\n* update shape\n\n* update\n\n* testing\n\n* new config\n\n* Update modeling_glmasr.py\n\n* draft change\n\n* update encoder config\n\n* darft PR with no embed\n\n* revert\n\n* revert\n\n* Update modular_glmasr.py\n\n* update\n\n* format\n\n* for doc update\n\n* add processor\n\n* rename\n\n* update rotray\n\n* update convert logtic\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* use //2\n\n* update for pytest\n\n* change modular\n\n* for sglang support\n\n* Update modeling_glmasr.py\n\n* audioflamingo3: use _get_audio_token_length\n\n* clean modular\n\n* integration tests\n\n* Glmasr -> GlmAsr\n\n* audioflamingo3: audio len and default prompt as attributes\n\n* modular update\n\n* fix\n\n* udpate integration tests\n\n* use auto dtype\n\n* conversion script update\n\n* fix\n\n* correct test values\n\n* make\n\n* test update\n\n* fix\n\n* make\n\n* auto updates\n\n* doc update\n\n* update doc\n\n* fix\n\n* fix import\n\n* doc update\n\n* fix\n\n* fix\n\n* doc update\n\n* make style\n\n* update max_audio_len\n\n* update repo\n\n* make\n\n* make\n\n* make\n\n---------\n\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>",
    "sha": "a7f29523361b2cc12e51c1f5133d95f122f6f45c",
    "files": [
        {
            "sha": "670854e4895dc99dc3d79179b7b3c89693e5b245",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -1053,6 +1053,8 @@\n         title: Gemma3n\n       - local: model_doc/git\n         title: GIT\n+      - local: model_doc/glmasr\n+        title: GLM-ASR\n       - local: model_doc/glm46v\n         title: Glm46V\n       - local: model_doc/glm4v"
        },
        {
            "sha": "7e7c8adc1d1244f11908c37693154f9034ddfed5",
            "filename": "docs/source/en/model_doc/glmasr.md",
            "status": "added",
            "additions": 174,
            "deletions": 0,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/docs%2Fsource%2Fen%2Fmodel_doc%2Fglmasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/docs%2Fsource%2Fen%2Fmodel_doc%2Fglmasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglmasr.md?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -0,0 +1,174 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-15.*\n+\n+\n+# GlmAsr\n+\n+## Overview\n+\n+**GLM-ASR-Nano-2512** is a robust, open-source speech recognition model with **1.5B parameters**. Designed for\n+real-world complexity, it outperforms OpenAI Whisper V3 on multiple benchmarks while maintaining a compact size.\n+\n+Key capabilities include:\n+\n+* **Exceptional Dialect Support**\n+  Beyond standard Mandarin and English, the model is highly optimized for **Cantonese (ç²¤è¯­)** and other dialects,\n+  effectively bridging the gap in dialectal speech recognition.\n+\n+* **Low-Volume Speech Robustness**\n+  Specifically trained for **\"Whisper/Quiet Speech\"** scenarios. It captures and accurately transcribes extremely\n+  low-volume audio that traditional models often miss.\n+\n+* **SOTA Performance**\n+  Achieves the **lowest average error rate (4.10)** among comparable open-source models, showing significant advantages\n+  in Chinese benchmarks (Wenet Meeting, Aishell-1, etc..).\n+\n+This model was contributed by [Eustache Le Bihan](https://huggingface.co/eustlb) and [Yuxuan Zhang](https://huggingface.co/ZHANGYUXUAN-zR).\n+you can check the [model card](https://huggingface.co/zai-org/GLM-ASR-Nano-2512) for more details and our \n+[github repo](https://github.com/zai-org/GLM-ASR).\n+\n+## Usage\n+\n+### Basic usage\n+\n+<options id=\"usage\">\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+from transformers import AutoModelForSeq2SeqLM, AutoProcessor\n+\n+processor = AutoProcessor.from_pretrained(\"zai-org/GLM-ASR-Nano-2512\")\n+model = AutoModelForSeq2SeqLM.from_pretrained(\"zai-org/GLM-ASR-Nano-2512\", dtype=\"auto\", device_map=\"auto\")\n+\n+inputs = processor.apply_transcription_request(\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\")\n+\n+inputs = inputs.to(model.device, dtype=model.dtype)\n+outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1] :], skip_special_tokens=True)\n+print(decoded_outputs)\n+```\n+</hfoption>\n+</hfoptions>\n+\n+### Advanced usage\n+\n+The processor's `apply_transcription_request` is equivalent to using the chat template in the following manner:\n+\n+```py\n+from transformers import GlmAsrForConditionalGeneration, AutoProcessor\n+\n+processor = GlmAsrForConditionalGeneration.from_pretrained(\"zai-org/GLM-ASR-Nano-2512\")\n+\n+inputs = processor.apply_transcription_request(\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\")\n+\n+# which is equivalent to\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"audio\",\n+                \"url\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n+            },\n+            {\"type\": \"text\", \"text\": \"Please transcribe this audio into text\"},\n+        ],\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+)\n+```\n+\n+One can also use audio arrays directly:\n+\n+```py\n+from transformers import GlmAsrForConditionalGeneration, AutoProcessor\n+from datasets import load_dataset\n+\n+processor = AutoProcessor.from_pretrained(\"zai-org/GLM-ASR-Nano-2512\")\n+model = GlmAsrForConditionalGeneration.from_pretrained(\"zai-org/GLM-ASR-Nano-2512\", dtype=\"auto\", device_map=\"auto\")\n+\n+# loading audio directly from dataset\n+ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+audio_array = ds[0][\"audio\"][\"array\"]\n+\n+inputs = processor.apply_transcription_request(audio_array)\n+\n+inputs = inputs.to(model.device, dtype=model.dtype)\n+outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1] :], skip_special_tokens=True)\n+print(decoded_outputs)\n+```\n+\n+### Batched inference\n+\n+You can process multiple audio files at once:\n+\n+```py\n+from transformers import GlmAsrForConditionalGeneration, AutoProcessor\n+\n+processor = AutoProcessor.from_pretrained(\"zai-org/GLM-ASR-Nano-2512\")\n+model = GlmAsrForConditionalGeneration.from_pretrained(\"zai-org/GLM-ASR-Nano-2512\", dtype=\"auto\", device_map=\"auto\")\n+\n+inputs = processor.apply_transcription_request([\n+    \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n+    \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n+])\n+\n+inputs = inputs.to(model.device, dtype=model.dtype)\n+outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1] :], skip_special_tokens=True)\n+print(decoded_outputs)\n+```\n+\n+## GlmAsrEncoderConfig\n+\n+[[autodoc]] GlmAsrEncoderConfig\n+\n+## GlmAsrConfig\n+\n+[[autodoc]] GlmAsrConfig\n+\n+## GlmAsrPreTrainedModel\n+\n+[[autodoc]] GlmAsrPreTrainedModel\n+    - forward\n+\n+## GlmAsrProcessor\n+\n+[[autodoc]] GlmAsrProcessor\n+\n+## GlmAsrEncoder\n+\n+[[autodoc]] GlmAsrEncoder\n+    - forward\n+\n+## GlmAsrForConditionalGeneration\n+\n+[[autodoc]] GlmAsrForConditionalGeneration\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "342f366f3bc8dc878e3809634c5c572a09230416",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -152,6 +152,7 @@\n     from .glm4v import *\n     from .glm4v_moe import *\n     from .glm46v import *\n+    from .glmasr import *\n     from .glpn import *\n     from .got_ocr2 import *\n     from .gpt2 import *"
        },
        {
            "sha": "ceb619bd4ee840079a886f18b65d20da961fabd0",
            "filename": "src/transformers/models/audioflamingo3/processing_audioflamingo3.py",
            "status": "modified",
            "additions": 27,
            "deletions": 18,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -32,9 +32,6 @@\n \n logger = logging.get_logger(__name__)\n \n-MAX_AUDIO_LEN = 10 * 60  # 10 minutes\n-DEFAULT_TRANSCRIPTION_PROMPT = \"Transcribe the input speech.\"\n-\n \n class AudioFlamingo3ProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n@@ -63,15 +60,19 @@ class AudioFlamingo3Processor(ProcessorMixin):\n     [`Qwen2TokenizerFast`]. See the [`~AudioFlamingo3Processor.__call__`] for more information.\n \n     Args:\n-        feature_extractor ([`WhisperFeatureExtractor`]):\n-            The feature extractor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`]):\n-            The tokenizer is a required input.\n-        chat_template (`Optional[str]`, *optional*):\n-            The Jinja template to use for formatting the conversation. If not provided, the tokenizer's default chat\n-            template will be used.\n-        audio_token (`Optional[str]`, *optional*, defaults to `\"<sound>\"`):\n-            Special token used to represent audio inputs in the chat template.\n+            feature_extractor ([`WhisperFeatureExtractor`]):\n+                The feature extractor is a required input.\n+            tokenizer ([`Qwen2TokenizerFast`]):\n+                The tokenizer is a required input.\n+            chat_template (`Optional[str]`, *optional*):\n+                The Jinja template to use for formatting the conversation. If not provided, the tokenizer's default chat\n+                template will be used.\n+            audio_token (`Optional[str]`, *optional*, defaults to `\"<sound>\"`):\n+                Special token used to represent audio inputs in the chat template.\n+            default_transcription_prompt (`str`, *optional*, defaults to `\"Transcribe the input speech.\"`):\n+                Default prompt to use for transcription tasks when applying transcription requests.\n+            max_audio_len (`int`, *optional*, defaults to 600):\n+                Maximum length of audio sequences in seconds. Audio longer than this will be truncated.\n     \"\"\"\n \n     def __init__(\n@@ -80,11 +81,20 @@ def __init__(\n         tokenizer,\n         chat_template=None,\n         audio_token=\"<sound>\",\n+        default_transcription_prompt=\"Transcribe the input speech.\",\n+        max_audio_len=600,\n     ):\n         self.audio_token = audio_token\n         self.audio_token_id = tokenizer.convert_tokens_to_ids(audio_token)\n+        self.default_transcription_prompt = default_transcription_prompt\n+        self.max_audio_len = max_audio_len\n         super().__init__(feature_extractor, tokenizer, chat_template=chat_template)\n \n+    def _get_audio_token_length(self, audio_lengths: \"torch.Tensor\") -> \"torch.Tensor\":\n+        conv_output_lengths = (audio_lengths - 1) // 2 + 1  # After conv2 downsampling\n+        audio_tokens_lengths = (conv_output_lengths - 2) // 2 + 1  # After avg pooling\n+        return audio_tokens_lengths\n+\n     def __call__(\n         self,\n         text: Union[TextInput, list[TextInput]],\n@@ -139,7 +149,7 @@ def __call__(\n \n             # Determine number of chunks per sample, and flatten\n             window_size = int(audio_kwargs[\"sampling_rate\"] * audio_kwargs[\"chunk_length\"])\n-            max_windows = int(MAX_AUDIO_LEN // audio_kwargs[\"chunk_length\"])\n+            max_windows = int(self.max_audio_len // audio_kwargs[\"chunk_length\"])\n \n             per_sample_windows: list[int] = []\n             flat_chunks: list[np.ndarray] = []\n@@ -149,7 +159,7 @@ def __call__(\n                 n_win = max(1, (n_samples + window_size - 1) // window_size)\n                 if n_win > max_windows:\n                     logger.warning(\n-                        f\"Audio duration ({n_samples / audio_kwargs['sampling_rate']:.1f}s) exceeds {MAX_AUDIO_LEN}s; truncating to first {MAX_AUDIO_LEN}s.\"\n+                        f\"Audio duration ({n_samples / audio_kwargs['sampling_rate']:.1f}s) exceeds {self.max_audio_len}s; truncating to first {self.max_audio_len}s.\"\n                     )\n                     n_win = max_windows\n                 per_sample_windows.append(n_win)\n@@ -167,8 +177,7 @@ def __call__(\n \n             # Compute sequence lengths token counting\n             audio_lengths = torch.stack([s.sum() for s in torch.split(padding_mask.sum(-1), per_sample_windows)])\n-            conv_output_lengths = (audio_lengths - 1) // 2 + 1  # After conv2 downsampling\n-            audio_tokens_lengths = (conv_output_lengths - 2) // 2 + 1  # After avg pooling\n+            audio_tokens_lengths = self._get_audio_token_length(audio_lengths)\n \n             # expand audio tokens in text\n             for i, audio_length in enumerate(audio_tokens_lengths):\n@@ -232,7 +241,7 @@ def apply_transcription_request(\n             raise ValueError(\"`audio` must contain at least one sample.\")\n \n         if prompt is None:\n-            prompts = [DEFAULT_TRANSCRIPTION_PROMPT] * batch_size\n+            prompts = [self.default_transcription_prompt] * batch_size\n         elif isinstance(prompt, str):\n             prompts = [prompt] * batch_size\n         elif isinstance(prompt, (list, tuple)):\n@@ -243,7 +252,7 @@ def apply_transcription_request(\n             prompts = []\n             for item in prompt:\n                 if item is None:\n-                    prompts.append(DEFAULT_TRANSCRIPTION_PROMPT)\n+                    prompts.append(self.default_transcription_prompt)\n                 elif isinstance(item, str):\n                     prompts.append(item)\n                 else:"
        },
        {
            "sha": "55dd1b820073479935d39ae88e3078b913ab62f8",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -180,6 +180,8 @@\n         (\"glm4v_moe_vision\", \"Glm4vMoeVisionConfig\"),\n         (\"glm4v_text\", \"Glm4vTextConfig\"),\n         (\"glm4v_vision\", \"Glm4vVisionConfig\"),\n+        (\"glmasr\", \"GlmAsrConfig\"),\n+        (\"glmasr_encoder\", \"GlmAsrEncoderConfig\"),\n         (\"glpn\", \"GLPNConfig\"),\n         (\"got_ocr2\", \"GotOcr2Config\"),\n         (\"gpt-sw3\", \"GPT2Config\"),\n@@ -632,6 +634,8 @@\n         (\"glm4v_moe_vision\", \"Glm4vMoeVisionModel\"),\n         (\"glm4v_text\", \"GLM4V\"),\n         (\"glm4v_vision\", \"Glm4vVisionModel\"),\n+        (\"glmasr\", \"GLM-ASR\"),\n+        (\"glmasr_encoder\", \"GLM-ASR Encoder\"),\n         (\"glpn\", \"GLPN\"),\n         (\"got_ocr2\", \"GOT-OCR2\"),\n         (\"gpt-sw3\", \"GPT-Sw3\"),\n@@ -973,6 +977,7 @@\n         (\"glm4v_moe_vision\", \"glm4v_moe\"),\n         (\"glm4v_text\", \"glm4v\"),\n         (\"glm4v_moe_text\", \"glm4v_moe\"),\n+        (\"glmasr_encoder\", \"glmasr\"),\n         (\"grounding-dino\", \"grounding_dino\"),\n         (\"mm-grounding-dino\", \"mm_grounding_dino\"),\n         (\"idefics3_vision\", \"idefics3\"),"
        },
        {
            "sha": "b36fe91f720a2920b956862b92df7a57b6a69551",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -47,6 +47,7 @@\n         (\"dia\", \"DiaFeatureExtractor\"),\n         (\"encodec\", \"EncodecFeatureExtractor\"),\n         (\"gemma3n\", \"Gemma3nAudioFeatureExtractor\"),\n+        (\"glmasr\", \"WhisperFeatureExtractor\"),\n         (\"granite_speech\", \"GraniteSpeechFeatureExtractor\"),\n         (\"hubert\", \"Wav2Vec2FeatureExtractor\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextFeatureExtractor\"),"
        },
        {
            "sha": "8b151b68e1df070bac37dcaf6eb03b69ccf28f8d",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -183,6 +183,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"glm4v_moe_vision\", \"Glm4vMoeVisionModel\"),\n         (\"glm4v_text\", \"Glm4vTextModel\"),\n         (\"glm4v_vision\", \"Glm4vVisionModel\"),\n+        (\"glmasr\", \"GlmAsrForConditionalGeneration\"),\n+        (\"glmasr_encoder\", \"GlmAsrEncoder\"),\n         (\"glpn\", \"GLPNModel\"),\n         (\"got_ocr2\", \"GotOcr2Model\"),\n         (\"gpt-sw3\", \"GPT2Model\"),\n@@ -479,6 +481,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"fsmt\", \"FSMTForConditionalGeneration\"),\n         (\"funnel\", \"FunnelForPreTraining\"),\n         (\"gemma3\", \"Gemma3ForConditionalGeneration\"),\n+        (\"glmasr\", \"GlmAsrForConditionalGeneration\"),\n         (\"gpt-sw3\", \"GPT2LMHeadModel\"),\n         (\"gpt2\", \"GPT2LMHeadModel\"),\n         (\"gpt_bigcode\", \"GPTBigCodeForCausalLM\"),\n@@ -1062,6 +1065,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n MODEL_FOR_MULTIMODAL_LM_MAPPING_NAMES = OrderedDict(\n     [\n         *list(MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES.items()),\n+        (\"glmasr\", \"GlmAsrForConditionalGeneration\"),\n         (\"granite_speech\", \"GraniteSpeechForConditionalGeneration\"),\n         (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextForConditionalGeneration\"),\n         (\"phi4_multimodal\", \"Phi4MultimodalForCausalLM\"),\n@@ -1072,6 +1076,13 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n     ]\n )\n \n+MODEL_FOR_MASKED_LM_MAPPING_NAMES = OrderedDict(\n+    [\n+        # Model for Masked LM mapping\n+        (\"albert\", \"AlbertForMaskedLM\"),\n+    ]\n+)\n+\n MODEL_FOR_MASKED_LM_MAPPING_NAMES = OrderedDict(\n     [\n         # Model for Masked LM mapping\n@@ -1168,6 +1179,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"blenderbot-small\", \"BlenderbotSmallForConditionalGeneration\"),\n         (\"encoder-decoder\", \"EncoderDecoderModel\"),\n         (\"fsmt\", \"FSMTForConditionalGeneration\"),\n+        (\"glmasr\", \"GlmAsrForConditionalGeneration\"),\n         (\"granite_speech\", \"GraniteSpeechForConditionalGeneration\"),\n         (\"led\", \"LEDForConditionalGeneration\"),\n         (\"longt5\", \"LongT5ForConditionalGeneration\"),\n@@ -1193,6 +1205,12 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n     ]\n )\n \n+MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = OrderedDict(\n+    [\n+        (\"dia\", \"DiaForConditionalGeneration\"),\n+    ]\n+)\n+\n MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = OrderedDict(\n     [\n         (\"dia\", \"DiaForConditionalGeneration\"),"
        },
        {
            "sha": "3d2e6ef5cbc7822905e2db079fad52b2d629b6a4",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -79,6 +79,7 @@\n         (\"glm46v\", \"Glm46VProcessor\"),\n         (\"glm4v\", \"Glm4vProcessor\"),\n         (\"glm4v_moe\", \"Glm4vProcessor\"),\n+        (\"glmasr\", \"GlmAsrProcessor\"),\n         (\"got_ocr2\", \"GotOcr2Processor\"),\n         (\"granite_speech\", \"GraniteSpeechProcessor\"),\n         (\"grounding-dino\", \"GroundingDinoProcessor\"),"
        },
        {
            "sha": "171a9fca6868cc7b8dfd62ccbd494448b972de23",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -154,6 +154,7 @@\n         (\"glm4_moe\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"glm4v\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"glm4v_moe\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n+        (\"glmasr\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"got_ocr2\", \"TokenizersBackend\" if is_tokenizers_available() else None),\n         (\"gpt-sw3\", \"GPTSw3Tokenizer\" if is_sentencepiece_available() else None),\n         (\"gpt2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),"
        },
        {
            "sha": "832397015fb0de77fa550a595d2d4402415c9281",
            "filename": "src/transformers/models/glmasr/__init__.py",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglmasr%2F__init__.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -0,0 +1,30 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_glmasr import *\n+    from .modeling_glmasr import *\n+    from .processing_glmasr import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "132dd4ee4639e7ab28ffe08428c38ce6dd9d5ab6",
            "filename": "src/transformers/models/glmasr/configuration_glmasr.py",
            "status": "added",
            "additions": 197,
            "deletions": 0,
            "changes": 197,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fconfiguration_glmasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fconfiguration_glmasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglmasr%2Fconfiguration_glmasr.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -0,0 +1,197 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class GlmAsrEncoderConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`GlmAsrEncoder`]. It is used to instantiate a\n+    glmasr audio encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the audio encoder of the glmasr\n+    architecture.\n+\n+    e.g. [zai-org/GLM-ASR-Nano-2512](https://huggingface.co/zai-org/GLM-ASR-Nano-2512)\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1280):\n+            Dimensionality of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 5120):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 20):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler.\n+        max_position_embeddings (`int`, *optional*, defaults to 1500):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        num_mel_bins (`int`, *optional*, defaults to 128):\n+            Number of mel features used per input features. Should correspond to the value used in the\n+            `GlmAsrProcessor` class.\n+\n+    ```python\n+    >>> from transformers import GlmAsrEncoderConfig, GlmAsrEncoder\n+\n+    >>> # Initializing a GlmAsrEncoderConfig\n+    >>> configuration = GlmAsrEncoderConfig()\n+\n+    >>> # Initializing a GlmAsrEncoder (with random weights)\n+    >>> model = GlmAsrEncoder(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"glmasr_encoder\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1280,\n+        intermediate_size=5120,\n+        num_hidden_layers=32,\n+        num_attention_heads=20,\n+        num_key_value_heads=None,\n+        hidden_act=\"gelu\",\n+        max_position_embeddings=1500,\n+        initializer_range=0.02,\n+        rope_parameters=None,\n+        attention_dropout=0.0,\n+        num_mel_bins=128,\n+        **kwargs,\n+    ):\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.head_dim = hidden_size // num_attention_heads\n+        self.max_position_embeddings = max_position_embeddings\n+        self.rope_parameters = rope_parameters\n+        self.attention_dropout = attention_dropout\n+        self.num_mel_bins = num_mel_bins\n+\n+        kwargs.setdefault(\"partial_rotary_factor\", 0.5)\n+        super().__init__(**kwargs)\n+\n+\n+class GlmAsrConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`GlmAsrForConditionalGeneration`]. It is used to instantiate an\n+    glmasr model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the glmasr-Mini-3B.\n+\n+    e.g. [zai-org/GLM-ASR-Nano-2512](https://huggingface.co/zai-org/GLM-ASR-Nano-2512)\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        audio_config (`Union[AutoConfig, dict]`, *optional*):\n+            The config object or dictionary of the audio encoder.\n+        text_config (`Union[AutoConfig, dict]`, *optional*):\n+            The config object or dictionary of the text model.\n+        audio_token_id (`int`, *optional*, defaults to 59260):\n+            The audio token index to encode the audio prompt.\n+        projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function (function or string) in the multi-modal projector.\n+\n+    ```python\n+    >>> from transformers import GlmAsrForConditionalGeneration, GlmAsrConfig\n+\n+    >>> # Initializing a glmasr configuration\n+    >>> configuration = GlmAsrConfig()\n+\n+    >>> # Initializing a GLM-ASR-Nano-2512 model with random weights\n+    >>> model = GlmAsrForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"glmasr\"\n+    sub_configs = {\"text_config\": AutoConfig, \"audio_config\": AutoConfig}\n+\n+    _default_text_config_kwargs = {\n+        \"vocab_size\": 59264,\n+        \"hidden_size\": 2048,\n+        \"intermediate_size\": 6144,\n+        \"num_hidden_layers\": 28,\n+        \"num_attention_heads\": 16,\n+        \"num_key_value_heads\": 4,\n+        \"max_position_embeddings\": 8192,\n+        \"rms_norm_eps\": 1e-05,\n+        \"use_cache\": True,\n+        \"eos_token_id\": [59246, 59253, 59255],\n+        \"rope_parameters\": {\"rope_theta\": 10000.0, \"rope_type\": \"default\"},\n+    }\n+\n+    def __init__(\n+        self,\n+        audio_config=None,\n+        text_config=None,\n+        audio_token_id=59260,\n+        projector_hidden_act=\"gelu\",\n+        **kwargs,\n+    ):\n+        if isinstance(audio_config, dict):\n+            audio_config[\"model_type\"] = audio_config.get(\"model_type\", \"glmasr_encoder\")\n+            audio_config = CONFIG_MAPPING[audio_config[\"model_type\"]](**audio_config)\n+        elif audio_config is None:\n+            audio_config = CONFIG_MAPPING[\"glmasr_encoder\"]()\n+        self.audio_config = audio_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](\n+                **{**self._default_text_config_kwargs, **text_config}\n+            )\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"llama\"](**self._default_text_config_kwargs)\n+        self.text_config = text_config\n+\n+        self.vocab_size = text_config.vocab_size\n+        self.hidden_size = text_config.hidden_size\n+        self.audio_token_id = audio_token_id\n+        self.projector_hidden_act = projector_hidden_act\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"GlmAsrEncoderConfig\", \"GlmAsrConfig\"]"
        },
        {
            "sha": "5a26906321ca44d415a2444819f1011940347173",
            "filename": "src/transformers/models/glmasr/convert_glmasr_weights_to_hf.py",
            "status": "added",
            "additions": 182,
            "deletions": 0,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fconvert_glmasr_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fconvert_glmasr_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglmasr%2Fconvert_glmasr_weights_to_hf.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -0,0 +1,182 @@\n+import argparse\n+import re\n+\n+import torch\n+from safetensors.torch import load_file\n+\n+from transformers import (\n+    GlmAsrConfig,\n+    GlmAsrForConditionalGeneration,\n+    GlmAsrProcessor,\n+    TokenizersBackend,\n+    WhisperFeatureExtractor,\n+)\n+from transformers.utils.hub import cached_file\n+\n+\n+chat_template = \"\"\"{%- macro to_text(content) -%}\n+{%- if content is string -%}\n+{{- content -}}\n+{%- elif content is iterable and content is not mapping -%}\n+{%- for item in content -%}\n+{%- if item is mapping and item.type == 'text' and item.text is defined -%}\n+{{- item.text -}}\n+{%- elif item is mapping and (item.type == 'audio' or 'audio' in item) -%}\n+<|begin_of_audio|><|pad|><|end_of_audio|><|user|>\n+{% elif item is string -%}\n+{{- item -}}\n+{%- endif -%}\n+{%- endfor -%}\n+{%- else -%}\n+{{- content -}}\n+{%- endif -%}\n+{%- endmacro -%}\n+{%- for m in messages -%}\n+{%- if m.role == 'system' -%}\n+<|system|>\n+{{ to_text(m.content) | trim }}\n+{%- elif m.role == 'user' -%}\n+<|user|>\n+{{ to_text(m.content) | trim }}\n+{%- elif m.role == 'assistant' -%}\n+<|assistant|>\n+{{ to_text(m.content) | trim }}\n+{%- endif -%}\n+{%- endfor -%}\n+{%- if add_generation_prompt -%}\n+<|assistant|>\n+{% endif -%}\"\"\"\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"^model\\.norm\\.weight$\":                                      r\"language_model.model.norm.weight\",\n+    r\"^model\\.(.*)$\":                                              r\"language_model.model.\\1\",\n+    r\"^lm_head\\.(.*)$\":                                            r\"language_model.lm_head.\\1\",\n+    r\"^audio_encoder\\.adapting\\.0\\.(.*)$\":                         r\"multi_modal_projector.linear_1.\\1\",\n+    r\"^audio_encoder\\.adapting\\.2\\.(.*)$\":                         r\"multi_modal_projector.linear_2.\\1\",\n+    r\"^audio_encoder\\.proj\\.(weight|bias)$\":                       r\"multi_modal_projector.\\1\",\n+    r\"^audio_encoder\\.whisper\\.(.*)$\":                             r\"audio_tower.\\1\",\n+    r\"^audio_encoder\\.layer_norm\\.(.*)$\":                          r\"audio_tower.norm.\\1\",\n+    r\"^audio_tower\\.layers\\.(\\d+)\\.self_attn\\.out_proj\\.(.*)$\":    r\"audio_tower.layers.\\1.self_attn.o_proj.\\2\",\n+    r\"^audio_tower\\.layers\\.(\\d+)\\.self_attn_layer_norm\\.(.*)$\":   r\"audio_tower.layers.\\1.input_layernorm.\\2\",\n+    r\"^audio_tower\\.layers\\.(\\d+)\\.final_layer_norm\\.(.*)$\":       r\"audio_tower.layers.\\1.post_attention_layernorm.\\2\",\n+    r\"^audio_tower\\.layers\\.(\\d+)\\.(fc[12])\\.(.*)$\":               r\"audio_tower.layers.\\1.mlp.\\2.\\3\",\n+}\n+# fmt: on\n+\n+\n+def permute_rope(tensor, config):\n+    # IMPORTANT: the original checkpoint applies partial rope (half dimension) in the interleaved manner\n+    # since we use a different rope implementation, we want to permute the order like:\n+    # original order: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n+    # permuted order: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n+\n+    if tensor.dim() == 2:\n+        dim1, dim2 = tensor.shape\n+    else:\n+        dim1 = tensor.shape[0]\n+\n+    n_heads = config.audio_config.num_attention_heads\n+    head_dim = config.audio_config.head_dim\n+    rope_dim = dim1 // 2\n+\n+    rope_indices = torch.arange(rope_dim)\n+    rope_indices = rope_indices.view(n_heads, rope_dim // n_heads // 2, 2)\n+    rope_indices = rope_indices.transpose(1, 2)\n+    rope_indices = rope_indices.reshape(n_heads, -1)\n+\n+    non_rope_start = head_dim // 2\n+    non_rope_indices = torch.arange(non_rope_start, head_dim, dtype=torch.long)\n+    non_rope_indices = non_rope_indices.expand(n_heads, -1)\n+\n+    head_offsets = torch.arange(n_heads, dtype=torch.long)[:, None] * (head_dim // 2)\n+    non_rope_indices = non_rope_indices + head_offsets.expand(n_heads, head_dim // 2)\n+\n+    combined_indices = torch.cat([rope_indices, non_rope_indices], dim=1)\n+    global_head_offsets = torch.arange(n_heads, dtype=torch.long)[:, None] * (head_dim // 2)\n+    combined_indices = combined_indices + global_head_offsets.expand(n_heads, head_dim)\n+\n+    permutation_indices = combined_indices.reshape(-1)\n+    tensor = tensor[permutation_indices]\n+\n+    return tensor\n+\n+\n+def convert_key(key, mapping):\n+    for pattern, replacement in mapping.items():\n+        key = re.sub(pattern, replacement, key)\n+    return key\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Convert GLM-ASR model weights to Hugging Face format\")\n+    parser.add_argument(\n+        \"--input_path_or_repo\",\n+        type=str,\n+        default=\"zai-org/GLM-ASR-Nano-2512\",\n+        help=\"Path to input model file or Hugging Face repository ID\",\n+    )\n+    parser.add_argument(\n+        \"--revision\",\n+        type=str,\n+        default=\"91967eab799804ab256a3819a085b92378906eb2\",\n+        help=\"Revision of the input repository\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        type=str,\n+        default=None,\n+        help=\"Output directory to save the converted model and processor\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        type=str,\n+        default=None,\n+        help=\"Repository ID to push the model and processor to Hub (if not provided, won't push)\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    path = cached_file(args.input_path_or_repo, \"model.safetensors\", revision=args.revision)\n+    state_dict = load_file(path)\n+\n+    config = GlmAsrConfig()\n+    model = GlmAsrForConditionalGeneration(config)\n+\n+    new_state_dict = {}\n+    for k, v in state_dict.items():\n+        new_key = convert_key(k, ORIGINAL_TO_CONVERTED_KEY_MAPPING)\n+\n+        # those are not used\n+        if new_key in [\n+            \"audio_encoder.audio_bos_eos_token.weight\",  # already present in the emb\n+            \"audio_tower.embed_positions.weight\",\n+            \"multi_modal_projector.bias\",\n+            \"multi_modal_projector.weight\",\n+        ]:\n+            continue\n+\n+        if \"audio_tower\" in new_key and (\"q_proj\" in new_key or \"k_proj\" in new_key):\n+            v = permute_rope(v, config)\n+\n+        new_state_dict[new_key] = v\n+\n+    model.load_state_dict(new_state_dict, strict=True, assign=True)\n+\n+    feature_extractor = WhisperFeatureExtractor(feature_size=128)\n+    tokenizer = TokenizersBackend.from_pretrained(args.input_path_or_repo, revision=args.revision)\n+    tokenizer.pad_token = tokenizer.eos_token\n+\n+    processor = GlmAsrProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer, chat_template=chat_template)\n+\n+    if args.output_dir:\n+        model.save_pretrained(args.output_dir)\n+        processor.save_pretrained(args.output_dir)\n+\n+    if args.push_to_hub:\n+        model.push_to_hub(args.push_to_hub)\n+        processor.push_to_hub(args.push_to_hub)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "121f13cc91b72a08b455113a56ecc258760ac97b",
            "filename": "src/transformers/models/glmasr/modeling_glmasr.py",
            "status": "added",
            "additions": 512,
            "deletions": 0,
            "changes": 512,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fmodeling_glmasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fmodeling_glmasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglmasr%2Fmodeling_glmasr.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -0,0 +1,512 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/glmasr/modular_glmasr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_glmasr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernelized_func\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_available\n+from ...utils.generic import check_model_inputs, maybe_autocast\n+from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_glmasr import GlmAsrConfig, GlmAsrEncoderConfig\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+\n+class GlmAsrRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: GlmAsrConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.register_buffer(\"original_inv_freq\", inv_freq.clone(), persistent=False)\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[GlmAsrConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        partial_rotary_factor = config.rope_parameters.get(\"partial_rotary_factor\", 1.0)\n+        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+        dim = int(head_dim * partial_rotary_factor)\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    rotary_dim = cos.shape[-1]\n+    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n+    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n+\n+    # Apply rotary embeddings on the first half or full tensor\n+    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)\n+    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)\n+\n+    # Concatenate back to full shape\n+    q_embed = torch.cat([q_embed, q_pass], dim=-1)\n+    k_embed = torch.cat([k_embed, k_pass], dim=-1)\n+    return q_embed, k_embed\n+\n+\n+@use_kernelized_func(apply_rotary_pos_emb)\n+class GlmAsrAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: GlmAsrConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = False\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=True)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=None,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class GlmAsrMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.act_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class GlmAsrEncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: GlmAsrConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = GlmAsrAttention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = GlmAsrMLP(config)\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class GlmAsrPreTrainedModel(PreTrainedModel):\n+    config: GlmAsrConfig\n+    base_model_prefix = \"model\"\n+    input_modalities = (\"audio\", \"text\")\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"GlmAsrAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+\n+# TODO: @eustlb, this is what WhisperEncoder should look like\n+class GlmAsrEncoder(GlmAsrPreTrainedModel):\n+    config: GlmAsrEncoderConfig\n+    main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n+    _no_split_modules = [\"GlmAsrEncoderLayer\"]\n+\n+    def __init__(self, config: GlmAsrEncoderConfig):\n+        super().__init__(config)\n+        self.conv1 = nn.Conv1d(config.num_mel_bins, config.hidden_size, kernel_size=3, padding=1)\n+        self.conv2 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=3, stride=2, padding=1)\n+\n+        self.layers = nn.ModuleList(\n+            [GlmAsrEncoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = nn.LayerNorm(config.hidden_size)\n+        self.rotary_emb = GlmAsrRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(self, input_features, **kwargs: Unpack[TransformersKwargs]):\n+        inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n+        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n+        inputs_embeds = inputs_embeds.transpose(1, 2)\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(\n+            hidden_states, position_ids=torch.arange(hidden_states.shape[1], device=hidden_states.device)[None, :]\n+        )\n+\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(hidden_states, position_embeddings=position_embeddings, **kwargs)\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+class GlmAsrMultiModalProjector(nn.Module):\n+    \"\"\"\n+    Audio adaptor (small MLP) that projects GlmAsrEncoder features\n+    to the LLM embedding space so they can replace `<sound>` tokens.\n+    \"\"\"\n+\n+    def __init__(self, config: GlmAsrConfig):\n+        super().__init__()\n+        self.linear_1 = nn.Linear(config.audio_config.intermediate_size, config.text_config.hidden_size * 2)\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(config.text_config.hidden_size * 2, config.text_config.hidden_size)\n+\n+    def forward(self, audio_features):\n+        hidden_states = self.linear_1(audio_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The GlmAsr model which consists of a fine-tuned Whisper encoder, a multi-modal projector and a Llama language model.\n+    \"\"\"\n+)\n+class GlmAsrForConditionalGeneration(GlmAsrPreTrainedModel, GenerationMixin):\n+    _keep_in_fp32_modules_strict = None\n+    _tp_plan = None\n+    _pp_plan = None\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.audio_tower = AutoModel.from_config(config.audio_config)\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        self.multi_modal_projector = GlmAsrMultiModalProjector(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_audio_features(\n+        self, input_features: torch.FloatTensor, input_features_mask: torch.Tensor\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        This method is used to get the audio embeddings from input features (a log mel spectrogram), meaning inferring the audio encoder and the multi-modal projector.\n+        Args:\n+            input_features (`torch.FloatTensor`):\n+                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n+                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            input_features_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n+                Mask to avoid performing attention on padded feature indices.\n+\n+        Returns:\n+            `torch.FloatTensor`:\n+                The audio embeddings.\n+        \"\"\"\n+        audio_outputs = self.audio_tower(input_features)\n+        audio_hidden_states = audio_outputs.last_hidden_state\n+        audio_hidden_states = audio_hidden_states.reshape(\n+            input_features.shape[0], -1, self.config.audio_config.intermediate_size\n+        )\n+        audio_embeds = self.multi_modal_projector(audio_hidden_states)\n+\n+        audio_lengths = input_features_mask.sum(-1)\n+        for padding, kernel_size, stride in [(1, 3, 1), (1, 3, 2)]:\n+            audio_lengths = (audio_lengths + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n+        merge_factor = 4\n+        post_lengths = (audio_lengths - merge_factor) // merge_factor + 1\n+\n+        valid_mask = torch.arange(audio_embeds.shape[1], device=post_lengths.device)[None, :] < post_lengths[:, None]\n+        audio_embeds = audio_embeds[valid_mask.to(audio_embeds.device)]\n+        return audio_embeds\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n+        input_features_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        input_features_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n+            Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import GlmAsrForConditionalGeneration, AutoProcessor\n+\n+        >>> model_id = \"zai-org/GLM-ASR-Nano-2512\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = GlmAsrForConditionalGeneration.from_pretrained(model_id, dtype=\"auto\", device_map=\"auto\")\n+        >>> inputs = processor.apply_transcription_request(\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\")\n+\n+        >>> inputs = inputs.to(model.device, dtype=model.dtype)\n+\n+        >>> outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+\n+        >>> decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1] :], skip_special_tokens=True)\n+        >>> print(decoded_outputs)\n+        ```\"\"\"\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if input_features is not None and input_ids is not None:\n+            audio_embeds = self.get_audio_features(input_features, input_features_mask)\n+\n+            # replace text-audio token placeholders with audio embeddings\n+            audio_token_mask = (input_ids == self.config.audio_token_id).unsqueeze(-1)\n+            inputs_embeds = inputs_embeds.masked_scatter(\n+                audio_token_mask.to(inputs_embeds.device), audio_embeds.to(inputs_embeds.device)\n+            )\n+\n+        outputs: CausalLMOutputWithPast = self.language_model(\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            labels=labels,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+        return outputs\n+\n+    def prepare_inputs_for_generation(self, *args, **kwargs):\n+        # Overwritten -- we should not pass input_features when we are in cached decoding stage\n+\n+        input_features = kwargs.pop(\"input_features\", None)\n+        input_features_mask = kwargs.pop(\"input_features_mask\", None)\n+        cache_position = kwargs.get(\"cache_position\")\n+\n+        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n+\n+        if cache_position is not None and cache_position[0] == 0:\n+            # input_features should only be passed when we are not in cached decoding stage\n+            if input_features is not None:\n+                model_inputs[\"input_features\"] = input_features\n+            if input_features_mask is not None:\n+                model_inputs[\"input_features_mask\"] = input_features_mask\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"GlmAsrEncoder\", \"GlmAsrForConditionalGeneration\", \"GlmAsrPreTrainedModel\"]"
        },
        {
            "sha": "35fe2986b2a15a93453747a572caecbfb6bd7c27",
            "filename": "src/transformers/models/glmasr/modular_glmasr.py",
            "status": "added",
            "additions": 433,
            "deletions": 0,
            "changes": 433,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fmodular_glmasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fmodular_glmasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglmasr%2Fmodular_glmasr.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -0,0 +1,433 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...activations import ACT2FN\n+from ...audio_utils import AudioInput, make_list_of_audio\n+from ...cache_utils import Cache\n+from ...feature_extraction_utils import BatchFeature\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, CausalLMOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_available, logging\n+from ...utils.generic import check_model_inputs\n+from ..audioflamingo3.modeling_audioflamingo3 import (\n+    AudioFlamingo3ForConditionalGeneration,\n+    AudioFlamingo3MultiModalProjector,\n+    AudioFlamingo3PreTrainedModel,\n+)\n+from ..audioflamingo3.processing_audioflamingo3 import AudioFlamingo3Processor, AudioFlamingo3ProcessorKwargs\n+from ..glm.modeling_glm import GlmRotaryEmbedding\n+from ..llama.modeling_llama import LlamaAttention, eager_attention_forward, rotate_half\n+from .configuration_glmasr import GlmAsrConfig, GlmAsrEncoderConfig\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class GlmAsrProcessorKwargs(AudioFlamingo3ProcessorKwargs): ...\n+\n+\n+class GlmAsrProcessor(AudioFlamingo3Processor):\n+    r\"\"\"\n+    Constructs an GlmAsr processor which wraps an GlmAsr feature extractor and an GlmAsr\n+    tokenizer into a single processor.\n+\n+    [`GlmAsrProcessor`] offers all the functionalities of [`WhisperFeatureExtractor`] and\n+    [`Qwen2TokenizerFast`]. See the [`~GlmAsrProcessor.__call__`] for more information.\n+\n+    Args:\n+            feature_extractor ([`WhisperFeatureExtractor`]):\n+                The feature extractor is a required input.\n+            tokenizer ([`Qwen2TokenizerFast`]):\n+                The tokenizer is a required input.\n+            chat_template (`Optional[str]`, *optional*):\n+                The Jinja template to use for formatting the conversation. If not provided, the tokenizer's default chat\n+                template will be used.\n+            audio_token (`Optional[str]`, *optional*, defaults to `\"<|pad|>`\"):\n+                Special token used to represent audio inputs in the chat template.\n+            default_transcription_prompt (`str`, *optional*, defaults to `\"Please transcribe this audio into text\"`):\n+                Default prompt to use for transcription tasks when applying transcription requests.\n+            max_audio_len (`int`, *optional*, defaults to 655):\n+                Maximum length of audio sequences in seconds. Audio longer than this will be truncated.\n+                655 gives approximately 8192 tokens, corresponding to the maximum sequence length of the text model.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        feature_extractor,\n+        tokenizer,\n+        chat_template=None,\n+        audio_token=\"<|pad|>\",\n+        default_transcription_prompt=\"Please transcribe this audio into text\",\n+        max_audio_len=655,\n+    ):\n+        super().__init__(\n+            feature_extractor,\n+            tokenizer,\n+            chat_template=chat_template,\n+            audio_token=audio_token,\n+            default_transcription_prompt=default_transcription_prompt,\n+            max_audio_len=max_audio_len,\n+        )\n+\n+    def _get_audio_token_length(self, audio_lengths: \"torch.Tensor\") -> \"torch.Tensor\":\n+        merge_factor = 4\n+        for padding, kernel_size, stride in [(1, 3, 1), (1, 3, 2)]:\n+            audio_lengths = (audio_lengths + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n+\n+        num_tokens = (audio_lengths - merge_factor) // merge_factor + 1\n+        return num_tokens\n+\n+    def apply_transcription_request(\n+        self,\n+        audio: Union[str, list[str], AudioInput],\n+        prompt: Optional[Union[str, list[str]]] = None,\n+        **kwargs: Unpack[GlmAsrProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Prepare inputs for automatic speech recognition without manually writing the default transcription prompt.\n+\n+        Args:\n+            audio (`str`, `list[str]`, `np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                Audio to transcribe. Strings are interpreted as local paths or URLs and will be loaded automatically by\n+                the chat template loader; NumPy arrays and PyTorch tensors are forwarded directly.\n+            prompt (`str` or `list[str]`, *optional*):\n+                Custom prompt(s) to include in the user turn. A list must be the same length as the batch. When `None`,\n+                each sample uses `\"Transcribe the input speech.\"`.\n+            **kwargs:\n+                Additional keyword arguments forwarded to [`~AudioFlamingo3Processor.apply_chat_template`] (for example\n+                `text_kwargs`, `audio_kwargs`, ...).\n+\n+        Returns:\n+            [`BatchFeature`]: Processor outputs ready to be passed to [`AudioFlamingo3ForConditionalGeneration.generate`].\n+\n+        \"\"\"\n+\n+        if isinstance(audio, str):\n+            audio_items: list[Union[str, np.ndarray]] = [audio]\n+        elif isinstance(audio, (list, tuple)) and audio and all(isinstance(el, str) for el in audio):\n+            audio_items = list(audio)\n+        else:\n+            audio_items = list(make_list_of_audio(audio))\n+            if is_torch_available():\n+                audio_items = [el.detach().cpu().numpy() if isinstance(el, torch.Tensor) else el for el in audio_items]\n+\n+        batch_size = len(audio_items)\n+        if batch_size == 0:\n+            raise ValueError(\"`audio` must contain at least one sample.\")\n+\n+        if prompt is None:\n+            prompts = [self.default_transcription_prompt] * batch_size\n+        elif isinstance(prompt, str):\n+            prompts = [prompt] * batch_size\n+        elif isinstance(prompt, (list, tuple)):\n+            if len(prompt) != batch_size:\n+                raise ValueError(\n+                    f\"Received {len(prompt)} prompt(s) for {batch_size} audio sample(s); counts must match.\"\n+                )\n+            prompts = []\n+            for item in prompt:\n+                if item is None:\n+                    prompts.append(self.default_transcription_prompt)\n+                elif isinstance(item, str):\n+                    prompts.append(item)\n+                else:\n+                    raise TypeError(\"Each prompt must be a string or `None`.\")\n+        else:\n+            raise TypeError(\"`prompt` must be a string, a sequence of strings, or `None`.\")\n+\n+        conversations = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"audio\", \"path\": audio_item}\n+                        if isinstance(audio_item, str)\n+                        else {\"type\": \"audio\", \"audio\": audio_item},\n+                        {\"type\": \"text\", \"text\": prompt_text},\n+                    ],\n+                }\n+            ]\n+            for prompt_text, audio_item in zip(prompts, audio_items)\n+        ]\n+\n+        return self.apply_chat_template(\n+            conversations,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+\n+class GlmAsrRotaryEmbedding(GlmRotaryEmbedding): ...\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    rotary_dim = cos.shape[-1]\n+    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n+    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n+\n+    # Apply rotary embeddings on the first half or full tensor\n+    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)\n+    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)\n+\n+    # Concatenate back to full shape\n+    q_embed = torch.cat([q_embed, q_pass], dim=-1)\n+    k_embed = torch.cat([k_embed, k_pass], dim=-1)\n+    return q_embed, k_embed\n+\n+\n+class GlmAsrAttention(LlamaAttention):\n+    def __init__(self, config: GlmAsrConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.is_causal = False\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=True)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=True)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=None,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class GlmAsrMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.act_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class GlmAsrEncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: GlmAsrConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = GlmAsrAttention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = GlmAsrMLP(config)\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+class GlmAsrPreTrainedModel(AudioFlamingo3PreTrainedModel): ...\n+\n+\n+# TODO: @eustlb, this is what WhisperEncoder should look like\n+class GlmAsrEncoder(GlmAsrPreTrainedModel):\n+    config: GlmAsrEncoderConfig\n+    main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n+    _no_split_modules = [\"GlmAsrEncoderLayer\"]\n+\n+    def __init__(self, config: GlmAsrEncoderConfig):\n+        super().__init__(config)\n+        self.conv1 = nn.Conv1d(config.num_mel_bins, config.hidden_size, kernel_size=3, padding=1)\n+        self.conv2 = nn.Conv1d(config.hidden_size, config.hidden_size, kernel_size=3, stride=2, padding=1)\n+\n+        self.layers = nn.ModuleList(\n+            [GlmAsrEncoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = nn.LayerNorm(config.hidden_size)\n+        self.rotary_emb = GlmAsrRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(self, input_features, **kwargs: Unpack[TransformersKwargs]):\n+        inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n+        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n+        inputs_embeds = inputs_embeds.transpose(1, 2)\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(\n+            hidden_states, position_ids=torch.arange(hidden_states.shape[1], device=hidden_states.device)[None, :]\n+        )\n+\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(hidden_states, position_embeddings=position_embeddings, **kwargs)\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+class GlmAsrMultiModalProjector(AudioFlamingo3MultiModalProjector):\n+    def __init__(self, config: GlmAsrConfig):\n+        super().__init__()\n+        self.linear_1 = nn.Linear(config.audio_config.intermediate_size, config.text_config.hidden_size * 2)\n+        self.linear_2 = nn.Linear(config.text_config.hidden_size * 2, config.text_config.hidden_size)\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The GlmAsr model which consists of a fine-tuned Whisper encoder, a multi-modal projector and a Llama language model.\n+    \"\"\"\n+)\n+class GlmAsrForConditionalGeneration(AudioFlamingo3ForConditionalGeneration):\n+    def get_audio_features(\n+        self, input_features: torch.FloatTensor, input_features_mask: torch.Tensor\n+    ) -> torch.FloatTensor:\n+        audio_outputs = self.audio_tower(input_features)\n+        audio_hidden_states = audio_outputs.last_hidden_state\n+        audio_hidden_states = audio_hidden_states.reshape(\n+            input_features.shape[0], -1, self.config.audio_config.intermediate_size\n+        )\n+        audio_embeds = self.multi_modal_projector(audio_hidden_states)\n+\n+        audio_lengths = input_features_mask.sum(-1)\n+        for padding, kernel_size, stride in [(1, 3, 1), (1, 3, 2)]:\n+            audio_lengths = (audio_lengths + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n+        merge_factor = 4\n+        post_lengths = (audio_lengths - merge_factor) // merge_factor + 1\n+\n+        valid_mask = torch.arange(audio_embeds.shape[1], device=post_lengths.device)[None, :] < post_lengths[:, None]\n+        audio_embeds = audio_embeds[valid_mask.to(audio_embeds.device)]\n+        return audio_embeds\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n+        input_features_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        input_features_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n+            Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import GlmAsrForConditionalGeneration, AutoProcessor\n+\n+        >>> model_id = \"zai-org/GLM-ASR-Nano-2512\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = GlmAsrForConditionalGeneration.from_pretrained(model_id, dtype=\"auto\", device_map=\"auto\")\n+        >>> inputs = processor.apply_transcription_request(\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\")\n+\n+        >>> inputs = inputs.to(model.device, dtype=model.dtype)\n+\n+        >>> outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+\n+        >>> decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1] :], skip_special_tokens=True)\n+        >>> print(decoded_outputs)\n+        ```\"\"\"\n+        return super().forward(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            labels=labels,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"GlmAsrEncoder\", \"GlmAsrForConditionalGeneration\", \"GlmAsrProcessor\", \"GlmAsrPreTrainedModel\"]"
        },
        {
            "sha": "7f60a21fc4f062bdfe94797b6fd52da728bfd913",
            "filename": "src/transformers/models/glmasr/processing_glmasr.py",
            "status": "added",
            "additions": 332,
            "deletions": 0,
            "changes": 332,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fprocessing_glmasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Fmodels%2Fglmasr%2Fprocessing_glmasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglmasr%2Fprocessing_glmasr.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -0,0 +1,332 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/glmasr/modular_glmasr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_glmasr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import re\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...audio_utils import AudioInput, make_list_of_audio\n+from ...feature_extraction_utils import BatchFeature\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import TextInput\n+from ...utils import is_torch_available, logging\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class GlmAsrProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": True,\n+        },\n+        \"audio_kwargs\": {\n+            \"sampling_rate\": 16000,\n+            \"chunk_length\": 30.0,\n+            \"return_attention_mask\": True,\n+            \"padding\": \"max_length\",\n+        },\n+        \"common_kwargs\": {\n+            \"return_tensors\": \"pt\",\n+            \"padding_side\": \"left\",\n+        },\n+    }\n+\n+\n+class GlmAsrProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs an GlmAsr processor which wraps an GlmAsr feature extractor and an GlmAsr\n+    tokenizer into a single processor.\n+\n+    [`GlmAsrProcessor`] offers all the functionalities of [`WhisperFeatureExtractor`] and\n+    [`Qwen2TokenizerFast`]. See the [`~GlmAsrProcessor.__call__`] for more information.\n+\n+    Args:\n+            feature_extractor ([`WhisperFeatureExtractor`]):\n+                The feature extractor is a required input.\n+            tokenizer ([`Qwen2TokenizerFast`]):\n+                The tokenizer is a required input.\n+            chat_template (`Optional[str]`, *optional*):\n+                The Jinja template to use for formatting the conversation. If not provided, the tokenizer's default chat\n+                template will be used.\n+            audio_token (`Optional[str]`, *optional*, defaults to `\"<|pad|>`\"):\n+                Special token used to represent audio inputs in the chat template.\n+            default_transcription_prompt (`str`, *optional*, defaults to `\"Please transcribe this audio into text\"`):\n+                Default prompt to use for transcription tasks when applying transcription requests.\n+            max_audio_len (`int`, *optional*, defaults to 655):\n+                Maximum length of audio sequences in seconds. Audio longer than this will be truncated.\n+                655 gives approximately 8192 tokens, corresponding to the maximum sequence length of the text model.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        feature_extractor,\n+        tokenizer,\n+        chat_template=None,\n+        audio_token=\"<|pad|>\",\n+        default_transcription_prompt=\"Please transcribe this audio into text\",\n+        max_audio_len=655,\n+    ):\n+        self.audio_token = audio_token\n+        self.audio_token_id = tokenizer.convert_tokens_to_ids(audio_token)\n+        self.default_transcription_prompt = default_transcription_prompt\n+        self.max_audio_len = max_audio_len\n+        super().__init__(feature_extractor, tokenizer, chat_template=chat_template)\n+\n+    def _get_audio_token_length(self, audio_lengths: \"torch.Tensor\") -> \"torch.Tensor\":\n+        merge_factor = 4\n+        for padding, kernel_size, stride in [(1, 3, 1), (1, 3, 2)]:\n+            audio_lengths = (audio_lengths + 2 * padding - (kernel_size - 1) - 1) // stride + 1\n+\n+        num_tokens = (audio_lengths - merge_factor) // merge_factor + 1\n+        return num_tokens\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, list[TextInput]],\n+        audio: Optional[AudioInput] = None,\n+        output_labels: Optional[bool] = False,\n+        **kwargs: Unpack[GlmAsrProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        Main method to prepare one or several text sequence(s) and audio waveform(s) for the model. This\n+        method expands `<sound>` placeholders in the text based on the post-pool frame counts of the\n+        audio windows, then tokenizes the provided strings as-is, and extracts log-mel features\n+        with [`WhisperFeatureExtractor`]. If `audio` is `None`, no audio processing is performed and\n+        the text is tokenized as-is (LM-only behavior).\n+\n+        Args:\n+            text (`str` or `list[str]`):\n+                Input sequence or batch of sequences.\n+            audio (`np.ndarray` or `list[np.ndarray]`):\n+                Input audio or batch of audios as NumPy arrays. If provided, there must be as many `text` inputs as\n+                `audio` inputs.\n+            output_labels (bool, *optional*, default=False):\n+                Whether to return labels for training.\n+\n+        Returns:\n+            [`BatchFeature`]: A dictionary with tokenized text (`input_ids`, `attention_mask`) and\n+            audio features (`input_features`, `input_features_mask`).\n+        \"\"\"\n+\n+        # Merge defaults with user kwargs\n+        call_kwargs = self._merge_kwargs(\n+            GlmAsrProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        text_kwargs = call_kwargs[\"text_kwargs\"]\n+        audio_kwargs = call_kwargs[\"audio_kwargs\"]\n+        return_tensors = text_kwargs.get(\"return_tensors\")\n+        if return_tensors != \"pt\":\n+            raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not (isinstance(text, (list, tuple)) and all(isinstance(t, str) for t in text)):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        audio_inputs = {}\n+        if audio is not None:\n+            audio = make_list_of_audio(audio)\n+            if len(text) != len(audio):\n+                raise ValueError(f\"Got {len(text)} text but {len(audio)} audios; they must match 1:1.\")\n+\n+            # Determine number of chunks per sample, and flatten\n+            window_size = int(audio_kwargs[\"sampling_rate\"] * audio_kwargs[\"chunk_length\"])\n+            max_windows = int(self.max_audio_len // audio_kwargs[\"chunk_length\"])\n+\n+            per_sample_windows: list[int] = []\n+            flat_chunks: list[np.ndarray] = []\n+\n+            for audio_el in audio:\n+                n_samples = int(audio_el.shape[0])\n+                n_win = max(1, (n_samples + window_size - 1) // window_size)\n+                if n_win > max_windows:\n+                    logger.warning(\n+                        f\"Audio duration ({n_samples / audio_kwargs['sampling_rate']:.1f}s) exceeds {self.max_audio_len}s; truncating to first {self.max_audio_len}s.\"\n+                    )\n+                    n_win = max_windows\n+                per_sample_windows.append(n_win)\n+\n+                time_cap = min(n_samples, n_win * window_size)\n+                for i in range(n_win):\n+                    start = i * window_size\n+                    end = min((i + 1) * window_size, time_cap)\n+                    flat_chunks.append(audio_el[start:end])\n+\n+            # Feature extraction\n+            audio_inputs = self.feature_extractor(flat_chunks, **audio_kwargs)\n+            padding_mask = audio_inputs.pop(\"attention_mask\")\n+            audio_inputs[\"input_features_mask\"] = padding_mask\n+\n+            # Compute sequence lengths token counting\n+            audio_lengths = torch.stack([s.sum() for s in torch.split(padding_mask.sum(-1), per_sample_windows)])\n+            audio_tokens_lengths = self._get_audio_token_length(audio_lengths)\n+\n+            # expand audio tokens in text\n+            for i, audio_length in enumerate(audio_tokens_lengths):\n+                expanded = re.sub(re.escape(self.audio_token), self.audio_token * audio_length, text[i])\n+                text[i] = expanded\n+\n+        # Tokenize\n+        text_inputs = self.tokenizer(text, **text_kwargs)\n+\n+        data = {**text_inputs, **audio_inputs}\n+        if output_labels:\n+            labels = data[\"input_ids\"].clone()\n+            labels[labels == self.audio_token_id] = -100\n+            labels[labels == self.tokenizer.pad_token_id] = -100\n+            data[\"labels\"] = labels\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    @property\n+    def model_input_names(self) -> list[str]:\n+        tok_names = self.tokenizer.model_input_names\n+        fea_names = self.feature_extractor.model_input_names\n+        return list(dict.fromkeys(tok_names + fea_names + [\"input_features_mask\"]))\n+\n+    def apply_transcription_request(\n+        self,\n+        audio: Union[str, list[str], AudioInput],\n+        prompt: Optional[Union[str, list[str]]] = None,\n+        **kwargs: Unpack[GlmAsrProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Prepare inputs for automatic speech recognition without manually writing the default transcription prompt.\n+\n+        Args:\n+            audio (`str`, `list[str]`, `np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                Audio to transcribe. Strings are interpreted as local paths or URLs and will be loaded automatically by\n+                the chat template loader; NumPy arrays and PyTorch tensors are forwarded directly.\n+            prompt (`str` or `list[str]`, *optional*):\n+                Custom prompt(s) to include in the user turn. A list must be the same length as the batch. When `None`,\n+                each sample uses `\"Transcribe the input speech.\"`.\n+            **kwargs:\n+                Additional keyword arguments forwarded to [`~AudioFlamingo3Processor.apply_chat_template`] (for example\n+                `text_kwargs`, `audio_kwargs`, ...).\n+\n+        Returns:\n+            [`BatchFeature`]: Processor outputs ready to be passed to [`AudioFlamingo3ForConditionalGeneration.generate`].\n+\n+        \"\"\"\n+\n+        if isinstance(audio, str):\n+            audio_items: list[Union[str, np.ndarray]] = [audio]\n+        elif isinstance(audio, (list, tuple)) and audio and all(isinstance(el, str) for el in audio):\n+            audio_items = list(audio)\n+        else:\n+            audio_items = list(make_list_of_audio(audio))\n+            if is_torch_available():\n+                audio_items = [el.detach().cpu().numpy() if isinstance(el, torch.Tensor) else el for el in audio_items]\n+\n+        batch_size = len(audio_items)\n+        if batch_size == 0:\n+            raise ValueError(\"`audio` must contain at least one sample.\")\n+\n+        if prompt is None:\n+            prompts = [self.default_transcription_prompt] * batch_size\n+        elif isinstance(prompt, str):\n+            prompts = [prompt] * batch_size\n+        elif isinstance(prompt, (list, tuple)):\n+            if len(prompt) != batch_size:\n+                raise ValueError(\n+                    f\"Received {len(prompt)} prompt(s) for {batch_size} audio sample(s); counts must match.\"\n+                )\n+            prompts = []\n+            for item in prompt:\n+                if item is None:\n+                    prompts.append(self.default_transcription_prompt)\n+                elif isinstance(item, str):\n+                    prompts.append(item)\n+                else:\n+                    raise TypeError(\"Each prompt must be a string or `None`.\")\n+        else:\n+            raise TypeError(\"`prompt` must be a string, a sequence of strings, or `None`.\")\n+\n+        conversations = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"audio\", \"path\": audio_item}\n+                        if isinstance(audio_item, str)\n+                        else {\"type\": \"audio\", \"audio\": audio_item},\n+                        {\"type\": \"text\", \"text\": prompt_text},\n+                    ],\n+                }\n+            ]\n+            for prompt_text, audio_item in zip(prompts, audio_items)\n+        ]\n+\n+        return self.apply_chat_template(\n+            conversations,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+    def batch_decode(self, *args, strip_prefix=False, **kwargs):\n+        \"\"\"\n+        Forward arguments to [`~PreTrainedTokenizer.batch_decode`] and optionally remove the assistant framing the model\n+        was trained to produce.\n+\n+        AF3 transcription requests respond with sentences such as `\"The spoken content of the audio is \\\"...\\\".\"`.\n+        Setting `strip_prefix=True` trims the fixed prefix for just the transcription text.\n+        \"\"\"\n+        decoded = self.tokenizer.batch_decode(*args, **kwargs)\n+        if strip_prefix:\n+            decoded = [self._strip_assistant_prefix_and_quotes(text) for text in decoded]\n+        return decoded\n+\n+    def _strip_assistant_prefix_and_quotes(self, text: str) -> str:\n+        \"\"\"\n+        Remove the assistant prefix and surrounding quotes from a decoded transcription string.\n+        \"\"\"\n+\n+        stripped = text.strip()\n+\n+        for prefix in (\n+            \"The spoken content of the audio is\",\n+            \"The transcription of the audio is\",\n+        ):\n+            if stripped.startswith(prefix):\n+                stripped = stripped[len(prefix) :].strip()\n+                break\n+\n+        if stripped.endswith(\".\"):\n+            stripped = stripped[:-1].strip()\n+\n+        if len(stripped) >= 2 and stripped[0] == stripped[-1] and stripped[0] in {\"'\", '\"'}:\n+            stripped = stripped[1:-1].strip()\n+\n+        return stripped\n+\n+\n+__all__ = [\"GlmAsrProcessor\"]"
        },
        {
            "sha": "d24188f65246c791cd772c99f10b6422dc37cb18",
            "filename": "src/transformers/tokenization_utils_tokenizers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_tokenizers.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -901,6 +901,8 @@ def _decode(\n \n         if isinstance(token_ids, int):\n             token_ids = [token_ids]\n+        if isinstance(token_ids, dict):\n+            token_ids = token_ids[\"input_ids\"]\n         return self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n \n     def _save_pretrained("
        },
        {
            "sha": "c02ff216fe7572efe143fe486467811e31c51254",
            "filename": "tests/models/audioflamingo3/test_modeling_audioflamingo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/tests%2Fmodels%2Faudioflamingo3%2Ftest_modeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/tests%2Fmodels%2Faudioflamingo3%2Ftest_modeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudioflamingo3%2Ftest_modeling_audioflamingo3.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -157,6 +157,7 @@ class AudioFlamingo3ForConditionalGenerationModelTest(ModelTesterMixin, Generati\n     \"\"\"\n \n     all_model_classes = (AudioFlamingo3ForConditionalGeneration,) if is_torch_available() else ()\n+    # TODO: @eustlb, this is incorrect\n     pipeline_model_mapping = (\n         {\n             \"text-to-speech\": AudioFlamingo3ForConditionalGeneration,"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/glmasr/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/tests%2Fmodels%2Fglmasr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/tests%2Fmodels%2Fglmasr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglmasr%2F__init__.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c"
        },
        {
            "sha": "6f01cb08f19e2198fcb29be5d4248ee78e58370f",
            "filename": "tests/models/glmasr/test_modeling_glmasr.py",
            "status": "added",
            "additions": 361,
            "deletions": 0,
            "changes": 361,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/tests%2Fmodels%2Fglmasr%2Ftest_modeling_glmasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/tests%2Fmodels%2Fglmasr%2Ftest_modeling_glmasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglmasr%2Ftest_modeling_glmasr.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -0,0 +1,361 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch glmasr model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+import pytest\n+\n+from transformers import (\n+    AutoProcessor,\n+    GlmAsrConfig,\n+    GlmAsrForConditionalGeneration,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class GlmAsrModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        ignore_index=-100,\n+        audio_token_id=0,\n+        seq_length=35,\n+        feat_seq_length=64,\n+        text_config={\n+            \"model_type\": \"llama\",\n+            \"intermediate_size\": 64,\n+            \"initializer_range\": 0.02,\n+            \"hidden_size\": 16,\n+            \"max_position_embeddings\": 52,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"use_labels\": True,\n+            \"use_mrope\": False,\n+            \"vocab_size\": 99,\n+            \"head_dim\": 8,\n+            \"pad_token_id\": 1,  # can't be the same as the audio token id\n+        },\n+        is_training=True,\n+        audio_config={\n+            \"model_type\": \"glmasr_encoder\",\n+            \"hidden_size\": 128,\n+            \"num_attention_heads\": 2,\n+            \"intermediate_size\": 512,\n+            \"num_hidden_layers\": 2,\n+            \"num_mel_bins\": 128,\n+            \"max_source_positions\": 32,\n+            \"initializer_range\": 0.02,\n+        },\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.audio_token_id = audio_token_id\n+        self.text_config = text_config\n+        self.audio_config = audio_config\n+        self.seq_length = seq_length\n+        self.feat_seq_length = feat_seq_length\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.is_training = is_training\n+\n+        self.batch_size = 3\n+        self.encoder_seq_length = seq_length\n+\n+    def get_config(self):\n+        return GlmAsrConfig(\n+            text_config=self.text_config,\n+            audio_config=self.audio_config,\n+            ignore_index=self.ignore_index,\n+            audio_token_id=self.audio_token_id,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        input_features_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.audio_config[\"num_mel_bins\"],\n+                self.feat_seq_length,\n+            ]\n+        )\n+        config = self.get_config()\n+        input_features_mask = torch.ones([self.batch_size, self.feat_seq_length], dtype=torch.bool).to(torch_device)\n+        return config, input_features_values, input_features_mask\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_features_values, input_features_mask = config_and_inputs\n+        num_audio_tokens_per_batch_idx = 8\n+\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n+        attention_mask[:, :1] = 0\n+\n+        input_ids[:, 1 : 1 + num_audio_tokens_per_batch_idx] = config.audio_token_id\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"input_features\": input_features_values,\n+            \"input_features_mask\": input_features_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class GlmAsrForConditionalGenerationModelTest(\n+    ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase\n+):\n+    \"\"\"\n+    Model tester for `GlmAsrForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (GlmAsrForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"audio-text-to-text\": GlmAsrForConditionalGeneration} if is_torch_available() else {}\n+\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = GlmAsrModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=GlmAsrConfig, has_text_modality=False)\n+\n+    @unittest.skip(\n+        reason=\"This test does not apply to GlmAsr since inputs_embeds corresponding to audio tokens are replaced when input features are provided.\"\n+    )\n+    def test_inputs_embeds_matches_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Compile not yet supported for GlmAsr models\")\n+    @pytest.mark.torch_compile_test\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Compile not yet supported for GlmAsr models\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+    @unittest.skip(reason=\"GlmAsr tests avoid right-padding equivalence; fusion is in-place.\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+    @unittest.skip(reason=\"GlmAsr has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        # GlmAsr is audio+text composite; verify SDPA toggles propagate to submodules.\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                # SDPA (default)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n+                audio_attn = \"sdpa\" if model.audio_tower._supports_sdpa else \"eager\"\n+\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model.audio_tower.config._attn_implementation == audio_attn)\n+\n+                # Eager\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.audio_tower.config._attn_implementation == \"eager\")\n+\n+                for _, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+\n+@require_torch\n+class GlmAsrForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.checkpoint_name = \"zai-org/GLM-ASR-Nano-2512\"\n+        self.processor = AutoProcessor.from_pretrained(self.checkpoint_name)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_single_batch_sub_30(self):\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"url\": \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/bcn_weather.mp3\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Please transcribe this audio into text\"},\n+                ],\n+            },\n+        ]\n+\n+        model = GlmAsrForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, device_map=torch_device, dtype=\"auto\"\n+        )\n+\n+        inputs = self.processor.apply_chat_template(\n+            conversation, tokenize=True, add_generation_prompt=True, return_dict=True\n+        ).to(model.device, dtype=model.dtype)\n+\n+        inputs_transcription = self.processor.apply_transcription_request(\n+            \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/bcn_weather.mp3\",\n+        ).to(model.device, dtype=model.dtype)\n+\n+        for key in inputs:\n+            self.assertTrue(torch.equal(inputs[key], inputs_transcription[key]))\n+\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+\n+        decoded_outputs = self.processor.batch_decode(\n+            outputs[:, inputs.input_ids.shape[1] :], skip_special_tokens=True\n+        )\n+\n+        EXPECTED_OUTPUT = [\n+            \"Yesterday it was thirty five degrees in Barcelona, but today the temperature will go down to minus twenty degrees.\"\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)\n+\n+    @slow\n+    def test_single_batch_over_30(self):\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"url\": \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/obama2.mp3\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Please transcribe this audio into text\"},\n+                ],\n+            },\n+        ]\n+\n+        model = GlmAsrForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, device_map=torch_device, dtype=\"auto\"\n+        )\n+\n+        inputs = self.processor.apply_chat_template(\n+            conversation, tokenize=True, add_generation_prompt=True, return_dict=True\n+        ).to(model.device, dtype=model.dtype)\n+\n+        inputs_transcription = self.processor.apply_transcription_request(\n+            \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/obama2.mp3\",\n+        ).to(model.device, dtype=model.dtype)\n+\n+        for key in inputs:\n+            self.assertTrue(torch.equal(inputs[key], inputs_transcription[key]))\n+\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+\n+        decoded_outputs = self.processor.batch_decode(\n+            outputs[:, inputs.input_ids.shape[1] :], skip_special_tokens=True\n+        )\n+\n+        EXPECTED_OUTPUT = [\n+            \"This week, I traveled to Chicago to deliver my final farewell address to the nation, following in the tradition of presidents before me. It was an opportunity to say thank you. Whether we've seen eye to eye or rarely agreed at all, my conversations with you, the American people, in living rooms and schools, at farms and on factory floors, at diners and on distant military outposts, all these conversations are what have kept me honest, kept me inspired, and kept me going. Every day, I learned from you. You made me a better president, and you made me a better man. Over the\"\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)\n+\n+    @slow\n+    def test_batched(self):\n+        conversation = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"audio\",\n+                            \"url\": \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/bcn_weather.mp3\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Please transcribe this audio into text\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"audio\",\n+                            \"url\": \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/obama2.mp3\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Please transcribe this audio into text\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+\n+        model = GlmAsrForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, device_map=torch_device, dtype=\"auto\"\n+        )\n+\n+        inputs = self.processor.apply_chat_template(\n+            conversation, tokenize=True, add_generation_prompt=True, return_dict=True\n+        ).to(model.device, dtype=model.dtype)\n+\n+        inputs_transcription = self.processor.apply_transcription_request(\n+            [\n+                \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/bcn_weather.mp3\",\n+                \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/obama2.mp3\",\n+            ],\n+        ).to(model.device, dtype=model.dtype)\n+\n+        for key in inputs:\n+            self.assertTrue(torch.equal(inputs[key], inputs_transcription[key]))\n+\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+\n+        decoded_outputs = self.processor.batch_decode(\n+            outputs[:, inputs.input_ids.shape[1] :], skip_special_tokens=True\n+        )\n+\n+        EXPECTED_OUTPUT = [\n+            \"Yesterday it was thirty five degrees in Barcelona, but today the temperature will go down to minus twenty degrees.\",\n+            \"This week, I traveled to Chicago to deliver my final farewell address to the nation, following in the tradition of presidents before me. It was an opportunity to say thank you. Whether we've seen eye to eye or rarely agreed at all, my conversations with you, the American people, in living rooms and schools, at farms and on factory floors, at diners and on distant military outposts, all these conversations are what have kept me honest, kept me inspired, and kept me going. Every day, I learned from you. You made me a better president, and you made me a better man. Over the\",\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)"
        },
        {
            "sha": "2d64f8d27e4c45cb402c860a1020255ae7328448",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7f29523361b2cc12e51c1f5133d95f122f6f45c/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7f29523361b2cc12e51c1f5133d95f122f6f45c/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=a7f29523361b2cc12e51c1f5133d95f122f6f45c",
            "patch": "@@ -99,6 +99,8 @@ class DecoratedItem:\n # docstrings instead. If formatting should be ignored for the docstring, you can put a comment # no-format on the\n # line before the docstring.\n OBJECTS_TO_IGNORE = {\n+    \"GlmAsrProcessor\",\n+    \"AudioFlamingo3Processor\",\n     \"ApertusConfig\",\n     \"Mxfp4Config\",\n     \"Qwen3OmniMoeConfig\","
        }
    ],
    "stats": {
        "total": 2300,
        "additions": 2282,
        "deletions": 18
    }
}