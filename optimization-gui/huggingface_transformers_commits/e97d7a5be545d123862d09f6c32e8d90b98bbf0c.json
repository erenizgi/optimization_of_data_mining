{
    "author": "ArthurZucker",
    "message": "add `_supports_flex_attn = True` for models that do support it (#35598)\n\n* add `_supports_flex_attn = True`\n\n* fix repo consistency",
    "sha": "e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
    "files": [
        {
            "sha": "12d7224b210522bd08d318957aabcf913a6a0601",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -704,6 +704,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "ecb944a9244fb336e32ae56f57eb3b65e0816c65",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -413,6 +413,7 @@ class CoherePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "5e36b8a36cfe8518bb51b5a5204228c6f200b43e",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -413,6 +413,7 @@ class Cohere2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "d9f08cc1aee7fac38187fd9cde86eaacf4a1083b",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -380,6 +380,7 @@ class GemmaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "fb60a611a4f0a327643cb2a3461456a154b45354",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -410,6 +410,7 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "aac6e471bdd4f56bae6b0c385cd5a7f1f822fdba",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -395,6 +395,7 @@ class GlmPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "3c5c52524f8419d14577b55d8370c9519938687e",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -395,6 +395,7 @@ class GranitePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "16d4ba4dacc9172a8eae16627d272d3b4b150342",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -384,6 +384,7 @@ class LlamaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "a44f96dd7b9cbef8c9b9b586b69667292299d5a6",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -356,6 +356,7 @@ class MistralPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "c7130a3a5f887510a176e72451ab7fd9a74fb34b",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -478,6 +478,7 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "6cb2baa4e009f1744fcfe2573ddeefa7f1167712",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -360,6 +360,7 @@ class OlmoPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "7219284bd862ca0775daae1e884cd4f1b43aa735",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -361,6 +361,7 @@ class Olmo2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "d4befc96d182ec70c4a8fc32145e22b52fcf23b6",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -762,6 +762,7 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "63d40070c2530ed004f9b19debb81a66ee7afc3c",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -356,6 +356,7 @@ class PhiPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "dd6d0d1dc3a7ad7ce70440ed8ca747ede4742bfb",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -422,6 +422,7 @@ class Phi3PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "b540dd18300ea2c672f3e8356bb5fa6c01bb661f",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -910,6 +910,7 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "e8d1bff8a29ac58080dd4c0b994bf1bac4f11d80",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -369,6 +369,7 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "3668c076d24c69659e5dfde5b7f315f5438fd8f4",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97d7a5be545d123862d09f6c32e8d90b98bbf0c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=e97d7a5be545d123862d09f6c32e8d90b98bbf0c",
            "patch": "@@ -360,6 +360,7 @@ class Starcoder2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 18,
        "deletions": 0
    }
}