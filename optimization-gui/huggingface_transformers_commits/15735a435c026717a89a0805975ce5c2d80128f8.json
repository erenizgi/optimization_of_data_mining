{
    "author": "MekkCyber",
    "message": "[Quantization]Â Fix Static FP8 Quantization (#42775)\n\n* fix\n\n* fix style\n\n* Update src/transformers/integrations/finegrained_fp8.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* fix\n\n* style\n\n* Update src/transformers/integrations/finegrained_fp8.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "15735a435c026717a89a0805975ce5c2d80128f8",
    "files": [
        {
            "sha": "94b7dd71c5fa50c95bb0376a9da64d4862314400",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/15735a435c026717a89a0805975ce5c2d80128f8/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/15735a435c026717a89a0805975ce5c2d80128f8/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=15735a435c026717a89a0805975ce5c2d80128f8",
            "patch": "@@ -466,9 +466,11 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n                     qinput, scale = act_quant(input, self.block_size[1])\n                 elif self.activation_scheme == \"static\":\n                     scale = self.activation_scale.to(torch.float32)\n-                    qinput = (input / scale).to(torch.float8_e4m3fn)\n+                    qinput = (input / scale).clamp(min=_FP8_MIN, max=_FP8_MAX).to(torch.float8_e4m3fn)\n+\n                 else:\n                     raise NotImplementedError(\"Not supported\")\n+\n                 output = w8a8_block_fp8_matmul_triton(\n                     qinput,\n                     weight,\n@@ -483,7 +485,8 @@ def forward(self, input: torch.Tensor) -> torch.Tensor:\n             torch_accelerator_module.synchronize()\n             if self.bias is not None:\n                 output = output + self.bias\n-            output = torch.nan_to_num(output, nan=0.0)\n+\n+            #            output = torch.nan_to_num(output, nan=0.0)\n             return output.to(dtype=input.dtype)\n \n "
        }
    ],
    "stats": {
        "total": 7,
        "additions": 5,
        "deletions": 2
    }
}