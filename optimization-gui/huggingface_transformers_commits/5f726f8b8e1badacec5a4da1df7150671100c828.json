{
    "author": "BlackSamorez",
    "message": "New HIGGS quantization interfaces, JIT kernel compilation support. (#36148)\n\n* new flute\r\n\r\n* new higgs working\r\n\r\n* small adjustments\r\n\r\n* progress and quallity\r\n\r\n* small updates\r\n\r\n* style\r\n\r\n---------\r\n\r\nCo-authored-by: Andrey Panferov <panferov.andrey3@wb.ru>\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "5f726f8b8e1badacec5a4da1df7150671100c828",
    "files": [
        {
            "sha": "3ba35eb4e4fd052c610beaab16211b2b43d2e01b",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f726f8b8e1badacec5a4da1df7150671100c828/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f726f8b8e1badacec5a4da1df7150671100c828/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=5f726f8b8e1badacec5a4da1df7150671100c828",
            "patch": "@@ -28,15 +28,12 @@\n \n \n if is_flute_available():\n-    import flute.utils\n+    from flute.integrations.higgs import prepare_data_transposed\n+    from flute.tune import TuneMetaData, qgemm_v2\n \n if is_hadamard_available():\n     from fast_hadamard_transform import hadamard_transform\n \n-if is_flute_available():\n-    import flute.utils\n-    from flute.integrations.higgs import prepare_data_transposed\n-\n \n def pad_to_block(tensor, dims, had_block_size, value=0):\n     pad_dims = [0 for _ in range(2 * len(tensor.shape))]\n@@ -464,14 +461,14 @@ def quantize_with_higgs(weight, bits: int = 4, p: int = 2, group_size: int = 256\n \n     # Quantize\n     codes = torch.empty(weight.shape[:-1], device=device, dtype=torch.uint8)\n-    for i in range(0, weight.shape[0], 64):\n-        codes[i : i + 64] = torch.argmax(2 * weight[i : i + 64] @ grid.T - grid_norm_2, dim=-1).to(torch.uint8)\n+    for i in range(0, weight.shape[0], 16):\n+        codes[i : i + 16] = torch.argmax(2 * weight[i : i + 16] @ grid.T - grid_norm_2, dim=-1).to(torch.uint8)\n     del weight\n \n     codes = codes.reshape(codes.shape[0], -1)\n     scales = scales / sqrt(hadamard_size)\n \n-    weight, scales, tables, tables2 = prepare_data_transposed(\n+    weight, scales, tables, tables2, tune_metadata = prepare_data_transposed(\n         codes,\n         torch.repeat_interleave(scales.to(dtype), hadamard_size // group_size, dim=1),\n         grid.to(dtype),\n@@ -480,13 +477,15 @@ def quantize_with_higgs(weight, bits: int = 4, p: int = 2, group_size: int = 256\n         vector_size=p,\n         dtype=dtype,\n         device=device,\n+        check_correctness=False,\n     )\n \n     return {\n         \"weight\": weight,\n         \"scales\": scales,\n         \"tables\": tables,\n         \"tables2\": tables2.view(dtype=torch.float16),\n+        \"tune_metadata\": tune_metadata,\n     }\n \n \n@@ -508,7 +507,6 @@ def __init__(\n         self.num_bits = num_bits\n         self.group_size = group_size\n         self.hadamard_size = hadamard_size\n-        self.num_sms_packed = nn.Parameter(torch.tensor(-1, dtype=torch.int32, device=device), requires_grad=False)\n \n         assert in_features % group_size == 0\n         assert num_bits in [2, 3, 4]\n@@ -531,23 +529,23 @@ def __init__(\n             self.register_parameter(\"bias\", None)\n \n         self.workspace = None  # must be set externally to be reused among layers\n+        self.tune_metadata: TuneMetaData = None  # must be set externally because architecture dependent\n \n     def forward(self, x):\n         x = pad_to_block(x, [-1], self.hadamard_size)\n \n         if self.workspace is None:\n             raise Exception(\"Workspace must be set before calling forward\")\n \n-        return flute.qgemm_hadamard(\n+        return qgemm_v2(\n             x,\n             self.weight,\n             self.scales,\n             self.tables,\n             self.tables2.view(dtype=torch.float32),\n             self.workspace,\n-            self.num_bits,\n-            self.group_size,\n-            self.hadamard_size,\n+            self.tune_metadata,\n+            hadamard_size=self.hadamard_size,\n         )\n \n "
        },
        {
            "sha": "83c102f16c3e303514d856508e4dc37d87163ffb",
            "filename": "src/transformers/quantizers/quantizer_higgs.py",
            "status": "modified",
            "additions": 34,
            "deletions": 64,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f726f8b8e1badacec5a4da1df7150671100c828/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f726f8b8e1badacec5a4da1df7150671100c828/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py?ref=5f726f8b8e1badacec5a4da1df7150671100c828",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n from typing import TYPE_CHECKING, Any, Dict, List, Optional\n \n+from ..utils.logging import tqdm\n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n \n@@ -30,20 +31,6 @@\n logger = logging.get_logger(__name__)\n \n \n-def get_num_sms_from_device(device):\n-    target_device_cc = torch.cuda.get_device_capability(device=device)\n-    if target_device_cc == (8, 6):\n-        return 84\n-    elif target_device_cc == (8, 0):\n-        return 108\n-    elif target_device_cc == (8, 9):\n-        return 128\n-    else:\n-        raise NotImplementedError(\n-            f\"Device capability {target_device_cc} not supported for FLUTE (yet?) to verify your device capability check out https://developer.nvidia.com/cuda-gpus\"\n-        )\n-\n-\n class HiggsHfQuantizer(HfQuantizer):\n     \"\"\"\n     Quantizer of the HIGGS method. Enables the loading of prequantized models and in-flight quantization of full-precision models.\n@@ -115,26 +102,24 @@ def create_quantized_param(\n             self.quantization_config.group_size,\n             self.quantization_config.hadamard_size,\n         )\n-\n         del param_value\n \n-        module, tensor_name = get_module_from_name(model, param_name)\n+        module, _ = get_module_from_name(model, param_name)\n+        module_name = \".\".join(param_name.split(\".\")[:-1])\n         for key, value in flute_dict.items():\n             if key in module._parameters:\n                 module._parameters[key] = torch.nn.Parameter(value, requires_grad=False)\n             elif key in module._buffers:\n                 module._buffers[key] = torch.nn.Buffer(value)\n+            elif key == \"tune_metadata\":\n+                module.tune_metadata = value\n+                self.quantization_config.tune_metadata[module_name] = value.to_dict()\n             else:\n                 raise ValueError(f\"Unexpected key {key} in module {module}\")\n \n         if unexpected_keys is not None and param_name in unexpected_keys:\n             unexpected_keys.remove(param_name)\n \n-        module.num_sms_packed = torch.nn.Parameter(\n-            torch.tensor(get_num_sms_from_device(target_device), device=target_device, dtype=torch.int32),\n-            requires_grad=False,\n-        )\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n@@ -149,57 +134,42 @@ def _process_model_before_weight_loading(\n         model.config.quantization_config = self.quantization_config\n \n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n-        import flute.utils\n+        from flute.tune import TuneMetaData, maybe_tune_and_repack\n+        from flute.utils import make_workspace_streamk\n \n         from ..integrations import HiggsLinear\n \n         flute_workspaces = {}\n-        for name, module in model.named_modules():\n-            if isinstance(module, HiggsLinear):\n-                # Every HiggsLinear needs a \"workspace\": a buffer for the unpacking operation.\n-                # This buffer needs to be on the same device as the weights, but can be reused across modules otherwise.\n-                if module.weight.device not in flute_workspaces:\n-                    flute_workspaces[module.weight.device] = flute.utils.make_workspace_streamk(\n-                        device=module.weight.device\n-                    )\n-                module.workspace = flute_workspaces[module.weight.device]\n-\n-                # FLUTE weights are packed in a way that is optimized for a specific number of SMs (GPU streaming multiprocessors).\n-                # If the model is loaded on a different device than the one it was saved on, we need to repack the weights.\n-                if module.num_sms_packed.item() != get_num_sms_from_device(module.weight.device):\n-                    new_device = module.weight.device\n-                    new_num_sms = get_num_sms_from_device(new_device)\n-                    module.weight.data = flute.utils.pack(\n-                        flute.utils.unpack(\n-                            weight=module.weight.data,\n-                            scales=module.scales.data,\n-                            workspace=module.workspace,\n-                            num_bits=module.num_bits,\n-                            group_size=module.group_size,\n-                            num_sms_packed=module.num_sms_packed.item(),\n-                        ).T.contiguous(),\n-                        module.num_bits,\n-                        module.group_size,\n-                    )\n-                    module.num_sms_packed = torch.nn.Parameter(\n-                        torch.tensor(new_num_sms, device=new_device, dtype=torch.int32),\n-                        requires_grad=False,\n-                    )\n+        flute_modules = {name: module for name, module in model.named_modules() if isinstance(module, HiggsLinear)}\n+        for name, module in tqdm(flute_modules.items(), desc=\"Repacking HIGGS modules\", leave=False):\n+            # Every HiggsLinear needs a \"workspace\": a buffer for the unpacking operation.\n+            # This buffer needs to be on the same device as the weights, but can be reused across modules otherwise.\n+            if module.weight.device not in flute_workspaces:\n+                flute_workspaces[module.weight.device] = make_workspace_streamk(device=module.weight.device)\n+            module.workspace = flute_workspaces[module.weight.device]\n+\n+            # FLUTE weights are packed in a way that is optimized for a specific number of SMs (GPU streaming multiprocessors).\n+            # If the model is loaded on a different device than the one it was saved on, we need to repack the weights.\n+            module.tune_metadata = TuneMetaData.from_dict(self.quantization_config.tune_metadata[name])\n+            module.weight.data, module.tune_metadata = maybe_tune_and_repack(\n+                weight=module.weight.data,\n+                scales=module.scales.data,\n+                metadata=module.tune_metadata,\n+            )\n+            self.quantization_config.tune_metadata[name] = module.tune_metadata.to_dict()\n \n     def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> List[str]:\n         from ..integrations import HiggsLinear\n \n-        not_missing_keys = []\n-        for name, module in model.named_modules():\n-            if isinstance(module, HiggsLinear):\n-                for missing in missing_keys:\n-                    if (\n-                        (name in missing or name in f\"{prefix}.{missing}\")\n-                        and not missing.endswith(\".weight\")\n-                        and not missing.endswith(\".bias\")\n-                    ):\n-                        not_missing_keys.append(missing)\n-        return [k for k in missing_keys if k not in not_missing_keys]\n+        higgs_names = {name for name, module in model.named_modules() if isinstance(module, HiggsLinear)}\n+\n+        def should_update(key: str) -> bool:\n+            if key.endswith(\".weight\") or key.endswith(\".bias\"):\n+                return False\n+            full_key = f\"{prefix}.{key}\"\n+            return any(name in key or name in full_key for name in higgs_names)\n+\n+        return [key for key in missing_keys if not should_update(key)]\n \n     @property\n     def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):"
        },
        {
            "sha": "aa7be764c520b4088df5f47cb3d02dcc05726ff7",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f726f8b8e1badacec5a4da1df7150671100c828/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f726f8b8e1badacec5a4da1df7150671100c828/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=5f726f8b8e1badacec5a4da1df7150671100c828",
            "patch": "@@ -639,7 +639,7 @@ def is_flax_available():\n \n def is_flute_available():\n     try:\n-        return importlib.util.find_spec(\"flute\") is not None and importlib.metadata.version(\"flute-kernel\") >= \"0.3.0\"\n+        return importlib.util.find_spec(\"flute\") is not None and importlib.metadata.version(\"flute-kernel\") >= \"0.4.1\"\n     except importlib.metadata.PackageNotFoundError:\n         return False\n "
        },
        {
            "sha": "ec8a5ef70d4f8d945c2aa989c33bd45eefe16f9c",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f726f8b8e1badacec5a4da1df7150671100c828/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f726f8b8e1badacec5a4da1df7150671100c828/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=5f726f8b8e1badacec5a4da1df7150671100c828",
            "patch": "@@ -1404,6 +1404,8 @@ class HiggsConfig(QuantizationConfigMixin):\n             Hadamard size for the HIGGS method. Default is 512. Input dimension of matrices is padded to this value. Decreasing this below 512 will reduce the quality of the quantization.\n         group_size (int, *optional*, defaults to 256):\n             Group size for the HIGGS method. Can be 64, 128 or 256. Decreasing it barely affects the performance. Default is 256. Must be a divisor of hadamard_size.\n+        tune_metadata ('dict', *optional*, defaults to {}):\n+            Module-wise metadata (gemm block shapes, GPU metadata, etc.) for saving the kernel tuning results. Default is an empty dictionary. Is set automatically during tuning.\n     \"\"\"\n \n     def __init__(\n@@ -1413,16 +1415,20 @@ def __init__(\n         modules_to_not_convert: Optional[List[str]] = None,\n         hadamard_size: int = 512,\n         group_size: int = 256,\n+        tune_metadata: Optional[Dict[str, Any]] = None,\n         **kwargs,\n     ):\n         if modules_to_not_convert is None:\n             modules_to_not_convert = [\"lm_head\"]\n+        if tune_metadata is None:\n+            tune_metadata = {}\n         self.quant_method = QuantizationMethod.HIGGS\n         self.bits = bits\n         self.p = p\n         self.modules_to_not_convert = modules_to_not_convert\n         self.hadamard_size = hadamard_size\n         self.group_size = group_size\n+        self.tune_metadata = tune_metadata\n \n         self.post_init()\n "
        },
        {
            "sha": "5c17ed63aa6d737f866a948133c19fc6f089412f",
            "filename": "tests/quantization/higgs/test_higgs.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f726f8b8e1badacec5a4da1df7150671100c828/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f726f8b8e1badacec5a4da1df7150671100c828/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py?ref=5f726f8b8e1badacec5a4da1df7150671100c828",
            "patch": "@@ -65,12 +65,12 @@ def test_from_dict(self):\n @require_accelerate\n # @require_read_token\n class HiggsTest(unittest.TestCase):\n-    model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n+    model_name = \"unsloth/Llama-3.2-1B\"\n \n-    input_text = \"A quick brown fox jumps over the\"\n+    input_text = \"Font test: A quick brown fox jumps over the\"\n     max_new_tokens = 2\n \n-    EXPECTED_OUTPUT = \"A quick brown fox jumps over the lazy dog\"\n+    EXPECTED_OUTPUT = \"Font test: A quick brown fox jumps over the lazy dog\"\n \n     device_map = \"cuda\"\n "
        }
    ],
    "stats": {
        "total": 136,
        "additions": 55,
        "deletions": 81
    }
}