{
    "author": "ArthurZucker",
    "message": "use a single for loop (#33148)\n\n* use a single for loop\r\n\r\n* oups\r\n\r\n* fixup\r\n\r\n* fix typo",
    "sha": "c409cd81777fb27aadc043ed3d8339dbc020fb3b",
    "files": [
        {
            "sha": "ba2340789970482cc296c6ed759953b9e11deccf",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 19,
            "deletions": 23,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -294,31 +294,27 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n-\n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            all_images.append(image)\n \n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         data = {\"pixel_values\": images}"
        },
        {
            "sha": "46d081973bb46866d651a39a6f35a002d9715a2a",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 17,
            "deletions": 23,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -311,32 +311,26 @@ def preprocess(\n         if input_data_format is None:\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n-\n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n \n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            all_images.append(image)\n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         data = {\"pixel_values\": images}"
        },
        {
            "sha": "515c2de0cfc300f2b0bea4376d5c10c8d2116280",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 15,
            "deletions": 20,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -280,31 +280,26 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n \n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n \n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n \n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n \n+            all_images.append(image)\n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         data = {\"pixel_values\": images}"
        },
        {
            "sha": "fa398821ca614ca1db8ab9f89b20ca15462759b2",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 18,
            "deletions": 23,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -319,31 +319,26 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n-\n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n \n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            all_images.append(image)\n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         data = {\"pixel_values\": images}"
        },
        {
            "sha": "bafb5f6e71adc088a67bf743f0bad668f9a5358c",
            "filename": "src/transformers/models/deit/image_processing_deit.py",
            "status": "modified",
            "additions": 15,
            "deletions": 20,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -270,31 +270,26 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n \n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n \n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n \n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n \n+            all_images.append(image)\n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         data = {\"pixel_values\": images}"
        },
        {
            "sha": "e7c3193ceab4cbdb4f18f4fa55e861599cb5c437",
            "filename": "src/transformers/models/deprecated/vit_hybrid/image_processing_vit_hybrid.py",
            "status": "modified",
            "additions": 15,
            "deletions": 20,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -312,31 +312,26 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n \n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n \n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n \n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n \n+            all_images.append(image)\n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         data = {\"pixel_values\": images}"
        },
        {
            "sha": "579e6d44c1435b7d392ccd1c59f9b534463e9346",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 18,
            "deletions": 23,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -409,31 +409,26 @@ def _preprocess(\n         \"\"\"\n         images = make_list_of_images(images)\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n-\n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n \n+            all_images.append(image)\n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         return images"
        },
        {
            "sha": "59d0d9d9447252db52414b3e7419c0d15110a8b3",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 18,
            "deletions": 23,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -272,31 +272,26 @@ def _preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n-\n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n \n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            all_images.append(image)\n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         return images"
        },
        {
            "sha": "7981947307fdc22e38d1b8fd8711a073319d4d0d",
            "filename": "src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1.py",
            "status": "modified",
            "additions": 15,
            "deletions": 20,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -276,31 +276,26 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n \n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n \n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n \n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n \n+            all_images.append(image)\n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         data = {\"pixel_values\": images}"
        },
        {
            "sha": "25d227bd582fb7f9a8be6c7c720a33961b36689c",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 23,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c409cd81777fb27aadc043ed3d8339dbc020fb3b/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=c409cd81777fb27aadc043ed3d8339dbc020fb3b",
            "patch": "@@ -279,31 +279,26 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_center_crop:\n-            images = [\n-                self.center_crop(image=image, size=crop_size, input_data_format=input_data_format) for image in images\n-            ]\n-\n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n \n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            all_images.append(image)\n         images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n         ]\n \n         data = {\"pixel_values\": images}"
        }
    ],
    "stats": {
        "total": 386,
        "additions": 168,
        "deletions": 218
    }
}