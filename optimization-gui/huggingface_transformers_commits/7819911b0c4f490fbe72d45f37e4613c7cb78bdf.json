{
    "author": "ydshieh",
    "message": "Use T4 single GPU runner with more CPU RAM (#37961)\n\nlarger T4 single GPU\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "7819911b0c4f490fbe72d45f37e4613c7cb78bdf",
    "files": [
        {
            "sha": "8366707845cded285f802a8f4ab9a72d8e274d1c",
            "filename": ".github/workflows/check_failed_model_tests.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fcheck_failed_model_tests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fcheck_failed_model_tests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fcheck_failed_model_tests.yml?ref=7819911b0c4f490fbe72d45f37e4613c7cb78bdf",
            "patch": "@@ -29,7 +29,7 @@ jobs:\n   run_models_gpu:\n     name: \" \"\n     runs-on:\n-      group: aws-g4dn-2xlarge-cache\n+      group: aws-g4dn-4xlarge-cache\n     container:\n       image: ${{ inputs.docker }}\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/"
        },
        {
            "sha": "b881bc38e590f5ad682d6ca37a015956b2dbee6a",
            "filename": ".github/workflows/doctest_job.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fdoctest_job.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fdoctest_job.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fdoctest_job.yml?ref=7819911b0c4f490fbe72d45f37e4613c7cb78bdf",
            "patch": "@@ -28,7 +28,7 @@ jobs:\n       matrix:\n         split_keys: ${{ fromJson(inputs.split_keys) }}\n     runs-on: \n-      group: aws-g4dn-2xlarge-cache\n+      group: aws-g4dn-4xlarge-cache\n     container:\n       image: huggingface/transformers-all-latest-gpu\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/"
        },
        {
            "sha": "bdf967a6d9ef0e9429ef13718f0cb4ebf599d28f",
            "filename": ".github/workflows/doctests.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fdoctests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fdoctests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fdoctests.yml?ref=7819911b0c4f490fbe72d45f37e4613c7cb78bdf",
            "patch": "@@ -15,7 +15,7 @@ jobs:\n   setup:\n     name: Setup\n     runs-on: \n-      group: aws-g4dn-2xlarge-cache\n+      group: aws-g4dn-4xlarge-cache\n     container:\n       image: huggingface/transformers-all-latest-gpu\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/"
        },
        {
            "sha": "b88304f7868d1ab33985d385a511bc7c951fe168",
            "filename": ".github/workflows/model_jobs.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fmodel_jobs.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fmodel_jobs.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs.yml?ref=7819911b0c4f490fbe72d45f37e4613c7cb78bdf",
            "patch": "@@ -107,7 +107,7 @@ jobs:\n         run: |\n           echo \"${{ inputs.machine_type }}\"\n \n-          if [ \"${{ inputs.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ inputs.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n           elif [ \"${{ inputs.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n             machine_type=multi-gpu"
        },
        {
            "sha": "61d0866e754a3f9f88d08232c8cb80e342a85ab1",
            "filename": ".github/workflows/self-comment-ci.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fself-comment-ci.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fself-comment-ci.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-comment-ci.yml?ref=7819911b0c4f490fbe72d45f37e4613c7cb78bdf",
            "patch": "@@ -185,7 +185,7 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.get-tests.outputs.models) }}\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n     runs-on:\n        group: '${{ matrix.machine_type }}'\n     container:\n@@ -239,7 +239,7 @@ jobs:\n         shell: bash\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n           elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n@@ -292,7 +292,7 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.get-tests.outputs.quantizations) }}\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -338,7 +338,7 @@ jobs:\n         shell: bash\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n           elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n             machine_type=multi-gpu"
        },
        {
            "sha": "1198148fd63ddcaa410a15ddaa8c439779bc74bd",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=7819911b0c4f490fbe72d45f37e4613c7cb78bdf",
            "patch": "@@ -49,7 +49,7 @@ jobs:\n     name: Setup\n     strategy:\n       matrix:\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -107,7 +107,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n         slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}\n     uses: ./.github/workflows/model_jobs.yml\n     with:\n@@ -125,7 +125,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n         slice_id: [0, 1]\n     uses: ./.github/workflows/model_jobs.yml\n     with:\n@@ -143,7 +143,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -177,7 +177,7 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n           elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n@@ -211,7 +211,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -246,7 +246,7 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n           elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n@@ -280,7 +280,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-2xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -314,7 +314,7 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n           elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n@@ -349,7 +349,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -411,7 +411,7 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n           elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n             machine_type=multi-gpu\n@@ -448,7 +448,7 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.setup.outputs.quantization_matrix) }}\n-        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n     runs-on:\n       group: '${{ matrix.machine_type }}'\n     container:\n@@ -491,7 +491,7 @@ jobs:\n         run: |\n           echo \"${{ matrix.machine_type }}\"\n \n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n             machine_type=single-gpu\n           elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n             machine_type=multi-gpu"
        },
        {
            "sha": "622773630330963e2cb600ab4a7b4b5b46672805",
            "filename": ".github/workflows/ssh-runner.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fssh-runner.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7819911b0c4f490fbe72d45f37e4613c7cb78bdf/.github%2Fworkflows%2Fssh-runner.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fssh-runner.yml?ref=7819911b0c4f490fbe72d45f37e4613c7cb78bdf",
            "patch": "@@ -35,7 +35,7 @@ jobs:\n         shell: bash\n         run: |\n           if [[ \"${{ github.event.inputs.num_gpus }}\" == \"single\" && \"${{ github.event.inputs.runner_type }}\" == \"t4\" ]]; then\n-            echo \"RUNNER=aws-g4dn-2xlarge-cache\" >> $GITHUB_ENV\n+            echo \"RUNNER=aws-g4dn-4xlarge-cache\" >> $GITHUB_ENV\n           elif [[ \"${{ github.event.inputs.num_gpus }}\" == \"multi\" && \"${{ github.event.inputs.runner_type }}\" == \"t4\" ]]; then\n             echo \"RUNNER=aws-g4dn-12xlarge-cache\" >> $GITHUB_ENV\n           elif [[ \"${{ github.event.inputs.num_gpus }}\" == \"single\" && \"${{ github.event.inputs.runner_type }}\" == \"a10\" ]]; then"
        }
    ],
    "stats": {
        "total": 44,
        "additions": 22,
        "deletions": 22
    }
}