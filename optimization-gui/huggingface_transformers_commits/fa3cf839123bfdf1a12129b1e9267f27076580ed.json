{
    "author": "LysandreJik",
    "message": "Migration guide: tokenization (#42477)\n\n* Adjust tokenization part\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Ita's changes\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "fa3cf839123bfdf1a12129b1e9267f27076580ed",
    "files": [
        {
            "sha": "a7b8cc294622f395d8457d290d9254a680bad4c9",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 120,
            "deletions": 81,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa3cf839123bfdf1a12129b1e9267f27076580ed/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa3cf839123bfdf1a12129b1e9267f27076580ed/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=fa3cf839123bfdf1a12129b1e9267f27076580ed",
            "patch": "@@ -74,22 +74,19 @@ While this is being implemented, expect varying levels of support across differe\n \n Linked PR: https://github.com/huggingface/transformers/pull/41580\n \n-\n-\n-\n ## Tokenization\n \n-Just as we moved towards a single backend library for model definition, we want `Tokenizer` to be a lot more intuitive.\n-With v5, you can now initialize an empty `LlamaTokenizer` and train it directly on your new task! \n+Just as we moved towards a single backend library for model definition, we want our tokenizers, and the `Tokenizer` object to be a lot more intuitive. With v5, tokenizer definition is much simpler; one can now initialize an empty `LlamaTokenizer` and train it directly on your corpus.\n \n Defining a new tokenizer object should be as simple as this:\n+\n ```python\n from transformers import TokenizersBackend, generate_merges\n from tokenizers import pre_tokenizers, Tokenizer\n from tokenizers.model import BPE\n \n class Llama5Tokenizer(TokenizersBackend):\n-    def __init__(self,        unk_token=\"<unk>\",bos_token=\"<s>\", eos_token=\"</s>\", vocab=None, merges=None ):\n+    def __init__(self, unk_token=\"<unk>\",bos_token=\"<s>\", eos_token=\"</s>\", vocab=None, merges=None ):\n         if vocab is None:\n             self._vocab = {\n                 str(unk_token): 0,\n@@ -119,110 +116,106 @@ class Llama5Tokenizer(TokenizersBackend):\n         )\n ```\n \n-And now if you call `Llama5Tokenizer()` you just get an empty, trainable tokenizer that follows the definition of the authors of `Llama5` (it does not exist yet :wink:).\n-\n-The above is the main motivation towards refactoring tokenization: we want people to just instantiate a tokenizer like they would a model, empty or not and with exactly what they defined.\n-\n-### Non-tokenizers\n-If you tokenizers is not common, or you just don't want to rely on `sentencepiece` nor `tokenizers` you can just import the `PythonBackend` (previousl `PreTrainedTokenzier`) which has all the API and logic for added tokens, encoding and decoding wieht them etc. \n-\n-If you want to have en less features, you can use the common `PreTrainedTokenizerBase` mixin, which mostly defines `transformers` tokenizer API: `encode`, `decode`, `vocab_size`, `get_vocab`, `convert_tokens_to_ids`, `convert_ids_to_tokens`, `from_pretrained`, `save_pretrained`, etc.\n+Once the tokenizer is defined as above, you can load it with the following: `Llama5Tokenizer()`. Doing this returns you an empty, trainable tokenizer that follows the definition of the authors of `Llama5` (it does not exist yet :wink:).\n \n-### Backend Architecture Changes\n+The above is the main motivation towards refactoring tokenization: we want tokenizers to behave similarly to models: trained or empty, and with exactly what is defined in their class definition.\n \n-**Moving away from \"slow\" vs \"fast\" tokenizers:**\n+### Backend Architecture Changes: moving away from the slow/fast tokenizer separation\n \n-Previously, transformers maintained two parallel implementations for many tokenizers:\n+Up to now, transformers maintained two parallel implementations for many tokenizers:\n - \"Slow\" tokenizers (`tokenization_<model>.py`) - Python-based implementations, often using [SentencePiece](https://github.com/google/sentencepiece) as the backend.\n - \"Fast\" tokenizers (`tokenization_<model>_fast.py`) - Rust-based implementations using the ü§ó [tokenizers](https://github.com/huggingface/tokenizers) library.\n \n In v5, we consolidate to a single tokenizer file per model: `tokenization_<model>.py`. This file will use the most appropriate backend available:\n \n-1. **TokenizersBackend** (preferred): Rust-based tokenizers from the ü§ó [tokenizers](https://github.com/huggingface/tokenizers) library. In general its performances are better, but it also offers a lot more features that are comonly adopted across the ecosystem, like handling additional tokens, easily update the state of the tokenizer, automatic parallelisation etc. \n-2. **SentencePieceBackend**: For models requiring SentencePiece\n-3. **PythonBackend**: Pure Python implementations\n-4. **MistralCommonBackend**: Relies on `MistralCommon`'s toknenization library. (Previously `MistralCommonTokenizer`)\n+1. **TokenizersBackend** (preferred): Rust-based tokenizers from the ü§ó [tokenizers](https://github.com/huggingface/tokenizers) library. In general it provides optimal performance, but it also offers a lot more features that are commonly adopted across the ecosystem:\n+  - handling additional tokens\n+  - a full python API for setting and updating \n+  - automatic parallelization,\n+  - automatic offsets\n+  - customization\n+  - training\n+2. **SentencePieceBackend**: for tokenizers requiring the `sentencepiece` library. It inherits from `PythonBackend`. \n+3. **PythonBackend**: a Python implementations of the features provided by `tokenizers`. Basically allows adding tokens.\n+4. **MistralCommonBackend**: relies on `MistralCommon`'s tokenization library. (Previously known as the `MistralCommonTokenizer`)\n \n The `AutoTokenizer` automatically selects the appropriate backend based on available files and dependencies. This is transparent, you continue to use `AutoTokenizer.from_pretrained()` as before. This allows transformers to be future-proof and modular to easily support future backends.\n \n+### Defining a tokenizers outside of the existing backends\n+\n+We enable users and tokenizer builders to define their own tokenizers from top to bottom. Tokenizers are usually defined using a backend such as `tokenizers`, `sentencepiece` or `mistral-common`, but we offer the possibility to design the tokenizer at a higher-level, without relying on those backends.\n+\n+To do so, you can import the `PythonBackend` (which was previously known as `PreTrainedTokenizer`). This class encapsulates all the logic related to added tokens, encoding, and decoding.\n+\n+If you want something even higher up the stack, then `PreTrainedTokenizerBase` is what `PythonBackend` inherits from. It contains the very basic tokenizer API features: \n+- `encode`\n+- `decode`\n+- `vocab_size`\n+- `get_vocab`\n+- `convert_tokens_to_ids`\n+- `convert_ids_to_tokens`\n+- `from_pretrained`\n+- `save_pretrained`\n+- among a few others\n \n ### API Changes\n \n-**1. Direct tokenizer initialization with vocab and merges:**\n+#### 1. Direct tokenizer initialization with vocab and merges\n \n-In v5, you can now initialize tokenizers directly with vocabulary and merges, enabling training custom tokenizers from scratch:\n+Starting with v5, we now enable initializing blank, untrained `tokenizers`-backed tokenizers:\n \n-```python\n-# v5: Initialize a blank tokenizer for training\n+```py\n+from transformers import LlamaTokenizer\n+\n+tokenizer = LlamaTokenizer()\n+```\n+\n+This tokenizer will therefore follow the definition of the `LlamaTokenizer` as defined in its class definition. It can then be trained on a corpus as can be seen in [the `tokenizers` documentation](https://huggingface.co/docs/tokenizers/training_from_memory).\n+\n+These tokenizers can also be initialized from vocab and merges (if necessary), like the previous \"slow\" tokenizers:\n+\n+```py\n from transformers import LlamaTokenizer\n \n-# Create a tokenizer with custom vocabulary and merges\n vocab = {\"<unk>\": 0, \"<s>\": 1, \"</s>\": 2, \"hello\": 3, \"world\": 4}\n merges = [(\"h\", \"e\"), (\"l\", \"l\"), (\"o\", \" \")]\n \n tokenizer = LlamaTokenizer(vocab=vocab, merges=merges)\n-\n-# Or initialize a blank tokenizer to train on your own dataset\n-tokenizer = LlamaTokenizer()  # Creates a blank Llama-like tokenizer\n ```\n-But you can no longer pass a vocab file. As this accounts for `from_pretrained` use-case.\n \n-**2. Simplified decoding API:**\n+This tokenizer will behave as a Llama-like tokenizer, with an updated vocabulary. This allows comparing different tokenizer classes with the same vocab; therefore enabling the comparison of different pre-tokenizers, normalizers, etc.\n+\n+‚ö†Ô∏è The `vocab_file` (as in, a path towards a file containing the vocabulary) cannot be used to initialize the `LlamaTokenizer` as loading from files is reserved to the `from_pretrained` method.\n+\n+#### 2. Simplified decoding API\n+\n+The `batch_decode` and `decode` methods have been unified to reflect behavior of the `encode` method. Both single and batch decoding now use the same `decode` method. See an example of the new behavior below:\n \n-The `batch_decode` method has been unified with `decode`. Both single and batch decoding now use the same method:\n ```python\n from transformers import AutoTokenizer\n tokenizer = AutoTokenizer.from_pretrained(\"t5-small\") \n inputs = [\"hey how are you?\", \"fine\"]\n tokenizer.decode(tokenizer.encode(inputs))\n ```\n+\n Gives:\n ```diff\n - 'hey how are you?</s> fine</s>'\n + ['hey how are you?</s>', 'fine</s>']\n ```\n \n-This is mostly because people get `list[list[int]]` out of `generate`, but then they would use `decode` because they use `encode` and would get:\n-```python\n-   ...: tokenizer.decode([[1,2], [1,4]])\n----------------------------------------------------------------------------\n-TypeError                                 Traceback (most recent call last)\n-Cell In[2], line 4\n-      2 tokenizer = AutoTokenizer.from_pretrained(\"t5-small\") \n-      3 inputs = [\"hey how are you?\", \"fine\"]\n-----> 4 tokenizer.decode([[1,2], [1,4]])\n-\n-File /raid/arthur/transformers/src/transformers/tokenization_utils_base.py:3948, in PreTrainedTokenizerBase.decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\n-   3945 # Convert inputs to python lists\n-   3946 token_ids = to_py_obj(token_ids)\n--> 3948 return self._decode(\n-   3949     token_ids=token_ids,\n-   3950     skip_special_tokens=skip_special_tokens,\n-   3951     clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n-   3952     **kwargs,\n-   3953 )\n-\n-File /raid/arthur/transformers/src/transformers/tokenization_utils_fast.py:682, in PreTrainedTokenizerFast._decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\n-    680 if isinstance(token_ids, int):\n-    681     token_ids = [token_ids]\n---> 682 text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n-    684 clean_up_tokenization_spaces = (\n-    685     clean_up_tokenization_spaces\n-    686     if clean_up_tokenization_spaces is not None\n-    687     else self.clean_up_tokenization_spaces\n-    688 )\n-    689 if clean_up_tokenization_spaces:\n-\n-TypeError: argument 'ids': 'list' object cannot be interpreted as an integer\n-```\n+We expect `encode` and `decode` to behave, as two sides of the same coin: `encode`, `process`, `decode`,  should work. \n+\n+> [!NOTE]\n+> A common use-case would be: `encode`, `model.generate`, `decode`.  However, using `generate` would return `list[list[int]]`, which would then be incompatible with `decode`.\n \n-**3. Unified encoding API:**\n+#### 3. Unified encoding API\n \n-The `encode_plus` is deprecated ‚Üí call directly with `__call__`\n+The `encode_plus` method is deprecated in favor of the single `__call__` method.\n \n-**3. `apply_chat_template` returns `BatchEncoding`:**\n+#### 4. `apply_chat_template` returns `BatchEncoding`\n \n-Previously, `apply_chat_template` returned `input_ids` for backward compatibility. In v5, it now consistently returns a `BatchEncoding` dict like other tokenizer methods:\n+Previously, `apply_chat_template` returned `input_ids` for backward compatibility. Starting with v5, it now consistently returns a `BatchEncoding` dict like other tokenizer methods.\n \n ```python\n # v5\n@@ -236,15 +229,17 @@ outputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n print(outputs.keys())  # dict_keys(['input_ids', 'attention_mask'])\n ```\n \n-#### Removed legacy configuration file saving:\n+#### 5. Removed legacy configuration file saving:\n+\n+We simplify the serialization of tokenization attributes:\n \n - `special_tokens_map.json` - special tokens are now stored in `tokenizer_config.json`.\n - `added_tokens.json` - added tokens are now stored in `tokenizer.json`.\n - `added_tokens_decoder` is only stored when there is no `tokenizer.json`.\n \n-When loading older tokenizers, these files are still read for backward compatibility, but new saves use the consolidated format.\n+When loading older tokenizers, these files are still read for backward compatibility, but new saves use the consolidated format. We're gradually moving towards consolidating attributes to fewer files so that other libraries and implementations may depend on them more reliably.\n \n-### Model-Specific Changes\n+#### 6. Model-Specific Changes\n \n Several models that had identical tokenizers now import from their base implementation:\n \n@@ -255,25 +250,69 @@ Several models that had identical tokenizers now import from their base implemen\n - **MT5** ‚Üí uses T5Tokenizer\n - **MVP** ‚Üí uses BartTokenizer\n \n-We're just gonna remove these files at term.\n+These modules will eventually be removed altogether.\n \n-**Removed T5-specific workarounds:**\n+**Removed T5-specific workarounds**\n \n The internal `_eventually_correct_t5_max_length` method has been removed. T5 tokenizers now handle max length consistently with other models.\n \n ### Testing Changes\n \n-Model-specific tokenization test files now focus on integration tests.\n-Common tokenization API tests (e.g., `add_tokens`, `encode`, `decode`) are now centralized and automatically applied across all tokenizers. This reduces test duplication and ensures consistent behavior\n-\n+A few testing changes specific to tokenizers have been applied:\n+- Model-specific tokenization test files now focus on integration tests.\n+- Common tokenization API tests (e.g., `add_tokens`, `encode`, `decode`) are now centralized and automatically applied across all tokenizers. This reduces test duplication and ensures consistent behavior\n \n For legacy implementations, the original BERT Python tokenizer code (including `WhitespaceTokenizer`, `BasicTokenizer`, etc.) is preserved in `bert_legacy.py` for reference purposes.\n \n-**Linked PRs:**\n-- https://github.com/huggingface/transformers/issues/40938\n-- https://github.com/huggingface/transformers/pull/40936\n-- https://github.com/huggingface/transformers/pull/41626\n+#### 7. Deprecated / Modified Features\n+\n+**Special Tokens Structure:**\n+- `SpecialTokensMixin`: Merged into `PreTrainedTokenizerBase` to simplify the tokenizer architecture.\n+- `special_tokens_map`: Now only stores named special token attributes (e.g., `bos_token`, `eos_token`). Use `extra_special_tokens` for additional special tokens (formerly `additional_special_tokens`). `all_special_tokens` includes both named and extra tokens.\n+\n+```python\n+# v4\n+tokenizer.special_tokens_map  # Included 'additional_special_tokens'\n+\n+# v5\n+tokenizer.special_tokens_map  # Only named tokens\n+tokenizer.extra_special_tokens  # Additional tokens\n+```\n+\n+- `special_tokens_map_extended` and `all_special_tokens_extended`: Removed. Access `AddedToken` objects directly from `_special_tokens_map` or `_extra_special_tokens` if needed.\n+- `additional_special_tokens`: Still accepted for backward compatibility but is automatically converted to `extra_special_tokens`.\n+\n+**Deprecated Methods:**\n+- `sanitize_special_tokens()`: Already deprecated in v4, removed in v5.\n+- `prepare_seq2seq_batch()`: Deprecated; use `__call__()` with `text_target` parameter instead.\n+\n+```python\n+# v4\n+model_inputs = tokenizer.prepare_seq2seq_batch(src_texts, tgt_texts, max_length=128)\n+\n+# v5\n+model_inputs = tokenizer(src_texts, text_target=tgt_texts, max_length=128, return_tensors=\"pt\")\n+model_inputs[\"labels\"] = model_inputs.pop(\"input_ids_target\")\n+```\n+\n+- `BatchEncoding.words()`: Deprecated; use `word_ids()` instead.\n+\n+**Removed Methods:**\n+- `create_token_type_ids_from_sequences()`: Removed from base class. Subclasses that need custom token type ID creation should implement this method directly.\n+- `clean_up_tokenization()`: Removed from base class. Now defined at model class level for models that need it (e.g., PLBart, CLVP, Wav2Vec2).\n+- `prepare_for_model()`, `build_inputs_with_special_tokens()`, `truncate_sequences()`: Moved from `tokenization_utils_base.py` to `tokenization_python.py` for `PythonBackend` tokenizers. `TokenizersBackend` provides model-ready input via `tokenize()` and `encode()`, so these methods are no longer needed in the base class.\n+- `_switch_to_input_mode()`, `_switch_to_target_mode()`, `as_target_tokenizer()`: Removed from base class. Use `__call__()` with `text_target` parameter instead.\n+\n+```python\n+# v4\n+with tokenizer.as_target_tokenizer():\n+    labels = tokenizer(tgt_texts, ...)\n+\n+# v5\n+labels = tokenizer(text_target=tgt_texts, ...)\n+```\n \n+- `parse_response()`: Removed from base class.\n \n ## Disclaimers for the RC0\n "
        }
    ],
    "stats": {
        "total": 201,
        "additions": 120,
        "deletions": 81
    }
}