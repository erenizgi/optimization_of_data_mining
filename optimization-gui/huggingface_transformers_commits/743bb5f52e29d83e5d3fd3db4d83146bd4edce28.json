{
    "author": "arpon-kapuria",
    "message": "chore: update Deformable_Detr model card (#39902)\n\n* chore: update Deformable_Detr model card\n\n* fix: added pipeline, automodel examples and checkpoints link\n\n* Update deformable_detr.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "743bb5f52e29d83e5d3fd3db4d83146bd4edce28",
    "files": [
        {
            "sha": "84c8de54962cc189f0a84a250f596cb366c42e84",
            "filename": "docs/source/en/model_doc/deformable_detr.md",
            "status": "modified",
            "additions": 61,
            "deletions": 21,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/743bb5f52e29d83e5d3fd3db4d83146bd4edce28/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/743bb5f52e29d83e5d3fd3db4d83146bd4edce28/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md?ref=743bb5f52e29d83e5d3fd3db4d83146bd4edce28",
            "patch": "@@ -14,43 +14,83 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Deformable DETR\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+\t<div class=\"flex flex-wrap space-x-1\">\n+\t\t<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+\t</div>\n </div>\n \n-## Overview\n-\n-The Deformable DETR model was proposed in [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://huggingface.co/papers/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.\n-Deformable DETR mitigates the slow convergence issues and limited feature spatial resolution of the original [DETR](detr) by leveraging a new deformable attention module which only attends to a small set of key sampling points around a reference.\n-\n-The abstract from the paper is the following:\n+# Deformable DETR\n \n-*DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.*\n+[Deformable DETR](https://huggingface.co/papers/2010.04159) improves on the original [DETR](./detr) by using a deformable attention module. This mechanism selectively attends to a small set of key sampling points around a reference. It improves training speed and improves accuracy.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/deformable_detr_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n <small> Deformable DETR architecture. Taken from the <a href=\"https://huggingface.co/papers/2010.04159\">original paper</a>.</small>\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/fundamentalvision/Deformable-DETR).\n+You can find all the available Deformable DETR checkpoints under the [SenseTime](https://huggingface.co/SenseTime) organization.\n \n-## Usage tips\n+> [!TIP]\n+> This model was contributed by [nielsr](https://huggingface.co/nielsr).\n+>\n+> Click on the Deformable DETR models in the right sidebar for more examples of how to apply Deformable DETR to different object detection and segmentation tasks.\n \n-- Training Deformable DETR is equivalent to training the original [DETR](detr) model. See the [resources](#resources) section below for demo notebooks.\n+The example below demonstrates how to perform object detection with the [`Pipeline`] and the [`AutoModel`] class.\n \n-## Resources\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```python\n+from transformers import pipeline\n+import torch\n+\n+pipeline = pipeline(\n+    \"object-detection\", \n+    model=\"SenseTime/deformable-detr\",\n+    torch_dtype=torch.float16,\n+    device_map=0\n+)\n+\n+pipeline(\"http://images.cocodataset.org/val2017/000000039769.jpg\")\n+```\n \n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with Deformable DETR.\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-<PipelineTag pipeline=\"object-detection\"/>\n+```python\n+from transformers import AutoImageProcessor, AutoModelForObjectDetection\n+from PIL import Image\n+import requests\n+import torch\n \n-- Demo notebooks regarding inference + fine-tuning on a custom dataset for [`DeformableDetrForObjectDetection`] can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Deformable-DETR).\n-- Scripts for finetuning [`DeformableDetrForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n-- See also: [Object detection task guide](../tasks/object_detection).\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+image_processor = AutoImageProcessor.from_pretrained(\"SenseTime/deformable-detr\")\n+model = AutoModelForObjectDetection.from_pretrained(\"SenseTime/deformable-detr\")\n+\n+# prepare image for the model\n+inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+\n+results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)\n+\n+for result in results:\n+    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n+        score, label = score.item(), label_id.item()\n+        box = [round(i, 2) for i in box.tolist()]\n+        print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Resources\n \n-If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+- Refer to this set of [notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Deformable-DETR) for inference and fine-tuning [`DeformableDetrForObjectDetection`] on a custom dataset.\n \n ## DeformableDetrImageProcessor\n "
        }
    ],
    "stats": {
        "total": 82,
        "additions": 61,
        "deletions": 21
    }
}