{
    "author": "cyyever",
    "message": "Use newer typing notation (#38934)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "43f07018cff3c305e9d3a3782e3a95100800be4c",
    "files": [
        {
            "sha": "ee28e08f2d43174f252c53cbf4dd3d0f9ecd5a3e",
            "filename": "src/transformers/generation/streamers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstreamers.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,7 @@\n \n import asyncio\n from queue import Queue\n-from typing import TYPE_CHECKING, Optional\n+from typing import TYPE_CHECKING\n \n \n if TYPE_CHECKING:\n@@ -206,7 +206,7 @@ class TextIteratorStreamer(TextStreamer):\n     \"\"\"\n \n     def __init__(\n-        self, tokenizer: AutoTokenizer, skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs\n+        self, tokenizer: AutoTokenizer, skip_prompt: bool = False, timeout: float | None = None, **decode_kwargs\n     ):\n         super().__init__(tokenizer, skip_prompt, **decode_kwargs)\n         self.text_queue = Queue()\n@@ -284,7 +284,7 @@ class AsyncTextIteratorStreamer(TextStreamer):\n     \"\"\"\n \n     def __init__(\n-        self, tokenizer: AutoTokenizer, skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs\n+        self, tokenizer: AutoTokenizer, skip_prompt: bool = False, timeout: float | None = None, **decode_kwargs\n     ):\n         super().__init__(tokenizer, skip_prompt, **decode_kwargs)\n         self.text_queue = asyncio.Queue()"
        },
        {
            "sha": "96a955d4b76a803067532d7a624292ddda959852",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n import os\n import re\n from functools import partial, reduce\n-from typing import Optional, Union\n \n import torch\n import torch.distributed as dist\n@@ -94,7 +93,7 @@ def initialize_tensor_parallelism(tp_plan, tp_size=None):\n     return tp_device, device_map, device_mesh\n \n \n-def _blocks_to_block_sizes(total_size: int, blocks: Union[int, list[int]]) -> list[int]:\n+def _blocks_to_block_sizes(total_size: int, blocks: int | list[int]) -> list[int]:\n     \"\"\"\n     Convert block count or proportions to block sizes.\n \n@@ -120,7 +119,7 @@ def _blocks_to_block_sizes(total_size: int, blocks: Union[int, list[int]]) -> li\n         return [single_size] * blocks\n \n \n-def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str]) -> Optional[str]:\n+def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str]) -> str | None:\n     \"\"\"\n     Get the TP style for a parameter from the TP plan.\n \n@@ -412,8 +411,8 @@ class GatherParallel(TensorParallelLayer):\n     def __init__(\n         self,\n         *,\n-        input_layouts: Optional[Placement] = None,\n-        output_layouts: Optional[Placement] = None,\n+        input_layouts: Placement | None = None,\n+        output_layouts: Placement | None = None,\n         use_local_output: bool = True,\n     ):\n         super().__init__()\n@@ -506,8 +505,8 @@ class ColwiseParallel(TensorParallelLayer):\n     def __init__(\n         self,\n         *,\n-        input_layouts: Optional[Placement] = None,\n-        output_layouts: Optional[Placement] = None,\n+        input_layouts: Placement | None = None,\n+        output_layouts: Placement | None = None,\n         use_local_output: bool = True,\n         use_dtensor=True,\n     ):\n@@ -596,8 +595,8 @@ class RowwiseParallel(TensorParallelLayer):\n     def __init__(\n         self,\n         *,\n-        input_layouts: Optional[Placement] = None,\n-        output_layouts: Optional[Placement] = None,\n+        input_layouts: Placement | None = None,\n+        output_layouts: Placement | None = None,\n         use_local_output: bool = True,\n         use_dtensor=True,\n     ):\n@@ -937,7 +936,7 @@ def shard_and_distribute_module(\n     return param\n \n \n-def verify_tp_plan(expected_keys: list[str], tp_plan: Optional[dict[str, str]]):\n+def verify_tp_plan(expected_keys: list[str], tp_plan: dict[str, str] | None):\n     \"\"\"\n     Verify the TP plan of the model, log a warning if the layers that were not sharded and the rules that were not applied.\n     \"\"\""
        },
        {
            "sha": "c7491b67f9aebb93b95632a5a7db2fd85cf5b4c7",
            "filename": "src/transformers/modeling_tf_outputs.py",
            "status": "modified",
            "additions": 35,
            "deletions": 36,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodeling_tf_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodeling_tf_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_outputs.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -16,7 +16,6 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Optional\n \n import tensorflow as tf\n \n@@ -44,7 +43,7 @@ class TFBaseModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -64,8 +63,8 @@ class TFBaseModelOutputWithNoAttention(ModelOutput):\n             Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    hidden_states: Optional[tuple[tf.Tensor, ...]] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    hidden_states: tuple[tf.Tensor, ...] | None = None\n \n \n @dataclass\n@@ -96,8 +95,8 @@ class TFBaseModelOutputWithPooling(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    pooler_output: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    pooler_output: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -119,9 +118,9 @@ class TFBaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n             Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    pooler_output: Optional[tf.Tensor] = None\n-    hidden_states: Optional[tuple[tf.Tensor, ...]] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    pooler_output: tf.Tensor | None = None\n+    hidden_states: tuple[tf.Tensor, ...] | None = None\n \n \n @dataclass\n@@ -164,8 +163,8 @@ class TFBaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    pooler_output: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    pooler_output: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -202,7 +201,7 @@ class TFBaseModelOutputWithPast(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -235,7 +234,7 @@ class TFBaseModelOutputWithCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n     cross_attentions: tuple[tf.Tensor] | None = None\n@@ -277,7 +276,7 @@ class TFBaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -334,7 +333,7 @@ class TFSeq2SeqModelOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     decoder_hidden_states: tuple[tf.Tensor] | None = None\n     decoder_attentions: tuple[tf.Tensor] | None = None\n@@ -368,7 +367,7 @@ class TFCausalLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -403,7 +402,7 @@ class TFCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -445,7 +444,7 @@ class TFCausalLMOutputWithCrossAttentions(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -476,7 +475,7 @@ class TFMaskedLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -530,7 +529,7 @@ class TFSeq2SeqLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     decoder_hidden_states: tuple[tf.Tensor] | None = None\n     decoder_attentions: tuple[tf.Tensor] | None = None\n@@ -565,7 +564,7 @@ class TFNextSentencePredictorOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -594,7 +593,7 @@ class TFSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -645,7 +644,7 @@ class TFSeq2SeqSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     decoder_hidden_states: tuple[tf.Tensor] | None = None\n     decoder_attentions: tuple[tf.Tensor] | None = None\n@@ -687,7 +686,7 @@ class TFSemanticSegmenterOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -719,7 +718,7 @@ class TFSemanticSegmenterOutputWithNoAttention(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n \n \n@@ -745,7 +744,7 @@ class TFImageClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -776,7 +775,7 @@ class TFMultipleChoiceModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -805,7 +804,7 @@ class TFTokenClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -836,8 +835,8 @@ class TFQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    start_logits: Optional[tf.Tensor] = None\n-    end_logits: Optional[tf.Tensor] = None\n+    start_logits: tf.Tensor | None = None\n+    end_logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -887,8 +886,8 @@ class TFSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    start_logits: Optional[tf.Tensor] = None\n-    end_logits: Optional[tf.Tensor] = None\n+    start_logits: tf.Tensor | None = None\n+    end_logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     decoder_hidden_states: tuple[tf.Tensor] | None = None\n     decoder_attentions: tuple[tf.Tensor] | None = None\n@@ -927,7 +926,7 @@ class TFSequenceClassifierOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -950,8 +949,8 @@ class TFImageClassifierOutputWithNoAttention(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n-    hidden_states: Optional[tuple[tf.Tensor, ...]] = None\n+    logits: tf.Tensor | None = None\n+    hidden_states: tuple[tf.Tensor, ...] | None = None\n \n \n @dataclass\n@@ -977,7 +976,7 @@ class TFMaskedImageModelingOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    reconstruction: Optional[tf.Tensor] = None\n+    reconstruction: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "5f70e4e2b9d0d4173a468106e68e97a5d5f32d0f",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 35,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -27,7 +27,7 @@\n import warnings\n from collections.abc import Mapping\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Callable, Optional, Union\n+from typing import TYPE_CHECKING, Any, Callable, Union\n \n import h5py\n import numpy as np\n@@ -1412,10 +1412,10 @@ def prepare_tf_dataset(\n         dataset: datasets.Dataset,  # noqa:F821\n         batch_size: int = 8,\n         shuffle: bool = True,\n-        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n-        collate_fn: Optional[Callable] = None,\n-        collate_fn_args: Optional[dict[str, Any]] = None,\n-        drop_remainder: Optional[bool] = None,\n+        tokenizer: PreTrainedTokenizerBase | None = None,\n+        collate_fn: Callable | None = None,\n+        collate_fn_args: dict[str, Any] | None = None,\n+        drop_remainder: bool | None = None,\n         prefetch: bool = True,\n     ):\n         \"\"\"\n@@ -1811,14 +1811,14 @@ def create_model_card(\n         self,\n         output_dir,\n         model_name: str,\n-        language: Optional[str] = None,\n-        license: Optional[str] = None,\n-        tags: Optional[str] = None,\n-        finetuned_from: Optional[str] = None,\n-        tasks: Optional[str] = None,\n-        dataset_tags: Optional[Union[str, list[str]]] = None,\n-        dataset: Optional[Union[str, list[str]]] = None,\n-        dataset_args: Optional[Union[str, list[str]]] = None,\n+        language: str | None = None,\n+        license: str | None = None,\n+        tags: str | None = None,\n+        finetuned_from: str | None = None,\n+        tasks: str | None = None,\n+        dataset_tags: str | list[str] | None = None,\n+        dataset: str | list[str] | None = None,\n+        dataset_args: str | list[str] | None = None,\n     ):\n         \"\"\"\n         Creates a draft of a model card using the information available to the `Trainer`.\n@@ -1887,7 +1887,7 @@ def set_input_embeddings(self, value):\n             self.build_in_name_scope()\n             main_layer.set_input_embeddings(value)\n \n-    def get_output_embeddings(self) -> Union[None, keras.layers.Layer]:\n+    def get_output_embeddings(self) -> None | keras.layers.Layer:\n         \"\"\"\n         Returns the model's output embeddings\n \n@@ -1924,7 +1924,7 @@ def set_output_embeddings(self, value):\n                 self.build_in_name_scope()\n                 lm_head.set_output_embeddings(value)\n \n-    def get_output_layer_with_bias(self) -> Union[None, keras.layers.Layer]:\n+    def get_output_layer_with_bias(self) -> None | keras.layers.Layer:\n         \"\"\"\n         Get the layer that handles a bias attribute in case the model has an LM head with weights tied to the\n         embeddings\n@@ -1937,7 +1937,7 @@ def get_output_layer_with_bias(self) -> Union[None, keras.layers.Layer]:\n         )\n         return self.get_lm_head()\n \n-    def get_prefix_bias_name(self) -> Union[None, str]:\n+    def get_prefix_bias_name(self) -> None | str:\n         \"\"\"\n         Get the concatenated _prefix name of the bias from the model name to the parent layer\n \n@@ -1947,7 +1947,7 @@ def get_prefix_bias_name(self) -> Union[None, str]:\n         warnings.warn(\"The method get_prefix_bias_name is deprecated. Please use `get_bias` instead.\", FutureWarning)\n         return None\n \n-    def get_bias(self) -> Union[None, dict[str, tf.Variable]]:\n+    def get_bias(self) -> None | dict[str, tf.Variable]:\n         \"\"\"\n         Dict of bias attached to an LM head. The key represents the name of the bias attribute.\n \n@@ -1989,9 +1989,7 @@ def get_lm_head(self) -> keras.layers.Layer:\n         \"\"\"\n         return None\n \n-    def resize_token_embeddings(\n-        self, new_num_tokens: Optional[int] = None\n-    ) -> Union[keras.layers.Embedding, tf.Variable]:\n+    def resize_token_embeddings(self, new_num_tokens: int | None = None) -> keras.layers.Embedding | tf.Variable:\n         \"\"\"\n         Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n \n@@ -2022,7 +2020,7 @@ def resize_token_embeddings(\n \n         return model_embeds\n \n-    def _v2_resized_token_embeddings(self, new_num_tokens: Optional[int] = None) -> keras.layers.Embedding:\n+    def _v2_resized_token_embeddings(self, new_num_tokens: int | None = None) -> keras.layers.Embedding:\n         \"\"\"\n         Resizes input token embeddings matrix of the model if `new_num_tokens != config.vocab_size`.\n \n@@ -2346,10 +2344,10 @@ def save_pretrained(\n         version=1,\n         push_to_hub=False,\n         signatures=None,\n-        max_shard_size: Union[int, str] = \"5GB\",\n+        max_shard_size: int | str = \"5GB\",\n         create_pr: bool = False,\n         safe_serialization: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -2525,16 +2523,16 @@ def save_pretrained(\n     @classmethod\n     def from_pretrained(\n         cls,\n-        pretrained_model_name_or_path: Optional[Union[str, os.PathLike]],\n+        pretrained_model_name_or_path: str | os.PathLike | None,\n         *model_args,\n-        config: Optional[Union[PretrainedConfig, str, os.PathLike]] = None,\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        config: PretrainedConfig | str | os.PathLike | None = None,\n+        cache_dir: str | os.PathLike | None = None,\n         ignore_mismatched_sizes: bool = False,\n         force_download: bool = False,\n         local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         revision: str = \"main\",\n-        use_safetensors: Optional[bool] = None,\n+        use_safetensors: bool | None = None,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -3121,13 +3119,13 @@ def from_pretrained(\n     def push_to_hub(\n         self,\n         repo_id: str,\n-        use_temp_dir: Optional[bool] = None,\n-        commit_message: Optional[str] = None,\n-        private: Optional[bool] = None,\n-        max_shard_size: Optional[Union[int, str]] = \"10GB\",\n-        token: Optional[Union[bool, str]] = None,\n+        use_temp_dir: bool | None = None,\n+        commit_message: str | None = None,\n+        private: bool | None = None,\n+        max_shard_size: int | str | None = \"10GB\",\n+        token: bool | str | None = None,\n         # (`use_auth_token` is deprecated: we have to keep it here as we don't have **kwargs)\n-        use_auth_token: Optional[Union[bool, str]] = None,\n+        use_auth_token: bool | str | None = None,\n         create_pr: bool = False,\n         **base_model_card_args,\n     ) -> str:\n@@ -3314,7 +3312,7 @@ class TFSharedEmbeddings(keras.layers.Layer):\n \n     # TODO (joao): flagged for detection due to embeddings refactor\n \n-    def __init__(self, vocab_size: int, hidden_size: int, initializer_range: Optional[float] = None, **kwargs):\n+    def __init__(self, vocab_size: int, hidden_size: int, initializer_range: float | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.vocab_size = vocab_size\n         self.hidden_size = hidden_size"
        },
        {
            "sha": "101ab63dc0545992fe68a53205f4ad81c607d9ca",
            "filename": "src/transformers/models/albert/modeling_tf_albert.py",
            "status": "modified",
            "additions": 48,
            "deletions": 49,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -164,10 +163,10 @@ def build(self, input_shape=None):\n     # Copied from transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.call\n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:\n@@ -403,7 +402,7 @@ def call(\n         output_attentions: bool,\n         output_hidden_states: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         layer_hidden_states = () if output_hidden_states else None\n         layer_attentions = () if output_attentions else None\n \n@@ -466,7 +465,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         hidden_states = self.embedding_hidden_mapping_in(inputs=hidden_states)\n         all_attentions = () if output_attentions else None\n         all_hidden_states = (hidden_states,) if output_hidden_states else None\n@@ -629,11 +628,11 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -749,9 +748,9 @@ class TFAlbertForPreTrainingOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: Optional[tf.Tensor] = None\n-    prediction_logits: Optional[tf.Tensor] = None\n-    sop_logits: Optional[tf.Tensor] = None\n+    loss: tf.Tensor | None = None\n+    prediction_logits: tf.Tensor | None = None\n+    sop_logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -879,11 +878,11 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         outputs = self.albert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -942,13 +941,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         sentence_order_label: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFAlbertForPreTrainingOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFAlbertForPreTrainingOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         Return:\n \n@@ -1070,12 +1069,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1193,12 +1192,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1290,12 +1289,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1382,13 +1381,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1485,12 +1484,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`"
        },
        {
            "sha": "0a6d2317d696f9a9d1edce17f256ceccc8134e49",
            "filename": "src/transformers/models/bart/modeling_tf_bart.py",
            "status": "modified",
            "additions": 52,
            "deletions": 53,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_tf_bart.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import random\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -103,7 +102,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n     return tf.tile(mask[None, None, :, :], (bsz, 1, 1, 1))\n \n \n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -129,7 +128,7 @@ def __init__(self, num_embeddings: int, embedding_dim: int, **kwargs):\n \n     def call(\n         self,\n-        input_shape: Optional[tf.TensorShape] = None,\n+        input_shape: tf.TensorShape | None = None,\n         past_key_values_length: int = 0,\n         position_ids: tf.Tensor | None = None,\n     ):\n@@ -184,7 +183,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -334,7 +333,7 @@ def call(\n         hidden_states: tf.Tensor,\n         attention_mask: np.ndarray | tf.Tensor | None,\n         layer_head_mask: tf.Tensor | None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tf.Tensor:\n         \"\"\"\n         Args:\n@@ -427,8 +426,8 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n-        past_key_value: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        training: Optional[bool] = False,\n+        past_key_value: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n@@ -750,7 +749,7 @@ class TFBartEncoder(keras.layers.Layer):\n         config: BartConfig\n     \"\"\"\n \n-    def __init__(self, config: BartConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: BartConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.dropout = keras.layers.Dropout(config.dropout)\n@@ -776,11 +775,11 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         \"\"\"\n         Args:\n             input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n@@ -910,7 +909,7 @@ class TFBartDecoder(keras.layers.Layer):\n         embed_tokens: output embedding\n     \"\"\"\n \n-    def __init__(self, config: BartConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: BartConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.padding_idx = config.pad_token_id\n@@ -938,13 +937,13 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         Args:\n             input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n@@ -1167,17 +1166,17 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[TFSeq2SeqModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFSeq2SeqModelOutput | tuple[tf.Tensor]:\n         # different to other models, Bart automatically creates decoder_input_ids from\n         # input_ids if no decoder_input_ids are provided\n         if decoder_input_ids is None and decoder_inputs_embeds is None:\n@@ -1297,17 +1296,17 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1429,17 +1428,17 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[TFBaseModelOutput] = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        encoder_outputs: TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSeq2SeqLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSeq2SeqLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1600,17 +1599,17 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[TFBaseModelOutput] = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        encoder_outputs: TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSeq2SeqSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSeq2SeqSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "1ca82f9f18200fafc165d851da0f24591b259aa8",
            "filename": "src/transformers/models/bert/modeling_tf_bert.py",
            "status": "modified",
            "additions": 65,
            "deletions": 66,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -20,7 +20,6 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -161,10 +160,10 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:\n@@ -589,12 +588,12 @@ def call(\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None,\n-        use_cache: Optional[bool],\n+        use_cache: bool | None,\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -851,13 +850,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -1048,10 +1047,10 @@ class TFBertForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    prediction_logits: Optional[tf.Tensor] = None\n-    seq_relationship_logits: Optional[tf.Tensor] = None\n-    hidden_states: Optional[Union[tuple[tf.Tensor], tf.Tensor]] = None\n-    attentions: Optional[Union[tuple[tf.Tensor], tf.Tensor]] = None\n+    prediction_logits: tf.Tensor | None = None\n+    seq_relationship_logits: tf.Tensor | None = None\n+    hidden_states: tuple[tf.Tensor] | tf.Tensor | None = None\n+    attentions: tuple[tf.Tensor] | tf.Tensor | None = None\n \n \n BERT_START_DOCSTRING = r\"\"\"\n@@ -1179,13 +1178,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1273,13 +1272,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         next_sentence_label: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBertForPreTrainingOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFBertForPreTrainingOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1405,12 +1404,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1509,15 +1508,15 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFCausalLMOutputWithCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1617,12 +1616,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         next_sentence_label: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFNextSentencePredictorOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFNextSentencePredictorOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         Return:\n \n@@ -1731,12 +1730,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1820,12 +1819,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1946,12 +1945,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -2045,13 +2044,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "78f4f6a6761e71087b5d232a42cb137595af9702",
            "filename": "src/transformers/models/blenderbot/modeling_tf_blenderbot.py",
            "status": "modified",
            "additions": 23,
            "deletions": 24,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_tf_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_tf_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_tf_blenderbot.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n import os\n import random\n import warnings\n-from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -104,7 +103,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -179,7 +178,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -330,7 +329,7 @@ def call(\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n         layer_head_mask: tf.Tensor,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ):\n         \"\"\"\n         Args:\n@@ -425,7 +424,7 @@ def call(\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n         past_key_value: tuple[tf.Tensor] | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n@@ -687,7 +686,7 @@ class TFBlenderbotEncoder(keras.layers.Layer):\n         config: BlenderbotConfig\n     \"\"\"\n \n-    def __init__(self, config: BlenderbotConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: BlenderbotConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.dropout = keras.layers.Dropout(config.dropout)\n@@ -859,7 +858,7 @@ class TFBlenderbotDecoder(keras.layers.Layer):\n         embed_tokens: output embedding\n     \"\"\"\n \n-    def __init__(self, config: BlenderbotConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: BlenderbotConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.padding_idx = config.pad_token_id\n@@ -1128,7 +1127,7 @@ def call(\n         head_mask=None,\n         decoder_head_mask=None,\n         cross_attn_head_mask=None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n         past_key_values=None,\n         inputs_embeds=None,\n         decoder_inputs_embeds=None,\n@@ -1230,7 +1229,7 @@ def get_decoder(self):\n         return self.model.decoder\n \n     @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n+    def from_pretrained(cls, pretrained_model_name_or_path: str | os.PathLike | None, *model_args, **kwargs):\n         if pretrained_model_name_or_path == \"facebook/blenderbot-90M\":\n             from ..blenderbot_small import TFBlenderbotSmallModel\n \n@@ -1262,17 +1261,17 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n         past_key_values: list[tf.Tensor] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[tuple[tf.Tensor], TFSeq2SeqModelOutput]:\n+    ) -> tuple[tf.Tensor] | TFSeq2SeqModelOutput:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1385,7 +1384,7 @@ def set_bias(self, value):\n         self.bias_layer.bias.assign(value[\"final_logits_bias\"])\n \n     @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, **kwargs):\n+    def from_pretrained(cls, pretrained_model_name_or_path: str | os.PathLike | None, *model_args, **kwargs):\n         if pretrained_model_name_or_path == \"facebook/blenderbot-90M\":\n             from ..blenderbot_small import TFBlenderbotSmallForConditionalGeneration\n \n@@ -1414,17 +1413,17 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n         past_key_values: list[tf.Tensor] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple[tf.Tensor], TFSeq2SeqLMOutput]:\n+        training: bool | None = False,\n+    ) -> tuple[tf.Tensor] | TFSeq2SeqLMOutput:\n         r\"\"\"\n         labels (`tf.tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "be7711801ed238e9e5b06c665e38befdb3d86a75",
            "filename": "src/transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py",
            "status": "modified",
            "additions": 22,
            "deletions": 23,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_tf_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_tf_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_tf_blenderbot_small.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import random\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -103,7 +102,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -179,7 +178,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -330,7 +329,7 @@ def call(\n         hidden_states: tf.Tensor,\n         attention_mask: np.ndarray | tf.Tensor | None,\n         layer_head_mask: tf.Tensor | None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tf.Tensor:\n         \"\"\"\n         Args:\n@@ -424,8 +423,8 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n-        past_key_value: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        training: Optional[bool] = False,\n+        past_key_value: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n@@ -691,7 +690,7 @@ class TFBlenderbotSmallEncoder(keras.layers.Layer):\n         config: BlenderbotSmallConfig\n     \"\"\"\n \n-    def __init__(self, config: BlenderbotSmallConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: BlenderbotSmallConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.dropout = keras.layers.Dropout(config.dropout)\n@@ -863,7 +862,7 @@ class TFBlenderbotSmallDecoder(keras.layers.Layer):\n         embed_tokens: output embedding\n     \"\"\"\n \n-    def __init__(self, config: BlenderbotSmallConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: BlenderbotSmallConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.padding_idx = config.pad_token_id\n@@ -1129,7 +1128,7 @@ def call(\n         head_mask=None,\n         decoder_head_mask=None,\n         cross_attn_head_mask=None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n         past_key_values=None,\n         inputs_embeds=None,\n         decoder_inputs_embeds=None,\n@@ -1247,17 +1246,17 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n         past_key_values: list[tf.Tensor] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[tuple[tf.Tensor], TFSeq2SeqModelOutput]:\n+    ) -> tuple[tf.Tensor] | TFSeq2SeqModelOutput:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1383,17 +1382,17 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[TFBaseModelOutput] = None,\n+        encoder_outputs: TFBaseModelOutput | None = None,\n         past_key_values: list[tf.Tensor] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple[tf.Tensor], TFSeq2SeqLMOutput]:\n+        training: bool | None = False,\n+    ) -> tuple[tf.Tensor] | TFSeq2SeqLMOutput:\n         r\"\"\"\n         labels (`tf.tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "a1a1f7928273c69de30b4ba1c190435d91dfd509",
            "filename": "src/transformers/models/blip/modeling_tf_blip.py",
            "status": "modified",
            "additions": 55,
            "deletions": 55,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,7 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import tensorflow as tf\n \n@@ -96,7 +96,7 @@ class TFBlipForConditionalGenerationModelOutput(ModelOutput):\n     loss: tuple[tf.Tensor] | None = None\n     logits: tuple[tf.Tensor] | None = None\n     image_embeds: tf.Tensor | None = None\n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n \n@@ -138,7 +138,7 @@ class TFBlipTextVisionModelOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     image_embeds: tf.Tensor | None = None\n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n \n@@ -179,7 +179,7 @@ class TFBlipImageTextMatchingModelOutput(ModelOutput):\n     itm_score: tf.Tensor | None = None\n     loss: tf.Tensor | None = None\n     image_embeds: tf.Tensor | None = None\n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     vision_pooler_output: tf.Tensor | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n@@ -209,10 +209,10 @@ class TFBlipOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits_per_image: Optional[tf.Tensor] = None\n-    logits_per_text: Optional[tf.Tensor] = None\n-    text_embeds: Optional[tf.Tensor] = None\n-    image_embeds: Optional[tf.Tensor] = None\n+    logits_per_image: tf.Tensor | None = None\n+    logits_per_text: tf.Tensor | None = None\n+    text_embeds: tf.Tensor | None = None\n+    image_embeds: tf.Tensor | None = None\n     text_model_output: TFBaseModelOutputWithPooling = None\n     vision_model_output: TFBaseModelOutputWithPooling = None\n \n@@ -309,9 +309,9 @@ def build(self, input_shape: tf.TensorShape = None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n     ) -> tf.Tensor:\n         \"\"\"\n         Applies embedding based on inputs tensor.\n@@ -367,8 +367,8 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n-        training: Optional[bool] = None,\n+        output_attentions: bool | None = False,\n+        training: bool | None = None,\n     ) -> tuple[tf.Tensor, tf.Tensor | None, tuple[tf.Tensor] | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -470,8 +470,8 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        output_attentions: Optional[bool] = False,\n-        training: Optional[bool] = None,\n+        output_attentions: bool | None = False,\n+        training: bool | None = None,\n     ) -> tuple[tf.Tensor]:\n         \"\"\"\n         Args:\n@@ -624,11 +624,11 @@ def call(\n         self,\n         inputs_embeds,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = None,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = None,\n+    ) -> tuple | TFBaseModelOutput:\n         r\"\"\"\n         Args:\n             inputs_embeds (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -723,11 +723,11 @@ def serving_output(self, output: TFBaseModelOutputWithPooling) -> TFBaseModelOut\n     def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = None,\n-    ) -> Union[tuple, TFBaseModelOutputWithPooling]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = None,\n+    ) -> tuple | TFBaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n \n@@ -861,12 +861,12 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         position_ids: tf.Tensor | None = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = None,\n-    ) -> Union[tuple, TFBlipOutput]:\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = None,\n+    ) -> tuple | TFBlipOutput:\n         # Use BLIP model's config for some fields (if specified) instead of those of vision & text components.\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -954,12 +954,12 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         position_ids: tf.Tensor | None = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = None,\n-    ) -> Union[tuple, TFBlipOutput]:\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = None,\n+    ) -> tuple | TFBlipOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1003,7 +1003,7 @@ def get_text_features(\n         input_ids: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         position_ids: tf.Tensor | None = None,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n     ) -> tf.Tensor:\n         r\"\"\"\n         Returns:\n@@ -1039,7 +1039,7 @@ def get_text_features(\n     def get_image_features(\n         self,\n         pixel_values: tf.Tensor | None = None,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n     ) -> tf.Tensor:\n         r\"\"\"\n         Returns:\n@@ -1116,12 +1116,12 @@ def call(\n         pixel_values: tf.Tensor,\n         input_ids: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = None,\n-    ) -> Union[tuple, TFBlipForConditionalGenerationModelOutput]:\n+        return_dict: bool | None = None,\n+        training: bool | None = None,\n+    ) -> tuple | TFBlipForConditionalGenerationModelOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1333,12 +1333,12 @@ def call(\n         decoder_input_ids: tf.Tensor | None = None,\n         decoder_attention_mask: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = None,\n-    ) -> Union[tuple, TFBlipTextVisionModelOutput]:\n+        return_dict: bool | None = None,\n+        training: bool | None = None,\n+    ) -> tuple | TFBlipTextVisionModelOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1586,13 +1586,13 @@ def call(\n         self,\n         input_ids: tf.Tensor,\n         pixel_values: tf.Tensor | None = None,\n-        use_itm_head: Optional[bool] = True,\n+        use_itm_head: bool | None = True,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = None,\n-    ) -> Union[tuple, TFBlipImageTextMatchingModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = None,\n+    ) -> tuple | TFBlipImageTextMatchingModelOutput:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "ec86f6d26acf858faf99509ee709ba1bc990575c",
            "filename": "src/transformers/models/blip/modeling_tf_blip_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import math\n-from typing import Optional\n \n import tensorflow as tf\n \n@@ -303,7 +302,7 @@ def __init__(self, config: BlipTextConfig, **kwargs):\n         self.dropout = keras.layers.Dropout(rate=config.hidden_dropout_prob)\n         self.config = config\n \n-    def call(self, hidden_states: tf.Tensor, input_tensor: tf.Tensor, training: Optional[bool] = None) -> tf.Tensor:\n+    def call(self, hidden_states: tf.Tensor, input_tensor: tf.Tensor, training: bool | None = None) -> tf.Tensor:\n         hidden_states = self.dense(inputs=hidden_states)\n         hidden_states = self.dropout(inputs=hidden_states, training=training)\n         hidden_states = self.LayerNorm(inputs=hidden_states + input_tensor)\n@@ -338,8 +337,8 @@ def call(\n         encoder_hidden_states: tf.Tensor | None = None,\n         encoder_attention_mask: tf.Tensor | None = None,\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n-        output_attentions: Optional[bool] = False,\n-        training: Optional[bool] = None,\n+        output_attentions: bool | None = False,\n+        training: bool | None = None,\n     ):\n         self_outputs = self.self(\n             hidden_states,"
        },
        {
            "sha": "0869902aa96245ead1bb22f2e818517288a35534",
            "filename": "src/transformers/models/camembert/modeling_tf_camembert.py",
            "status": "modified",
            "additions": 47,
            "deletions": 48,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_tf_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_tf_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_tf_camembert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n \n import math\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -695,12 +694,12 @@ def call(\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None,\n-        use_cache: Optional[bool],\n+        use_cache: bool | None,\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -809,13 +808,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -1008,13 +1007,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutputWithPoolingAndCrossAttentions]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutputWithPoolingAndCrossAttentions:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1164,12 +1163,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1294,12 +1293,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1390,12 +1389,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1482,12 +1481,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1586,13 +1585,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1706,14 +1705,14 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutputWithCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if"
        },
        {
            "sha": "2873293571f467076ff510a241695ddf3b006030",
            "filename": "src/transformers/models/clip/modeling_tf_clip.py",
            "status": "modified",
            "additions": 52,
            "deletions": 52,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import tensorflow as tf\n@@ -55,7 +55,7 @@\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -108,10 +108,10 @@ class TFCLIPOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits_per_image: Optional[tf.Tensor] = None\n-    logits_per_text: Optional[tf.Tensor] = None\n-    text_embeds: Optional[tf.Tensor] = None\n-    image_embeds: Optional[tf.Tensor] = None\n+    logits_per_image: tf.Tensor | None = None\n+    logits_per_text: tf.Tensor | None = None\n+    text_embeds: tf.Tensor | None = None\n+    image_embeds: tf.Tensor | None = None\n     text_model_output: TFBaseModelOutputWithPooling = None\n     vision_model_output: TFBaseModelOutputWithPooling = None\n \n@@ -225,9 +225,9 @@ def build(self, input_shape: tf.TensorShape = None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n     ) -> tf.Tensor:\n         \"\"\"\n         Applies embedding based on inputs tensor.\n@@ -498,7 +498,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -560,7 +560,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         input_shape = shape_list(input_ids)\n \n         embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n@@ -677,11 +677,11 @@ def call(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         if input_ids is None:\n             raise ValueError(\"You have to specify input_ids\")\n \n@@ -728,7 +728,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         embedding_output = self.embeddings(pixel_values=pixel_values)\n         embedding_output = self.pre_layernorm(inputs=embedding_output)\n \n@@ -790,11 +790,11 @@ def get_input_embeddings(self) -> keras.layers.Layer:\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -892,9 +892,9 @@ def get_text_features(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         if input_ids is None:\n@@ -924,9 +924,9 @@ def get_text_features(\n     def get_image_features(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         if pixel_values is None:\n@@ -952,12 +952,12 @@ def call(\n         pixel_values: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFCLIPOutput, tuple[tf.Tensor]]:\n+    ) -> TFCLIPOutput | tuple[tf.Tensor]:\n         if input_ids is None:\n             raise ValueError(\"You have to specify either input_ids\")\n         if pixel_values is None:\n@@ -1191,11 +1191,11 @@ def call(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1250,11 +1250,11 @@ def __init__(self, config: CLIPVisionConfig, *inputs, **kwargs):\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1313,9 +1313,9 @@ def get_text_features(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         r\"\"\"\n@@ -1351,9 +1351,9 @@ def get_text_features(\n     def get_image_features(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         r\"\"\"\n@@ -1397,12 +1397,12 @@ def call(\n         pixel_values: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFCLIPOutput, tuple[tf.Tensor]]:\n+    ) -> TFCLIPOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "47c720f5c12c36a67ed3441d4dcccea329b69f70",
            "filename": "src/transformers/models/convbert/modeling_tf_convbert.py",
            "status": "modified",
            "additions": 37,
            "deletions": 39,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -16,8 +16,6 @@\n \n from __future__ import annotations\n \n-from typing import Optional, Union\n-\n import numpy as np\n import tensorflow as tf\n \n@@ -106,10 +104,10 @@ def build(self, input_shape=None):\n     # Copied from transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.call\n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:\n@@ -860,16 +858,16 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        attention_mask: Optional[Union[np.array, tf.Tensor]] = None,\n-        token_type_ids: Optional[Union[np.array, tf.Tensor]] = None,\n-        position_ids: Optional[Union[np.array, tf.Tensor]] = None,\n-        head_mask: Optional[Union[np.array, tf.Tensor]] = None,\n+        attention_mask: np.array | tf.Tensor | None = None,\n+        token_type_ids: np.array | tf.Tensor | None = None,\n+        position_ids: np.array | tf.Tensor | None = None,\n+        head_mask: np.array | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         outputs = self.convbert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -995,12 +993,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFMaskedLMOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFMaskedLMOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1120,12 +1118,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFSequenceClassifierOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFSequenceClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1208,12 +1206,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFMultipleChoiceModelOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFMultipleChoiceModelOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1316,12 +1314,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFTokenClassifierOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFTokenClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1399,13 +1397,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: tf.Tensor | None = None,\n         end_positions: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFQuestionAnsweringModelOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFQuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "4419230ff8933e43288c85ebf1dcdf917368b250",
            "filename": "src/transformers/models/convnext/modeling_tf_convnext.py",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_tf_convnext.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -16,8 +16,6 @@\n \n from __future__ import annotations\n \n-from typing import Optional, Union\n-\n import numpy as np\n import tensorflow as tf\n \n@@ -238,7 +236,7 @@ def __init__(\n         kernel_size: int = 2,\n         stride: int = 2,\n         depth: int = 2,\n-        drop_path_rates: Optional[list[float]] = None,\n+        drop_path_rates: list[float] | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -365,10 +363,10 @@ def __init__(self, config: ConvNextConfig, add_pooling_layer: bool = True, **kwa\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n@@ -503,10 +501,10 @@ def __init__(self, config, *inputs, add_pooling_layer=True, **kwargs):\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -589,11 +587,11 @@ def __init__(self, config: ConvNextConfig, *inputs, **kwargs):\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "cf72a01134c61a19d54e915dd1b575d5fe336422",
            "filename": "src/transformers/models/convnextv2/modeling_tf_convnextv2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_tf_convnextv2.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -16,8 +16,6 @@\n \n from __future__ import annotations\n \n-from typing import Optional, Union\n-\n import numpy as np\n import tensorflow as tf\n \n@@ -279,7 +277,7 @@ def __init__(\n         kernel_size: int = 2,\n         stride: int = 2,\n         depth: int = 2,\n-        drop_path_rates: Optional[list[float]] = None,\n+        drop_path_rates: list[float] | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -367,9 +365,9 @@ def __init__(self, config: ConvNextV2Config, **kwargs):\n     def call(\n         self,\n         hidden_states: tf.Tensor,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-    ) -> Union[tuple, TFBaseModelOutputWithNoAttention]:\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n+    ) -> tuple | TFBaseModelOutputWithNoAttention:\n         all_hidden_states = () if output_hidden_states else None\n \n         for i, layer_module in enumerate(self.stages):\n@@ -411,10 +409,10 @@ def __init__(self, config: ConvNextV2Config, **kwargs):\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n@@ -557,10 +555,10 @@ def __init__(self, config: ConvNextV2Config, *inputs, **kwargs):\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndNoAttention, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPoolingAndNoAttention | tuple[tf.Tensor]:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n@@ -627,11 +625,11 @@ def __init__(self, config: ConvNextV2Config, *inputs, **kwargs):\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFImageClassifierOutputWithNoAttention, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFImageClassifierOutputWithNoAttention | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "e774621e196a55db611aae1665c0d6974e09f65e",
            "filename": "src/transformers/models/ctrl/modeling_tf_ctrl.py",
            "status": "modified",
            "additions": 28,
            "deletions": 30,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,8 +17,6 @@\n \n from __future__ import annotations\n \n-from typing import Optional, Union\n-\n import numpy as np\n import tensorflow as tf\n \n@@ -303,18 +301,18 @@ def _prune_heads(self, heads_to_prune):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutputWithPast]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutputWithPast:\n         # If using past key value states, only the last tokens\n         # should be given as an input\n         if past_key_values is not None:\n@@ -594,18 +592,18 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutputWithPast]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutputWithPast:\n         outputs = self.transformer(\n             input_ids=input_ids,\n             past_key_values=past_key_values,\n@@ -722,19 +720,19 @@ def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFCausalLMOutputWithPast]:\n+        training: bool | None = False,\n+    ) -> tuple | TFCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,\n@@ -835,19 +833,19 @@ def get_output_embeddings(self):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFSequenceClassifierOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFSequenceClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "20fb8a2ee7796fcb47f089499fe868dfcf6e52c4",
            "filename": "src/transformers/models/cvt/modeling_tf_cvt.py",
            "status": "modified",
            "additions": 18,
            "deletions": 19,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import collections.abc\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -65,8 +64,8 @@ class TFBaseModelOutputWithCLSToken(ModelOutput):\n             the initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    cls_token_value: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    cls_token_value: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n \n \n@@ -766,10 +765,10 @@ def __init__(self, config: CvtConfig, **kwargs):\n     def call(\n         self,\n         pixel_values: TFModelInputType,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithCLSToken, tuple[tf.Tensor]]:\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithCLSToken | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         hidden_state = pixel_values\n         # When running on CPU, `keras.layers.Conv2D` doesn't support (batch_size, num_channels, height, width)\n@@ -821,10 +820,10 @@ def __init__(self, config: CvtConfig, **kwargs):\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithCLSToken, tuple[tf.Tensor]]:\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithCLSToken | tuple[tf.Tensor]:\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -929,10 +928,10 @@ def __init__(self, config: CvtConfig, *inputs, **kwargs):\n     def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithCLSToken, tuple[tf.Tensor]]:\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithCLSToken | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1015,10 +1014,10 @@ def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n         labels: tf.Tensor | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFImageClassifierOutputWithNoAttention, tuple[tf.Tensor]]:\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFImageClassifierOutputWithNoAttention | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "0fa0fe1f811ee1748956c2481d28b0ad1ef516e0",
            "filename": "src/transformers/models/data2vec/modeling_tf_data2vec_vision.py",
            "status": "modified",
            "additions": 30,
            "deletions": 31,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n import collections.abc\n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -90,8 +89,8 @@ class TFData2VecVisionModelOutputWithPooling(TFBaseModelOutputWithPooling):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    pooler_output: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    pooler_output: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -258,7 +257,7 @@ def build(self, input_shape=None):\n \n \n class TFData2VecVisionSelfAttention(keras.layers.Layer):\n-    def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] = None, **kwargs):\n+    def __init__(self, config: Data2VecVisionConfig, window_size: tuple | None = None, **kwargs):\n         super().__init__(**kwargs)\n \n         if config.hidden_size % config.num_attention_heads != 0:\n@@ -306,7 +305,7 @@ def call(\n         hidden_states: tf.Tensor,\n         head_mask: tf.Tensor,\n         output_attentions: bool,\n-        relative_position_bias: Optional[TFData2VecVisionRelativePositionBias] = None,\n+        relative_position_bias: TFData2VecVisionRelativePositionBias | None = None,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         batch_size = shape_list(hidden_states)[0]\n@@ -402,7 +401,7 @@ def build(self, input_shape=None):\n \n \n class TFData2VecVisionAttention(keras.layers.Layer):\n-    def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] = None, **kwargs):\n+    def __init__(self, config: Data2VecVisionConfig, window_size: tuple | None = None, **kwargs):\n         super().__init__(**kwargs)\n \n         self.attention = TFData2VecVisionSelfAttention(config, window_size=window_size, name=\"attention\")\n@@ -416,7 +415,7 @@ def call(\n         input_tensor: tf.Tensor,\n         head_mask: tf.Tensor,\n         output_attentions: bool,\n-        relative_position_bias: Optional[TFData2VecVisionRelativePositionBias] = None,\n+        relative_position_bias: TFData2VecVisionRelativePositionBias | None = None,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         self_outputs = self.attention(\n@@ -504,7 +503,7 @@ class TFData2VecVisionLayer(keras.layers.Layer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(\n-        self, config: Data2VecVisionConfig, window_size: Optional[tuple] = None, drop_path_rate: float = 0.0, **kwargs\n+        self, config: Data2VecVisionConfig, window_size: tuple | None = None, drop_path_rate: float = 0.0, **kwargs\n     ):\n         super().__init__(**kwargs)\n         self.config = config\n@@ -570,7 +569,7 @@ def call(\n         hidden_states: tf.Tensor,\n         head_mask: tf.Tensor,\n         output_attentions: bool,\n-        relative_position_bias: Optional[TFData2VecVisionRelativePositionBias] = None,\n+        relative_position_bias: TFData2VecVisionRelativePositionBias | None = None,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         self_attention_outputs = self.attention(\n@@ -667,7 +666,7 @@ def call(self, inputs=None) -> tf.Tensor:\n \n \n class TFData2VecVisionEncoder(keras.layers.Layer):\n-    def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] = None, **kwargs):\n+    def __init__(self, config: Data2VecVisionConfig, window_size: tuple | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         if config.use_shared_relative_position_bias:\n@@ -696,7 +695,7 @@ def call(\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+    ) -> tuple | TFBaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -783,11 +782,11 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         bool_masked_pos: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple, TFData2VecVisionModelOutputWithPooling]:\n+    ) -> tuple | TFData2VecVisionModelOutputWithPooling:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -995,11 +994,11 @@ def call(\n         pixel_values: TFModelInputType | None = None,\n         bool_masked_pos: tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple, TFData2VecVisionModelOutputWithPooling]:\n+    ) -> tuple | TFData2VecVisionModelOutputWithPooling:\n         r\"\"\"\n         bool_masked_pos (`tf.Tensor` of shape `(batch_size, num_patches)`, *optional*):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n@@ -1059,12 +1058,12 @@ def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n@@ -1121,10 +1120,10 @@ def __init__(\n         self,\n         in_channels: int,\n         out_channels: int,\n-        kernel_size: Union[int, tuple[int, int]],\n+        kernel_size: int | tuple[int, int],\n         padding: str = \"valid\",\n         bias: bool = False,\n-        dilation: Union[int, tuple[int, int]] = 1,\n+        dilation: int | tuple[int, int] = 1,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -1462,7 +1461,7 @@ def __init__(\n         config: Data2VecVisionConfig,\n         in_index: int = 2,\n         kernel_size: int = 3,\n-        dilation: Union[int, tuple[int, int]] = 1,\n+        dilation: int | tuple[int, int] = 1,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -1599,10 +1598,10 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         labels: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, TFSemanticSegmenterOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+    ) -> tuple | TFSemanticSegmenterOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, height, width)`, *optional*):\n             Ground truth semantic segmentation maps for computing the loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "40d23fc28b9475623dc8a94d5539c5a530b3f376",
            "filename": "src/transformers/models/deberta/modeling_tf_deberta.py",
            "status": "modified",
            "additions": 46,
            "deletions": 47,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import math\n from collections.abc import Sequence\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -208,9 +207,9 @@ def call(\n         self,\n         input_tensor: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: Optional[tf.Tensor] = None,\n-        relative_pos: Optional[tf.Tensor] = None,\n-        rel_embeddings: Optional[tf.Tensor] = None,\n+        query_states: tf.Tensor | None = None,\n+        relative_pos: tf.Tensor | None = None,\n+        rel_embeddings: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n@@ -319,9 +318,9 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: Optional[tf.Tensor] = None,\n-        relative_pos: Optional[tf.Tensor] = None,\n-        rel_embeddings: Optional[tf.Tensor] = None,\n+        query_states: tf.Tensor | None = None,\n+        relative_pos: tf.Tensor | None = None,\n+        rel_embeddings: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n@@ -409,13 +408,13 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: Optional[tf.Tensor] = None,\n-        relative_pos: Optional[tf.Tensor] = None,\n+        query_states: tf.Tensor | None = None,\n+        relative_pos: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -651,9 +650,9 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: Optional[tf.Tensor] = None,\n-        relative_pos: Optional[tf.Tensor] = None,\n-        rel_embeddings: Optional[tf.Tensor] = None,\n+        query_states: tf.Tensor | None = None,\n+        relative_pos: tf.Tensor | None = None,\n+        rel_embeddings: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n@@ -881,11 +880,11 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n-        mask: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n+        mask: tf.Tensor | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         \"\"\"\n@@ -1074,11 +1073,11 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -1255,11 +1254,11 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         outputs = self.deberta(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1314,12 +1313,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1404,12 +1403,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1497,12 +1496,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1579,13 +1578,13 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "d71891ac19c00a8bc3ddb63e547b404570059a7a",
            "filename": "src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py",
            "status": "modified",
            "additions": 51,
            "deletions": 53,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -16,8 +16,6 @@\n \n from __future__ import annotations\n \n-from typing import Optional, Union\n-\n import numpy as np\n import tensorflow as tf\n \n@@ -192,9 +190,9 @@ def call(\n         self,\n         input_tensor: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: Optional[tf.Tensor] = None,\n-        relative_pos: Optional[tf.Tensor] = None,\n-        rel_embeddings: Optional[tf.Tensor] = None,\n+        query_states: tf.Tensor | None = None,\n+        relative_pos: tf.Tensor | None = None,\n+        rel_embeddings: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n@@ -306,9 +304,9 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: Optional[tf.Tensor] = None,\n-        relative_pos: Optional[tf.Tensor] = None,\n-        rel_embeddings: Optional[tf.Tensor] = None,\n+        query_states: tf.Tensor | None = None,\n+        relative_pos: tf.Tensor | None = None,\n+        rel_embeddings: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n@@ -485,13 +483,13 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: Optional[tf.Tensor] = None,\n-        relative_pos: Optional[tf.Tensor] = None,\n+        query_states: tf.Tensor | None = None,\n+        relative_pos: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         if len(shape_list(attention_mask)) <= 2:\n             input_mask = attention_mask\n         else:\n@@ -718,9 +716,9 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: Optional[tf.Tensor] = None,\n-        relative_pos: Optional[tf.Tensor] = None,\n-        rel_embeddings: Optional[tf.Tensor] = None,\n+        query_states: tf.Tensor | None = None,\n+        relative_pos: tf.Tensor | None = None,\n+        rel_embeddings: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n@@ -985,11 +983,11 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n-        mask: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n+        mask: tf.Tensor | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         \"\"\"\n@@ -1181,11 +1179,11 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -1364,11 +1362,11 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         outputs = self.deberta(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1424,12 +1422,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1515,12 +1513,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1609,12 +1607,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1692,13 +1690,13 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1793,12 +1791,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`"
        },
        {
            "sha": "3c56eee87911edc445641e0bbc14f094e1c5efa7",
            "filename": "src/transformers/models/deit/modeling_tf_deit.py",
            "status": "modified",
            "additions": 24,
            "deletions": 25,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n import collections.abc\n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -88,9 +87,9 @@ class token).\n             the self-attention heads.\n     \"\"\"\n \n-    logits: Optional[tf.Tensor] = None\n-    cls_logits: Optional[tf.Tensor] = None\n-    distillation_logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n+    cls_logits: tf.Tensor | None = None\n+    distillation_logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -550,7 +549,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -630,12 +629,12 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         bool_masked_pos: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor, ...]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor, ...]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -780,12 +779,12 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         bool_masked_pos: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n         training: bool = False,\n-    ) -> Union[tuple, TFBaseModelOutputWithPooling]:\n+    ) -> tuple | TFBaseModelOutputWithPooling:\n         outputs = self.deit(\n             pixel_values=pixel_values,\n             bool_masked_pos=bool_masked_pos,\n@@ -910,12 +909,12 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         bool_masked_pos: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n         training: bool = False,\n-    ) -> Union[tuple, TFMaskedImageModelingOutput]:\n+    ) -> tuple | TFMaskedImageModelingOutput:\n         r\"\"\"\n         bool_masked_pos (`tf.Tensor` of type bool and shape `(batch_size, num_patches)`):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n@@ -1046,12 +1045,12 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         labels: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n         training: bool = False,\n-    ) -> Union[tf.Tensor, TFImageClassifierOutput]:\n+    ) -> tf.Tensor | TFImageClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n@@ -1171,12 +1170,12 @@ def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         interpolate_pos_encoding: bool = False,\n         training: bool = False,\n-    ) -> Union[tuple, TFDeiTForImageClassificationWithTeacherOutput]:\n+    ) -> tuple | TFDeiTForImageClassificationWithTeacherOutput:\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.deit("
        },
        {
            "sha": "3c7830d633448bde3b8a420f0d84cdd070498882",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 12,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -20,7 +20,6 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -543,9 +542,9 @@ def call(\n         mems: list[tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n     ):\n@@ -690,7 +689,7 @@ class TFTransfoXLModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     mems: list[tf.Tensor] = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -723,7 +722,7 @@ class TFTransfoXLLMHeadModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    prediction_scores: Optional[tf.Tensor] = None\n+    prediction_scores: tf.Tensor | None = None\n     mems: list[tf.Tensor] = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -757,7 +756,7 @@ class TFTransfoXLSequenceClassifierOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     mems: list[tf.Tensor] = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -1047,12 +1046,12 @@ def call(\n         mems: list[tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFTransfoXLSequenceClassifierOutputWithPast]:\n+        training: bool | None = False,\n+    ) -> tuple | TFTransfoXLSequenceClassifierOutputWithPast:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "a2efa1105c1cde9236e9c32627a5773cb38cc862",
            "filename": "src/transformers/models/distilbert/modeling_tf_distilbert.py",
            "status": "modified",
            "additions": 30,
            "deletions": 31,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_tf_distilbert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n from __future__ import annotations\n \n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -594,11 +593,11 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         outputs = self.distilbert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -697,12 +696,12 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -794,12 +793,12 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -882,12 +881,12 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -969,12 +968,12 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1071,13 +1070,13 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "aef83e6c55fbe27ea57e48bf2baca515999010cb",
            "filename": "src/transformers/models/dpr/modeling_tf_dpr.py",
            "status": "modified",
            "additions": 16,
            "deletions": 17,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -68,7 +67,7 @@ class TFDPRContextEncoderOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    pooler_output: Optional[tf.Tensor] = None\n+    pooler_output: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n \n@@ -96,7 +95,7 @@ class TFDPRQuestionEncoderOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    pooler_output: Optional[tf.Tensor] = None\n+    pooler_output: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n \n@@ -127,9 +126,9 @@ class TFDPRReaderOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    start_logits: Optional[tf.Tensor] = None\n-    end_logits: Optional[tf.Tensor] = None\n-    relevance_logits: Optional[tf.Tensor] = None\n+    start_logits: tf.Tensor | None = None\n+    end_logits: tf.Tensor | None = None\n+    relevance_logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n \n@@ -155,15 +154,15 @@ def __init__(self, config: DPRConfig, **kwargs):\n     @unpack_inputs\n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         token_type_ids: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor, ...]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor, ...]:\n         outputs = self.bert_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -226,14 +225,14 @@ def __init__(self, config: DPRConfig, **kwargs):\n     @unpack_inputs\n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = False,\n         training: bool = False,\n-    ) -> Union[TFDPRReaderOutput, tuple[tf.Tensor, ...]]:\n+    ) -> TFDPRReaderOutput | tuple[tf.Tensor, ...]:\n         # notations: N - number of questions in a batch, M - number of passages per questions, L - sequence length\n         n_passages, sequence_length = shape_list(input_ids) if input_ids is not None else shape_list(inputs_embeds)[:2]\n         # feed encoder\n@@ -296,15 +295,15 @@ def __init__(self, config: DPRConfig, **kwargs):\n     @unpack_inputs\n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         token_type_ids: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = False,\n         training: bool = False,\n-    ) -> Union[TFDPRReaderOutput, tuple[tf.Tensor, ...]]:\n+    ) -> TFDPRReaderOutput | tuple[tf.Tensor, ...]:\n         outputs = self.encoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -329,15 +328,15 @@ def __init__(self, config: DPRConfig, **kwargs):\n     @unpack_inputs\n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         token_type_ids: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = False,\n         training: bool = False,\n-    ) -> Union[TFDPRReaderOutput, tuple[tf.Tensor, ...]]:\n+    ) -> TFDPRReaderOutput | tuple[tf.Tensor, ...]:\n         outputs = self.encoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "3a5c33e503d7386df5c2be0fc10a079ee4fe014a",
            "filename": "src/transformers/models/electra/modeling_tf_electra.py",
            "status": "modified",
            "additions": 51,
            "deletions": 52,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n import math\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -461,12 +460,12 @@ def call(\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None,\n-        use_cache: Optional[bool],\n+        use_cache: bool | None,\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -601,10 +600,10 @@ def build(self, input_shape=None):\n     # Copied from transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.call\n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:\n@@ -806,13 +805,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -931,7 +930,7 @@ class TFElectraForPreTrainingOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -1057,13 +1056,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1139,11 +1138,11 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFElectraForPreTrainingOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFElectraForPreTrainingOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1281,12 +1280,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1410,12 +1409,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1496,12 +1495,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1607,12 +1606,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1695,13 +1694,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "05cd02e3bcef5dc4043c74a4310ee0c8feef744a",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n import inspect\n import re\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -204,9 +203,9 @@ class TFEncoderDecoderModel(TFPreTrainedModel, TFCausalLanguageModelingLoss):\n \n     def __init__(\n         self,\n-        config: Optional[PretrainedConfig] = None,\n-        encoder: Optional[TFPreTrainedModel] = None,\n-        decoder: Optional[TFPreTrainedModel] = None,\n+        config: PretrainedConfig | None = None,\n+        encoder: TFPreTrainedModel | None = None,\n+        decoder: TFPreTrainedModel | None = None,\n     ):\n         if config is None and (encoder is None or decoder is None):\n             raise ValueError(\"Either a configuration or an encoder and a decoder has to be provided.\")\n@@ -311,8 +310,8 @@ def tf_to_pt_weight_rename(self, tf_weight):\n     @classmethod\n     def from_encoder_decoder_pretrained(\n         cls,\n-        encoder_pretrained_model_name_or_path: Optional[str] = None,\n-        decoder_pretrained_model_name_or_path: Optional[str] = None,\n+        encoder_pretrained_model_name_or_path: str | None = None,\n+        decoder_pretrained_model_name_or_path: str | None = None,\n         *model_args,\n         **kwargs,\n     ) -> TFPreTrainedModel:\n@@ -465,13 +464,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         **kwargs,\n-    ) -> Union[TFSeq2SeqLMOutput, tuple[tf.Tensor]]:\n+    ) -> TFSeq2SeqLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "3fd066868f0e86cd2e130ad170eb38c4791f4f88",
            "filename": "src/transformers/models/esm/modeling_tf_esm.py",
            "status": "modified",
            "additions": 26,
            "deletions": 27,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_tf_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_tf_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_tf_esm.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import os\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -331,7 +330,7 @@ def call(\n         encoder_hidden_states: tf.Tensor | None = None,\n         encoder_attention_mask: tf.Tensor | None = None,\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         mixed_query_layer = self.query(hidden_states)\n@@ -934,13 +933,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -1117,13 +1116,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1222,11 +1221,11 @@ def call(\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1370,11 +1369,11 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1457,11 +1456,11 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`."
        },
        {
            "sha": "e2bc25cb6d53f0428be3f6c1c25e67d67edab739",
            "filename": "src/transformers/models/esm/openfold_utils/rigid_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Frigid_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Frigid_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Frigid_utils.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,7 @@\n \n from collections.abc import Sequence\n from functools import cache\n-from typing import Any, Callable, Optional\n+from typing import Any, Callable\n \n import numpy as np\n import torch\n@@ -78,8 +78,8 @@ def rot_vec_mul(r: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n @cache\n def identity_rot_mats(\n     batch_dims: tuple[int, ...],\n-    dtype: Optional[torch.dtype] = None,\n-    device: Optional[torch.device] = None,\n+    dtype: torch.dtype | None = None,\n+    device: torch.device | None = None,\n     requires_grad: bool = True,\n ) -> torch.Tensor:\n     rots = torch.eye(3, dtype=dtype, device=device, requires_grad=requires_grad)\n@@ -93,8 +93,8 @@ def identity_rot_mats(\n @cache\n def identity_trans(\n     batch_dims: tuple[int, ...],\n-    dtype: Optional[torch.dtype] = None,\n-    device: Optional[torch.device] = None,\n+    dtype: torch.dtype | None = None,\n+    device: torch.device | None = None,\n     requires_grad: bool = True,\n ) -> torch.Tensor:\n     trans = torch.zeros((*batch_dims, 3), dtype=dtype, device=device, requires_grad=requires_grad)\n@@ -104,8 +104,8 @@ def identity_trans(\n @cache\n def identity_quats(\n     batch_dims: tuple[int, ...],\n-    dtype: Optional[torch.dtype] = None,\n-    device: Optional[torch.device] = None,\n+    dtype: torch.dtype | None = None,\n+    device: torch.device | None = None,\n     requires_grad: bool = True,\n ) -> torch.Tensor:\n     quat = torch.zeros((*batch_dims, 4), dtype=dtype, device=device, requires_grad=requires_grad)\n@@ -260,8 +260,8 @@ class Rotation:\n \n     def __init__(\n         self,\n-        rot_mats: Optional[torch.Tensor] = None,\n-        quats: Optional[torch.Tensor] = None,\n+        rot_mats: torch.Tensor | None = None,\n+        quats: torch.Tensor | None = None,\n         normalize_quats: bool = True,\n     ):\n         \"\"\"\n@@ -295,8 +295,8 @@ def __init__(\n     @staticmethod\n     def identity(\n         shape,\n-        dtype: Optional[torch.dtype] = None,\n-        device: Optional[torch.device] = None,\n+        dtype: torch.dtype | None = None,\n+        device: torch.device | None = None,\n         requires_grad: bool = True,\n         fmt: str = \"quat\",\n     ) -> Rotation:\n@@ -682,7 +682,7 @@ def cuda(self) -> Rotation:\n         else:\n             raise ValueError(\"Both rotations are None\")\n \n-    def to(self, device: Optional[torch.device], dtype: Optional[torch.dtype]) -> Rotation:\n+    def to(self, device: torch.device | None, dtype: torch.dtype | None) -> Rotation:\n         \"\"\"\n         Analogous to the to() method of torch Tensors\n \n@@ -734,7 +734,7 @@ class Rigid:\n     dimensions of its component parts.\n     \"\"\"\n \n-    def __init__(self, rots: Optional[Rotation], trans: Optional[torch.Tensor]):\n+    def __init__(self, rots: Rotation | None, trans: torch.Tensor | None):\n         \"\"\"\n         Args:\n             rots: A [*, 3, 3] rotation tensor\n@@ -786,8 +786,8 @@ def __init__(self, rots: Optional[Rotation], trans: Optional[torch.Tensor]):\n     @staticmethod\n     def identity(\n         shape: tuple[int, ...],\n-        dtype: Optional[torch.dtype] = None,\n-        device: Optional[torch.device] = None,\n+        dtype: torch.dtype | None = None,\n+        device: torch.device | None = None,\n         requires_grad: bool = True,\n         fmt: str = \"quat\",\n     ) -> Rigid:"
        },
        {
            "sha": "88b7ae9f0c9ddd96e761b3739a035c55a217dec8",
            "filename": "src/transformers/models/flaubert/modeling_tf_flaubert.py",
            "status": "modified",
            "additions": 39,
            "deletions": 40,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_tf_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_tf_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_tf_flaubert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -22,7 +22,6 @@\n import random\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -261,14 +260,14 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutput:\n         outputs = self.transformer(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -544,14 +543,14 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutput:\n         # removed: src_enc=None, src_len=None\n \n         if input_ids is not None and inputs_embeds is not None:\n@@ -808,7 +807,7 @@ class TFFlaubertWithLMHeadModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -864,14 +863,14 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFFlaubertWithLMHeadModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFFlaubertWithLMHeadModelOutput:\n         transformer_outputs = self.transformer(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -940,15 +939,15 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1031,16 +1030,16 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1138,15 +1137,15 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1251,15 +1250,15 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         if input_ids is not None:\n             num_choices = shape_list(input_ids)[1]\n             seq_length = shape_list(input_ids)[2]"
        },
        {
            "sha": "86982ba295459c93d5ca44cb2e2d4dae9aa50b4e",
            "filename": "src/transformers/models/funnel/modeling_tf_funnel.py",
            "status": "modified",
            "additions": 33,
            "deletions": 34,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -1104,7 +1103,7 @@ class TFFunnelForPreTrainingOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -1224,11 +1223,11 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFBaseModelOutput]:\n+    ) -> tuple[tf.Tensor] | TFBaseModelOutput:\n         return self.funnel(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1280,11 +1279,11 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFBaseModelOutput]:\n+    ) -> tuple[tf.Tensor] | TFBaseModelOutput:\n         return self.funnel(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1336,12 +1335,12 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         **kwargs,\n-    ) -> Union[tuple[tf.Tensor], TFFunnelForPreTrainingOutput]:\n+    ) -> tuple[tf.Tensor] | TFFunnelForPreTrainingOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1426,12 +1425,12 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFMaskedLMOutput]:\n+    ) -> tuple[tf.Tensor] | TFMaskedLMOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1509,12 +1508,12 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFSequenceClassifierOutput]:\n+    ) -> tuple[tf.Tensor] | TFSequenceClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1598,12 +1597,12 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFMultipleChoiceModelOutput]:\n+    ) -> tuple[tf.Tensor] | TFMultipleChoiceModelOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1705,12 +1704,12 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFTokenClassifierOutput]:\n+    ) -> tuple[tf.Tensor] | TFTokenClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1793,13 +1792,13 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFQuestionAnsweringModelOutput]:\n+    ) -> tuple[tf.Tensor] | TFQuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "27b2e20c37ee8b79ed39ab3e1ac19c2a9b89170e",
            "filename": "src/transformers/models/gpt2/modeling_tf_gpt2.py",
            "status": "modified",
            "additions": 37,
            "deletions": 38,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -396,20 +395,20 @@ def _prune_heads(self, heads_to_prune):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -628,8 +627,8 @@ class TFGPT2DoubleHeadsModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: Optional[tf.Tensor] = None\n-    mc_logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n+    mc_logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -764,20 +763,20 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -880,21 +879,21 @@ def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutputWithCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -991,19 +990,19 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         mc_token_ids: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFGPT2DoubleHeadsModelOutput, tuple[tf.Tensor]]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFGPT2DoubleHeadsModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         mc_token_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, num_choices)`, *optional*, default to index of the last token of the input):\n             Index of the classification token in each input sequence. Selected in the range `[0, input_ids.size(-1) -\n@@ -1145,19 +1144,19 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutputWithPast, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutputWithPast | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "ce4bbc6d2fb5fc43a68525a0a5bae5482e727cda",
            "filename": "src/transformers/models/gptj/modeling_tf_gptj.py",
            "status": "modified",
            "additions": 29,
            "deletions": 31,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -16,8 +16,6 @@\n \n from __future__ import annotations\n \n-from typing import Optional, Union\n-\n import numpy as np\n import tensorflow as tf\n \n@@ -203,7 +201,7 @@ def _attn(\n     def call(\n         self,\n         hidden_states: tf.Tensor,\n-        layer_past: Optional[tuple[tf.Tensor, tf.Tensor]] = None,\n+        layer_past: tuple[tf.Tensor, tf.Tensor] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         position_ids: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n@@ -428,7 +426,7 @@ def call(\n         output_hidden_states=None,\n         return_dict=None,\n         training=False,\n-    ) -> Union[TFBaseModelOutputWithPast, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPast | tuple[tf.Tensor]:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -694,18 +692,18 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPast, tuple[tf.Tensor]]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPast | tuple[tf.Tensor]:\n         r\"\"\"\n         use_cache (`bool`, *optional*, defaults to `True`):\n             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n@@ -794,19 +792,19 @@ def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutputWithPast, tuple[tf.Tensor]]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutputWithPast | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`np.ndarray` or `tf.Tensor` of shape `(batch_size, input_ids_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n@@ -902,19 +900,19 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutputWithPast, tuple[tf.Tensor]]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutputWithPast | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`np.ndarray` or `tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1019,19 +1017,19 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`np.ndarray` or `tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "19fdfe94e40e00b46ce033a907899bfa937f0dd2",
            "filename": "src/transformers/models/groupvit/modeling_tf_groupvit.py",
            "status": "modified",
            "additions": 61,
            "deletions": 61,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,7 @@\n import collections.abc\n import math\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import tensorflow as tf\n@@ -79,7 +79,7 @@\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -253,11 +253,11 @@ class TFGroupViTModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits_per_image: Optional[tf.Tensor] = None\n-    logits_per_text: Optional[tf.Tensor] = None\n-    segmentation_logits: Optional[tf.Tensor] = None\n-    text_embeds: Optional[tf.Tensor] = None\n-    image_embeds: Optional[tf.Tensor] = None\n+    logits_per_image: tf.Tensor | None = None\n+    logits_per_text: tf.Tensor | None = None\n+    segmentation_logits: tf.Tensor | None = None\n+    text_embeds: tf.Tensor | None = None\n+    image_embeds: tf.Tensor | None = None\n     text_model_output: TFBaseModelOutputWithPooling = None\n     vision_model_output: TFBaseModelOutputWithPooling = None\n \n@@ -646,9 +646,9 @@ def build(self, input_shape: tf.TensorShape = None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n     ) -> tf.Tensor:\n         \"\"\"\n         Applies embedding based on inputs tensor.\n@@ -809,9 +809,9 @@ class TFGroupViTMLP(keras.layers.Layer):\n     def __init__(\n         self,\n         config: GroupViTVisionConfig,\n-        hidden_size: Optional[int] = None,\n-        intermediate_size: Optional[int] = None,\n-        output_size: Optional[int] = None,\n+        hidden_size: int | None = None,\n+        intermediate_size: int | None = None,\n+        output_size: int | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -898,10 +898,10 @@ def transpose_for_scores(self, tensor: tf.Tensor, batch_size: int) -> tf.Tensor:\n     def call(\n         self,\n         hidden_states: tf.Tensor,\n-        attention_mask: Optional[tf.Tensor] = None,\n-        causal_attention_mask: Optional[tf.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        encoder_hidden_states: Optional[tf.Tensor] = None,\n+        attention_mask: tf.Tensor | None = None,\n+        causal_attention_mask: tf.Tensor | None = None,\n+        output_attentions: bool | None = None,\n+        encoder_hidden_states: tf.Tensor | None = None,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -1060,7 +1060,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+    ) -> tuple | TFBaseModelOutput:\n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -1121,7 +1121,7 @@ def call(\n         output_attentions: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+    ) -> tuple | TFBaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n         all_groupings = () if output_attentions else None\n \n@@ -1180,7 +1180,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         input_shape = shape_list(input_ids)\n \n         embedding_output = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n@@ -1292,7 +1292,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[tuple, TFBaseModelOutputWithPooling]:\n+    ) -> tuple | TFBaseModelOutputWithPooling:\n         embedding_output = self.embeddings(pixel_values)\n \n         encoder_outputs = self.encoder(\n@@ -1356,11 +1356,11 @@ def call(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         if input_ids is None:\n             raise ValueError(\"You have to specify input_ids\")\n \n@@ -1407,11 +1407,11 @@ def get_input_embeddings(self) -> keras.layers.Layer:\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -1518,9 +1518,9 @@ def get_text_features(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         if input_ids is None:\n@@ -1552,9 +1552,9 @@ def get_text_features(\n     def get_image_features(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         if pixel_values is None:\n@@ -1582,13 +1582,13 @@ def call(\n         pixel_values: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_segmentation: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        output_segmentation: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFGroupViTModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFGroupViTModelOutput | tuple[tf.Tensor]:\n         if input_ids is None:\n             raise ValueError(\"You have to specify either input_ids\")\n         if pixel_values is None:\n@@ -1867,11 +1867,11 @@ def call(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1926,11 +1926,11 @@ def __init__(self, config: GroupViTVisionConfig, *inputs, **kwargs):\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1989,9 +1989,9 @@ def get_text_features(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         r\"\"\"\n@@ -2028,9 +2028,9 @@ def get_text_features(\n     def get_image_features(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         r\"\"\"\n@@ -2075,13 +2075,13 @@ def call(\n         pixel_values: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n-        return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_segmentation: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        return_loss: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        output_segmentation: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFGroupViTModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFGroupViTModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "0b7ed531dbc15ca35fd558ffc2107b62039eb5e7",
            "filename": "src/transformers/models/hubert/modeling_tf_hubert.py",
            "status": "modified",
            "additions": 25,
            "deletions": 25,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,7 @@\n from __future__ import annotations\n \n import warnings\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import tensorflow as tf\n@@ -152,7 +152,7 @@ def _compute_mask_indices(\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -701,7 +701,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -896,7 +896,7 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         attn_residual = hidden_states\n@@ -956,7 +956,7 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         attn_residual = hidden_states\n@@ -1007,11 +1007,11 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -1090,11 +1090,11 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -1260,7 +1260,7 @@ def call(\n         inputs_embeds: tf.Tensor | None = None,\n         output_attentions: tf.Tensor | None = None,\n         output_hidden_states: tf.Tensor | None = None,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         **kwargs: Any,\n     ):\n@@ -1445,11 +1445,11 @@ def call(\n         position_ids: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         \"\"\"\n \n         Returns:\n@@ -1549,12 +1549,12 @@ def call(\n         position_ids: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutput, tuple[tf.Tensor]]:\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,"
        },
        {
            "sha": "769c297e514b93d170a53b535317303bf598eed4",
            "filename": "src/transformers/models/idefics/modeling_tf_idefics.py",
            "status": "modified",
            "additions": 76,
            "deletions": 77,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -22,7 +22,6 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -91,11 +90,11 @@ class TFIdeficsBaseModelOutputWithPast(ModelOutput):\n             image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    past_key_values: Optional[tuple[tuple[tf.Tensor]]] = None\n-    hidden_states: Optional[tuple[tf.Tensor]] = None\n-    attentions: Optional[tuple[tf.Tensor]] = None\n-    image_hidden_states: Optional[tuple[tf.Tensor]] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    past_key_values: tuple[tuple[tf.Tensor]] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n+    image_hidden_states: tuple[tf.Tensor] | None = None\n \n \n @dataclass\n@@ -132,12 +131,12 @@ class TFIdeficsCausalLMOutputWithPast(ModelOutput):\n             image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n-    loss: Optional[tf.Tensor] = None\n-    logits: Optional[tf.Tensor] = None\n-    past_key_values: Optional[list[tf.Tensor]] = None\n-    hidden_states: Optional[tuple[tf.Tensor]] = None\n-    attentions: Optional[tuple[tf.Tensor]] = None\n-    image_hidden_states: Optional[tuple[tf.Tensor]] = None\n+    loss: tf.Tensor | None = None\n+    logits: tf.Tensor | None = None\n+    past_key_values: list[tf.Tensor] | None = None\n+    hidden_states: tuple[tf.Tensor] | None = None\n+    attentions: tuple[tf.Tensor] | None = None\n+    image_hidden_states: tuple[tf.Tensor] | None = None\n \n \n def expand_inputs_for_generation(\n@@ -278,7 +277,7 @@ def __init__(\n         num_embeddings,\n         num_additional_embeddings,\n         embedding_dim,\n-        partially_freeze: Optional[bool] = False,\n+        partially_freeze: bool | None = False,\n         dtype=None,\n         **kwargs,\n     ) -> None:\n@@ -658,13 +657,13 @@ def _shape(self, tensor: tf.Tensor, seq_len: int, bsz: int):\n     def call(\n         self,\n         hidden_states: tf.Tensor,\n-        key_value_states: Optional[tf.Tensor] = None,\n-        attention_mask: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        past_key_value: Optional[tuple[tf.Tensor]] = None,\n+        key_value_states: tf.Tensor | None = None,\n+        attention_mask: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        past_key_value: tuple[tf.Tensor] | None = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n-    ) -> tuple[tf.Tensor, Optional[tf.Tensor], Optional[tuple[tf.Tensor]]]:\n+    ) -> tuple[tf.Tensor, tf.Tensor | None, tuple[tf.Tensor] | None]:\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         is_cross_attention = self.is_cross_attention or key_value_states is not None\n \n@@ -791,13 +790,13 @@ def __init__(self, config: IdeficsConfig, **kwargs):\n     def call(\n         self,\n         hidden_states: tf.Tensor,\n-        attention_mask: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        past_key_value: Optional[tuple[tf.Tensor]] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n+        attention_mask: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        past_key_value: tuple[tf.Tensor] | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = False,\n         training=False,\n-    ) -> tuple[tf.Tensor, Optional[tuple[tf.Tensor, tf.Tensor]]]:\n+    ) -> tuple[tf.Tensor, tuple[tf.Tensor, tf.Tensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`tf.Tensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -979,14 +978,14 @@ def build(self, input_shape):\n     def call(\n         self,\n         hidden_states: tf.Tensor,\n-        attention_mask: Optional[tf.Tensor] = None,\n-        image_hidden_states: Optional[tf.Tensor] = None,\n-        image_attention_mask: Optional[tf.Tensor] = None,\n-        cross_attention_gate: Optional[tf.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n-        past_key_value: Optional[tuple[tf.Tensor]] = None,\n-    ) -> tuple[tf.Tensor, Optional[tuple[tf.Tensor, tf.Tensor]]]:\n+        attention_mask: tf.Tensor | None = None,\n+        image_hidden_states: tf.Tensor | None = None,\n+        image_attention_mask: tf.Tensor | None = None,\n+        cross_attention_gate: tf.Tensor | None = None,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = False,\n+        past_key_value: tuple[tf.Tensor] | None = None,\n+    ) -> tuple[tf.Tensor, tuple[tf.Tensor, tf.Tensor] | None]:\n         \"\"\"\n         Args:\n             hidden_states (`tf.Tensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -1255,21 +1254,21 @@ def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_em\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        attention_mask: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        past_key_values: Optional[list[tf.Tensor]] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n-        pixel_values: Optional[tf.Tensor] = None,\n-        image_encoder_embeddings: Optional[tf.Tensor] = None,\n-        perceiver_embeddings: Optional[tf.Tensor] = None,\n-        image_attention_mask: Optional[tf.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = False,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = None,\n-    ) -> Union[TFIdeficsBaseModelOutputWithPast, tuple[tf.Tensor]]:\n+        attention_mask: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        past_key_values: list[tf.Tensor] | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n+        pixel_values: tf.Tensor | None = None,\n+        image_encoder_embeddings: tf.Tensor | None = None,\n+        perceiver_embeddings: tf.Tensor | None = None,\n+        image_attention_mask: tf.Tensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        interpolate_pos_encoding: bool | None = False,\n+        return_dict: bool | None = None,\n+        training: bool | None = None,\n+    ) -> TFIdeficsBaseModelOutputWithPast | tuple[tf.Tensor]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1554,21 +1553,21 @@ def __init__(self, config: IdeficsConfig, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        attention_mask: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        past_key_values: Optional[list[tf.Tensor]] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n-        pixel_values: Optional[tf.Tensor] = None,\n-        image_encoder_embeddings: Optional[tf.Tensor] = None,\n-        perceiver_embeddings: Optional[tf.Tensor] = None,\n-        image_attention_mask: Optional[tf.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = False,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = None,\n-    ) -> Union[TFIdeficsBaseModelOutputWithPast, tuple[tf.Tensor]]:\n+        attention_mask: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        past_key_values: list[tf.Tensor] | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n+        pixel_values: tf.Tensor | None = None,\n+        image_encoder_embeddings: tf.Tensor | None = None,\n+        perceiver_embeddings: tf.Tensor | None = None,\n+        image_attention_mask: tf.Tensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        interpolate_pos_encoding: bool | None = False,\n+        return_dict: bool | None = None,\n+        training: bool | None = None,\n+    ) -> TFIdeficsBaseModelOutputWithPast | tuple[tf.Tensor]:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1659,22 +1658,22 @@ def tie_weights(self):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        attention_mask: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        past_key_values: Optional[list[tf.Tensor]] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n-        pixel_values: Optional[tf.Tensor] = None,\n-        image_encoder_embeddings: Optional[tf.Tensor] = None,\n-        perceiver_embeddings: Optional[tf.Tensor] = None,\n-        image_attention_mask: Optional[tf.Tensor] = None,\n-        labels: Optional[tf.Tensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = False,\n-        return_dict: Optional[bool] = None,\n+        attention_mask: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        past_key_values: list[tf.Tensor] | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n+        pixel_values: tf.Tensor | None = None,\n+        image_encoder_embeddings: tf.Tensor | None = None,\n+        perceiver_embeddings: tf.Tensor | None = None,\n+        image_attention_mask: tf.Tensor | None = None,\n+        labels: tf.Tensor | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        interpolate_pos_encoding: bool | None = False,\n+        return_dict: bool | None = None,\n         training=False,\n-    ) -> Union[TFIdeficsCausalLMOutputWithPast, tuple[tf.Tensor]]:\n+    ) -> TFIdeficsCausalLMOutputWithPast | tuple[tf.Tensor]:\n         r\"\"\"\n             labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "eecda6d251d014e634da0053ddeadd5ff8907a30",
            "filename": "src/transformers/models/layoutlm/modeling_tf_layoutlm.py",
            "status": "modified",
            "additions": 36,
            "deletions": 37,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import math\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -127,11 +126,11 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        bbox: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        bbox: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         \"\"\"\n@@ -584,12 +583,12 @@ def call(\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None,\n-        use_cache: Optional[bool],\n+        use_cache: bool | None,\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -825,11 +824,11 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -1070,11 +1069,11 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1175,12 +1174,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1304,12 +1303,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1440,12 +1439,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1572,13 +1571,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "c0586d58835ee372bf2c307564fc8593c3c3b57e",
            "filename": "src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py",
            "status": "modified",
            "additions": 47,
            "deletions": 63,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_tf_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_tf_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_tf_layoutlmv3.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import collections\n import math\n-from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -231,7 +230,7 @@ def create_position_ids(self, input_ids: tf.Tensor, inputs_embeds: tf.Tensor) ->\n     def call(\n         self,\n         input_ids: tf.Tensor | None = None,\n-        bbox: Optional[tf.Tensor] = None,\n+        bbox: tf.Tensor | None = None,\n         token_type_ids: tf.Tensor | None = None,\n         position_ids: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n@@ -341,7 +340,7 @@ def transpose_for_scores(self, x: tf.Tensor):\n         x = tf.reshape(x, new_shape)\n         return tf.transpose(x, perm=[0, 2, 1, 3])  # batch_size, num_heads, seq_length, attention_head_size\n \n-    def cogview_attention(self, attention_scores: tf.Tensor, alpha: Union[float, int] = 32):\n+    def cogview_attention(self, attention_scores: tf.Tensor, alpha: float | int = 32):\n         \"\"\"\n         https://huggingface.co/papers/2105.13290 Section 2.4 Stabilization of training: Precision Bottleneck Relaxation\n         (PB-Relax). A replacement of the original keras.layers.Softmax(axis=-1)(attention_scores). Seems the new\n@@ -363,7 +362,7 @@ def call(\n         rel_pos: tf.Tensor | None = None,\n         rel_2d_pos: tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], tuple[tf.Tensor, tf.Tensor]]:\n+    ) -> tuple[tf.Tensor] | tuple[tf.Tensor, tf.Tensor]:\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n         query_layer = self.transpose_for_scores(self.query(hidden_states))\n@@ -468,7 +467,7 @@ def call(\n         rel_pos: tf.Tensor | None = None,\n         rel_2d_pos: tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], tuple[tf.Tensor, tf.Tensor]]:\n+    ) -> tuple[tf.Tensor] | tuple[tf.Tensor, tf.Tensor]:\n         self_outputs = self.self_attention(\n             hidden_states,\n             attention_mask,\n@@ -571,7 +570,7 @@ def call(\n         rel_pos: tf.Tensor | None = None,\n         rel_2d_pos: tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], tuple[tf.Tensor, tf.Tensor]]:\n+    ) -> tuple[tf.Tensor] | tuple[tf.Tensor, tf.Tensor]:\n         self_attention_outputs = self.attention(\n             hidden_states,\n             attention_mask,\n@@ -711,12 +710,7 @@ def call(\n         return_dict: bool = True,\n         position_ids: tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[\n-        TFBaseModelOutput,\n-        tuple[tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor, tf.Tensor],\n-    ]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor] | tuple[tf.Tensor, tf.Tensor] | tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -926,7 +920,7 @@ def get_extended_attention_mask(self, attention_mask: tf.Tensor) -> tf.Tensor:\n \n         return extended_attention_mask\n \n-    def get_head_mask(self, head_mask: tf.Tensor | None) -> Union[tf.Tensor, list[tf.Tensor | None]]:\n+    def get_head_mask(self, head_mask: tf.Tensor | None) -> tf.Tensor | list[tf.Tensor | None]:\n         if head_mask is None:\n             return [None] * self.config.num_hidden_layers\n \n@@ -962,16 +956,11 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         pixel_values: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[\n-        TFBaseModelOutput,\n-        tuple[tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor, tf.Tensor],\n-    ]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor] | tuple[tf.Tensor, tf.Tensor] | tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n         # This method can be called with a variety of modalities:\n         # 1. text + layout\n         # 2. text + layout + image\n@@ -1274,16 +1263,11 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         pixel_values: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[\n-        TFBaseModelOutput,\n-        tuple[tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor, tf.Tensor],\n-    ]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor] | tuple[tf.Tensor, tf.Tensor] | tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1413,19 +1397,19 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         labels: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         bbox: tf.Tensor | None = None,\n         pixel_values: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[\n-        TFSequenceClassifierOutput,\n-        tuple[tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor, tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor],\n-    ]:\n+        training: bool | None = False,\n+    ) -> (\n+        TFSequenceClassifierOutput\n+        | tuple[tf.Tensor]\n+        | tuple[tf.Tensor, tf.Tensor]\n+        | tuple[tf.Tensor, tf.Tensor, tf.Tensor]\n+        | tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]\n+    ):\n         \"\"\"\n         Returns:\n \n@@ -1539,18 +1523,18 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         labels: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         pixel_values: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[\n-        TFTokenClassifierOutput,\n-        tuple[tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor, tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor],\n-    ]:\n+        training: bool | None = False,\n+    ) -> (\n+        TFTokenClassifierOutput\n+        | tuple[tf.Tensor]\n+        | tuple[tf.Tensor, tf.Tensor]\n+        | tuple[tf.Tensor, tf.Tensor, tf.Tensor]\n+        | tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]\n+    ):\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1668,19 +1652,19 @@ def call(\n         inputs_embeds: tf.Tensor | None = None,\n         start_positions: tf.Tensor | None = None,\n         end_positions: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n         bbox: tf.Tensor | None = None,\n         pixel_values: tf.Tensor | None = None,\n-        return_dict: Optional[bool] = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[\n-        TFQuestionAnsweringModelOutput,\n-        tuple[tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor, tf.Tensor],\n-        tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor],\n-    ]:\n+    ) -> (\n+        TFQuestionAnsweringModelOutput\n+        | tuple[tf.Tensor]\n+        | tuple[tf.Tensor, tf.Tensor]\n+        | tuple[tf.Tensor, tf.Tensor, tf.Tensor]\n+        | tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]\n+    ):\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "36d6677731c5803de4e4542f142600688bbc7e35",
            "filename": "src/transformers/models/led/modeling_tf_led.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import random\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -100,7 +99,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -1470,7 +1469,7 @@ class TFLEDEncoderBaseModelOutput(ModelOutput):\n             in the sequence.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     global_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -1533,7 +1532,7 @@ class TFLEDSeq2SeqModelOutput(ModelOutput):\n             in the sequence.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     decoder_hidden_states: tuple[tf.Tensor, ...] | None = None\n     decoder_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -1600,7 +1599,7 @@ class TFLEDSeq2SeqLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     decoder_hidden_states: tuple[tf.Tensor, ...] | None = None\n     decoder_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -1731,7 +1730,7 @@ class TFLEDEncoder(keras.layers.Layer):\n         config: LEDConfig\n     \"\"\"\n \n-    def __init__(self, config: LEDConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: LEDConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.dropout = keras.layers.Dropout(config.dropout)\n@@ -2001,7 +2000,7 @@ class TFLEDDecoder(keras.layers.Layer):\n         embed_tokens: output embedding\n     \"\"\"\n \n-    def __init__(self, config: LEDConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: LEDConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.padding_idx = config.pad_token_id\n@@ -2253,7 +2252,7 @@ def call(\n         decoder_attention_mask=None,\n         head_mask=None,\n         decoder_head_mask=None,\n-        encoder_outputs: Optional[Union[tuple, TFLEDEncoderBaseModelOutput]] = None,\n+        encoder_outputs: tuple | TFLEDEncoderBaseModelOutput | None = None,\n         global_attention_mask=None,\n         past_key_values=None,\n         inputs_embeds=None,\n@@ -2509,7 +2508,7 @@ def call(\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         encoder_outputs: TFLEDEncoderBaseModelOutput | None = None,\n         global_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: tuple[tuple[Union[np.ndarray, tf.Tensor]]] | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         use_cache: bool | None = None,"
        },
        {
            "sha": "5ff41747d20c4036cc60e3fa3288f1cbff903b0e",
            "filename": "src/transformers/models/longformer/modeling_tf_longformer.py",
            "status": "modified",
            "additions": 40,
            "deletions": 41,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -93,7 +92,7 @@ class TFLongformerBaseModelOutput(ModelOutput):\n             in the sequence.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     global_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -140,8 +139,8 @@ class TFLongformerBaseModelOutputWithPooling(ModelOutput):\n             in the sequence.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    pooler_output: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    pooler_output: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     global_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -187,7 +186,7 @@ class TFLongformerMaskedLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     global_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -235,8 +234,8 @@ class TFLongformerQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    start_logits: Optional[tf.Tensor] = None\n-    end_logits: Optional[tf.Tensor] = None\n+    start_logits: tf.Tensor | None = None\n+    end_logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     global_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -282,7 +281,7 @@ class TFLongformerSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     global_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -330,7 +329,7 @@ class TFLongformerMultipleChoiceModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     global_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -376,7 +375,7 @@ class TFLongformerTokenClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     global_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -2138,11 +2137,11 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFLongformerBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFLongformerBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         outputs = self.longformer(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -2208,12 +2207,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFLongformerMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFLongformerMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -2304,13 +2303,13 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFLongformerQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFLongformerQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -2477,12 +2476,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         global_attention_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFLongformerSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFLongformerSequenceClassifierOutput | tuple[tf.Tensor]:\n         if input_ids is not None and not isinstance(input_ids, tf.Tensor):\n             input_ids = tf.convert_to_tensor(input_ids, dtype=tf.int64)\n         elif input_ids is not None:\n@@ -2603,12 +2602,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         global_attention_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFLongformerMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFLongformerMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -2721,12 +2720,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         global_attention_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        labels: Optional[Union[np.array, tf.Tensor]] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFLongformerTokenClassifierOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        labels: np.array | tf.Tensor | None = None,\n+        training: bool | None = False,\n+    ) -> TFLongformerTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`."
        },
        {
            "sha": "aee9fb7857967945f03f5c0cfcfaa0e1adb7b50e",
            "filename": "src/transformers/models/lxmert/modeling_tf_lxmert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_tf_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_tf_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_tf_lxmert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -20,7 +20,6 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -1109,11 +1108,11 @@ def call(\n         visual_attention_mask: np.ndarray | tf.Tensor | None = None,\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple, TFLxmertModelOutput]:\n+    ) -> tuple | TFLxmertModelOutput:\n         outputs = self.lxmert(\n             input_ids,\n             visual_feats,"
        },
        {
            "sha": "c989cfa15f5ac60e7d193dc3474661b47827166d",
            "filename": "src/transformers/models/marian/modeling_tf_marian.py",
            "status": "modified",
            "additions": 20,
            "deletions": 21,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_tf_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_tf_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_tf_marian.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import random\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -103,7 +102,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -216,7 +215,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -367,7 +366,7 @@ def call(\n         hidden_states: tf.Tensor,\n         attention_mask: np.ndarray | tf.Tensor | None,\n         layer_head_mask: tf.Tensor | None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tf.Tensor:\n         \"\"\"\n         Args:\n@@ -461,8 +460,8 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n-        past_key_value: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        training: Optional[bool] = False,\n+        past_key_value: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n@@ -723,7 +722,7 @@ class TFMarianEncoder(keras.layers.Layer):\n         config: MarianConfig\n     \"\"\"\n \n-    def __init__(self, config: MarianConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: MarianConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.dropout = keras.layers.Dropout(config.dropout)\n@@ -753,9 +752,9 @@ def call(\n         inputs_embeds: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ):\n         \"\"\"\n@@ -890,7 +889,7 @@ class TFMarianDecoder(keras.layers.Layer):\n         embed_tokens: output embedding\n     \"\"\"\n \n-    def __init__(self, config: MarianConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: MarianConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.padding_idx = config.pad_token_id\n@@ -924,10 +923,10 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ):\n         r\"\"\"\n@@ -1154,14 +1153,14 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Optional[tuple[tuple[tf.Tensor]]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[tf.Tensor]] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         **kwargs,\n     ):"
        },
        {
            "sha": "ac29bfeac76ff333483ea7396f70f9f2fded5c39",
            "filename": "src/transformers/models/mbart/modeling_tf_mbart.py",
            "status": "modified",
            "additions": 40,
            "deletions": 43,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_tf_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_tf_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_tf_mbart.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import random\n-from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -102,7 +101,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -129,7 +128,7 @@ def __init__(self, num_embeddings: int, embedding_dim: int, **kwargs):\n \n     def call(\n         self,\n-        input_shape: Optional[tf.TensorShape] = None,\n+        input_shape: tf.TensorShape | None = None,\n         past_key_values_length: int = 0,\n         position_ids: tf.Tensor | None = None,\n     ):\n@@ -185,7 +184,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -335,7 +334,7 @@ def call(\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n         layer_head_mask: tf.Tensor,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ):\n         \"\"\"\n         Args:\n@@ -429,7 +428,7 @@ def call(\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n         past_key_value: tuple[tf.Tensor] | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n@@ -713,7 +712,7 @@ class TFMBartEncoder(keras.layers.Layer):\n         config: MBartConfig\n     \"\"\"\n \n-    def __init__(self, config: MBartConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: MBartConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.dropout = keras.layers.Dropout(config.dropout)\n@@ -746,11 +745,11 @@ def call(\n         inputs_embeds: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         \"\"\"\n         Args:\n             input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n@@ -892,7 +891,7 @@ class TFMBartDecoder(keras.layers.Layer):\n         embed_tokens: output embedding\n     \"\"\"\n \n-    def __init__(self, config: MBartConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: MBartConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.padding_idx = config.pad_token_id\n@@ -928,14 +927,12 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[\n-        TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]\n-    ]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n         r\"\"\"\n         Args:\n             input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n@@ -1169,17 +1166,17 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[TFSeq2SeqModelOutput, tf.Tensor]:\n+    ) -> TFSeq2SeqModelOutput | tf.Tensor:\n         if decoder_input_ids is None and decoder_inputs_embeds is None:\n             use_cache = False\n \n@@ -1293,17 +1290,17 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[TFSeq2SeqModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFSeq2SeqModelOutput | tuple[tf.Tensor]:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1429,17 +1426,17 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[TFBaseModelOutput] = None,\n-        past_key_values: Optional[tuple[tuple[tf.Tensor]]] = None,\n+        encoder_outputs: TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[tf.Tensor]] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSeq2SeqLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSeq2SeqLMOutput | tuple[tf.Tensor]:\n         \"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "e4d148aa76c5a07847a05495569cf5d1942364d0",
            "filename": "src/transformers/models/mobilebert/modeling_tf_mobilebert.py",
            "status": "modified",
            "additions": 42,
            "deletions": 43,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_tf_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_tf_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_tf_mobilebert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -1063,8 +1062,8 @@ class TFMobileBertForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    prediction_logits: Optional[tf.Tensor] = None\n-    seq_relationship_logits: Optional[tf.Tensor] = None\n+    prediction_logits: tf.Tensor | None = None\n+    seq_relationship_logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -1191,11 +1190,11 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutputWithPooling]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutputWithPooling:\n         outputs = self.mobilebert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1252,13 +1251,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         next_sentence_label: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFMobileBertForPreTrainingOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFMobileBertForPreTrainingOutput:\n         r\"\"\"\n         Return:\n \n@@ -1369,12 +1368,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFMaskedLMOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFMaskedLMOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1471,12 +1470,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         next_sentence_label: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFNextSentencePredictorOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFNextSentencePredictorOutput:\n         r\"\"\"\n         Return:\n \n@@ -1587,12 +1586,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFSequenceClassifierOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFSequenceClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1687,13 +1686,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFQuestionAnsweringModelOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFQuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss.\n@@ -1796,12 +1795,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFMultipleChoiceModelOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFMultipleChoiceModelOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1914,12 +1913,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFTokenClassifierOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFTokenClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`."
        },
        {
            "sha": "bd656d171b111a5d21dfdc7626f5b3430531bc87",
            "filename": "src/transformers/models/mobilevit/modeling_tf_mobilevit.py",
            "status": "modified",
            "additions": 16,
            "deletions": 18,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_tf_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_tf_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_tf_mobilevit.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,8 +18,6 @@\n \n from __future__ import annotations\n \n-from typing import Optional, Union\n-\n import tensorflow as tf\n \n from ...activations_tf import get_tf_activation\n@@ -61,7 +59,7 @@\n _IMAGE_CLASS_EXPECTED_OUTPUT = \"tabby, tabby cat\"\n \n \n-def make_divisible(value: int, divisor: int = 8, min_value: Optional[int] = None) -> int:\n+def make_divisible(value: int, divisor: int = 8, min_value: int | None = None) -> int:\n     \"\"\"\n     Ensure that all layers have a channel count that is divisible by `divisor`. This function is taken from the\n     original TensorFlow repo. It can be seen here:\n@@ -88,7 +86,7 @@ def __init__(\n         bias: bool = False,\n         dilation: int = 1,\n         use_normalization: bool = True,\n-        use_activation: Union[bool, str] = True,\n+        use_activation: bool | str = True,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -764,7 +762,7 @@ def call(\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n         training: bool = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+    ) -> tuple | TFBaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n \n         for i, layer_module in enumerate(self.layers):\n@@ -830,10 +828,10 @@ class PreTrainedModel\n     def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFBaseModelOutputWithPooling]:\n+    ) -> tuple[tf.Tensor] | TFBaseModelOutputWithPooling:\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n@@ -998,10 +996,10 @@ def __init__(self, config: MobileViTConfig, expand_output: bool = True, *inputs,\n     def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFBaseModelOutputWithPooling]:\n+    ) -> tuple[tf.Tensor] | TFBaseModelOutputWithPooling:\n         output = self.mobilevit(pixel_values, output_hidden_states, return_dict, training=training)\n         return output\n \n@@ -1046,11 +1044,11 @@ def __init__(self, config: MobileViTConfig, *inputs, **kwargs) -> None:\n     def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n-        output_hidden_states: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFImageClassifierOutputWithNoAttention]:\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFImageClassifierOutputWithNoAttention:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n@@ -1287,10 +1285,10 @@ def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n         labels: tf.Tensor | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple, TFSemanticSegmenterOutputWithNoAttention]:\n+    ) -> tuple | TFSemanticSegmenterOutputWithNoAttention:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, height, width)`, *optional*):\n             Ground truth semantic segmentation maps for computing the loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "1afea867df35413e13616b25309fa62b336b2712",
            "filename": "src/transformers/models/mpnet/modeling_tf_mpnet.py",
            "status": "modified",
            "additions": 33,
            "deletions": 34,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_tf_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_tf_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_tf_mpnet.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n \n import math\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -765,15 +764,15 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        attention_mask: Optional[Union[np.array, tf.Tensor]] = None,\n-        position_ids: Optional[Union[np.array, tf.Tensor]] = None,\n-        head_mask: Optional[Union[np.array, tf.Tensor]] = None,\n+        attention_mask: np.array | tf.Tensor | None = None,\n+        position_ids: np.array | tf.Tensor | None = None,\n+        head_mask: np.array | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         outputs = self.mpnet(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -887,12 +886,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1002,16 +1001,16 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        attention_mask: Optional[Union[np.array, tf.Tensor]] = None,\n-        position_ids: Optional[Union[np.array, tf.Tensor]] = None,\n-        head_mask: Optional[Union[np.array, tf.Tensor]] = None,\n+        attention_mask: np.array | tf.Tensor | None = None,\n+        position_ids: np.array | tf.Tensor | None = None,\n+        head_mask: np.array | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1090,12 +1089,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1191,12 +1190,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1272,18 +1271,18 @@ def __init__(self, config, *inputs, **kwargs):\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        attention_mask: Optional[Union[np.array, tf.Tensor]] = None,\n-        position_ids: Optional[Union[np.array, tf.Tensor]] = None,\n-        head_mask: Optional[Union[np.array, tf.Tensor]] = None,\n+        attention_mask: np.array | tf.Tensor | None = None,\n+        position_ids: np.array | tf.Tensor | None = None,\n+        head_mask: np.array | tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: tf.Tensor | None = None,\n         end_positions: tf.Tensor | None = None,\n         training: bool = False,\n         **kwargs,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "0235159633b4fc6410f1e973dca90c29a1ddaa08",
            "filename": "src/transformers/models/openai/modeling_tf_openai.py",
            "status": "modified",
            "additions": 27,
            "deletions": 28,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -293,11 +292,11 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutput:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -429,8 +428,8 @@ class TFOpenAIGPTDoubleHeadsModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: Optional[tf.Tensor] = None\n-    mc_logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n+    mc_logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -557,11 +556,11 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutput:\n         outputs = self.transformer(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -620,12 +619,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFCausalLMOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFCausalLMOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,\n@@ -708,11 +707,11 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         mc_token_ids: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFOpenAIGPTDoubleHeadsModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFOpenAIGPTDoubleHeadsModelOutput:\n         r\"\"\"\n         mc_token_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, num_choices)`, *optional*, default to index of the last token of the input):\n             Index of the classification token in each input sequence. Selected in the range `[0, input_ids.size(-1) -\n@@ -853,12 +852,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFSequenceClassifierOutput]:\n+        training: bool | None = False,\n+    ) -> tuple | TFSequenceClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "38fb203d4c3a22f61a3c5ff64f2f159e07226937",
            "filename": "src/transformers/models/opt/modeling_tf_opt.py",
            "status": "modified",
            "additions": 34,
            "deletions": 36,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_tf_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_tf_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_tf_opt.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -16,8 +16,6 @@\n \n from __future__ import annotations\n \n-from typing import Optional, Union\n-\n import numpy as np\n import tensorflow as tf\n \n@@ -78,7 +76,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -157,7 +155,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -312,10 +310,10 @@ def call(\n         hidden_states: tf.Tensor,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        past_key_value: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        training: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n+        past_key_value: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        training: bool | None = False,\n+        output_attentions: bool | None = False,\n+        use_cache: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n@@ -578,13 +576,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPast, tuple[tf.Tensor]]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPast | tuple[tf.Tensor]:\n         r\"\"\"\n         Args:\n             input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n@@ -780,15 +778,15 @@ def call(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[TFBaseModelOutputWithPast, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPast | tuple[tf.Tensor]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -860,15 +858,15 @@ def call(\n         input_ids: TFModelInputType | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[TFBaseModelOutputWithPast, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPast | tuple[tf.Tensor]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -963,19 +961,19 @@ def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=\n     def call(\n         self,\n         input_ids: TFModelInputType | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[TFCausalLMOutputWithPast, tuple[tf.Tensor]]:\n+    ) -> TFCausalLMOutputWithPast | tuple[tf.Tensor]:\n         r\"\"\"\n         Args:\n             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "d159fc00138dcb6ca2f14213857b6b21e97c6865",
            "filename": "src/transformers/models/pegasus/modeling_tf_pegasus.py",
            "status": "modified",
            "additions": 37,
            "deletions": 38,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_tf_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_tf_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_tf_pegasus.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import random\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -104,7 +103,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -218,7 +217,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -369,7 +368,7 @@ def call(\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n         layer_head_mask: tf.Tensor,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ):\n         \"\"\"\n         Args:\n@@ -464,7 +463,7 @@ def call(\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n         past_key_value: tuple[tf.Tensor] | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n@@ -726,7 +725,7 @@ class TFPegasusEncoder(keras.layers.Layer):\n         config: PegasusConfig\n     \"\"\"\n \n-    def __init__(self, config: PegasusConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: PegasusConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.dropout = keras.layers.Dropout(config.dropout)\n@@ -757,10 +756,10 @@ def call(\n         inputs_embeds: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n     ):\n         \"\"\"\n         Args:\n@@ -899,7 +898,7 @@ class TFPegasusDecoder(keras.layers.Layer):\n         embed_tokens: output embedding\n     \"\"\"\n \n-    def __init__(self, config: PegasusConfig, embed_tokens: Optional[keras.layers.Embedding] = None, **kwargs):\n+    def __init__(self, config: PegasusConfig, embed_tokens: keras.layers.Embedding | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.config = config\n         self.padding_idx = config.pad_token_id\n@@ -933,12 +932,12 @@ def call(\n         encoder_attention_mask: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[tf.Tensor]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        past_key_values: tuple[tuple[tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n     ):\n         r\"\"\"\n         Args:\n@@ -1169,15 +1168,15 @@ def call(\n         head_mask: tf.Tensor | None = None,\n         decoder_head_mask: tf.Tensor | None = None,\n         cross_attn_head_mask: tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Optional[tuple[tuple[tf.Tensor]]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[tf.Tensor]] | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         decoder_inputs_embeds: tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n     ):\n         if decoder_input_ids is None and decoder_inputs_embeds is None:\n@@ -1290,17 +1289,17 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         **kwargs,\n-    ) -> Union[TFSeq2SeqModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFSeq2SeqModelOutput | tuple[tf.Tensor]:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1426,17 +1425,17 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[TFBaseModelOutput] = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        encoder_outputs: TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFSeq2SeqLMOutput, tuple[tf.Tensor]]:\n+    ) -> TFSeq2SeqLMOutput | tuple[tf.Tensor]:\n         \"\"\"\n         labels (`tf.tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "1d8c3f1fc8f0c3fc93f46a7080e78a450821f77c",
            "filename": "src/transformers/models/rag/modeling_tf_rag.py",
            "status": "modified",
            "additions": 29,
            "deletions": 30,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n \n import copy\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -115,7 +114,7 @@ class TFRetrievAugLMMarginOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     doc_scores: tf.Tensor | None = None\n     retrieved_doc_embeds: tf.Tensor | None = None\n@@ -198,7 +197,7 @@ class TFRetrievAugLMOutput(ModelOutput):\n             average in the self-attention heads.\n     \"\"\"\n \n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     past_key_values: list[tf.Tensor] | None = None\n     doc_scores: tf.Tensor | None = None\n     retrieved_doc_embeds: tf.Tensor | None = None\n@@ -232,8 +231,8 @@ class TFRagPreTrainedModel(TFPreTrainedModel):\n     @classmethod\n     def from_pretrained_question_encoder_generator(\n         cls,\n-        question_encoder_pretrained_model_name_or_path: Optional[str] = None,\n-        generator_pretrained_model_name_or_path: Optional[str] = None,\n+        question_encoder_pretrained_model_name_or_path: str | None = None,\n+        generator_pretrained_model_name_or_path: str | None = None,\n         retriever: RagRetriever = None,\n         *model_args,\n         **kwargs,\n@@ -499,11 +498,11 @@ class TFRagModel(TFRagPreTrainedModel):\n \n     def __init__(\n         self,\n-        config: Optional[PretrainedConfig] = None,\n-        question_encoder: Optional[TFPreTrainedModel] = None,\n-        generator: Optional[TFPreTrainedModel] = None,\n-        retriever: Optional[RagRetriever] = None,\n-        load_weight_prefix: Optional[str] = None,\n+        config: PretrainedConfig | None = None,\n+        question_encoder: TFPreTrainedModel | None = None,\n+        generator: TFPreTrainedModel | None = None,\n+        retriever: RagRetriever | None = None,\n+        load_weight_prefix: str | None = None,\n         **kwargs,\n     ):\n         assert config is not None or (question_encoder is not None and generator is not None), (\n@@ -554,7 +553,7 @@ def call(\n         encoder_outputs: np.ndarray | tf.Tensor | None = None,\n         decoder_input_ids: np.ndarray | tf.Tensor | None = None,\n         decoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: tuple[tuple[Union[np.ndarray, tf.Tensor]]] | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         doc_scores: np.ndarray | tf.Tensor | None = None,\n         context_input_ids: np.ndarray | tf.Tensor | None = None,\n         context_attention_mask: np.ndarray | tf.Tensor | None = None,\n@@ -741,10 +740,10 @@ class TFRagTokenForGeneration(TFRagPreTrainedModel, TFCausalLanguageModelingLoss\n \n     def __init__(\n         self,\n-        config: Optional[PretrainedConfig] = None,\n-        question_encoder: Optional[TFPreTrainedModel] = None,\n-        generator: Optional[TFPreTrainedModel] = None,\n-        retriever: Optional[RagRetriever] = None,\n+        config: PretrainedConfig | None = None,\n+        question_encoder: TFPreTrainedModel | None = None,\n+        generator: TFPreTrainedModel | None = None,\n+        retriever: RagRetriever | None = None,\n         **kwargs,\n     ):\n         assert config is not None or (question_encoder is not None and generator is not None), (\n@@ -859,7 +858,7 @@ def call(\n         decoder_input_ids: np.ndarray | tf.Tensor | None = None,\n         decoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         encoder_outputs: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: tuple[tuple[Union[np.ndarray, tf.Tensor]]] | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         doc_scores: np.ndarray | tf.Tensor | None = None,\n         context_input_ids: np.ndarray | tf.Tensor | None = None,\n         context_attention_mask: np.ndarray | tf.Tensor | None = None,\n@@ -1321,10 +1320,10 @@ class TFRagSequenceForGeneration(TFRagPreTrainedModel, TFCausalLanguageModelingL\n \n     def __init__(\n         self,\n-        config: Optional[PretrainedConfig] = None,\n-        question_encoder: Optional[TFPreTrainedModel] = None,\n-        generator: Optional[TFPreTrainedModel] = None,\n-        retriever: Optional[RagRetriever] = None,\n+        config: PretrainedConfig | None = None,\n+        question_encoder: TFPreTrainedModel | None = None,\n+        generator: TFPreTrainedModel | None = None,\n+        retriever: RagRetriever | None = None,\n         **kwargs,\n     ):\n         assert config is not None or (question_encoder is not None and generator is not None), (\n@@ -1373,22 +1372,22 @@ def call(\n         decoder_input_ids: np.ndarray | tf.Tensor | None = None,\n         decoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         encoder_outputs: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         doc_scores: np.ndarray | tf.Tensor | None = None,\n         context_input_ids: np.ndarray | tf.Tensor | None = None,\n         context_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_retrieved: Optional[bool] = None,\n-        n_docs: Optional[int] = None,\n-        exclude_bos_score: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        output_retrieved: bool | None = None,\n+        n_docs: int | None = None,\n+        exclude_bos_score: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        reduce_loss: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        reduce_loss: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         **kwargs,  # needs kwargs for generation\n-    ) -> Union[tuple[tf.Tensor], TFRetrievAugLMMarginOutput]:\n+    ) -> tuple[tf.Tensor] | TFRetrievAugLMMarginOutput:\n         r\"\"\"\n         exclude_bos_score (`bool`, *optional*):\n             Only relevant if `labels` is passed. If `True`, the score of the BOS token is disregarded when computing"
        },
        {
            "sha": "baf7b6e8adc9500521498b618c4640815c0bfe7a",
            "filename": "src/transformers/models/rembert/modeling_tf_rembert.py",
            "status": "modified",
            "additions": 50,
            "deletions": 51,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -106,10 +105,10 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:\n@@ -550,7 +549,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         hidden_states = self.embedding_hidden_mapping_in(inputs=hidden_states)\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n@@ -766,13 +765,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -1063,13 +1062,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1150,12 +1149,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1246,14 +1245,14 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutputWithCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1364,12 +1363,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1449,12 +1448,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1559,12 +1558,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1643,13 +1642,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "c5c56b85d5f37433c627572e495a12a034022ab0",
            "filename": "src/transformers/models/roberta/modeling_tf_roberta.py",
            "status": "modified",
            "additions": 47,
            "deletions": 48,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_tf_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_tf_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_tf_roberta.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n \n import math\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -596,12 +595,12 @@ def call(\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None,\n-        use_cache: Optional[bool],\n+        use_cache: bool | None,\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -709,13 +708,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -1005,13 +1004,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutputWithPoolingAndCrossAttentions]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutputWithPoolingAndCrossAttentions:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1156,12 +1155,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1259,14 +1258,14 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutputWithCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1417,12 +1416,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1506,12 +1505,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1614,12 +1613,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1704,13 +1703,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "0a370f390269240fb02d55f493fbf68905f99977",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_tf_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 47,
            "deletions": 48,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_tf_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_tf_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_tf_roberta_prelayernorm.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n \n import math\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -597,12 +596,12 @@ def call(\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None,\n-        use_cache: Optional[bool],\n+        use_cache: bool | None,\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -707,13 +706,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -1007,13 +1006,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutputWithPoolingAndCrossAttentions]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutputWithPoolingAndCrossAttentions:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1165,12 +1164,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1275,14 +1274,14 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutputWithCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1437,12 +1436,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1529,12 +1528,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1638,12 +1637,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1729,13 +1728,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "e07374e9fdf55f553750d7b489ad978cd3a8d550",
            "filename": "src/transformers/models/roformer/modeling_tf_roformer.py",
            "status": "modified",
            "additions": 43,
            "deletions": 44,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_tf_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_tf_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_tf_roformer.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import math\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -156,9 +155,9 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         \"\"\"\n@@ -524,7 +523,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -715,11 +714,11 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -934,11 +933,11 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         outputs = self.roformer(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -993,12 +992,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1072,12 +1071,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,\n@@ -1198,12 +1197,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1282,12 +1281,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1389,12 +1388,12 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1471,13 +1470,13 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "62ac3ca2ad2c63a17e1f561336961315e01ed2da",
            "filename": "src/transformers/models/sam/modeling_tf_sam.py",
            "status": "modified",
            "additions": 20,
            "deletions": 21,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -21,7 +21,6 @@\n \n import collections\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -71,7 +70,7 @@ class TFSamVisionEncoderOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: tf.Tensor | None = None\n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n \n@@ -105,8 +104,8 @@ class TFSamImageSegmentationOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    iou_scores: Optional[tf.Tensor] = None\n-    pred_masks: Optional[tf.Tensor] = None\n+    iou_scores: tf.Tensor | None = None\n+    pred_masks: tf.Tensor | None = None\n     vision_hidden_states: tuple[tf.Tensor, ...] | None = None\n     vision_attentions: tuple[tf.Tensor, ...] | None = None\n     mask_decoder_attentions: tuple[tf.Tensor, ...] | None = None\n@@ -431,10 +430,10 @@ def call(\n         point_embeddings: tf.Tensor,\n         image_embeddings: tf.Tensor,\n         image_positional_embeddings: tf.Tensor,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+    ) -> tuple | TFBaseModelOutput:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -613,7 +612,7 @@ def call(\n         sparse_prompt_embeddings: tf.Tensor,\n         dense_prompt_embeddings: tf.Tensor,\n         multimask_output: bool,\n-        output_attentions: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n     ) -> tuple[tf.Tensor, tf.Tensor]:\n         batch_size, num_channels, height, width = shape_list(image_embeddings)\n         point_batch_size = tf.math.maximum(1, tf.shape(sparse_prompt_embeddings)[1])\n@@ -857,8 +856,8 @@ def _embed_boxes(self, boxes: tf.Tensor) -> tf.Tensor:\n \n     def call(\n         self,\n-        batch_size: Optional[int],\n-        input_points: Optional[tuple[tf.Tensor, tf.Tensor]],\n+        batch_size: int | None,\n+        input_points: tuple[tf.Tensor, tf.Tensor] | None,\n         input_labels: tf.Tensor | None,\n         input_boxes: tf.Tensor | None,\n         input_masks: tf.Tensor | None,\n@@ -1119,8 +1118,8 @@ def window_unpartition(\n     def call(\n         self,\n         hidden_states: tf.Tensor,\n-        output_attentions: Optional[bool] = False,\n-        training: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor]:\n         residual = hidden_states\n \n@@ -1268,11 +1267,11 @@ def get_input_embeddings(self):\n     def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFSamVisionEncoderOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFSamVisionEncoderOutput:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1506,9 +1505,9 @@ def get_image_wide_positional_embeddings(self):\n     def get_image_embeddings(\n         self,\n         pixel_values,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n     ):\n         r\"\"\"\n         Returns the image embeddings by passing the pixel values through the vision encoder."
        },
        {
            "sha": "d9d000bab0fe612c5268f33a6162abeb975b13ca",
            "filename": "src/transformers/models/segformer/modeling_tf_segformer.py",
            "status": "modified",
            "additions": 24,
            "deletions": 25,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_tf_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_tf_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_tf_segformer.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import math\n-from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -169,7 +168,7 @@ def call(\n         width: int,\n         output_attentions: bool = False,\n         training: bool = False,\n-    ) -> Union[tf.Tensor, tuple[tf.Tensor, tf.Tensor]]:\n+    ) -> tf.Tensor | tuple[tf.Tensor, tf.Tensor]:\n         batch_size = shape_list(hidden_states)[0]\n         num_channels = shape_list(hidden_states)[2]\n \n@@ -272,7 +271,7 @@ def __init__(\n \n     def call(\n         self, hidden_states: tf.Tensor, height: int, width: int, output_attentions: bool = False\n-    ) -> Union[tf.Tensor, tuple[tf.Tensor, tf.Tensor]]:\n+    ) -> tf.Tensor | tuple[tf.Tensor, tf.Tensor]:\n         self_outputs = self.self(hidden_states, height, width, output_attentions)\n \n         attention_output = self.dense_output(self_outputs[0])\n@@ -325,8 +324,8 @@ def __init__(\n         self,\n         config: SegformerConfig,\n         in_features: int,\n-        hidden_features: Optional[int] = None,\n-        out_features: Optional[int] = None,\n+        hidden_features: int | None = None,\n+        out_features: int | None = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -499,11 +498,11 @@ def __init__(self, config: SegformerConfig, **kwargs):\n     def call(\n         self,\n         pixel_values: tf.Tensor,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n         training: bool = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+    ) -> tuple | TFBaseModelOutput:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -580,11 +579,11 @@ def __init__(self, config: SegformerConfig, **kwargs):\n     def call(\n         self,\n         pixel_values: tf.Tensor,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+    ) -> tuple | TFBaseModelOutput:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -714,11 +713,11 @@ def __init__(self, config: SegformerConfig, *inputs, **kwargs):\n     def call(\n         self,\n         pixel_values: tf.Tensor,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+    ) -> tuple | TFBaseModelOutput:\n         outputs = self.segformer(\n             pixel_values,\n             output_attentions=output_attentions,\n@@ -767,10 +766,10 @@ def call(\n         self,\n         pixel_values: tf.Tensor | None = None,\n         labels: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, TFSequenceClassifierOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+    ) -> tuple | TFSequenceClassifierOutput:\n         outputs = self.segformer(\n             pixel_values,\n             output_attentions=output_attentions,\n@@ -951,10 +950,10 @@ def call(\n         self,\n         pixel_values: tf.Tensor,\n         labels: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, TFSemanticSegmenterOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+    ) -> tuple | TFSemanticSegmenterOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, height, width)`, *optional*):\n             Ground truth semantic segmentation maps for computing the loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "e0fc6dd47d0cf5c7e773e77df5a2da873a3f2327",
            "filename": "src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 18,
            "deletions": 19,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import random\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -102,7 +101,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -173,15 +172,15 @@ def build(self, input_shape=None):\n class TFSpeech2TextSinusoidalPositionalEmbedding(keras.layers.Layer):\n     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n \n-    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None, **kwargs):\n+    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: int | None = None, **kwargs):\n         super().__init__(**kwargs)\n         self.offset = 2\n         self.embedding_dim = embedding_dim\n         self.padding_idx = padding_idx\n         self.embedding_weights = self._get_embedding(num_positions + self.offset, embedding_dim, padding_idx)\n \n     @staticmethod\n-    def _get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None) -> tf.Tensor:\n+    def _get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: int | None = None) -> tf.Tensor:\n         \"\"\"\n         Build sinusoidal embeddings. This matches the implementation in tensor2tensor, but differs slightly from the\n         description in Section 3.5 of \"Attention Is All You Need\".\n@@ -214,7 +213,7 @@ def call(self, input_ids: tf.Tensor, past_key_values_length: int = 0) -> tf.Tens\n \n     @staticmethod\n     def create_position_ids_from_input_ids(\n-        input_ids: tf.Tensor, padding_idx: int, past_key_values_length: Optional[int] = 0\n+        input_ids: tf.Tensor, padding_idx: int, past_key_values_length: int | None = 0\n     ) -> tf.Tensor:\n         \"\"\"\n         Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding\n@@ -271,7 +270,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -1349,15 +1348,15 @@ def call(\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n         encoder_outputs: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         **kwargs,\n-    ) -> Union[tuple, TFSeq2SeqModelOutput]:\n+    ) -> tuple | TFSeq2SeqModelOutput:\n         outputs = self.model(\n             input_features=input_features,\n             attention_mask=attention_mask,\n@@ -1448,16 +1447,16 @@ def call(\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n         encoder_outputs: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs,\n-    ) -> Union[tuple, TFSeq2SeqLMOutput]:\n+    ) -> tuple | TFSeq2SeqLMOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "0435a14b0e54c2d25f6a29b32bae3800801ca708",
            "filename": "src/transformers/models/swin/modeling_tf_swin.py",
            "status": "modified",
            "additions": 30,
            "deletions": 30,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_tf_swin.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -22,7 +22,7 @@\n from collections.abc import Iterable\n from dataclasses import dataclass\n from functools import partial\n-from typing import Any, Callable, Optional, Union\n+from typing import Any, Callable\n \n import tensorflow as tf\n \n@@ -92,7 +92,7 @@ class TFSwinEncoderOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     reshaped_hidden_states: tuple[tf.Tensor, ...] | None = None\n@@ -127,7 +127,7 @@ class TFSwinModelOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     pooler_output: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n@@ -164,7 +164,7 @@ class TFSwinMaskedImageModelingOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    reconstruction: Optional[tf.Tensor] = None\n+    reconstruction: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     reshaped_hidden_states: tuple[tf.Tensor, ...] | None = None\n@@ -209,7 +209,7 @@ class TFSwinImageClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n     reshaped_hidden_states: tuple[tf.Tensor, ...] | None = None\n@@ -308,7 +308,7 @@ def build(self, input_shape: tf.TensorShape) -> None:\n                 self.dropout.build(None)\n \n     def call(\n-        self, pixel_values: tf.Tensor, bool_masked_pos: Optional[bool] = None, training: bool = False\n+        self, pixel_values: tf.Tensor, bool_masked_pos: bool | None = None, training: bool = False\n     ) -> tuple[tf.Tensor, tuple[int, int]]:\n         embeddings, output_dimensions = self.patch_embeddings(pixel_values, training=training)\n         embeddings = self.norm(embeddings, training=training)\n@@ -413,7 +413,7 @@ class TFSwinPatchMerging(keras.layers.Layer):\n     \"\"\"\n \n     def __init__(\n-        self, input_resolution: tuple[int, int], dim: int, norm_layer: Optional[Callable] = None, **kwargs\n+        self, input_resolution: tuple[int, int], dim: int, norm_layer: Callable | None = None, **kwargs\n     ) -> None:\n         super().__init__(**kwargs)\n         self.input_resolution = input_resolution\n@@ -475,7 +475,7 @@ def build(self, input_shape=None):\n class TFSwinDropPath(keras.layers.Layer):\n     \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n \n-    def __init__(self, drop_prob: Optional[float] = None, scale_by_keep: bool = True, **kwargs) -> None:\n+    def __init__(self, drop_prob: float | None = None, scale_by_keep: bool = True, **kwargs) -> None:\n         super().__init__(**kwargs)\n         self.drop_prob = drop_prob\n         self.scale_by_keep = scale_by_keep\n@@ -908,7 +908,7 @@ def __init__(\n         depth: int,\n         num_heads: int,\n         drop_path: list[float],\n-        downsample: Optional[Callable],\n+        downsample: Callable | None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -945,7 +945,7 @@ def call(\n         hidden_states: tf.Tensor,\n         input_dimensions: tuple[int, int],\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor, ...]:\n         height, width = input_dimensions\n@@ -1015,7 +1015,7 @@ def call(\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor, ...], TFSwinEncoderOutput]:\n+    ) -> tuple[tf.Tensor, ...] | TFSwinEncoderOutput:\n         all_input_dimensions = ()\n         all_hidden_states = () if output_hidden_states else None\n         all_reshaped_hidden_states = () if output_hidden_states else None\n@@ -1157,9 +1157,9 @@ class AdaptiveAveragePooling1D(keras.layers.Layer):\n \n     def __init__(\n         self,\n-        output_size: Union[int, Iterable[int]],\n+        output_size: int | Iterable[int],\n         reduce_function: Callable = tf.reduce_mean,\n-        data_format: Optional[str] = None,\n+        data_format: str | None = None,\n         **kwargs,\n     ) -> None:\n         self.data_format = normalize_data_format(data_format)\n@@ -1225,7 +1225,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    def get_head_mask(self, head_mask: Optional[Any]) -> list:\n+    def get_head_mask(self, head_mask: Any | None) -> list:\n         if head_mask is not None:\n             raise NotImplementedError\n         return [None] * len(self.config.depths)\n@@ -1236,11 +1236,11 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         bool_masked_pos: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFSwinModelOutput, tuple[tf.Tensor, ...]]:\n+    ) -> TFSwinModelOutput | tuple[tf.Tensor, ...]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1332,11 +1332,11 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         bool_masked_pos: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFSwinModelOutput, tuple[tf.Tensor, ...]]:\n+    ) -> TFSwinModelOutput | tuple[tf.Tensor, ...]:\n         r\"\"\"\n         bool_masked_pos (`tf.Tensor` of shape `(batch_size, num_patches)`, *optional*):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n@@ -1449,11 +1449,11 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         bool_masked_pos: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple, TFSwinMaskedImageModelingOutput]:\n+    ) -> tuple | TFSwinMaskedImageModelingOutput:\n         r\"\"\"\n         bool_masked_pos (`tf.Tensor` of shape `(batch_size, num_patches)`):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n@@ -1583,11 +1583,11 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         labels: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor, ...], TFSwinImageClassifierOutput]:\n+    ) -> tuple[tf.Tensor, ...] | TFSwinImageClassifierOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "78ef8586748467ce08f124037a48e97ec377e506",
            "filename": "src/transformers/models/t5/modeling_tf_t5.py",
            "status": "modified",
            "additions": 19,
            "deletions": 20,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_tf_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_tf_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_tf_t5.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -21,7 +21,6 @@\n import itertools\n import math\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -1210,15 +1209,15 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         encoder_outputs: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFSeq2SeqModelOutput]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFSeq2SeqModelOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1387,16 +1386,16 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         encoder_outputs: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFSeq2SeqLMOutput]:\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFSeq2SeqLMOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,\n@@ -1620,11 +1619,11 @@ def call(\n         attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutput]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutput:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "624df1fba1766f16b32b0138b01930eec502a788",
            "filename": "src/transformers/models/tapas/modeling_tf_tapas.py",
            "status": "modified",
            "additions": 31,
            "deletions": 32,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tf_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tf_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tf_tapas.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n import enum\n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -111,7 +110,7 @@ class TFTableQuestionAnsweringOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     logits_aggregation: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n@@ -170,10 +169,10 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: Optional[tf.Tensor] = None,\n-        position_ids: Optional[tf.Tensor] = None,\n-        token_type_ids: Optional[tf.Tensor] = None,\n-        inputs_embeds: Optional[tf.Tensor] = None,\n+        input_ids: tf.Tensor | None = None,\n+        position_ids: tf.Tensor | None = None,\n+        token_type_ids: tf.Tensor | None = None,\n+        inputs_embeds: tf.Tensor | None = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         \"\"\"\n@@ -627,12 +626,12 @@ def call(\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None,\n-        use_cache: Optional[bool],\n+        use_cache: bool | None,\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -865,11 +864,11 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -1100,11 +1099,11 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1182,12 +1181,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1404,12 +1403,12 @@ def call(\n         float_answer: np.ndarray | tf.Tensor | None = None,\n         numeric_values: np.ndarray | tf.Tensor | None = None,\n         numeric_values_scale: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTableQuestionAnsweringOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTableQuestionAnsweringOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         table_mask (`tf.Tensor` of shape `(batch_size, seq_length)`, *optional*):\n             Mask for the table. Indicates which tokens belong to the table (1). Question tokens, table headers and\n@@ -1731,12 +1730,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "e1274cacfb2ecdf627f49c816bd3381fdaee3894",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 12,
            "deletions": 13,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import re\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -187,9 +186,9 @@ class TFVisionEncoderDecoderModel(TFPreTrainedModel, TFCausalLanguageModelingLos\n \n     def __init__(\n         self,\n-        config: Optional[PretrainedConfig] = None,\n-        encoder: Optional[TFPreTrainedModel] = None,\n-        decoder: Optional[TFPreTrainedModel] = None,\n+        config: PretrainedConfig | None = None,\n+        encoder: TFPreTrainedModel | None = None,\n+        decoder: TFPreTrainedModel | None = None,\n     ):\n         if config is None and (encoder is None or decoder is None):\n             raise ValueError(\"Either a configuration or an encoder and a decoder has to be provided.\")\n@@ -309,8 +308,8 @@ def tf_to_pt_weight_rename(self, tf_weight):\n     @classmethod\n     def from_encoder_decoder_pretrained(\n         cls,\n-        encoder_pretrained_model_name_or_path: Optional[str] = None,\n-        decoder_pretrained_model_name_or_path: Optional[str] = None,\n+        encoder_pretrained_model_name_or_path: str | None = None,\n+        decoder_pretrained_model_name_or_path: str | None = None,\n         *model_args,\n         **kwargs,\n     ) -> TFPreTrainedModel:\n@@ -462,17 +461,17 @@ def call(\n         pixel_values: np.ndarray | tf.Tensor | None = None,\n         decoder_input_ids: np.ndarray | tf.Tensor | None = None,\n         decoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[Union[tuple, TFBaseModelOutput]] = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        encoder_outputs: tuple | TFBaseModelOutput | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         decoder_inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         **kwargs,\n-    ) -> Union[TFSeq2SeqLMOutput, tuple[tf.Tensor]]:\n+    ) -> TFSeq2SeqLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "62ec6c955bc6e2c81d36c04b9297e4b9e9eb7660",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -17,7 +17,6 @@\n from __future__ import annotations\n \n import re\n-from typing import Optional, Union\n \n import tensorflow as tf\n \n@@ -178,9 +177,9 @@ class TFVisionTextDualEncoderModel(TFPreTrainedModel):\n \n     def __init__(\n         self,\n-        config: Optional[VisionTextDualEncoderConfig] = None,\n-        vision_model: Optional[TFPreTrainedModel] = None,\n-        text_model: Optional[TFPreTrainedModel] = None,\n+        config: VisionTextDualEncoderConfig | None = None,\n+        vision_model: TFPreTrainedModel | None = None,\n+        text_model: TFPreTrainedModel | None = None,\n     ):\n         if config is None and (vision_model is None or text_model is None):\n             raise ValueError(\"Either a configuration or an vision and a text model has to be provided\")\n@@ -351,13 +350,13 @@ def call(\n         pixel_values: tf.Tensor | None = None,\n         attention_mask: tf.Tensor | None = None,\n         position_ids: tf.Tensor | None = None,\n-        return_loss: Optional[bool] = None,\n+        return_loss: bool | None = None,\n         token_type_ids: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFCLIPOutput]:\n+    ) -> tuple[tf.Tensor] | TFCLIPOutput:\n         r\"\"\"\n         Returns:\n \n@@ -465,8 +464,8 @@ def call(\n     @classmethod\n     def from_vision_text_pretrained(\n         cls,\n-        vision_model_name_or_path: Optional[str] = None,\n-        text_model_name_or_path: Optional[str] = None,\n+        vision_model_name_or_path: str | None = None,\n+        text_model_name_or_path: str | None = None,\n         *model_args,\n         **kwargs,\n     ) -> TFPreTrainedModel:"
        },
        {
            "sha": "80d785e321144ba7675a93aab08d573712791d17",
            "filename": "src/transformers/models/vit/modeling_tf_vit.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_tf_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_tf_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_tf_vit.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import collections.abc\n import math\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -514,7 +513,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -583,12 +582,12 @@ def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        interpolate_pos_encoding: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n@@ -756,12 +755,12 @@ def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        interpolate_pos_encoding: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPooling, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPooling | tuple[tf.Tensor]:\n         outputs = self.vit(\n             pixel_values=pixel_values,\n             head_mask=head_mask,\n@@ -854,13 +853,13 @@ def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        interpolate_pos_encoding: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,"
        },
        {
            "sha": "d0184e92b37b283e935659787968ff92d0944bbd",
            "filename": "src/transformers/models/vit_mae/modeling_tf_vit_mae.py",
            "status": "modified",
            "additions": 25,
            "deletions": 26,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_tf_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_tf_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_tf_vit_mae.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -20,7 +20,6 @@\n import math\n from copy import deepcopy\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -74,9 +73,9 @@ class TFViTMAEModelOutput(ModelOutput):\n             the self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    mask: Optional[tf.Tensor] = None\n-    ids_restore: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    mask: tf.Tensor | None = None\n+    ids_restore: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -99,7 +98,7 @@ class TFViTMAEDecoderOutput(ModelOutput):\n             the self-attention heads.\n     \"\"\"\n \n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -129,9 +128,9 @@ class TFViTMAEForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n-    mask: Optional[tf.Tensor] = None\n-    ids_restore: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n+    mask: tf.Tensor | None = None\n+    ids_restore: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -314,7 +313,7 @@ def random_masking(self, sequence: tf.Tensor, noise: tf.Tensor | None = None):\n         return sequence_unmasked, mask, ids_restore\n \n     def call(\n-        self, pixel_values: tf.Tensor, noise: Optional[tf.Tensor] = None, interpolate_pos_encoding: bool = False\n+        self, pixel_values: tf.Tensor, noise: tf.Tensor | None = None, interpolate_pos_encoding: bool = False\n     ) -> tf.Tensor:\n         batch_size, num_channels, height, width = shape_list(pixel_values)\n         embeddings = self.patch_embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n@@ -708,7 +707,7 @@ def call(\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n \n@@ -775,14 +774,14 @@ class PreTrainedModel\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        noise: Optional[tf.Tensor] = None,\n+        noise: tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[TFViTMAEModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFViTMAEModelOutput | tuple[tf.Tensor]:\n         embedding_output, mask, ids_restore = self.embeddings(\n             pixel_values=pixel_values,\n             training=training,\n@@ -943,14 +942,14 @@ def get_input_embeddings(self):\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        noise: Optional[tf.Tensor] = None,\n+        noise: tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[TFViTMAEModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFViTMAEModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n \n@@ -1219,7 +1218,7 @@ def patchify(self, pixel_values, interpolate_pos_encoding: bool = False):\n         )\n         return patchified_pixel_values\n \n-    def unpatchify(self, patchified_pixel_values, original_image_size: Optional[tuple[int, int]] = None):\n+    def unpatchify(self, patchified_pixel_values, original_image_size: tuple[int, int] | None = None):\n         \"\"\"\n         Args:\n             patchified_pixel_values (`tf.Tensor` of shape `(batch_size, num_patches, patch_size**2 * num_channels)`:\n@@ -1294,14 +1293,14 @@ def forward_loss(self, pixel_values, pred, mask, interpolate_pos_encoding: bool\n     def call(\n         self,\n         pixel_values: TFModelInputType | None = None,\n-        noise: Optional[tf.Tensor] = None,\n+        noise: tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         interpolate_pos_encoding: bool = False,\n-    ) -> Union[TFViTMAEForPreTrainingOutput, tuple[tf.Tensor]]:\n+    ) -> TFViTMAEForPreTrainingOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "dd116a7c80d56840b24124937992ba29edf2b40c",
            "filename": "src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py",
            "status": "modified",
            "additions": 29,
            "deletions": 29,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,7 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import tensorflow as tf\n@@ -78,8 +78,8 @@ class TFWav2Vec2BaseModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n-    extract_features: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n+    extract_features: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor] | None = None\n     attentions: tuple[tf.Tensor] | None = None\n \n@@ -184,7 +184,7 @@ def _compute_mask_indices(\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -729,7 +729,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -922,7 +922,7 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         attn_residual = hidden_states\n@@ -981,7 +981,7 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n         training: bool = False,\n     ) -> tuple[tf.Tensor]:\n         attn_residual = hidden_states\n@@ -1031,11 +1031,11 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -1113,11 +1113,11 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+        output_attentions: bool | None = False,\n+        output_hidden_states: bool | None = False,\n+        return_dict: bool | None = True,\n+        training: bool | None = False,\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n@@ -1281,9 +1281,9 @@ def call(\n         position_ids: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n         **kwargs: Any,\n     ):\n@@ -1516,11 +1516,11 @@ def call(\n         position_ids: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         \"\"\"\n \n         Returns:\n@@ -1620,12 +1620,12 @@ def call(\n         position_ids: tf.Tensor | None = None,\n         head_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n         labels: tf.Tensor | None = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutput, tuple[tf.Tensor]]:\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,"
        },
        {
            "sha": "02f7dc5a9b07ee5b52613f7bbb8a2f0b3061bd0b",
            "filename": "src/transformers/models/whisper/modeling_tf_whisper.py",
            "status": "modified",
            "additions": 28,
            "deletions": 29,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,6 @@\n \n import math\n import random\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -111,7 +110,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -129,7 +128,7 @@ def __init__(\n         self,\n         num_positions: int,\n         embedding_dim: int,\n-        padding_idx: Optional[int] = None,\n+        padding_idx: int | None = None,\n         embedding_initializer=None,\n         **kwargs,\n     ):\n@@ -197,7 +196,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -1265,15 +1264,15 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        decoder_inputs_embeds: Optional[tuple[Union[np.ndarray, tf.Tensor]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        encoder_outputs: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        decoder_inputs_embeds: tuple[np.ndarray | tf.Tensor] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFSeq2SeqModelOutput]:\n+    ) -> tuple[tf.Tensor] | TFSeq2SeqModelOutput:\n         r\"\"\"\n         Returns:\n \n@@ -1388,16 +1387,16 @@ def call(\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         decoder_head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        encoder_outputs: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        decoder_inputs_embeds: Optional[tuple[Union[np.ndarray, tf.Tensor]]] = None,\n+        encoder_outputs: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        decoder_inputs_embeds: tuple[np.ndarray | tf.Tensor] | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[tuple[tf.Tensor], TFSeq2SeqLMOutput]:\n+    ) -> tuple[tf.Tensor] | TFSeq2SeqLMOutput:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`\n@@ -1476,15 +1475,15 @@ def call(\n \n     def generate(\n         self,\n-        inputs: Optional[tf.Tensor] = None,\n-        generation_config: Optional[GenerationConfig] = None,\n-        logits_processor: Optional[TFLogitsProcessorList] = None,\n-        seed: Optional[list[int]] = None,\n-        return_timestamps: Optional[bool] = None,\n-        task: Optional[str] = None,\n-        language: Optional[str] = None,\n-        is_multilingual: Optional[bool] = None,\n-        prompt_ids: Optional[tf.Tensor] = None,\n+        inputs: tf.Tensor | None = None,\n+        generation_config: GenerationConfig | None = None,\n+        logits_processor: TFLogitsProcessorList | None = None,\n+        seed: list[int] | None = None,\n+        return_timestamps: bool | None = None,\n+        task: str | None = None,\n+        language: str | None = None,\n+        is_multilingual: bool | None = None,\n+        prompt_ids: tf.Tensor | None = None,\n         return_token_timestamps=None,\n         **kwargs,\n     ):"
        },
        {
            "sha": "16b661dd120ad39ddec3616926874d374259cd95",
            "filename": "src/transformers/models/xglm/modeling_tf_xglm.py",
            "status": "modified",
            "additions": 31,
            "deletions": 31,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -18,7 +18,7 @@\n \n import math\n import random\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import tensorflow as tf\n@@ -57,7 +57,7 @@\n LARGE_NEGATIVE = -1e8\n \n \n-def create_sinusoidal_positions(num_positions: int, embedding_dim: int, padding_idx: Optional[int]) -> tf.Tensor:\n+def create_sinusoidal_positions(num_positions: int, embedding_dim: int, padding_idx: int | None) -> tf.Tensor:\n     half_dim = embedding_dim // 2\n     emb = math.log(10000) / (half_dim - 1)\n     emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n@@ -81,7 +81,7 @@ def create_sinusoidal_positions(num_positions: int, embedding_dim: int, padding_\n \n \n def _create_position_ids_from_input_ids(\n-    input_ids: tf.Tensor, past_key_values_length: int, padding_idx: Optional[int]\n+    input_ids: tf.Tensor, past_key_values_length: int, padding_idx: int | None\n ) -> tf.Tensor:\n     \"\"\"\n     Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n@@ -94,7 +94,7 @@ def _create_position_ids_from_input_ids(\n \n \n def _create_position_ids_from_inputs_embeds(\n-    inputs_embeds: tf.Tensor, past_key_values_length: int, padding_idx: Optional[int]\n+    inputs_embeds: tf.Tensor, past_key_values_length: int, padding_idx: int | None\n ) -> tf.Tensor:\n     \"\"\"\n     Args:\n@@ -129,7 +129,7 @@ def _make_causal_mask(input_ids_shape: tf.TensorShape, past_key_values_length: i\n \n \n # Copied from transformers.models.bart.modeling_tf_bart._expand_mask\n-def _expand_mask(mask: tf.Tensor, tgt_len: Optional[int] = None):\n+def _expand_mask(mask: tf.Tensor, tgt_len: int | None = None):\n     \"\"\"\n     Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n     \"\"\"\n@@ -184,7 +184,7 @@ def call(\n         past_key_value: tuple[tuple[tf.Tensor]] | None = None,\n         attention_mask: tf.Tensor | None = None,\n         layer_head_mask: tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor | None]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -356,7 +356,7 @@ def call(\n         layer_head_mask: tf.Tensor | None = None,\n         cross_attn_layer_head_mask: tf.Tensor | None = None,\n         past_key_value: tuple[tf.Tensor] | None = None,\n-        training: Optional[bool] = False,\n+        training: bool | None = False,\n     ) -> tuple[tf.Tensor, tf.Tensor, tuple[tuple[tf.Tensor]]]:\n         \"\"\"\n         Args:\n@@ -459,7 +459,7 @@ class TFXGLMMainLayer(keras.layers.Layer):\n     config_class = XGLMConfig\n \n     def __init__(\n-        self, config: XGLMConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, *inputs, **kwargs: Any\n+        self, config: XGLMConfig, embed_tokens: TFSharedEmbeddings | None = None, *inputs, **kwargs: Any\n     ) -> None:\n         super().__init__(*inputs, **kwargs)\n \n@@ -525,15 +525,15 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs: Any,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -796,7 +796,7 @@ class TFXGLMModel(TFXGLMPreTrainedModel):\n     \"\"\"\n \n     def __init__(\n-        self, config: XGLMConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, *inputs: Any, **kwargs: Any\n+        self, config: XGLMConfig, embed_tokens: TFSharedEmbeddings | None = None, *inputs: Any, **kwargs: Any\n     ) -> None:\n         super().__init__(config, *inputs, **kwargs)\n \n@@ -818,15 +818,15 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs: Any,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -872,7 +872,7 @@ class TFXGLMForCausalLM(TFXGLMPreTrainedModel, TFCausalLanguageModelingLoss):\n     ]\n \n     def __init__(\n-        self, config: XGLMConfig, embed_tokens: Optional[TFSharedEmbeddings] = None, *inputs: Any, **kwargs: Any\n+        self, config: XGLMConfig, embed_tokens: TFSharedEmbeddings | None = None, *inputs: Any, **kwargs: Any\n     ) -> None:\n         super().__init__(config, *inputs, **kwargs)\n \n@@ -929,16 +929,16 @@ def call(\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         cross_attn_head_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n         **kwargs: Any,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFCausalLMOutputWithCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`np.ndarray` or `tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set"
        },
        {
            "sha": "db89b4686f84bd6e19fdb340cbe91e3b9573c678",
            "filename": "src/transformers/models/xlm/modeling_tf_xlm.py",
            "status": "modified",
            "additions": 27,
            "deletions": 28,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_tf_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_tf_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_tf_xlm.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -21,7 +21,6 @@\n import itertools\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -400,7 +399,7 @@ def call(\n         output_hidden_states=None,\n         return_dict=None,\n         training=False,\n-    ) -> Union[TFBaseModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutput | tuple[tf.Tensor]:\n         # removed: src_enc=None, src_len=None\n \n         if input_ids is not None and inputs_embeds is not None:\n@@ -599,7 +598,7 @@ class TFXLMWithLMHeadModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n \n@@ -881,14 +880,14 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFXLMWithLMHeadModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFXLMWithLMHeadModelOutput | tuple[tf.Tensor]:\n         transformer_outputs = self.transformer(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -957,15 +956,15 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1068,15 +1067,15 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         if input_ids is not None:\n             num_choices = shape_list(input_ids)[1]\n             seq_length = shape_list(input_ids)[2]\n@@ -1184,15 +1183,15 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1273,16 +1272,16 @@ def call(\n         token_type_ids: np.ndarray | tf.Tensor | None = None,\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         lengths: np.ndarray | tf.Tensor | None = None,\n-        cache: Optional[dict[str, tf.Tensor]] = None,\n+        cache: dict[str, tf.Tensor] | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "0def1bfdb00da7422d0957f330936ebc74e99e38",
            "filename": "src/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py",
            "status": "modified",
            "additions": 47,
            "deletions": 48,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_tf_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_tf_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_tf_xlm_roberta.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -19,7 +19,6 @@\n \n import math\n import warnings\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -686,12 +685,12 @@ def call(\n         encoder_hidden_states: tf.Tensor | None,\n         encoder_attention_mask: tf.Tensor | None,\n         past_key_values: tuple[tuple[tf.Tensor]] | None,\n-        use_cache: Optional[bool],\n+        use_cache: bool | None,\n         output_attentions: bool,\n         output_hidden_states: bool,\n         return_dict: bool,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPastAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPastAndCrossAttentions | tuple[tf.Tensor]:\n         all_hidden_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n@@ -800,13 +799,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFBaseModelOutputWithPoolingAndCrossAttentions, tuple[tf.Tensor]]:\n+    ) -> TFBaseModelOutputWithPoolingAndCrossAttentions | tuple[tf.Tensor]:\n         if not self.config.is_decoder:\n             use_cache = False\n \n@@ -1000,13 +999,13 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[tuple, TFBaseModelOutputWithPoolingAndCrossAttentions]:\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool | None = False,\n+    ) -> tuple | TFBaseModelOutputWithPoolingAndCrossAttentions:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1153,12 +1152,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMaskedLMOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMaskedLMOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n@@ -1261,14 +1260,14 @@ def call(\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n         encoder_hidden_states: np.ndarray | tf.Tensor | None = None,\n         encoder_attention_mask: np.ndarray | tf.Tensor | None = None,\n-        past_key_values: Optional[tuple[tuple[Union[np.ndarray, tf.Tensor]]]] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        past_key_values: tuple[tuple[np.ndarray | tf.Tensor]] | None = None,\n+        use_cache: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFCausalLMOutputWithCrossAttentions, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFCausalLMOutputWithCrossAttentions | tuple[tf.Tensor]:\n         r\"\"\"\n         encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1421,12 +1420,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFSequenceClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFSequenceClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1513,12 +1512,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFMultipleChoiceModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFMultipleChoiceModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1622,12 +1621,12 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFTokenClassifierOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFTokenClassifierOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1713,13 +1712,13 @@ def call(\n         position_ids: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n-        training: Optional[bool] = False,\n-    ) -> Union[TFQuestionAnsweringModelOutput, tuple[tf.Tensor]]:\n+        training: bool | None = False,\n+    ) -> TFQuestionAnsweringModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "451d26c844d8dc3c054a26c532e070e00460e22c",
            "filename": "src/transformers/models/xlnet/modeling_tf_xlnet.py",
            "status": "modified",
            "additions": 43,
            "deletions": 44,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_tf_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43f07018cff3c305e9d3a3782e3a95100800be4c/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_tf_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_tf_xlnet.py?ref=43f07018cff3c305e9d3a3782e3a95100800be4c",
            "patch": "@@ -21,7 +21,6 @@\n \n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n import tensorflow as tf\n@@ -201,7 +200,7 @@ def call(\n         mems: np.ndarray | tf.Tensor | None = None,\n         target_mapping: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n         training: bool = False,\n     ):\n         if g is not None:\n@@ -390,7 +389,7 @@ def call(\n         mems: np.ndarray | tf.Tensor | None = None,\n         target_mapping: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n-        output_attentions: Optional[bool] = False,\n+        output_attentions: bool | None = False,\n         training: bool = False,\n     ):\n         outputs = self.rel_attn(\n@@ -631,10 +630,10 @@ def call(\n         input_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_mems: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_mems: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n     ):\n         if training and use_mems is None:\n@@ -863,7 +862,7 @@ class TFXLNetModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: Optional[tf.Tensor] = None\n+    last_hidden_state: tf.Tensor | None = None\n     mems: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n@@ -900,7 +899,7 @@ class TFXLNetLMHeadModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     mems: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n@@ -934,7 +933,7 @@ class TFXLNetForSequenceClassificationOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     mems: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n@@ -968,7 +967,7 @@ class TFXLNetForTokenClassificationOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     mems: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n@@ -1004,7 +1003,7 @@ class TFXLNetForMultipleChoiceOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: Optional[tf.Tensor] = None\n+    logits: tf.Tensor | None = None\n     mems: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n@@ -1040,8 +1039,8 @@ class TFXLNetForQuestionAnsweringSimpleOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    start_logits: Optional[tf.Tensor] = None\n-    end_logits: Optional[tf.Tensor] = None\n+    start_logits: tf.Tensor | None = None\n+    end_logits: tf.Tensor | None = None\n     mems: list[tf.Tensor] | None = None\n     hidden_states: tuple[tf.Tensor, ...] | None = None\n     attentions: tuple[tf.Tensor, ...] | None = None\n@@ -1189,12 +1188,12 @@ def call(\n         input_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_mems: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_mems: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         training: bool = False,\n-    ) -> Union[TFXLNetModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFXLNetModelOutput | tuple[tf.Tensor]:\n         outputs = self.transformer(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n@@ -1297,13 +1296,13 @@ def call(\n         input_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_mems: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_mems: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFXLNetLMHeadModelOutput, tuple[tf.Tensor]]:\n+    ) -> TFXLNetLMHeadModelOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,\n@@ -1432,13 +1431,13 @@ def call(\n         input_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_mems: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_mems: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFXLNetForSequenceClassificationOutput, tuple[tf.Tensor]]:\n+    ) -> TFXLNetForSequenceClassificationOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1533,13 +1532,13 @@ def call(\n         target_mapping: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_mems: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_mems: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFXLNetForMultipleChoiceOutput, tuple[tf.Tensor]]:\n+    ) -> TFXLNetForMultipleChoiceOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ..., num_choices]`\n@@ -1647,13 +1646,13 @@ def call(\n         input_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_mems: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_mems: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         labels: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFXLNetForTokenClassificationOutput, tuple[tf.Tensor]]:\n+    ) -> TFXLNetForTokenClassificationOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n@@ -1737,14 +1736,14 @@ def call(\n         input_mask: np.ndarray | tf.Tensor | None = None,\n         head_mask: np.ndarray | tf.Tensor | None = None,\n         inputs_embeds: np.ndarray | tf.Tensor | None = None,\n-        use_mems: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        use_mems: bool | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n         start_positions: np.ndarray | tf.Tensor | None = None,\n         end_positions: np.ndarray | tf.Tensor | None = None,\n         training: bool = False,\n-    ) -> Union[TFXLNetForQuestionAnsweringSimpleOutput, tuple[tf.Tensor]]:\n+    ) -> TFXLNetForQuestionAnsweringSimpleOutput | tuple[tf.Tensor]:\n         r\"\"\"\n         start_positions (`tf.Tensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        }
    ],
    "stats": {
        "total": 4439,
        "additions": 2176,
        "deletions": 2263
    }
}