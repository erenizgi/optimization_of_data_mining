{
    "author": "2015aroras",
    "message": "Rename OLMo November to OLMo2 (#34864)\n\n* Rename/move OLMo Nov files to OLMo2\r\n\r\n* Rename Olmo1124 and its variants to Olmo2",
    "sha": "9121ab8fe87a296e57f9846e70153b1a3c555d75",
    "files": [
        {
            "sha": "0d2b752d5ad93fada34e7f9aba998d990a9576fa",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -516,8 +516,8 @@\n         title: NystrÃ¶mformer\n       - local: model_doc/olmo\n         title: OLMo\n-      - local: model_doc/olmo_1124\n-        title: OLMo November 2024\n+      - local: model_doc/olmo2\n+        title: OLMo2\n       - local: model_doc/olmoe\n         title: OLMoE\n       - local: model_doc/open-llama"
        },
        {
            "sha": "8a9ccf45b69c26610385431426ac51ae3b829b29",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -240,7 +240,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                        [Nougat](model_doc/nougat)                        |       âœ…        |         âœ…         |      âœ…      |\n |                 [NystrÃ¶mformer](model_doc/nystromformer)                 |       âœ…        |         âŒ         |      âŒ      |\n |                          [OLMo](model_doc/olmo)                          |       âœ…        |         âŒ         |      âŒ      |\n-|                [OLMo November 2024](model_doc/olmo_1124)                 |       âœ…        |         âŒ         |      âŒ      |\n+|                         [OLMo2](model_doc/olmo2)                         |       âœ…        |         âŒ         |      âŒ      |\n |                         [OLMoE](model_doc/olmoe)                         |       âœ…        |         âŒ         |      âŒ      |\n |                   [OmDet-Turbo](model_doc/omdet-turbo)                   |       âœ…        |         âŒ         |      âŒ      |\n |                     [OneFormer](model_doc/oneformer)                     |       âœ…        |         âŒ         |      âŒ      |"
        },
        {
            "sha": "8ca3326660b3f4893af25634574a36791da7b03b",
            "filename": "docs/source/en/model_doc/olmo2.md",
            "status": "renamed",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo2.md?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -14,11 +14,11 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# OLMo November 2024\n+# OLMo2\n \n ## Overview\n \n-The OLMo November 2024 model is a successor of the OLMo model, which was proposed in\n+The OLMo2 model is the successor of the OLMo model, which was proposed in\n [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838).\n \n  The architectural changes from the original OLMo model to this model are:\n@@ -31,16 +31,16 @@ This model was contributed by [shanearora](https://huggingface.co/shanearora).\n The original code can be found [here](https://github.com/allenai/OLMo/tree/main/olmo).\n \n \n-## Olmo1124Config\n+## Olmo2Config\n \n-[[autodoc]] Olmo1124Config\n+[[autodoc]] Olmo2Config\n \n-## Olmo1124Model\n+## Olmo2Model\n \n-[[autodoc]] Olmo1124Model\n+[[autodoc]] Olmo2Model\n     - forward\n \n-## Olmo1124ForCausalLM\n+## Olmo2ForCausalLM\n \n-[[autodoc]] Olmo1124ForCausalLM\n+[[autodoc]] Olmo2ForCausalLM\n     - forward",
            "previous_filename": "docs/source/en/model_doc/olmo_1124.md"
        },
        {
            "sha": "8a106cae7386f101b12d8579573dea3a2db164c5",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -77,7 +77,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [Nemotron](https://huggingface.co/docs/transformers/model_doc/nemotron)\n * [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)\n * [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)\n-* [OLMo November 2024](https://huggingface.co/docs/transformers/model_doc/olmo_1124#transformers.Olmo1124Model)\n+* [OLMo2](https://huggingface.co/docs/transformers/model_doc/olmo2#transformers.Olmo2Model)\n * [OLMoE](https://huggingface.co/docs/transformers/model_doc/olmoe#transformers.OlmoeModel)\n * [OPT](https://huggingface.co/docs/transformers/model_doc/opt#transformers.OPTModel)\n * [PaliGemma](https://huggingface.co/docs/transformers/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration)\n@@ -261,7 +261,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)\n * [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)\n * [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)\n-* [OLMo November 2024](https://huggingface.co/docs/transformers/model_doc/olmo_1124#transformers.Olmo1124Model)\n+* [OLMo2](https://huggingface.co/docs/transformers/model_doc/olmo2#transformers.Olmo2Model)\n * [OLMoE](https://huggingface.co/docs/transformers/model_doc/olmoe#transformers.OlmoeModel)\n * [OPT](https://huggingface.co/docs/transformers/en/model_doc/opt)\n * [PaliGemma](https://huggingface.co/docs/transformers/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration)"
        },
        {
            "sha": "ce2a2553d6d7be79c60377faec1b621932b34c3c",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -620,7 +620,7 @@\n     \"models.nougat\": [\"NougatProcessor\"],\n     \"models.nystromformer\": [\"NystromformerConfig\"],\n     \"models.olmo\": [\"OlmoConfig\"],\n-    \"models.olmo_1124\": [\"Olmo1124Config\"],\n+    \"models.olmo2\": [\"Olmo2Config\"],\n     \"models.olmoe\": [\"OlmoeConfig\"],\n     \"models.omdet_turbo\": [\n         \"OmDetTurboConfig\",\n@@ -2920,11 +2920,11 @@\n             \"OlmoPreTrainedModel\",\n         ]\n     )\n-    _import_structure[\"models.olmo_1124\"].extend(\n+    _import_structure[\"models.olmo2\"].extend(\n         [\n-            \"Olmo1124ForCausalLM\",\n-            \"Olmo1124Model\",\n-            \"Olmo1124PreTrainedModel\",\n+            \"Olmo2ForCausalLM\",\n+            \"Olmo2Model\",\n+            \"Olmo2PreTrainedModel\",\n         ]\n     )\n     _import_structure[\"models.olmoe\"].extend(\n@@ -5514,7 +5514,7 @@\n         NystromformerConfig,\n     )\n     from .models.olmo import OlmoConfig\n-    from .models.olmo_1124 import Olmo1124Config\n+    from .models.olmo2 import Olmo2Config\n     from .models.olmoe import OlmoeConfig\n     from .models.omdet_turbo import (\n         OmDetTurboConfig,\n@@ -7533,10 +7533,10 @@\n             OlmoModel,\n             OlmoPreTrainedModel,\n         )\n-        from .models.olmo_1124 import (\n-            Olmo1124ForCausalLM,\n-            Olmo1124Model,\n-            Olmo1124PreTrainedModel,\n+        from .models.olmo2 import (\n+            Olmo2ForCausalLM,\n+            Olmo2Model,\n+            Olmo2PreTrainedModel,\n         )\n         from .models.olmoe import (\n             OlmoeForCausalLM,"
        },
        {
            "sha": "2d2a3b41d4378bb38dc63c237103591641d8ce6d",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -177,7 +177,7 @@\n     nougat,\n     nystromformer,\n     olmo,\n-    olmo_1124,\n+    olmo2,\n     olmoe,\n     omdet_turbo,\n     oneformer,"
        },
        {
            "sha": "4ab6d3922826579544280fd38d9d81e8ea81b363",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -195,7 +195,7 @@\n         (\"nougat\", \"VisionEncoderDecoderConfig\"),\n         (\"nystromformer\", \"NystromformerConfig\"),\n         (\"olmo\", \"OlmoConfig\"),\n-        (\"olmo_1124\", \"Olmo1124Config\"),\n+        (\"olmo2\", \"Olmo2Config\"),\n         (\"olmoe\", \"OlmoeConfig\"),\n         (\"omdet-turbo\", \"OmDetTurboConfig\"),\n         (\"oneformer\", \"OneFormerConfig\"),\n@@ -511,7 +511,7 @@\n         (\"nougat\", \"Nougat\"),\n         (\"nystromformer\", \"NystrÃ¶mformer\"),\n         (\"olmo\", \"OLMo\"),\n-        (\"olmo_1124\", \"OLMo November 2024\"),\n+        (\"olmo2\", \"OLMo2\"),\n         (\"olmoe\", \"OLMoE\"),\n         (\"omdet-turbo\", \"OmDet-Turbo\"),\n         (\"oneformer\", \"OneFormer\"),"
        },
        {
            "sha": "2c519a7dc42ca58c27859bcdb7b982fd250bb41f",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -184,7 +184,7 @@\n         (\"nllb-moe\", \"NllbMoeModel\"),\n         (\"nystromformer\", \"NystromformerModel\"),\n         (\"olmo\", \"OlmoModel\"),\n-        (\"olmo_1124\", \"Olmo1124Model\"),\n+        (\"olmo2\", \"Olmo2Model\"),\n         (\"olmoe\", \"OlmoeModel\"),\n         (\"omdet-turbo\", \"OmDetTurboForObjectDetection\"),\n         (\"oneformer\", \"OneFormerModel\"),\n@@ -517,7 +517,7 @@\n         (\"mvp\", \"MvpForCausalLM\"),\n         (\"nemotron\", \"NemotronForCausalLM\"),\n         (\"olmo\", \"OlmoForCausalLM\"),\n-        (\"olmo_1124\", \"Olmo1124ForCausalLM\"),\n+        (\"olmo2\", \"Olmo2ForCausalLM\"),\n         (\"olmoe\", \"OlmoeForCausalLM\"),\n         (\"open-llama\", \"OpenLlamaForCausalLM\"),\n         (\"openai-gpt\", \"OpenAIGPTLMHeadModel\"),"
        },
        {
            "sha": "e246bf3094c9cb5103737edff2bdd5a0748ed387",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -348,7 +348,7 @@\n                 ),\n             ),\n             (\"olmo\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"olmo_1124\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+            (\"olmo2\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"olmoe\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n             (\n                 \"omdet-turbo\","
        },
        {
            "sha": "e2161a4948b5e32f600af135c33330c2e2c353c7",
            "filename": "src/transformers/models/olmo2/__init__.py",
            "status": "renamed",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2F__init__.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -18,8 +18,8 @@\n \n \n if TYPE_CHECKING:\n-    from .configuration_olmo_1124 import *\n-    from .modeling_olmo_1124 import *\n+    from .configuration_olmo2 import *\n+    from .modeling_olmo2 import *\n else:\n     import sys\n ",
            "previous_filename": "src/transformers/models/olmo_1124/__init__.py"
        },
        {
            "sha": "144520f87ed7f9c5a46dba8e36e29b1dc0e09d04",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "renamed",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -1,27 +1,27 @@\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#           This file was automatically generated from src/transformers/models/olmo_1124/modular_olmo_1124.py.\n+#           This file was automatically generated from src/transformers/models/olmo2/modular_olmo2.py.\n #               Do NOT edit this file manually as any edits will be overwritten by the generation of\n #             the file from the modular. If any change should be done, please apply the change to the\n-#                          modular_olmo_1124.py file directly. One of our CI enforces this.\n+#                          modular_olmo2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n \n from ...configuration_utils import PretrainedConfig\n \n \n-class Olmo1124Config(PretrainedConfig):\n+class Olmo2Config(PretrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`Olmo1124Model`]. It is used to instantiate an OLMo November 2024\n+    This is the configuration class to store the configuration of a [`Olmo2Model`]. It is used to instantiate an OLMo2\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the [allenai/Olmo1124-7B-hf](https://huggingface.co/allenai/Olmo1124-7B-hf).\n+    defaults will yield a similar configuration to that of the [allenai/Olmo2-7B-1124-hf](https://huggingface.co/allenai/Olmo2-7B-1124-hf).\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n \n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 50304):\n-            Vocabulary size of the Olmo1124 model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`Olmo1124Model`]\n+            Vocabulary size of the Olmo2 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Olmo2Model`]\n         hidden_size (`int`, *optional*, defaults to 4096):\n             Dimension of the hidden representations.\n         intermediate_size (`int`, *optional*, defaults to 11008):\n@@ -73,20 +73,20 @@ class Olmo1124Config(PretrainedConfig):\n             The epsilon used by the rms normalization layers.\n \n     ```python\n-    >>> from transformers import Olmo1124Model, Olmo1124Config\n+    >>> from transformers import Olmo2Model, Olmo2Config\n \n-    >>> # Initializing a Olmo November 2024 7B style configuration\n-    >>> configuration = Olmo1124Config()\n+    >>> # Initializing a Olmo2 7B style configuration\n+    >>> configuration = Olmo2Config()\n \n-    >>> # Initializing a model from the Olmo November 2024 7B style configuration\n-    >>> model = Olmo1124Model(configuration)\n+    >>> # Initializing a model from the Olmo2 7B style configuration\n+    >>> model = Olmo2Model(configuration)\n \n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n     ```\n     \"\"\"\n \n-    model_type = \"olmo_1124\"\n+    model_type = \"olmo2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__(\n@@ -163,4 +163,4 @@ def _rope_scaling_validation(self):\n             raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")\n \n \n-__all__ = [\"Olmo1124Config\"]\n+__all__ = [\"Olmo2Config\"]",
            "previous_filename": "src/transformers/models/olmo_1124/configuration_olmo_1124.py"
        },
        {
            "sha": "43837fc14c2561247f3f5af1fdf2e3b063bd422b",
            "filename": "src/transformers/models/olmo2/convert_olmo2_weights_to_hf.py",
            "status": "renamed",
            "additions": 34,
            "deletions": 34,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2Fconvert_olmo2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2Fconvert_olmo2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconvert_olmo2_weights_to_hf.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -23,24 +23,24 @@\n import yaml\n from tokenizers import Tokenizer\n \n-from transformers import Olmo1124Config, Olmo1124ForCausalLM\n+from transformers import Olmo2Config, Olmo2ForCausalLM\n from transformers.models.gpt2.tokenization_gpt2_fast import GPT2TokenizerFast\n \n \n \"\"\"\n Sample usage:\n \n ```\n-python src/transformers/models/olmo_1124/convert_olmo_1124_weights_to_hf.py \\\n-    --input_dir /path/to/downloaded/olmo_1124/weights --model_size 7B --output_dir /output/path\n+python src/transformers/models/olmo2/convert_olmo2_weights_to_hf.py \\\n+    --input_dir /path/to/downloaded/olmo2/weights --model_size 7B --output_dir /output/path\n ```\n \n Thereafter, models can be loaded via:\n \n ```py\n-from transformers import Olmo1124ForCausalLM, AutoTokenizer\n+from transformers import Olmo2ForCausalLM, AutoTokenizer\n \n-model = Olmo1124ForCausalLM.from_pretrained(\"/output/path\")\n+model = Olmo2ForCausalLM.from_pretrained(\"/output/path\")\n tokenizer = AutoTokenizer.from_pretrained(\"/output/path\")\n ```\n \n@@ -77,26 +77,26 @@ def write_model(\n     os.makedirs(tmp_model_path, exist_ok=True)\n \n     config_path = Path(input_base_path) / \"config.yaml\"\n-    olmo_1124_config = yaml.safe_load(config_path.read_text())[\"model\"]\n+    olmo2_config = yaml.safe_load(config_path.read_text())[\"model\"]\n \n-    if not olmo_1124_config.get(\"attention_layer_norm\", False):\n-        raise RuntimeError(\"OLMo November 2024 checkpoints must have attention layer norm\")\n-    if not olmo_1124_config.get(\"norm_after\", False):\n-        raise RuntimeError(\"OLMo November 2024 checkpoints must set norm_after to True\")\n+    if not olmo2_config.get(\"attention_layer_norm\", False):\n+        raise RuntimeError(\"OLMo2 checkpoints must have attention layer norm\")\n+    if not olmo2_config.get(\"norm_after\", False):\n+        raise RuntimeError(\"OLMo2 checkpoints must set norm_after to True\")\n \n-    n_layers = olmo_1124_config[\"n_layers\"]\n-    n_heads = olmo_1124_config[\"n_heads\"]\n-    dim = olmo_1124_config[\"d_model\"]\n+    n_layers = olmo2_config[\"n_layers\"]\n+    n_heads = olmo2_config[\"n_heads\"]\n+    dim = olmo2_config[\"d_model\"]\n     dims_per_head = dim // n_heads\n-    base = olmo_1124_config[\"rope_theta\"]\n+    base = olmo2_config[\"rope_theta\"]\n     inv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n-    max_position_embeddings = olmo_1124_config[\"max_sequence_length\"]\n+    max_position_embeddings = olmo2_config[\"max_sequence_length\"]\n \n-    vocab_size = olmo_1124_config.get(\"embedding_size\", olmo_1124_config[\"vocab_size\"])\n+    vocab_size = olmo2_config.get(\"embedding_size\", olmo2_config[\"vocab_size\"])\n \n-    if olmo_1124_config.get(\"n_kv_heads\", None) is not None:\n-        num_key_value_heads = olmo_1124_config[\"n_kv_heads\"]  # for GQA / MQA\n-    elif olmo_1124_config[\"multi_query_attention\"]:  # compatibility with other checkpoints\n+    if olmo2_config.get(\"n_kv_heads\", None) is not None:\n+        num_key_value_heads = olmo2_config[\"n_kv_heads\"]  # for GQA / MQA\n+    elif olmo2_config[\"multi_query_attention\"]:  # compatibility with other checkpoints\n         num_key_value_heads = 1\n     else:\n         num_key_value_heads = n_heads\n@@ -167,29 +167,29 @@ def write_model(\n     index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n     write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n \n-    if olmo_1124_config.get(\"mlp_hidden_size\", None) is not None:\n-        intermediate_size = olmo_1124_config[\"mlp_hidden_size\"] // 2\n+    if olmo2_config.get(\"mlp_hidden_size\", None) is not None:\n+        intermediate_size = olmo2_config[\"mlp_hidden_size\"] // 2\n     else:\n-        intermediate_size = (dim * olmo_1124_config[\"mlp_ratio\"]) // 2\n+        intermediate_size = (dim * olmo2_config[\"mlp_ratio\"]) // 2\n \n-    if fix_eos_token_id and olmo_1124_config[\"eos_token_id\"] == 0:\n+    if fix_eos_token_id and olmo2_config[\"eos_token_id\"] == 0:\n         # Fixing a bug in OLMo where eos token id was incorrectly set\n         print(\"Changing eos_token_id from 0 to 50279.\")\n-        olmo_1124_config[\"eos_token_id\"] = 50279\n+        olmo2_config[\"eos_token_id\"] = 50279\n \n-    config = Olmo1124Config(\n+    config = Olmo2Config(\n         vocab_size=vocab_size,\n         hidden_size=dim,\n         intermediate_size=intermediate_size,\n         num_hidden_layers=n_layers,\n         num_attention_heads=n_heads,\n         num_key_value_heads=num_key_value_heads,\n         max_position_embeddings=max_position_embeddings,\n-        pad_token_id=olmo_1124_config[\"pad_token_id\"],\n+        pad_token_id=olmo2_config[\"pad_token_id\"],\n         bos_token_id=None,\n-        eos_token_id=olmo_1124_config[\"eos_token_id\"],\n-        tie_word_embeddings=olmo_1124_config[\"weight_tying\"],\n-        rms_norm_eps=olmo_1124_config[\"layer_norm_eps\"],\n+        eos_token_id=olmo2_config[\"eos_token_id\"],\n+        tie_word_embeddings=olmo2_config[\"weight_tying\"],\n+        rms_norm_eps=olmo2_config[\"layer_norm_eps\"],\n         rope_theta=base,\n     )\n     config.save_pretrained(tmp_model_path)\n@@ -202,8 +202,8 @@ def write_model(\n     if include_tokenizer:\n         _write_tokenizer(model_path, config, input_base_path, tokenizer_path)\n \n-    print(\"Loading the checkpoint in a OLMo November 2024 model.\")\n-    model = Olmo1124ForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.float32, low_cpu_mem_usage=True)\n+    print(\"Loading the checkpoint in a OLMo2 model.\")\n+    model = Olmo2ForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.float32, low_cpu_mem_usage=True)\n     # Avoid saving this as part of the config.\n     del model.config._name_or_path\n     print(\"Saving in the Transformers format.\")\n@@ -216,7 +216,7 @@ def write_model(\n \n def _write_tokenizer(\n     output_path: Path,\n-    config: Olmo1124Config,\n+    config: Olmo2Config,\n     checkpoint_dir: str,\n     input_tokenizer_path: Path | None,\n ) -> None:\n@@ -251,7 +251,7 @@ def main():\n     parser.add_argument(\n         \"--input_dir\",\n         required=True,\n-        help=\"Location of OLMo November 2024 weights, which contains config.yaml and model.pt.\",\n+        help=\"Location of OLMo2 weights, which contains config.yaml and model.pt.\",\n     )\n     parser.add_argument(\n         \"--no_tokenizer\",\n@@ -263,7 +263,7 @@ def main():\n         \"--tokenizer_json_path\",\n         type=Path,\n         default=None,\n-        help=\"Location of OLMo November 2024 tokenizer json file. Defaults to what is set in the config file.\",\n+        help=\"Location of OLMo2 tokenizer json file. Defaults to what is set in the config file.\",\n     )\n     parser.add_argument(\n         \"--output_dir\",",
            "previous_filename": "src/transformers/models/olmo_1124/convert_olmo_1124_weights_to_hf.py"
        },
        {
            "sha": "bdf53376a1e8b640e6ca734e596b0ea9f484455b",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "renamed",
            "additions": 67,
            "deletions": 67,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -1,8 +1,8 @@\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#           This file was automatically generated from src/transformers/models/olmo_1124/modular_olmo_1124.py.\n+#           This file was automatically generated from src/transformers/models/olmo2/modular_olmo2.py.\n #               Do NOT edit this file manually as any edits will be overwritten by the generation of\n #             the file from the modular. If any change should be done, please apply the change to the\n-#                          modular_olmo_1124.py file directly. One of our CI enforces this.\n+#                          modular_olmo2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n from typing import List, Optional, Tuple, Union\n@@ -25,7 +25,7 @@\n     logging,\n     replace_return_docstrings,\n )\n-from .configuration_olmo_1124 import Olmo1124Config\n+from .configuration_olmo2 import Olmo2Config\n \n \n if is_flash_attn_2_available():\n@@ -34,13 +34,13 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"Olmo1124Config\"\n+_CONFIG_FOR_DOC = \"Olmo2Config\"\n \n \n-class Olmo1124RMSNorm(nn.Module):\n+class Olmo2RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n-        Olmo1124RMSNorm is equivalent to T5LayerNorm\n+        Olmo2RMSNorm is equivalent to T5LayerNorm\n         \"\"\"\n         super().__init__()\n         self.weight = nn.Parameter(torch.ones(hidden_size))\n@@ -57,9 +57,9 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Olmo1124\n+# copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Olmo2\n # TODO(joao): add me back asap :)\n-class Olmo1124RotaryEmbedding(nn.Module):\n+class Olmo2RotaryEmbedding(nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n         super().__init__()\n         self.scaling_factor = scaling_factor\n@@ -88,10 +88,10 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Olmo1124\n+# copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Olmo2\n # TODO(joao): add me back asap :)\n-class Olmo1124LinearScalingRotaryEmbedding(Olmo1124RotaryEmbedding):\n-    \"\"\"Olmo1124RotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n+class Olmo2LinearScalingRotaryEmbedding(Olmo2RotaryEmbedding):\n+    \"\"\"Olmo2RotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n     def forward(self, x, position_ids):\n         # difference to the original RoPE: a scaling factor is aplied to the position ids\n@@ -100,10 +100,10 @@ def forward(self, x, position_ids):\n         return cos, sin\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Olmo1124\n+# copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Olmo2\n # TODO(joao): add me back asap :)\n-class Olmo1124DynamicNTKScalingRotaryEmbedding(Olmo1124RotaryEmbedding):\n-    \"\"\"Olmo1124RotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n+class Olmo2DynamicNTKScalingRotaryEmbedding(Olmo2RotaryEmbedding):\n+    \"\"\"Olmo2RotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n \n     def forward(self, x, position_ids):\n         # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n@@ -167,12 +167,12 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-class Olmo1124Attention(nn.Module):\n+class Olmo2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    # copied from transformers.models.llama.modeling_llama.LlamaAttention.__init__ with Llama->Olmo1124\n+    # copied from transformers.models.llama.modeling_llama.LlamaAttention.__init__ with Llama->Olmo2\n     # TODO(joao): add me back asap :)\n-    def __init__(self, config: Olmo1124Config, layer_idx: Optional[int] = None):\n+    def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n@@ -204,12 +204,12 @@ def __init__(self, config: Olmo1124Config, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n         self._init_rope()\n-        self.q_norm = Olmo1124RMSNorm(self.num_heads * self.head_dim, config.rms_norm_eps)\n-        self.k_norm = Olmo1124RMSNorm(self.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n+        self.q_norm = Olmo2RMSNorm(self.num_heads * self.head_dim, config.rms_norm_eps)\n+        self.k_norm = Olmo2RMSNorm(self.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n \n     def _init_rope(self):\n         if self.config.rope_scaling is None:\n-            self.rotary_emb = Olmo1124RotaryEmbedding(\n+            self.rotary_emb = Olmo2RotaryEmbedding(\n                 self.head_dim,\n                 max_position_embeddings=self.max_position_embeddings,\n                 base=self.rope_theta,\n@@ -218,14 +218,14 @@ def _init_rope(self):\n             scaling_type = self.config.rope_scaling[\"type\"]\n             scaling_factor = self.config.rope_scaling[\"factor\"]\n             if scaling_type == \"linear\":\n-                self.rotary_emb = Olmo1124LinearScalingRotaryEmbedding(\n+                self.rotary_emb = Olmo2LinearScalingRotaryEmbedding(\n                     self.head_dim,\n                     max_position_embeddings=self.max_position_embeddings,\n                     scaling_factor=scaling_factor,\n                     base=self.rope_theta,\n                 )\n             elif scaling_type == \"dynamic\":\n-                self.rotary_emb = Olmo1124DynamicNTKScalingRotaryEmbedding(\n+                self.rotary_emb = Olmo2DynamicNTKScalingRotaryEmbedding(\n                     self.head_dim,\n                     max_position_embeddings=self.max_position_embeddings,\n                     scaling_factor=scaling_factor,\n@@ -295,13 +295,13 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class Olmo1124FlashAttention2(Olmo1124Attention):\n+class Olmo2FlashAttention2(Olmo2Attention):\n     \"\"\"\n-    Olmo1124 flash attention module. This module inherits from `Olmo1124Attention` as the weights of the module stays\n+    Olmo2 flash attention module. This module inherits from `Olmo2Attention` as the weights of the module stays\n     untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n     flash attention and deal with padding tokens in case the input contains any of them.\n \n-    OLMo November 2024 flash attention module. This module inherits from `Olmo1124Attention` as the weights of the module stays\n+    OLMo2 flash attention module. This module inherits from `Olmo2Attention` as the weights of the module stays\n     untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n@@ -403,14 +403,14 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class Olmo1124SdpaAttention(Olmo1124Attention):\n+class Olmo2SdpaAttention(Olmo2Attention):\n     \"\"\"\n-    Olmo1124 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Olmo1124Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    Olmo2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `Olmo2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n     SDPA API.\n     \"\"\"\n \n-    # Adapted from Olmo1124Attention.forward\n+    # Adapted from Olmo2Attention.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -424,7 +424,7 @@ def forward(\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"Olmo1124Model is using Olmo1124SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                \"Olmo2Model is using Olmo2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                 'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n@@ -479,7 +479,7 @@ def forward(\n         return attn_output, None, past_key_value\n \n \n-class Olmo1124MLP(nn.Module):\n+class Olmo2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n@@ -494,23 +494,23 @@ def forward(self, x):\n         return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n \n \n-OLMO_1124_ATTENTION_CLASSES = {\n-    \"eager\": Olmo1124Attention,\n-    \"flash_attention_2\": Olmo1124FlashAttention2,\n-    \"sdpa\": Olmo1124SdpaAttention,\n+OLMO2_ATTENTION_CLASSES = {\n+    \"eager\": Olmo2Attention,\n+    \"flash_attention_2\": Olmo2FlashAttention2,\n+    \"sdpa\": Olmo2SdpaAttention,\n }\n \n \n-class Olmo1124DecoderLayer(nn.Module):\n-    def __init__(self, config: Olmo1124Config, layer_idx: int):\n+class Olmo2DecoderLayer(nn.Module):\n+    def __init__(self, config: Olmo2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = OLMO_1124_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n+        self.self_attn = OLMO2_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n \n-        self.mlp = Olmo1124MLP(config)\n-        self.post_attention_layernorm = Olmo1124RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_feedforward_layernorm = Olmo1124RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.mlp = Olmo2MLP(config)\n+        self.post_attention_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     # copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer.forward\n     # TODO(joao): add me back asap :)\n@@ -574,7 +574,7 @@ def forward(\n         return outputs\n \n \n-OLMO_1124_START_DOCSTRING = r\"\"\"\n+OLMO2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n     etc.)\n@@ -584,22 +584,22 @@ def forward(\n     and behavior.\n \n     Parameters:\n-        config ([`Olmo1124Config`]):\n+        config ([`Olmo2Config`]):\n             Model configuration class with all the parameters of the model. Initializing with a config file does not\n             load the weights associated with the model, only the configuration. Check out the\n             [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n \n @add_start_docstrings(\n-    \"The bare Olmo1124 Model outputting raw hidden-states without any specific head on top.\",\n-    OLMO_1124_START_DOCSTRING,\n+    \"The bare Olmo2 Model outputting raw hidden-states without any specific head on top.\",\n+    OLMO2_START_DOCSTRING,\n )\n-class Olmo1124PreTrainedModel(PreTrainedModel):\n-    config_class = Olmo1124Config\n+class Olmo2PreTrainedModel(PreTrainedModel):\n+    config_class = Olmo2Config\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"Olmo1124DecoderLayer\"]\n+    _no_split_modules = [\"Olmo2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n@@ -619,7 +619,7 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-OLMO_1124_INPUTS_DOCSTRING = r\"\"\"\n+OLMO2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n@@ -695,27 +695,27 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"The bare Olmo1124 Model outputting raw hidden-states without any specific head on top.\",\n-    OLMO_1124_START_DOCSTRING,\n+    \"The bare Olmo2 Model outputting raw hidden-states without any specific head on top.\",\n+    OLMO2_START_DOCSTRING,\n )\n-class Olmo1124Model(Olmo1124PreTrainedModel):\n+class Olmo2Model(Olmo2PreTrainedModel):\n     \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Olmo1124DecoderLayer`]\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Olmo2DecoderLayer`]\n \n     Args:\n-        config: Olmo1124Config\n+        config: Olmo2Config\n     \"\"\"\n \n-    def __init__(self, config: Olmo1124Config):\n+    def __init__(self, config: Olmo2Config):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n \n         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n         self.layers = nn.ModuleList(\n-            [Olmo1124DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+            [Olmo2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.norm = Olmo1124RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.norm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.gradient_checkpointing = False\n \n         # Initialize weights and apply final processing\n@@ -727,7 +727,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n-    @add_start_docstrings_to_model_forward(OLMO_1124_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(OLMO2_INPUTS_DOCSTRING)\n     # copied from transformers.models.llama.modeling_llama.LlamaModel.forward\n     # TODO(joao): add me back asap :)\n     def forward(\n@@ -971,13 +971,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->OLMO_1124,Llama->Olmo1124\n-class Olmo1124ForCausalLM(Olmo1124PreTrainedModel, GenerationMixin):\n+# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->OLMO2,Llama->Olmo2\n+class Olmo2ForCausalLM(Olmo2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n-    def __init__(self, config: Olmo1124Config):\n+    def __init__(self, config: Olmo2Config):\n         super().__init__(config)\n-        self.model = Olmo1124Model(config)\n+        self.model = Olmo2Model(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n \n@@ -1002,7 +1002,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    @add_start_docstrings_to_model_forward(OLMO_1124_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(OLMO2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     # Ignore copy\n     def forward(\n@@ -1038,10 +1038,10 @@ def forward(\n         Example:\n \n         ```python\n-        >>> from transformers import AutoTokenizer, Olmo1124ForCausalLM\n+        >>> from transformers import AutoTokenizer, Olmo2ForCausalLM\n \n-        >>> model = Olmo1124ForCausalLM.from_pretrained(\"allenai/Olmo1124-1B-hf\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/Olmo1124-1B-hf\")\n+        >>> model = Olmo2ForCausalLM.from_pretrained(\"allenai/Olmo2-1B-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/Olmo2-1B-hf\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n@@ -1093,4 +1093,4 @@ def forward(\n         )\n \n \n-__all__ = [\"Olmo1124ForCausalLM\", \"Olmo1124Model\", \"Olmo1124PreTrainedModel\"]\n+__all__ = [\"Olmo2ForCausalLM\", \"Olmo2Model\", \"Olmo2PreTrainedModel\"]",
            "previous_filename": "src/transformers/models/olmo_1124/modeling_olmo_1124.py"
        },
        {
            "sha": "393d17c59c1a8b713cbbb9e3ffb7e32a8b16b99f",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "renamed",
            "additions": 42,
            "deletions": 42,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -28,20 +28,20 @@\n logger = logging.get_logger(__name__)\n \n \n-class Olmo1124Config(OlmoConfig):\n+class Olmo2Config(OlmoConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`Olmo1124Model`]. It is used to instantiate an OLMo November 2024\n+    This is the configuration class to store the configuration of a [`Olmo2Model`]. It is used to instantiate an OLMo2\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the [allenai/Olmo1124-7B-hf](https://huggingface.co/allenai/Olmo1124-7B-hf).\n+    defaults will yield a similar configuration to that of the [allenai/Olmo2-7B-1124-hf](https://huggingface.co/allenai/Olmo2-7B-1124-hf).\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information.\n \n \n     Args:\n         vocab_size (`int`, *optional*, defaults to 50304):\n-            Vocabulary size of the Olmo1124 model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`Olmo1124Model`]\n+            Vocabulary size of the Olmo2 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Olmo2Model`]\n         hidden_size (`int`, *optional*, defaults to 4096):\n             Dimension of the hidden representations.\n         intermediate_size (`int`, *optional*, defaults to 11008):\n@@ -93,20 +93,20 @@ class Olmo1124Config(OlmoConfig):\n             The epsilon used by the rms normalization layers.\n \n     ```python\n-    >>> from transformers import Olmo1124Model, Olmo1124Config\n+    >>> from transformers import Olmo2Model, Olmo2Config\n \n-    >>> # Initializing a Olmo November 2024 7B style configuration\n-    >>> configuration = Olmo1124Config()\n+    >>> # Initializing a Olmo2 7B style configuration\n+    >>> configuration = Olmo2Config()\n \n-    >>> # Initializing a model from the Olmo November 2024 7B style configuration\n-    >>> model = Olmo1124Model(configuration)\n+    >>> # Initializing a model from the Olmo2 7B style configuration\n+    >>> model = Olmo2Model(configuration)\n \n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n     ```\n     \"\"\"\n \n-    model_type = \"olmo_1124\"\n+    model_type = \"olmo2\"\n \n     def __init__(\n         self,\n@@ -157,21 +157,21 @@ def __init__(\n         del self.clip_qkv\n \n \n-class Olmo1124RMSNorm(LlamaRMSNorm):\n+class Olmo2RMSNorm(LlamaRMSNorm):\n     pass\n \n \n-ALL_LAYERNORM_LAYERS.append(Olmo1124RMSNorm)\n+ALL_LAYERNORM_LAYERS.append(Olmo2RMSNorm)\n \n \n-# Olmo1124 attention is identical to OLMo attention except:\n+# Olmo2 attention is identical to OLMo attention except:\n # - Norm is applied to attention queries and keys.\n # - No qkv clipping.\n-class Olmo1124Attention(OlmoAttention):\n-    def __init__(self, config: Olmo1124Config, layer_idx: Optional[int] = None):\n+class Olmo2Attention(OlmoAttention):\n+    def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         super().__init__(config, layer_idx=layer_idx)\n-        self.q_norm = Olmo1124RMSNorm(self.num_heads * self.head_dim, config.rms_norm_eps)\n-        self.k_norm = Olmo1124RMSNorm(self.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n+        self.q_norm = Olmo2RMSNorm(self.num_heads * self.head_dim, config.rms_norm_eps)\n+        self.k_norm = Olmo2RMSNorm(self.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n \n     def forward(\n         self,\n@@ -234,15 +234,15 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class Olmo1124FlashAttention2(OlmoFlashAttention2, Olmo1124Attention):\n+class Olmo2FlashAttention2(OlmoFlashAttention2, Olmo2Attention):\n     \"\"\"\n-    OLMo November 2024 flash attention module. This module inherits from `Olmo1124Attention` as the weights of the module stays\n+    OLMo2 flash attention module. This module inherits from `Olmo2Attention` as the weights of the module stays\n     untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n     flash attention and deal with padding tokens in case the input contains any of them.\n     \"\"\"\n \n     def __init__(self, *args, **kwargs):\n-        Olmo1124Attention.__init__(*args, **kwargs)\n+        Olmo2Attention.__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n@@ -338,8 +338,8 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class Olmo1124SdpaAttention(OlmoSdpaAttention, Olmo1124Attention):\n-    # Adapted from Olmo1124Attention.forward\n+class Olmo2SdpaAttention(OlmoSdpaAttention, Olmo2Attention):\n+    # Adapted from Olmo2Attention.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -353,7 +353,7 @@ def forward(\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n             logger.warning_once(\n-                \"Olmo1124Model is using Olmo1124SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                \"Olmo2Model is using Olmo2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                 'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n             )\n             return super().forward(\n@@ -408,14 +408,14 @@ def forward(\n         return attn_output, None, past_key_value\n \n \n-# The OLMo November 2024 layers are identical to those of the OLMo model except:\n+# The OLMo2 layers are identical to those of the OLMo model except:\n # - RMSNorm is used instead of standard layer norm.\n # - Norm is applied after attention/feedforward rather than before.\n-class Olmo1124DecoderLayer(OlmoDecoderLayer):\n-    def __init__(self, config: Olmo1124Config, layer_idx: int):\n+class Olmo2DecoderLayer(OlmoDecoderLayer):\n+    def __init__(self, config: Olmo2Config, layer_idx: int):\n         super().__init__(config, layer_idx=layer_idx)\n-        self.post_attention_layernorm = Olmo1124RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_feedforward_layernorm = Olmo1124RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         del self.input_layernorm\n \n     def forward(\n@@ -459,31 +459,31 @@ def forward(\n         return outputs\n \n \n-class Olmo1124PreTrainedModel(OlmoPreTrainedModel):\n+class Olmo2PreTrainedModel(OlmoPreTrainedModel):\n     pass\n \n \n-# The OLMo November 2024 model is identical to the OLMo model, except RMSNorm is used instead of\n+# The OLMo2 model is identical to the OLMo model, except RMSNorm is used instead of\n # standard layer norm for the output norm.\n-class Olmo1124Model(OlmoModel):\n-    def __init__(self, config: Olmo1124Config):\n+class Olmo2Model(OlmoModel):\n+    def __init__(self, config: Olmo2Config):\n         super().__init__(config)\n         self.layers = nn.ModuleList(\n-            [Olmo1124DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+            [Olmo2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.norm = Olmo1124RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.norm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n \n # The heads now only need to redefine the model inside to the correct `RobertaModel`\n-class Olmo1124ForCausalLM(OlmoForCausalLM):\n-    def __init__(self, config: Olmo1124Config):\n+class Olmo2ForCausalLM(OlmoForCausalLM):\n+    def __init__(self, config: Olmo2Config):\n         super().__init__(config)\n-        self.model = Olmo1124Model(config)\n+        self.model = Olmo2Model(config)\n \n \n __all__ = [\n-    \"Olmo1124Config\",\n-    \"Olmo1124ForCausalLM\",\n-    \"Olmo1124Model\",\n-    \"Olmo1124PreTrainedModel\",\n+    \"Olmo2Config\",\n+    \"Olmo2ForCausalLM\",\n+    \"Olmo2Model\",\n+    \"Olmo2PreTrainedModel\",\n ]",
            "previous_filename": "src/transformers/models/olmo_1124/modular_olmo_1124.py"
        },
        {
            "sha": "1238f058783c18bc9f7572a259a3f31caad4a9e4",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -6758,21 +6758,21 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class Olmo1124ForCausalLM(metaclass=DummyObject):\n+class Olmo2ForCausalLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class Olmo1124Model(metaclass=DummyObject):\n+class Olmo2Model(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class Olmo1124PreTrainedModel(metaclass=DummyObject):\n+class Olmo2PreTrainedModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/olmo2/__init__.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/tests%2Fmodels%2Folmo2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/tests%2Fmodels%2Folmo2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2F__init__.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "previous_filename": "tests/models/olmo_1124/__init__.py"
        },
        {
            "sha": "fe6dcfdb540a671d32c7b9ba2abbf7e4b5cf6ec7",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "renamed",
            "additions": 30,
            "deletions": 30,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/9121ab8fe87a296e57f9846e70153b1a3c555d75/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9121ab8fe87a296e57f9846e70153b1a3c555d75/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=9121ab8fe87a296e57f9846e70153b1a3c555d75",
            "patch": "@@ -12,14 +12,14 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"Testing suite for the PyTorch OLMo November 2024 model.\"\"\"\n+\"\"\"Testing suite for the PyTorch OLMo2 model.\"\"\"\n \n import unittest\n \n from packaging import version\n from parameterized import parameterized\n \n-from transformers import Olmo1124Config, is_torch_available, set_seed\n+from transformers import Olmo2Config, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.models.auto.tokenization_auto import AutoTokenizer\n from transformers.testing_utils import (\n@@ -39,12 +39,12 @@\n     import torch\n \n     from transformers import (\n-        Olmo1124ForCausalLM,\n-        Olmo1124Model,\n+        Olmo2ForCausalLM,\n+        Olmo2Model,\n     )\n \n \n-class Olmo1124ModelTester:\n+class Olmo2ModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -119,7 +119,7 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n \n     def get_config(self):\n-        return Olmo1124Config(\n+        return Olmo2Config(\n             vocab_size=self.vocab_size,\n             hidden_size=self.hidden_size,\n             num_hidden_layers=self.num_hidden_layers,\n@@ -138,7 +138,7 @@ def get_config(self):\n     def create_and_check_model(\n         self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n     ):\n-        model = Olmo1124Model(config=config)\n+        model = Olmo2Model(config=config)\n         model.to(torch_device)\n         model.eval()\n         result = model(input_ids, attention_mask=input_mask)\n@@ -158,7 +158,7 @@ def create_and_check_model_as_decoder(\n         encoder_attention_mask,\n     ):\n         config.add_cross_attention = True\n-        model = Olmo1124Model(config)\n+        model = Olmo2Model(config)\n         model.to(torch_device)\n         model.eval()\n         result = model(\n@@ -187,7 +187,7 @@ def create_and_check_for_causal_lm(\n         encoder_hidden_states,\n         encoder_attention_mask,\n     ):\n-        model = Olmo1124ForCausalLM(config=config)\n+        model = Olmo2ForCausalLM(config=config)\n         model.to(torch_device)\n         model.eval()\n         result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n@@ -207,7 +207,7 @@ def create_and_check_decoder_model_past_large_inputs(\n     ):\n         config.is_decoder = True\n         config.add_cross_attention = True\n-        model = Olmo1124ForCausalLM(config=config)\n+        model = Olmo2ForCausalLM(config=config)\n         model.to(torch_device)\n         model.eval()\n \n@@ -271,13 +271,13 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Olmo1124ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (Olmo1124Model, Olmo1124ForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (Olmo1124ForCausalLM,) if is_torch_available() else ()\n+class Olmo2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (Olmo2Model, Olmo2ForCausalLM) if is_torch_available() else ()\n+    all_generative_model_classes = (Olmo2ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n-            \"feature-extraction\": Olmo1124Model,\n-            \"text-generation\": Olmo1124ForCausalLM,\n+            \"feature-extraction\": Olmo2Model,\n+            \"text-generation\": Olmo2ForCausalLM,\n         }\n         if is_torch_available()\n         else {}\n@@ -290,8 +290,8 @@ class Olmo1124ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n     model_split_percents = [0.5, 0.7, 0.8]\n \n     def setUp(self):\n-        self.model_tester = Olmo1124ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Olmo1124Config, hidden_size=37)\n+        self.model_tester = Olmo2ModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Olmo2Config, hidden_size=37)\n \n     def test_config(self):\n         self.config_tester.run_common_tests()\n@@ -300,7 +300,7 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    @unittest.skip(reason=\"OLMo November 2024 does not support head pruning.\")\n+    @unittest.skip(reason=\"OLMo2 does not support head pruning.\")\n     def test_headmasking(self):\n         pass\n \n@@ -310,7 +310,7 @@ def test_model_various_embeddings(self):\n             config_and_inputs[0].position_embedding_type = type\n             self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    @unittest.skip(reason=\"OLMo November 2024 buffers include complex numbers, which breaks this test\")\n+    @unittest.skip(reason=\"OLMo2 buffers include complex numbers, which breaks this test\")\n     def test_save_load_fast_init_from_base(self):\n         pass\n \n@@ -321,15 +321,15 @@ def test_model_rope_scaling(self, scaling_type):\n         long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n \n         set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        original_model = Olmo1124Model(config)\n+        original_model = Olmo2Model(config)\n         original_model.to(torch_device)\n         original_model.eval()\n         original_short_output = original_model(short_input).last_hidden_state\n         original_long_output = original_model(long_input).last_hidden_state\n \n         set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n         config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n-        scaled_model = Olmo1124Model(config)\n+        scaled_model = Olmo2Model(config)\n         scaled_model.to(torch_device)\n         scaled_model.eval()\n         scaled_short_output = scaled_model(short_input).last_hidden_state\n@@ -347,11 +347,11 @@ def test_model_rope_scaling(self, scaling_type):\n \n \n @require_torch\n-class Olmo1124IntegrationTest(unittest.TestCase):\n+class Olmo2IntegrationTest(unittest.TestCase):\n     @slow\n     def test_model_7b_logits(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n-        model = Olmo1124ForCausalLM.from_pretrained(\"shanearora/OLMo-7B-1124-hf\", device_map=\"auto\")\n+        model = Olmo2ForCausalLM.from_pretrained(\"shanearora/OLMo2-7B-1124-hf\", device_map=\"auto\")\n         out = model(torch.tensor(input_ids)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor(\n@@ -366,8 +366,8 @@ def test_model_7b_logits(self):\n     def test_model_7b_greedy_generation(self):\n         EXPECTED_TEXT_COMPLETION = \"\"\"Simply put, the theory of relativity states that 1) the speed of light is constant, 2) the speed of light is the fastest speed possible, and 3) the speed of light is the same for all observers, regardless of their relative motion. The theory of relativity is based on the idea that the speed of light is constant. This means that\"\"\"\n         prompt = \"Simply put, the theory of relativity states that \"\n-        tokenizer = AutoTokenizer.from_pretrained(\"shanearora/OLMo-7B-1124-hf\", device_map=\"auto\")\n-        model = Olmo1124ForCausalLM.from_pretrained(\"shanearora/OLMo-7B-1124-hf\", device_map=\"auto\")\n+        tokenizer = AutoTokenizer.from_pretrained(\"shanearora/OLMo2-7B-1124-hf\", device_map=\"auto\")\n+        model = Olmo2ForCausalLM.from_pretrained(\"shanearora/OLMo2-7B-1124-hf\", device_map=\"auto\")\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n \n         # greedy generation outputs\n@@ -377,7 +377,7 @@ def test_model_7b_greedy_generation(self):\n \n     @require_tokenizers\n     def test_simple_encode_decode(self):\n-        rust_tokenizer = AutoTokenizer.from_pretrained(\"shanearora/OLMo-7B-1124-hf\")\n+        rust_tokenizer = AutoTokenizer.from_pretrained(\"shanearora/OLMo2-7B-1124-hf\")\n \n         self.assertEqual(rust_tokenizer.encode(\"This is a test\"), [2028, 374, 264, 1296])\n         self.assertEqual(rust_tokenizer.decode([2028, 374, 264, 1296], skip_special_tokens=True), \"This is a test\")\n@@ -414,9 +414,9 @@ def test_export_static_cache(self):\n             convert_and_export_with_cache,\n         )\n \n-        olmo_1124_model = \"shanearora/OLMo-7B-1124-hf\"\n+        olmo2_model = \"shanearora/OLMo2-7B-1124-hf\"\n \n-        tokenizer = AutoTokenizer.from_pretrained(olmo_1124_model, pad_token=\"</s>\", padding_side=\"right\")\n+        tokenizer = AutoTokenizer.from_pretrained(olmo2_model, pad_token=\"</s>\", padding_side=\"right\")\n         EXPECTED_TEXT_COMPLETION = [\n             \"Simply put, the theory of relativity states that 1) the speed of light is constant, 2) the speed of light\",\n         ]\n@@ -439,8 +439,8 @@ def test_export_static_cache(self):\n                 \"max_cache_len\": max_generation_length,\n             },\n         )\n-        model = Olmo1124ForCausalLM.from_pretrained(\n-            olmo_1124_model,\n+        model = Olmo2ForCausalLM.from_pretrained(\n+            olmo2_model,\n             device_map=device,\n             torch_dtype=dtype,\n             attn_implementation=attn_implementation,",
            "previous_filename": "tests/models/olmo_1124/test_modeling_olmo_1124.py"
        }
    ],
    "stats": {
        "total": 442,
        "additions": 221,
        "deletions": 221
    }
}