{
    "author": "Cyrilvallez",
    "message": "Fix modular edge case + modular sorting order (#35562)\n\n* look-ahead negation\r\n\r\n* re add examples by default\r\n\r\n* Fix the bug in topological sort\r\n\r\n* Update create_dependency_mapping.py\r\n\r\n* start adding test\r\n\r\n* finalize test\r\n\r\n* more tests\r\n\r\n* style\r\n\r\n* style",
    "sha": "46276f9a7f99a9380ec86f7ecad14b3818b78a54",
    "files": [
        {
            "sha": "59637e02d3f11cb7fe511f71a7220b4d948f7a66",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=46276f9a7f99a9380ec86f7ecad14b3818b78a54",
            "patch": "@@ -43,7 +43,7 @@ class MyNewModelConfig(PretrainedConfig):\n             The non-linear activation function (function or string) in the decoder.\n         max_position_embeddings (`int`, *optional*, defaults to 2048):\n             The maximum sequence length that this model might ever be used with. MyNewModel 1 supports up to 2048 tokens,\n-            MyNewModel 2 up to 4096, CodeMyNewModel up to 16384.\n+            MyNewModel 2 up to 4096, CodeLlama up to 16384.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n@@ -110,7 +110,7 @@ class MyNewModelConfig(PretrainedConfig):\n         mlp_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n         head_dim (`int`, *optional*):\n-            The attention head dimension. If None, it will default to hidden_size // num_heads\n+            The attention head dimension. If None, it will default to hidden_size // num_attention_heads\n \n     ```python\n     >>> from transformers import MyNewModelModel, MyNewModelConfig"
        },
        {
            "sha": "382b87bd38471e1ba7bc927138e2fb8646cdfabb",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=46276f9a7f99a9380ec86f7ecad14b3818b78a54",
            "patch": "@@ -597,7 +597,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "df23a83b3411447747170ae2e6fdc52077c0f28e",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=46276f9a7f99a9380ec86f7ecad14b3818b78a54",
            "patch": "@@ -597,7 +597,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "9288b1a29305780150859fe6653aab44d888099a",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=46276f9a7f99a9380ec86f7ecad14b3818b78a54",
            "patch": "@@ -602,7 +602,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "1f5aa55c46909efbfa5163d818956fd1dbd1bd59",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46276f9a7f99a9380ec86f7ecad14b3818b78a54/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=46276f9a7f99a9380ec86f7ecad14b3818b78a54",
            "patch": "@@ -519,7 +519,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "7d6d6af3824dffb26645f7baa44e5b760a595523",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/46276f9a7f99a9380ec86f7ecad14b3818b78a54/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46276f9a7f99a9380ec86f7ecad14b3818b78a54/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=46276f9a7f99a9380ec86f7ecad14b3818b78a54",
            "patch": "@@ -612,11 +612,7 @@ def _init_weights(self, module):\n \n \n class DiffLlamaRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: DiffLlamaConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: DiffLlamaConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n@@ -898,7 +894,7 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n "
        },
        {
            "sha": "f5e133ce1fea469c753cda2462c7009b3ed63c04",
            "filename": "tests/repo_utils/modular/test_conversion_order.py",
            "status": "added",
            "additions": 63,
            "deletions": 0,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/46276f9a7f99a9380ec86f7ecad14b3818b78a54/tests%2Frepo_utils%2Fmodular%2Ftest_conversion_order.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46276f9a7f99a9380ec86f7ecad14b3818b78a54/tests%2Frepo_utils%2Fmodular%2Ftest_conversion_order.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Frepo_utils%2Fmodular%2Ftest_conversion_order.py?ref=46276f9a7f99a9380ec86f7ecad14b3818b78a54",
            "patch": "@@ -0,0 +1,63 @@\n+import os\n+import sys\n+import unittest\n+\n+\n+ROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))\n+sys.path.append(os.path.join(ROOT_DIR, \"utils\"))\n+\n+import create_dependency_mapping  # noqa: E402\n+\n+\n+# This is equivalent to `all` in the current library state (as of 09/01/2025)\n+MODEL_ROOT = os.path.join(\"src\", \"transformers\", \"models\")\n+FILES_TO_PARSE = [\n+    os.path.join(MODEL_ROOT, \"starcoder2\", \"modular_starcoder2.py\"),\n+    os.path.join(MODEL_ROOT, \"gemma\", \"modular_gemma.py\"),\n+    os.path.join(MODEL_ROOT, \"olmo2\", \"modular_olmo2.py\"),\n+    os.path.join(MODEL_ROOT, \"diffllama\", \"modular_diffllama.py\"),\n+    os.path.join(MODEL_ROOT, \"granite\", \"modular_granite.py\"),\n+    os.path.join(MODEL_ROOT, \"gemma2\", \"modular_gemma2.py\"),\n+    os.path.join(MODEL_ROOT, \"mixtral\", \"modular_mixtral.py\"),\n+    os.path.join(MODEL_ROOT, \"olmo\", \"modular_olmo.py\"),\n+    os.path.join(MODEL_ROOT, \"rt_detr\", \"modular_rt_detr.py\"),\n+    os.path.join(MODEL_ROOT, \"qwen2\", \"modular_qwen2.py\"),\n+    os.path.join(MODEL_ROOT, \"llava_next_video\", \"modular_llava_next_video.py\"),\n+    os.path.join(MODEL_ROOT, \"cohere2\", \"modular_cohere2.py\"),\n+    os.path.join(MODEL_ROOT, \"modernbert\", \"modular_modernbert.py\"),\n+    os.path.join(MODEL_ROOT, \"colpali\", \"modular_colpali.py\"),\n+    os.path.join(MODEL_ROOT, \"deformable_detr\", \"modular_deformable_detr.py\"),\n+    os.path.join(MODEL_ROOT, \"aria\", \"modular_aria.py\"),\n+    os.path.join(MODEL_ROOT, \"ijepa\", \"modular_ijepa.py\"),\n+    os.path.join(MODEL_ROOT, \"bamba\", \"modular_bamba.py\"),\n+    os.path.join(MODEL_ROOT, \"dinov2_with_registers\", \"modular_dinov2_with_registers.py\"),\n+    os.path.join(MODEL_ROOT, \"instructblipvideo\", \"modular_instructblipvideo.py\"),\n+    os.path.join(MODEL_ROOT, \"glm\", \"modular_glm.py\"),\n+    os.path.join(MODEL_ROOT, \"phi\", \"modular_phi.py\"),\n+    os.path.join(MODEL_ROOT, \"mistral\", \"modular_mistral.py\"),\n+    os.path.join(MODEL_ROOT, \"phi3\", \"modular_phi3.py\"),\n+    os.path.join(MODEL_ROOT, \"cohere\", \"modular_cohere.py\"),\n+]\n+\n+\n+def appear_after(model1: str, model2: str, priority_list: list[str]) -> bool:\n+    \"\"\"Return True if `model1` appear after `model2` in `priority_list`.\"\"\"\n+    return priority_list.index(model1) > priority_list.index(model2)\n+\n+\n+class ConversionOrderTest(unittest.TestCase):\n+    def test_conversion_order(self):\n+        # Find the order\n+        priority_list = create_dependency_mapping.find_priority_list(FILES_TO_PARSE)\n+        # Extract just the model names\n+        model_priority_list = [file.rsplit(\"modular_\")[-1].replace(\".py\", \"\") for file in priority_list]\n+\n+        # These are based on what the current library order should be (as of 09/01/2025)\n+        self.assertTrue(appear_after(\"mixtral\", \"mistral\", model_priority_list))\n+        self.assertTrue(appear_after(\"gemma2\", \"gemma\", model_priority_list))\n+        self.assertTrue(appear_after(\"starcoder2\", \"mistral\", model_priority_list))\n+        self.assertTrue(appear_after(\"olmo2\", \"olmo\", model_priority_list))\n+        self.assertTrue(appear_after(\"diffllama\", \"mistral\", model_priority_list))\n+        self.assertTrue(appear_after(\"cohere2\", \"gemma2\", model_priority_list))\n+        self.assertTrue(appear_after(\"cohere2\", \"cohere\", model_priority_list))\n+        self.assertTrue(appear_after(\"phi3\", \"mistral\", model_priority_list))"
        },
        {
            "sha": "5cf38cdd1f815d9dbd9aeb566aba2cf7afaabeaf",
            "filename": "utils/create_dependency_mapping.py",
            "status": "modified",
            "additions": 22,
            "deletions": 44,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/46276f9a7f99a9380ec86f7ecad14b3818b78a54/utils%2Fcreate_dependency_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46276f9a7f99a9380ec86f7ecad14b3818b78a54/utils%2Fcreate_dependency_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dependency_mapping.py?ref=46276f9a7f99a9380ec86f7ecad14b3818b78a54",
            "patch": "@@ -3,51 +3,29 @@\n \n \n # Function to perform topological sorting\n-def topological_sort(dependencies):\n-    new_dependencies = {}\n-    graph = defaultdict(list)\n+def topological_sort(dependencies: dict):\n+    # Nodes are the name of the models to convert (we only add those to the graph)\n+    nodes = {node.rsplit(\"modular_\", 1)[1].replace(\".py\", \"\") for node in dependencies.keys()}\n+    # This will be a graph from models to convert, to models to convert that should be converted before (as they are a dependency)\n+    graph = {}\n+    name_mapping = {}\n     for node, deps in dependencies.items():\n-        node_name = node.split(\"/\")[-2]\n-        for dep in deps:\n-            dep_name = dep.split(\".\")[-2]\n-            if dep_name == node_name:\n-                # Skip self dependencies for topological sort as they create cycles\n-                continue\n-            if \"example\" not in node and \"auto\" not in dep and node_name not in graph[dep_name]:\n-                graph[dep_name].append(node_name)\n-        new_dependencies[node_name] = node\n-\n-    # Create a graph and in-degree count for each node\n-    def filter_one_by_one(filtered_list, reverse):\n-        if len(reverse) == 0:\n-            return filtered_list\n-\n-        graph = defaultdict(list)\n-        # Build the graph\n-        for node, deps in reverse.items():\n-            for dep in deps:\n-                graph[dep].append(node)\n-\n-        base_modules = set(reverse.keys()) - set(graph.keys())\n-        if base_modules == reverse.keys():\n-            # we are at the end\n-            return filtered_list + list(graph.keys())\n-        to_add = []\n-        for k in graph.keys():\n-            if len(graph[k]) == 1 and graph[k][0] in base_modules:\n-                if graph[k][0] in reverse:\n-                    del reverse[graph[k][0]]\n-                if k not in filtered_list:\n-                    to_add += [k]\n-        for k in base_modules:\n-            if k not in filtered_list:\n-                to_add += [k]\n-        filtered_list += list(to_add)\n-        return filter_one_by_one(filtered_list, reverse)\n-\n-    final_order = filter_one_by_one([], graph)\n-\n-    return [new_dependencies.get(k) for k in final_order if k in new_dependencies]\n+        node_name = node.rsplit(\"modular_\", 1)[1].replace(\".py\", \"\")\n+        dep_names = {dep.split(\".\")[-2] for dep in deps}\n+        dependencies = {dep for dep in dep_names if dep in nodes and dep != node_name}\n+        graph[node_name] = dependencies\n+        name_mapping[node_name] = node\n+\n+    sorting_list = []\n+    while len(graph) > 0:\n+        # Find the nodes with 0 out-degree\n+        leaf_nodes = {node for node in graph if len(graph[node]) == 0}\n+        # Add them to the list\n+        sorting_list += list(leaf_nodes)\n+        # Remove the leafs from the graph (and from the deps of other nodes)\n+        graph = {node: deps - leaf_nodes for node, deps in graph.items() if node not in leaf_nodes}\n+\n+    return [name_mapping[x] for x in sorting_list]\n \n \n # Function to extract class and import info from a file"
        },
        {
            "sha": "7cba82f6df13bf2bea5a4830a5bbda360cebf8d7",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/46276f9a7f99a9380ec86f7ecad14b3818b78a54/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/46276f9a7f99a9380ec86f7ecad14b3818b78a54/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=46276f9a7f99a9380ec86f7ecad14b3818b78a54",
            "patch": "@@ -58,7 +58,7 @@ def get_module_source_from_name(module_name: str) -> str:\n def preserve_case_replace(text, patterns: dict, default_name: str):\n     # Create a regex pattern to match all variations\n     regex_pattern = \"|\".join(re.escape(key) for key in patterns.keys())\n-    compiled_regex = re.compile(f\"({regex_pattern})(.|$)\", re.IGNORECASE | re.DOTALL)\n+    compiled_regex = re.compile(f\"(?<![a-z0-9])({regex_pattern})(.|$)\", re.IGNORECASE | re.DOTALL)\n \n     def replace(match):\n         matched_pattern = match.group(1)\n@@ -1691,9 +1691,13 @@ def save_modeling_file(modular_file, converted_file):\n     args = parser.parse_args()\n     if args.files_to_parse == [\"all\"]:\n         args.files_to_parse = glob.glob(\"src/transformers/models/**/modular_*.py\", recursive=True)\n-        args.files_to_parse += glob.glob(\"examples/**/modular_*.py\", recursive=True)\n+    if args.files_to_parse == [\"examples\"]:\n+        args.files_to_parse = glob.glob(\"examples/**/modular_*.py\", recursive=True)\n \n-    for file_name in find_priority_list(args.files_to_parse):\n+    priority_list = find_priority_list(args.files_to_parse)\n+    assert len(priority_list) == len(args.files_to_parse), \"Some files will not be converted\"\n+\n+    for file_name in priority_list:\n         print(f\"Converting {file_name} to a single model single file format\")\n         module_path = file_name.replace(\"/\", \".\").replace(\".py\", \"\").replace(\"src.\", \"\")\n         converted_files = convert_modular_file(file_name)"
        }
    ],
    "stats": {
        "total": 159,
        "additions": 100,
        "deletions": 59
    }
}