{
    "author": "Cyrilvallez",
    "message": "Update Mistral converter (#35967)\n\n* Update convert_mistral_weights_to_hf.py\r\n\r\n* Update convert_mistral_weights_to_hf.py\r\n\r\n* update\r\n\r\n* style\r\n\r\n* move it to integrations\r\n\r\n* style\r\n\r\n* trigger CIs\r\n\r\n* trigger CIs",
    "sha": "ad3059892391debd25bb3adcfed127523db16d90",
    "files": [
        {
            "sha": "78172329277e4f5ac0b01795a123bdebe7bd8f51",
            "filename": "src/transformers/integrations/mistral.py",
            "status": "added",
            "additions": 105,
            "deletions": 0,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3059892391debd25bb3adcfed127523db16d90/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3059892391debd25bb3adcfed127523db16d90/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmistral.py?ref=ad3059892391debd25bb3adcfed127523db16d90",
            "patch": "@@ -0,0 +1,105 @@\n+from tokenizers import Regex, Tokenizer, decoders, pre_tokenizers, processors\n+from tokenizers.models import BPE\n+\n+from transformers import LlamaTokenizerFast\n+from transformers.convert_slow_tokenizer import bytes_to_unicode\n+\n+\n+class MistralConverter:\n+    \"\"\"\n+    A general tiktoken converter.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab=None,\n+        pattern=r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n+        add_prefix_space=False,\n+        additional_special_tokens=None,\n+        *args,\n+        **kwargs,\n+    ):\n+        super().__init__(*args)\n+        self.vocab = vocab\n+        self.pattern = pattern\n+        self.add_prefix_space = add_prefix_space\n+        self.additional_special_tokens = additional_special_tokens\n+\n+    def extract_vocab_merges_from_model(self, vocab: str):\n+        bpe_ranks = vocab\n+        byte_encoder = bytes_to_unicode()\n+\n+        def token_bytes_to_string(b):\n+            return \"\".join([byte_encoder[ord(char)] for char in b.decode(\"latin-1\")])\n+\n+        merges = []\n+        vocab = {}\n+        for idx, (token, rank) in enumerate(bpe_ranks.items()):\n+            if token not in self.additional_special_tokens:\n+                vocab[token_bytes_to_string(token)] = idx\n+                if len(token) == 1:\n+                    continue\n+                local = []\n+                for index in range(1, len(token)):\n+                    piece_l, piece_r = token[:index], token[index:]\n+                    if piece_l in bpe_ranks and piece_r in bpe_ranks and (piece_l + piece_r) in bpe_ranks:\n+                        local.append((piece_l, piece_r, rank))\n+                local = sorted(local, key=lambda x: (bpe_ranks[x[0]], bpe_ranks[x[1]]), reverse=False)\n+                merges.extend(local)\n+            else:\n+                vocab[token] = idx\n+        merges = sorted(merges, key=lambda val: val[2], reverse=False)\n+        merges = [(token_bytes_to_string(val[0]), token_bytes_to_string(val[1])) for val in merges]\n+        return vocab, merges\n+\n+    def tokenizer(self):\n+        vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab)\n+        tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=False))\n+        if hasattr(tokenizer.model, \"ignore_merges\"):\n+            tokenizer.model.ignore_merges = True\n+        return tokenizer\n+\n+    def converted(self) -> Tokenizer:\n+        tokenizer = self.tokenizer()\n+        tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+            [\n+                pre_tokenizers.Split(Regex(self.pattern), behavior=\"isolated\", invert=False),\n+                pre_tokenizers.ByteLevel(add_prefix_space=self.add_prefix_space, use_regex=False),\n+            ]\n+        )\n+        tokenizer.decoder = decoders.ByteLevel()\n+        tokenizer.add_special_tokens(self.additional_special_tokens)\n+\n+        tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n+\n+        return tokenizer\n+\n+\n+def convert_tekken_tokenizer(tokenizer_file: str):\n+    \"\"\"Convert a \"tekken\" tokenizer to a fast Tokenizer.\"\"\"\n+    # Tekken format -- need to use the Converter\n+\n+    from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n+\n+    # Load directly using their lib\n+    mistral_tokenizer = MistralTokenizer.from_file(tokenizer_file)\n+\n+    # Extract vocab and special tokens\n+    vocab = mistral_tokenizer.instruct_tokenizer.tokenizer._tekken_token2id_nospecial\n+    all_special = [\n+        token.value if hasattr(token, \"value\") else token\n+        for token in mistral_tokenizer.instruct_tokenizer.tokenizer._all_special_tokens\n+    ]\n+    specials_tokens = {token: all_special.index(token) for token in all_special}\n+    specials_tokens.update(vocab)\n+    vocab = specials_tokens\n+\n+    # Convert\n+    tokenizer = LlamaTokenizerFast(\n+        tokenizer_object=MistralConverter(vocab=vocab, additional_special_tokens=all_special).converted(),\n+    )\n+\n+    # Post-process\n+    tokenizer.add_special_tokens({\"additional_special_tokens\": all_special})\n+\n+    return tokenizer"
        },
        {
            "sha": "1fc4ad90e4f5616f1b0ea8944845ad18ca8e534f",
            "filename": "src/transformers/models/mistral/convert_mistral_weights_to_hf.py",
            "status": "modified",
            "additions": 37,
            "deletions": 31,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3059892391debd25bb3adcfed127523db16d90/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3059892391debd25bb3adcfed127523db16d90/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py?ref=ad3059892391debd25bb3adcfed127523db16d90",
            "patch": "@@ -15,25 +15,14 @@\n import json\n import os\n import re\n-import warnings\n \n import torch\n from safetensors.torch import load_file\n \n-from transformers import LlamaTokenizer, MistralConfig, MistralForCausalLM\n+from transformers import AutoTokenizer, LlamaTokenizerFast, MistralConfig, MistralForCausalLM\n+from transformers.integrations.mistral import convert_tekken_tokenizer\n \n \n-try:\n-    from transformers import LlamaTokenizerFast\n-\n-    tokenizer_class = LlamaTokenizerFast\n-except ImportError as e:\n-    warnings.warn(e)\n-    warnings.warn(\n-        \"The converted tokenizer will be the `slow` tokenizer. To use the fast, update your `tokenizers` library and re-run the tokenizer conversion\"\n-    )\n-    tokenizer_class = LlamaTokenizer\n-\n # fmt: off\n STATE_DICT_MAPPING = {\n     # CausalLM keys\n@@ -87,23 +76,24 @@ def convert_state_dict(original_state_dict: dict, config: MistralConfig):\n     \"\"\"Convert a state dict file, when a single `nn.Module` is never sharded in different files (usual case).\"\"\"\n     new_dict = {}\n \n-    n_heads = config.num_attention_heads\n-    dim = config.hidden_size\n-    dims_per_head = dim // n_heads\n+    num_attention_heads = config.num_attention_heads\n+    hidden_size = config.hidden_size\n+    head_dim = config.head_dim\n     num_key_value_heads = config.num_key_value_heads\n-    key_value_dim = dims_per_head * num_key_value_heads\n+    key_value_dim = head_dim * num_key_value_heads\n+    query_dim = head_dim * num_attention_heads\n \n     for old_key, tensor in original_state_dict.items():\n         new_key = map_old_key_to_new(old_key)\n \n         if \"q_proj\" in new_key:\n-            tensor = tensor.view(n_heads, dims_per_head, dim).reshape(dim, dim)\n-            tensor = permute_for_rope(tensor, n_heads, dim, dim)\n+            tensor = tensor.view(num_attention_heads, head_dim, hidden_size).reshape(query_dim, hidden_size)\n+            tensor = permute_for_rope(tensor, num_attention_heads, query_dim, hidden_size)\n         elif \"k_proj\" in new_key:\n-            tensor = tensor.view(num_key_value_heads, dims_per_head, dim).reshape(key_value_dim, dim)\n-            tensor = permute_for_rope(tensor, num_key_value_heads, key_value_dim, dim)\n+            tensor = tensor.view(num_key_value_heads, head_dim, hidden_size).reshape(key_value_dim, hidden_size)\n+            tensor = permute_for_rope(tensor, num_key_value_heads, key_value_dim, hidden_size)\n         elif \"v_proj\" in new_key:\n-            tensor = tensor.view(num_key_value_heads, dims_per_head, dim).reshape(key_value_dim, dim)\n+            tensor = tensor.view(num_key_value_heads, head_dim, hidden_size).reshape(key_value_dim, hidden_size)\n \n         new_dict[new_key] = tensor\n     return new_dict\n@@ -169,7 +159,7 @@ def convert_state_dict_sharded(loaded_shards: list[dict], config: MistralConfig)\n     return new_dict\n \n \n-def convert_config(original_config: dict, max_position_embeddings: int):\n+def convert_config(original_config: dict, max_position_embeddings: int = 32768):\n     key_mapping = {\n         \"hidden_size\": \"dim\",\n         \"num_hidden_layers\": \"n_layers\",\n@@ -191,9 +181,7 @@ def convert_config(original_config: dict, max_position_embeddings: int):\n         \"n_kv_heads\", new_config_kwargs[\"num_attention_heads\"]\n     )\n     new_config_kwargs[\"rope_theta\"] = original_config.get(\"rope_theta\", 10000.0)\n-\n-    # This is never provided in `params.json`, we provide it manually\n-    new_config_kwargs[\"max_position_embeddings\"] = max_position_embeddings\n+    new_config_kwargs[\"max_position_embeddings\"] = original_config.get(\"max_seq_len\", max_position_embeddings)\n \n     # This may sometimes be a string in `params.json`\n     if new_config_kwargs[\"sliding_window\"] is not None:\n@@ -230,11 +218,23 @@ def convert_and_write_model(input_dir: str, output_dir: str, max_position_embedd\n     model.save_pretrained(output_dir)\n \n \n-def convert_and_write_tokenizer(input_dir: str, output_dir: str):\n+def convert_and_write_tokenizer(input_dir: str, output_dir: str, tokenizer_template_name: str = \"\"):\n     \"\"\"Convert the tokenizer and save it.\"\"\"\n-    # May have .v3 or .v7 at the end\n-    tokenizer_file = [file for file in os.listdir(input_dir) if \"tokenizer.model\" in file][0]\n-    tokenizer = tokenizer_class(os.path.join(input_dir, tokenizer_file))\n+    # Tekken format\n+    if \"tekken.json\" in os.listdir(input_dir):\n+        tokenizer_file = os.path.join(input_dir, \"tekken.json\")\n+        tokenizer = convert_tekken_tokenizer(tokenizer_file)\n+    else:\n+        # May have .v3 or .v7 at the end\n+        tokenizer_file = [file for file in os.listdir(input_dir) if \"tokenizer.model\" in file][0]\n+        tokenizer = LlamaTokenizerFast(os.path.join(input_dir, tokenizer_file))\n+\n+    # Load a chat template from another model\n+    if tokenizer_template_name != \"\":\n+        template_tok = AutoTokenizer.from_pretrained(tokenizer_template_name)\n+        tokenizer.chat_template = template_tok.chat_template\n+\n+    # Finally save it\n     tokenizer.save_pretrained(output_dir)\n \n \n@@ -248,6 +248,12 @@ def main():\n         \"output_dir\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n+    parser.add_argument(\n+        \"--template_name\",\n+        type=str,\n+        default=\"\",\n+        help=\"Another model name from which to copy the chat template.\",\n+    )\n     parser.add_argument(\n         \"--max_position_embeddings\",\n         type=int,\n@@ -269,7 +275,7 @@ def main():\n \n     if not args.tokenizer_only:\n         convert_and_write_model(args.input_dir, args.output_dir, args.max_position_embeddings, args.modules_are_split)\n-    convert_and_write_tokenizer(args.input_dir, args.output_dir)\n+    convert_and_write_tokenizer(args.input_dir, args.output_dir, args.template_name)\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "ee1f1e9eb5eddd11975ca1e28da1c4ad035df90b",
            "filename": "src/transformers/models/pixtral/convert_pixtral_weights_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/ad3059892391debd25bb3adcfed127523db16d90/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ad3059892391debd25bb3adcfed127523db16d90/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py?ref=ad3059892391debd25bb3adcfed127523db16d90",
            "patch": "@@ -19,8 +19,6 @@\n import torch\n from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n from safetensors.torch import load_file as safe_load_file\n-from tokenizers import Regex, Tokenizer, decoders, pre_tokenizers, processors\n-from tokenizers.models import BPE\n \n from transformers import (\n     LlavaConfig,\n@@ -30,7 +28,6 @@\n     PixtralProcessor,\n     PixtralVisionConfig,\n )\n-from transformers.convert_slow_tokenizer import bytes_to_unicode\n \n \n \"\"\"\n@@ -87,76 +84,6 @@\n }\n \n \n-class MistralConverter:\n-    \"\"\"\n-    A general tiktoken converter.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        vocab=None,\n-        pattern=r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n-        add_prefix_space=False,\n-        additional_special_tokens=None,\n-        *args,\n-        **kwargs,\n-    ):\n-        super().__init__(*args)\n-        self.vocab = vocab\n-        self.pattern = pattern\n-        self.add_prefix_space = add_prefix_space\n-        self.additional_special_tokens = additional_special_tokens\n-\n-    def extract_vocab_merges_from_model(self, vocab: str):\n-        bpe_ranks = vocab\n-        byte_encoder = bytes_to_unicode()\n-\n-        def token_bytes_to_string(b):\n-            return \"\".join([byte_encoder[ord(char)] for char in b.decode(\"latin-1\")])\n-\n-        merges = []\n-        vocab = {}\n-        for idx, (token, rank) in enumerate(bpe_ranks.items()):\n-            if token not in self.additional_special_tokens:\n-                vocab[token_bytes_to_string(token)] = idx\n-                if len(token) == 1:\n-                    continue\n-                local = []\n-                for index in range(1, len(token)):\n-                    piece_l, piece_r = token[:index], token[index:]\n-                    if piece_l in bpe_ranks and piece_r in bpe_ranks and (piece_l + piece_r) in bpe_ranks:\n-                        local.append((piece_l, piece_r, rank))\n-                local = sorted(local, key=lambda x: (bpe_ranks[x[0]], bpe_ranks[x[1]]), reverse=False)\n-                merges.extend(local)\n-            else:\n-                vocab[token] = idx\n-        merges = sorted(merges, key=lambda val: val[2], reverse=False)\n-        merges = [(token_bytes_to_string(val[0]), token_bytes_to_string(val[1])) for val in merges]\n-        return vocab, merges\n-\n-    def tokenizer(self):\n-        vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab)\n-        tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=False))\n-        if hasattr(tokenizer.model, \"ignore_merges\"):\n-            tokenizer.model.ignore_merges = True\n-        return tokenizer\n-\n-    def converted(self) -> Tokenizer:\n-        tokenizer = self.tokenizer()\n-        tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n-            [\n-                pre_tokenizers.Split(Regex(self.pattern), behavior=\"isolated\", invert=False),\n-                pre_tokenizers.ByteLevel(add_prefix_space=self.add_prefix_space, use_regex=False),\n-            ]\n-        )\n-        tokenizer.decoder = decoders.ByteLevel()\n-        tokenizer.add_special_tokens(self.additional_special_tokens)\n-\n-        tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n-\n-        return tokenizer\n-\n-\n def convert_mistral_tokenizer(model_file):\n     from transformers import LlamaTokenizer\n "
        }
    ],
    "stats": {
        "total": 246,
        "additions": 142,
        "deletions": 104
    }
}