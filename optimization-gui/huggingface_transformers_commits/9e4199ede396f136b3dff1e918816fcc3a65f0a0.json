{
    "author": "remi-or",
    "message": "Gemma3 fixes (#41572)\n\n* Multiple device error fix\n\n* FA2 equivalence fix\n\n* Move the train fwd in cfg test\n\n* Style\n\n* Added comment\n\n* Made the comment more clear",
    "sha": "9e4199ede396f136b3dff1e918816fcc3a65f0a0",
    "files": [
        {
            "sha": "aa21cf995ebec352a1ea54c1b17854cdeed134eb",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e4199ede396f136b3dff1e918816fcc3a65f0a0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e4199ede396f136b3dff1e918816fcc3a65f0a0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=9e4199ede396f136b3dff1e918816fcc3a65f0a0",
            "patch": "@@ -798,7 +798,7 @@ def create_causal_mask_mapping(\n         is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n         new_image_start = is_image & ~is_previous_image\n         image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        image_group_ids = torch.where(is_image, image_group_ids, -1)\n         mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n             token_type_ids.to(cache_position.device), image_group_ids\n         )"
        },
        {
            "sha": "a814d200a172a584bba54375136ff6119de955fa",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e4199ede396f136b3dff1e918816fcc3a65f0a0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e4199ede396f136b3dff1e918816fcc3a65f0a0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=9e4199ede396f136b3dff1e918816fcc3a65f0a0",
            "patch": "@@ -764,7 +764,7 @@ def create_causal_mask_mapping(\n         is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n         new_image_start = is_image & ~is_previous_image\n         image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-        image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+        image_group_ids = torch.where(is_image, image_group_ids, -1)\n         mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n             token_type_ids.to(cache_position.device), image_group_ids\n         )"
        },
        {
            "sha": "9f466bcd298f64ae9757d9a7ff3009426e9923f8",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e4199ede396f136b3dff1e918816fcc3a65f0a0/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e4199ede396f136b3dff1e918816fcc3a65f0a0/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=9e4199ede396f136b3dff1e918816fcc3a65f0a0",
            "patch": "@@ -19,6 +19,7 @@\n \n import pytest\n from parameterized import parameterized\n+from pytest import mark\n \n from transformers import (\n     AutoModelForCausalLM,\n@@ -33,9 +34,11 @@\n     is_flash_attn_2_available,\n     require_deterministic_for_xpu,\n     require_flash_attn,\n+    require_flash_attn_3,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_gpu,\n     require_torch_large_accelerator,\n     slow,\n     torch_device,\n@@ -342,6 +345,20 @@ def test_automodelforcausallm(self):\n             for_causal_lm = AutoModelForCausalLM.from_pretrained(tmp_dir)\n             self.assertIsInstance(for_causal_lm, Gemma3ForConditionalGeneration)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_2\", test_fwd_in_train=False)\n+\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @mark.flash_attn_3_test\n+    @slow\n+    def test_flash_attn_3_from_config(self):\n+        self.flash_attn_from_config(attn_implementation=\"flash_attention_3\", test_fwd_in_train=False)\n+\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "0e0c444f8eec1a953b98d1a38bdda5125247c34d",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e4199ede396f136b3dff1e918816fcc3a65f0a0/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e4199ede396f136b3dff1e918816fcc3a65f0a0/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=9e4199ede396f136b3dff1e918816fcc3a65f0a0",
            "patch": "@@ -2976,7 +2976,7 @@ def test_model_is_small(self):\n \n     def flash_attn_inference_equivalence(\n         self, attn_implementation: str, padding_side: str, atol: float = 4e-2, rtol: float = 4e-2\n-    ):\n+    ) -> None:\n         r\"\"\"\n         Tests the equivalence between the eager and flash attention implementations.\n         This test is only for inference and runs with `dtype=torch.bfloat16`.\n@@ -3114,9 +3114,6 @@ def flash_attn_inference_equivalence(\n                 torch.testing.assert_close(logits_1_eager, logits_1_fa, atol=atol, rtol=rtol)\n                 if padding_side == \"left\":\n                     torch.testing.assert_close(logits_2_eager[1:], logits_2_fa[1:], atol=atol, rtol=rtol)\n-                    # Check it can run in training mode\n-                    model.train()\n-                    _ = model(**second_inputs)\n                 else:\n                     torch.testing.assert_close(logits_2_eager[:-1], logits_2_fa[:-1], atol=atol, rtol=rtol)\n \n@@ -3651,7 +3648,7 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n \n         assert not loss.isnan().any()\n \n-    def flash_attn_from_config(self, attn_implementation: str):\n+    def flash_attn_from_config(self, attn_implementation: str, test_fwd_in_train: bool = True):\n         r\"\"\"\n         Tests if the model can be loaded with `attn_implementation` from the config and if the\n         weights are not randomly initialized.\n@@ -3669,6 +3666,14 @@ def flash_attn_from_config(self, attn_implementation: str):\n                 config, attn_implementation=attn_implementation, dtype=torch.bfloat16\n             ).to(torch_device)\n \n+            # By default, we perform the forward pass in train mode, because it's more sctrict than eval mode. If the\n+            # forward pass is successful in train mode, it will also be successful in eval mode. But since some models\n+            # (eg. gemma3) need different inputs in train mode we have the option to test the forward pass in eval mode.\n+            if test_fwd_in_train:\n+                fa_model = fa_model.train()\n+            else:\n+                fa_model = fa_model.eval()\n+\n             dummy_input = inputs_dict[fa_model.main_input_name]\n             if dummy_input.dtype in [torch.float32, torch.float16]:\n                 dummy_input = dummy_input.to(torch.bfloat16)"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 29,
        "deletions": 7
    }
}