{
    "author": "guangy10",
    "message": "Fix Qwen models export with torch 2.7 (#37985)\n\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "0b037fd425da1f98ea54e6b63d11e7d223b1e516",
    "files": [
        {
            "sha": "acb784f6ab238dea2d41e856008d0fc5951623ba",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b037fd425da1f98ea54e6b63d11e7d223b1e516/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b037fd425da1f98ea54e6b63d11e7d223b1e516/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=0b037fd425da1f98ea54e6b63d11e7d223b1e516",
            "patch": "@@ -31,6 +31,7 @@\n     slow,\n     torch_device,\n )\n+from transformers.utils.import_utils import is_torch_greater_or_equal\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -490,7 +491,8 @@ def test_export_static_cache(self):\n         max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n \n         # Static Cache + export\n-        exported_program = convert_and_export_with_cache(model)\n+        strict = is_torch_greater_or_equal(\"2.7.0\")  # Due to https://github.com/pytorch/pytorch/issues/150994\n+        exported_program = convert_and_export_with_cache(model, strict=strict)\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        },
        {
            "sha": "44eb7474fa834f083134464cfb79d13135c9cfa8",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b037fd425da1f98ea54e6b63d11e7d223b1e516/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b037fd425da1f98ea54e6b63d11e7d223b1e516/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=0b037fd425da1f98ea54e6b63d11e7d223b1e516",
            "patch": "@@ -31,6 +31,7 @@\n     slow,\n     torch_device,\n )\n+from transformers.utils.import_utils import is_torch_greater_or_equal\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -453,17 +454,23 @@ def test_export_static_cache(self):\n         qwen_model = \"Qwen/Qwen3-0.6B-Base\"\n \n         tokenizer = AutoTokenizer.from_pretrained(qwen_model, pad_token=\"</s>\", padding_side=\"right\")\n-        EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% plain, unflavoured, and unadulterated. It is\"]\n+        if is_torch_greater_or_equal(\"2.7.0\"):\n+            strict = False  # Due to https://github.com/pytorch/pytorch/issues/150994\n+            EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% plain, unsalted, unsweetened, and unflavored.\"]\n+        else:\n+            strict = True\n+            EXPECTED_TEXT_COMPLETION = [\"My favourite condiment is 100% plain, unflavoured, and unadulterated. It is\"]\n+\n         max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n             \"input_ids\"\n         ].shape[-1]\n-\n-        # Load model\n         device = \"cpu\"\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\"\n         batch_size = 1\n+\n+        # Load model\n         model = Qwen3ForCausalLM.from_pretrained(\n             qwen_model,\n             device_map=device,\n@@ -486,7 +493,7 @@ def test_export_static_cache(self):\n         max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n \n         # Static Cache + export\n-        exported_program = convert_and_export_with_cache(model)\n+        exported_program = convert_and_export_with_cache(model, strict=strict)\n         ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 14,
        "deletions": 5
    }
}