{
    "author": "tanhuajie",
    "message": "Add MLCD model (#36182)\n\n* Add MLCD model\n\n* Update codes for auto-mapping\n\n* Add test scripts for MLCD\n\n* Update doc for MLCD model\n\n* Fix import error\n\n* Fix import error\n\n* Fix CI error for attention_outputs\n\n* Fix code style for CI\n\n* Fix code style for CI\n\n* Fix code style for CI\n\n* Fix code style for CI\n\n* Fix code style for CI\n\n* Fix CI error for initialization\n\n* Fix code style for CI\n\n* Fix code style for CI\n\n* Reformat codes and docs for CI test\n\n* Reformat codes and docs for CI test\n\n* Remove unused attributes for CI test\n\n* Fix style for CI test\n\n* List MLCD in flash_attn doc\n\n* Fix: typos, modulars, refactors from suggestions\n\n* Refactoring convert_mlcd_weights_to_hf.py from suggestions\n\n* Fix: docs conflicts\n\n* Fix error for CI test\n\n* Fix style for CI test\n\n* Add integration test for MLCD\n\n* Refactoring by class inheritance\n\n* Fix: refactor attention interface, adjust codes\n\n* Fix: merging conflicts\n\n* Fix: merging conflicts\n\n* Fix: style for CI test\n\n* Fix: style for CI test\n\n* Fix: set test_resize_embeddings to be False\n\n* Fix: initializer for CI test\n\n* Fix: conflicts, CI test, warning and refactoring\n\n* Fix: merging conflicts\n\n* Refactor\n\n* Update docs\n\n* Fix mistakes\n\n* Remove unused args and fix multi-gpu error\n\n* Revert position_embeddings\n\n* Solve conflicts\n\n* Solve conflicts\n\n* Remove dummy\n\n* Update _init_weights\n\n* Update _init_weights\n\n* Update _init_weights for CI test",
    "sha": "6f7ea1cf006414231610906e7710a84281960c24",
    "files": [
        {
            "sha": "f1a8029b09405dc359206a133d13a65fbf0ee5c3",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -737,6 +737,8 @@\n         title: Mask2Former\n       - local: model_doc/maskformer\n         title: MaskFormer\n+      - local: model_doc/mlcd\n+        title: MLCD\n       - local: model_doc/mobilenet_v1\n         title: MobileNetV1\n       - local: model_doc/mobilenet_v2"
        },
        {
            "sha": "66d87d3e3ffcbf6977339a9a776de0025c306311",
            "filename": "docs/source/en/model_doc/mlcd.md",
            "status": "added",
            "additions": 81,
            "deletions": 0,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/docs%2Fsource%2Fen%2Fmodel_doc%2Fmlcd.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/docs%2Fsource%2Fen%2Fmodel_doc%2Fmlcd.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmlcd.md?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -0,0 +1,81 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# MLCD\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## Overview\n+\n+The MLCD models were released by the DeepGlint-AI team in [unicom](https://github.com/deepglint/unicom), which focuses on building foundational visual models for large multimodal language models using large-scale datasets such as LAION400M and COYO700M, and employs sample-to-cluster contrastive learning to optimize performance. MLCD models are primarily used for multimodal visual large language models, such as LLaVA.\n+\n+ðŸ”¥**MLCD-ViT-bigG**ðŸ”¥ series is the state-of-the-art vision transformer model enhanced with 2D Rotary Position Embedding (RoPE2D), achieving superior performance on document understanding and visual question answering tasks. Developed by DeepGlint AI, this model demonstrates exceptional capabilities in processing complex visual-language interactions.\n+\n+Tips:\n+\n+- We adopted the official [LLaVA-NeXT](https://github.com/LLaVA-VL/LLaVA-NeXT) and the official training dataset [LLaVA-NeXT-Data](https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Data) for evaluating the foundational visual models.\n+\n+- The language model is [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct). \n+\n+Result: \n+\n+| Vision Tower                                                                                  | RoPE2D | ChartQA   | DocVQA    | InfoVQA   | OCRBench   | MMMU      |\n+| :-------------------------------------------------------------------------------------------- | :----: | :-------- | :-------- | :-------- | :--------- | :-------- |\n+| CLIP (ViT-L-14-336px)                                                                         |   Ã—    | 66.52     | 75.21     | 38.88     | 525.00     | 44.20     |\n+| SigLIP (ViT-SO400M-384px)                                                                     |   Ã—    | 69.28     | 76.71     | 41.38     | 554.00     | 46.78     |\n+| DFN5B (ViT-H-14-378px)                                                                        |   Ã—    | 64.36     | 70.87     | 38.59     | 473.00     | **48.00** |\n+| **[MLCD (ViT-L-14-336px)](https://huggingface.co/DeepGlint-AI/mlcd-vit-large-patch14-336)**   |   Ã—    | 67.84     | 76.46     | 43.48     | 531.00     | 44.30     |\n+| **[MLCD (ViT-bigG-14-336px)](https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-336)** |   âˆš    | 71.07     | 79.63     | 44.38     | 572.00     | 46.78     |\n+| **[MLCD (ViT-bigG-14-448px)](https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-448)** |   âˆš    | **73.80** | **83.34** | **46.59** | **582.00** | 46.00     |\n+\n+\n+## Usage\n+\n+```python\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, MLCDVisionModel\n+\n+# Load model and processor\n+model = MLCDVisionModel.from_pretrained(\"DeepGlint-AI/mlcd-vit-bigG-patch14-448\")\n+processor = AutoProcessor.from_pretrained(\"DeepGlint-AI/mlcd-vit-bigG-patch14-448\")\n+\n+# Process single image\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+inputs = processor(images=image, return_tensors=\"pt\")\n+\n+# Generate outputs\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+\n+# Get visual features\n+features = outputs.last_hidden_state\n+\n+print(f\"Extracted features shape: {features.shape}\")\n+```\n+\n+## MLCDVisionConfig\n+\n+[[autodoc]] MLCDVisionConfig\n+\n+## MLCDVisionModel\n+\n+[[autodoc]] MLCDVisionModel\n+    - forward"
        },
        {
            "sha": "73961f4a6a8731497b6318c5c6f0fe89dcde122b",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -179,6 +179,7 @@\n     from .mistral import *\n     from .mistral3 import *\n     from .mixtral import *\n+    from .mlcd import *\n     from .mllama import *\n     from .mluke import *\n     from .mobilebert import *"
        },
        {
            "sha": "1fbc2cb168fc02e3e295ab1e69d5bccd9f700af2",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -200,6 +200,7 @@\n         (\"mistral\", \"MistralConfig\"),\n         (\"mistral3\", \"Mistral3Config\"),\n         (\"mixtral\", \"MixtralConfig\"),\n+        (\"mlcd\", \"MLCDVisionConfig\"),\n         (\"mllama\", \"MllamaConfig\"),\n         (\"mobilebert\", \"MobileBertConfig\"),\n         (\"mobilenet_v1\", \"MobileNetV1Config\"),\n@@ -559,6 +560,7 @@\n         (\"mistral\", \"Mistral\"),\n         (\"mistral3\", \"Mistral3\"),\n         (\"mixtral\", \"Mixtral\"),\n+        (\"mlcd\", \"MLCD\"),\n         (\"mllama\", \"Mllama\"),\n         (\"mluke\", \"mLUKE\"),\n         (\"mms\", \"MMS\"),"
        },
        {
            "sha": "a536f5b45e5ef2f37f48d3c408da4355a20bfb23",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -114,6 +114,7 @@\n             (\"maskformer\", (\"MaskFormerImageProcessor\",)),\n             (\"mgp-str\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n+            (\"mlcd\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"mllama\", (\"MllamaImageProcessor\",)),\n             (\"mobilenet_v1\", (\"MobileNetV1ImageProcessor\",)),\n             (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\", \"MobileNetV2ImageProcessorFast\")),"
        },
        {
            "sha": "e630333602cfe4dbfb17df9b34bd4aabf4301887",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -183,6 +183,7 @@\n         (\"mimi\", \"MimiModel\"),\n         (\"mistral\", \"MistralModel\"),\n         (\"mixtral\", \"MixtralModel\"),\n+        (\"mlcd\", \"MLCDVisionModel\"),\n         (\"mobilebert\", \"MobileBertModel\"),\n         (\"mobilenet_v1\", \"MobileNetV1Model\"),\n         (\"mobilenet_v2\", \"MobileNetV2Model\"),\n@@ -640,6 +641,7 @@\n         (\"imagegpt\", \"ImageGPTModel\"),\n         (\"levit\", \"LevitModel\"),\n         (\"llama4\", \"Llama4VisionModel\"),\n+        (\"mlcd\", \"MLCDVisionModel\"),\n         (\"mllama\", \"MllamaVisionModel\"),\n         (\"mobilenet_v1\", \"MobileNetV1Model\"),\n         (\"mobilenet_v2\", \"MobileNetV2Model\"),"
        },
        {
            "sha": "3556177d5eb8e493aa5e051785b980b0e10d9c4a",
            "filename": "src/transformers/models/mlcd/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2F__init__.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_mlcd import *\n+    from .modeling_mlcd import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "f28a5f1a7cab3d453c3bf07caea9fa9fd25a593b",
            "filename": "src/transformers/models/mlcd/configuration_mlcd.py",
            "status": "added",
            "additions": 117,
            "deletions": 0,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconfiguration_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconfiguration_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconfiguration_mlcd.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -0,0 +1,117 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/mlcd/modular_mlcd.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_mlcd.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class MLCDVisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MLCDVisionModel`]. It is used to instantiate a MLCD\n+    vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the vision encoder of the MLCD\n+    [DeepGlint-AI/mlcd-vit-bigG-patch14-336](https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-336) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1664):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 8192):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        projection_dim (`int`, *optional*, defaults to 1024):\n+            Dimensionality of text and vision projection layers.\n+        num_hidden_layers (`int`, *optional*, defaults to 48):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        image_size (`int`, *optional*, defaults to 336):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_factor (`float`, *optional*, defaults to 1.0):\n+            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n+            testing).\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MLCDVisionConfig, MLCDVisionModel\n+\n+    >>> # Initializing a MLCDVisionConfig with DeepGlint-AI/mlcd-vit-bigG-patch14-336 style configuration\n+    >>> configuration = MLCDVisionConfig()\n+\n+    >>> # Initializing a MLCDVisionModel (with random weights) from the DeepGlint-AI/mlcd-vit-bigG-patch14-336 style configuration\n+    >>> model = MLCDVisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"mlcd_vision_model\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1664,\n+        intermediate_size=8192,\n+        num_hidden_layers=48,\n+        num_attention_heads=16,\n+        num_key_value_groups=1,\n+        num_channels=3,\n+        image_size=336,\n+        patch_size=14,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-5,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        initializer_factor=1.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_groups = num_key_value_groups\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.image_size = image_size\n+        self.initializer_range = initializer_range\n+        self.initializer_factor = initializer_factor\n+        self.attention_dropout = attention_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.hidden_act = hidden_act\n+\n+\n+__all__ = [\"MLCDVisionConfig\"]"
        },
        {
            "sha": "0f74b64737a217a87d63cccb1f48c209328a6494",
            "filename": "src/transformers/models/mlcd/convert_mlcd_weights_to_hf.py",
            "status": "added",
            "additions": 336,
            "deletions": 0,
            "changes": 336,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconvert_mlcd_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconvert_mlcd_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fconvert_mlcd_weights_to_hf.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -0,0 +1,336 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert MLCD checkpoints from the original repository.\n+\n+URL: https://github.com/deepglint/unicom/tree/main\n+\"\"\"\n+\n+import argparse\n+import collections\n+import os\n+import re\n+\n+import numpy as np\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import CLIPImageProcessor\n+\n+from ...utils import logging\n+from .configuration_mlcd import MLCDVisionConfig\n+from .modeling_mlcd import MLCDVisionModel\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+\n+COMMON_CONFIG_PARAMS = {\n+    \"mlcd-vit-bigG-patch14-336\": {\n+        \"hidden_size\": 1664,\n+        \"image_size\": 336,\n+        \"intermediate_size\": 8192,\n+        \"num_attention_heads\": 16,\n+        \"num_hidden_layers\": 48,\n+        \"patch_size\": 14,\n+        \"projection_dim\": 1024,\n+    },\n+    \"mlcd-vit-bigG-patch14-448\": {\n+        \"hidden_size\": 1664,\n+        \"image_size\": 448,\n+        \"intermediate_size\": 8192,\n+        \"num_attention_heads\": 16,\n+        \"num_hidden_layers\": 48,\n+        \"patch_size\": 14,\n+        \"projection_dim\": 1024,\n+    },\n+}\n+\n+MODEL_NAME_TO_CHECKPOINT_PATH = {\n+    # base checkpoints\n+    \"mlcd-vit-bigG-patch14-336\": \"MLCD_ViT_bigG_14_336px_pytorch.pt\",\n+    \"mlcd-vit-bigG-patch14-448\": \"MLCD_ViT_bigG_14_448px_pytorch.pt\",\n+}\n+\n+# fmt: off\n+EXPECTED_OUTPUTS = {\n+    \"mlcd-vit-bigG-patch14-336\": torch.tensor([\n+        [-0.8921, -0.1069,  0.2989,  0.6018, -0.5892],\n+        [ 0.4093, -1.4592,  0.6048, -0.5147, -0.5929],\n+        [ 0.7796, -0.7133, -0.5649, -0.7843, -0.5548],\n+        [ 0.0041,  0.0286,  0.4310, -0.1403, -0.2399],\n+        [ 0.0839,  0.2152, -0.3822, -0.1668, -0.7886]\n+    ]),\n+    \"mlcd-vit-bigG-patch14-448\": torch.tensor([\n+        [-0.8978, -0.1181,  0.4769,  0.4761, -0.5779],\n+        [ 0.2640, -2.6150,  0.4853,  0.5743, -1.1003],\n+        [ 0.3314, -0.3328, -0.4305, -0.1874, -0.7701],\n+        [-1.5174, -1.0238, -1.1854,  0.1749, -0.8786],\n+        [ 0.2323, -0.8346, -0.9680, -0.2951,  0.0867],\n+    ]),\n+}\n+# fmt: on\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # Vision embeddings\n+    r\"conv1.weight\":                                                r\"vision_model.embeddings.patch_embedding.weight\",\n+    r\"class_embedding\":                                             r\"vision_model.embeddings.class_embedding\",\n+    r\"vision_rotary_embedding\":                                     r\"vision_model.vision_rotary_embedding\",\n+    r\"class_pos_emb\":                                               r\"vision_model.class_pos_emb\",\n+    # Vision encoder\n+    r\"transformer.resblocks_(\\d+).ln_1.weight\":                     r\"vision_model.encoder.layers.\\1.layer_norm1.weight\",\n+    r\"transformer.resblocks_(\\d+).ln_1.bias\":                       r\"vision_model.encoder.layers.\\1.layer_norm1.bias\",\n+    r\"transformer.resblocks_(\\d+).ln_2.weight\":                     r\"vision_model.encoder.layers.\\1.layer_norm2.weight\",\n+    r\"transformer.resblocks_(\\d+).ln_2.bias\":                       r\"vision_model.encoder.layers.\\1.layer_norm2.bias\",\n+    r\"transformer.resblocks_(\\d+).mlp.c_fc.weight\":                 r\"vision_model.encoder.layers.\\1.mlp.fc1.weight\",\n+    r\"transformer.resblocks_(\\d+).mlp.c_fc.bias\":                   r\"vision_model.encoder.layers.\\1.mlp.fc1.bias\",\n+    r\"transformer.resblocks_(\\d+).mlp.c_proj.weight\":               r\"vision_model.encoder.layers.\\1.mlp.fc2.weight\",\n+    r\"transformer.resblocks_(\\d+).mlp.c_proj.bias\":                 r\"vision_model.encoder.layers.\\1.mlp.fc2.bias\",\n+    r\"transformer.resblocks_(\\d+).attn.(q|k|v|out)_proj.weight\":    r\"vision_model.encoder.layers.\\1.self_attn.\\2_proj.weight\",\n+    r\"transformer.resblocks_(\\d+).attn.(q|k|v|out)_proj.bias\":      r\"vision_model.encoder.layers.\\1.self_attn.\\2_proj.bias\",\n+    # Vision norm\n+    r\"ln_post.weight\":                                              r\"vision_model.post_layernorm.weight\",\n+    r\"ln_post.bias\":                                                r\"vision_model.post_layernorm.bias\",\n+    r\"ln_pre.weight\":                                               r\"vision_model.pre_layernorm.weight\",\n+    r\"ln_pre.bias\":                                                 r\"vision_model.pre_layernorm.bias\",\n+}\n+# fmt: on\n+\n+\n+# --------------------------------------------------------------------------------------------\n+# Model objects: configuration, image processor\n+# --------------------------------------------------------------------------------------------\n+\n+\n+def get_mlcd_config(model_name: str) -> MLCDVisionConfig:\n+    \"\"\"\n+    Create a configuration for the MLCD model based on the model name.\n+    \"\"\"\n+    assert model_name in COMMON_CONFIG_PARAMS, f\"Model {model_name} not found in the list of COMMON_CONFIG_PARAMS.\"\n+    config_params = COMMON_CONFIG_PARAMS[model_name]\n+    config = MLCDVisionConfig(\n+        hidden_size=config_params[\"hidden_size\"],\n+        image_size=config_params[\"image_size\"],\n+        intermediate_size=config_params[\"intermediate_size\"],\n+        num_attention_heads=config_params[\"num_attention_heads\"],\n+        num_hidden_layers=config_params[\"num_hidden_layers\"],\n+        patch_size=config_params[\"patch_size\"],\n+        projection_dim=config_params[\"projection_dim\"],\n+    )\n+    return config\n+\n+\n+def get_mlcd_image_processor(model_name: str) -> CLIPImageProcessor:\n+    \"\"\"\n+    Create an image processor for the MLCD model based on the model name.\n+    \"\"\"\n+    assert model_name in COMMON_CONFIG_PARAMS, f\"Model {model_name} not found in the list of COMMON_CONFIG_PARAMS.\"\n+    config_params = COMMON_CONFIG_PARAMS[model_name]\n+    image_processor = CLIPImageProcessor(\n+        do_center_crop=True,\n+        do_normalize=True,\n+        do_resize=True,\n+        feature_extractor_type=\"CLIPFeatureExtractor\",\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n+        resample=3,\n+        size=config_params[\"image_size\"],\n+        crop_size=config_params[\"image_size\"],\n+    )\n+    return image_processor\n+\n+\n+# --------------------------------------------------------------------------------------------\n+# Helper functions for state dict conversion\n+# --------------------------------------------------------------------------------------------\n+\n+\n+def flatten_nested_dict(params: dict, parent_key: str = \"\", sep: str = \".\") -> dict:\n+    \"\"\"\n+    Flatten a nested original checkpoint dictionary into a flat dictionary.\n+    \"\"\"\n+    items = []\n+    for k, v in params.items():\n+        new_key = parent_key + sep + k if parent_key else k\n+        if isinstance(v, collections.abc.MutableMapping):\n+            items.extend(flatten_nested_dict(v, new_key, sep=sep).items())\n+        else:\n+            items.append((new_key, v))\n+    return dict(items)\n+\n+\n+def split_resblocks_layers(state_dict: dict) -> dict:\n+    \"\"\"\n+    Split the resblocks weight into layers. In some cases they are concatenated in\n+    the original checkpoints.\n+    \"\"\"\n+    # Make shallow copy\n+    state_dict = state_dict.copy()\n+    # Split resblocks weight into layers\n+    keys = list(state_dict.keys())\n+    for key in keys:\n+        if \".resblocks.\" in key:\n+            weight = state_dict.pop(key)\n+            for i, weight_i in enumerate(weight):\n+                new_name = key.replace(\"resblocks\", f\"resblocks_{i}\")\n+                state_dict[new_name] = weight_i\n+    return state_dict\n+\n+\n+def chunk_qkv_for_attn(state_dict: dict) -> dict:\n+    \"\"\"\n+    Chunk the q/k/v weights and biases for the attention layers.\n+    \"\"\"\n+    # Make shallow copy\n+    state_dict = state_dict.copy()\n+    # Read and process q/k/v weights and biases\n+    keys = list(state_dict.keys())\n+    for key in keys:\n+        if \".in_proj.\" in key:\n+            weight = state_dict.pop(key)\n+            qkv_weights = weight.chunk(3, dim=0)\n+            for name, weight_i in zip([\"q_proj\", \"k_proj\", \"v_proj\"], qkv_weights):\n+                new_name = key.replace(\"in_proj\", name)\n+                state_dict[new_name] = weight_i\n+    return state_dict\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: list) -> dict:\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+# --------------------------------------------------------------------------------------------\n+# Convert model\n+# --------------------------------------------------------------------------------------------\n+\n+\n+@torch.no_grad()\n+def convert_mlcd_checkpoint(model_name, input_dir, output_dir, verify_hidden_state=True, push_to_hub=False):\n+    \"\"\"\n+    Copy/paste/tweak model's weights to our MLCD structure.\n+    \"\"\"\n+\n+    # Define MLCD configuration\n+    config = get_mlcd_config(model_name)\n+\n+    checkpoint = MODEL_NAME_TO_CHECKPOINT_PATH[model_name]\n+    checkpoint_path = os.path.join(input_dir, checkpoint)\n+    assert os.path.exists(checkpoint_path), f\"Checkpoint path ({checkpoint_path}) not found.\"\n+\n+    # Load original checkpoint\n+    print(f\"Loading checkpoint from {checkpoint_path}...\")\n+    state_dict = torch.load(checkpoint_path, \"cpu\")\n+\n+    # Flatten nested dictionary\n+    print(\"Flattening nested dictionary...\")\n+    state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n+    if \"positional_embedding\" in state_dict:\n+        state_dict.pop(\"positional_embedding\")\n+    state_dict = flatten_nested_dict(state_dict)\n+    state_dict = split_resblocks_layers(state_dict)\n+    state_dict = chunk_qkv_for_attn(state_dict)\n+\n+    # Rename and transform weights\n+    print(\"Renaming and transforming weights...\")\n+    original_keys = list(state_dict.keys())\n+    hf_keys = convert_old_keys_to_new_keys(original_keys)\n+    new_state_dict = {}\n+    for original_key in original_keys:\n+        new_key = hf_keys[original_key]\n+        parameter = state_dict.pop(original_key)\n+        new_state_dict[new_key] = torch.from_numpy(parameter)\n+\n+    # load HuggingFace model\n+    print(\"Loading HuggingFace model...\")\n+    model = MLCDVisionModel(config).eval()\n+    model.load_state_dict(new_state_dict)\n+\n+    # Create processor\n+    print(\"Creating processor...\")\n+    image_processor = get_mlcd_image_processor(model_name)\n+\n+    # Verify hidden state\n+    if verify_hidden_state:\n+        print(\"Verifying hidden state for {model_name}...\")\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+        pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n+        last_hidden_state = model(pixel_values, output_hidden_states=True).last_hidden_state[0, :5, :5]\n+        expected_hidden_state = EXPECTED_OUTPUTS[model_name]\n+        np.testing.assert_allclose(last_hidden_state.cpu().numpy(), expected_hidden_state.numpy(), atol=1e-4)\n+\n+    # Save model\n+    if output_dir is not None:\n+        dst_dir = os.path.join(output_dir, model_name)\n+        print(f\"Saving model {model_name} to {dst_dir}...\")\n+        model.save_pretrained(dst_dir)\n+        print(f\"Saving processor to {dst_dir}...\")\n+        image_processor.save_pretrained(dst_dir)\n+\n+    if push_to_hub:\n+        print(f\"Pushing model and processor for {model_name} to the HuggingFace Hub...\")\n+        model.push_to_hub(f\"deepglint-hf/{model_name}\", private=True)\n+        image_processor.push_to_hub(f\"deepglint-hf/{model_name}\", private=True)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"mlcd-vit-bigG-patch14-448\",\n+        type=str,\n+        choices=MODEL_NAME_TO_CHECKPOINT_PATH.keys(),\n+        help=\"Name of the model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--input_dir\",\n+        default=\"mlcd/original\",\n+        help=\"Location of MLCD original weights\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=\"mlcd/checkpoint\",\n+        help=\"Location to write HF model and processor\",\n+    )\n+    parser.add_argument(\n+        \"--verify_hidden_state\",\n+        action=\"store_true\",\n+        help=\"Whether to verify hidden_state against the original implementation.\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ðŸ¤— hub.\"\n+    )\n+\n+    args = parser.parse_args()\n+    convert_mlcd_checkpoint(\n+        args.model_name, args.input_dir, args.output_dir, args.verify_hidden_state, args.push_to_hub\n+    )"
        },
        {
            "sha": "ec8524baac329beda7e7bfcb0e4220cae3215cab",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "added",
            "additions": 679,
            "deletions": 0,
            "changes": 679,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -0,0 +1,679 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/mlcd/modular_mlcd.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_mlcd.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Callable, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    logging,\n+    torch_int,\n+)\n+from .configuration_mlcd import MLCDVisionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class MLCDMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class MLCDRotaryEmbedding(nn.Module):\n+    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n+        super().__init__()\n+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, num_patches_height: int, num_patches_width: int) -> torch.Tensor:\n+        \"\"\"\n+        Calculate the Rotary Position Embedding (RoPE) for MLCDVisionModel based on the grid size.\n+\n+        Args:\n+            num_patches_height (int): Number of patches in the height dimension.\n+            num_patches_width (int): Number of patches in the width dimension.\n+\n+        Returns:\n+            torch.Tensor: Rotary positional embeddings for the given grid size.\n+        \"\"\"\n+        # Generate position IDs for height and width dimensions\n+        hpos_ids = (\n+            torch.arange(num_patches_height, device=self.inv_freq.device).unsqueeze(1).expand(-1, num_patches_width)\n+        )\n+        wpos_ids = (\n+            torch.arange(num_patches_width, device=self.inv_freq.device).unsqueeze(0).expand(num_patches_height, -1)\n+        )\n+\n+        # Flatten and stack the position IDs\n+        pos_ids = torch.stack([hpos_ids.flatten(), wpos_ids.flatten()], dim=-1)\n+\n+        # Generate the full rotary positional embeddings for the maximum grid size\n+        max_grid_size = max(num_patches_height, num_patches_width)\n+        seq = torch.arange(max_grid_size, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        rotary_pos_emb_full = torch.outer(seq, self.inv_freq)\n+\n+        # Select and flatten the embeddings based on the position IDs\n+        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n+\n+        return rotary_pos_emb\n+\n+\n+class MLCDVisionEmbeddings(nn.Module):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.image_size = config.image_size\n+        self.patch_size = config.patch_size\n+\n+        self.class_embedding = nn.Parameter(torch.randn(self.embed_dim))\n+\n+        self.patch_embedding = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=self.embed_dim,\n+            kernel_size=self.patch_size,\n+            stride=self.patch_size,\n+            bias=False,\n+        )\n+\n+        self.num_patches = (self.image_size // self.patch_size) ** 2\n+        self.num_positions = self.num_patches + 1\n+        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embedding(self.position_ids)\n+\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n+        batch_size = pixel_values.shape[0]\n+        target_dtype = self.patch_embedding.weight.dtype\n+        # patch_embeds -> shape = [batch, width, grid, grid]\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))\n+        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n+\n+        class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n+        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n+\n+        return embeddings\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def apply_rotary_pos_emb_vision(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    orig_q_dtype = q.dtype\n+    orig_k_dtype = k.dtype\n+    q, k = q.float(), k.float()\n+    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    q_embed = q_embed.to(orig_q_dtype)\n+    k_embed = k_embed.to(orig_k_dtype)\n+    return q_embed, k_embed\n+\n+\n+class MLCDAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\n+    Multi-headed attention with RoPE. Refer to papers:\n+        - Attention is all you need:\n+            https://arxiv.org/abs/1706.03762\n+        - RoFormer: Enhanced Transformer with Rotary Position Embedding:\n+            https://arxiv.org/abs/2104.09864\n+    \"\"\"\n+\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.num_key_value_groups = config.num_key_value_groups\n+        self.is_causal = False\n+\n+    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n+        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+        batch_size, seq_length = hidden_states.shape[:-1]\n+\n+        # Each of shape: [batch_size, seq_length, num_heads, head_dim]\n+        query_states = self.q_proj(hidden_states).reshape((batch_size, seq_length, self.num_heads, self.head_dim))\n+        key_states = self.k_proj(hidden_states).reshape((batch_size, seq_length, self.num_heads, self.head_dim))\n+        value_states = self.v_proj(hidden_states).reshape((batch_size, seq_length, self.num_heads, self.head_dim))\n+\n+        # Apply positional embeddings\n+        cos = position_embeddings[0].unsqueeze(0).float()\n+        sin = position_embeddings[1].unsqueeze(0).float()\n+        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n+\n+        # Each of shape: [batch_size, num_heads, seq_length, head_dim]\n+        query_states = query_states.permute(0, 2, 1, 3).contiguous()\n+        key_states = key_states.permute(0, 2, 1, 3).contiguous()\n+        value_states = value_states.permute(0, 2, 1, 3).contiguous()\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scale,\n+            is_causal=self.is_causal,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.permute(1, 0, 2, 3).contiguous()  # [seq_length, batch_size, num_heads, head_dim]\n+        attn_output = attn_output.view(seq_length, batch_size, -1)  # [seq_length, batch_size, embedding_dim]\n+        attn_output = self.out_proj(attn_output)\n+        attn_output = attn_output.permute(1, 0, 2).contiguous()  # [batch_size, seq_length, embedding_dim]\n+        return attn_output, attn_weights\n+\n+\n+class MLCDEncoderLayer(nn.Module):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.self_attn = MLCDAttention(config)\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = MLCDMLP(config)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.FloatTensor]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`):\n+                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n+                Represents the hidden states from the previous layer or the input embeddings.\n+            position_embeddings (`Tuple[torch.Tensor, torch.Tensor]`):\n+                A tuple of two tensors, each of shape `(batch, seq_len, embed_dim)`.\n+                Represents absolute positional embeddings for the query and key in the attention mechanism.\n+            attention_mask (`torch.FloatTensor`):\n+                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n+            output_attentions (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class MLCDEncoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`MLCDEncoderLayer`].\n+\n+    Args:\n+        config: MLCDVisionConfig\n+    \"\"\"\n+\n+    def __init__(self, config: MLCDVisionConfig):\n+        \"\"\"Overwrite dummy `MLCDConfig` to `MLCDVisionConfig`.\"\"\"\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([MLCDEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        inputs_embeds: torch.FloatTensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            position_embeddings (`Tuple[torch.Tensor, torch.Tensor]`):\n+                A tuple of two tensors, each of shape `(batch, seq_len, embed_dim)`.\n+                Represents absolute positional embeddings for the query and key in the attention mechanism.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        hidden_states = inputs_embeds\n+        for idx, encoder_layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    position_embeddings,\n+                    attention_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states=hidden_states,\n+                    position_embeddings=position_embeddings,\n+                    attention_mask=attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n+        )\n+\n+\n+MLCD_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+class MLCDVisionTransformer(nn.Module):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = MLCDVisionEmbeddings(config)\n+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.encoder = MLCDEncoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n+        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n+\n+    @add_start_docstrings_to_model_forward(MLCD_VISION_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n+        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n+        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n+        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n+        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n+\n+        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.pre_layrnorm(hidden_states)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            position_embeddings=position_embeddings,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        if not return_dict:\n+            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class MLCDPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = MLCDVisionConfig\n+    base_model_prefix = \"mlcd\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_factor\n+        if isinstance(module, MLCDVisionEmbeddings):\n+            factor = self.config.initializer_factor\n+            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+        elif isinstance(module, MLCDAttention):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            out_proj_std = (module.embed_dim**-0.5) * factor\n+            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+        elif isinstance(module, MLCDMLP):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n+            nn.init.normal_(module.fc1.weight, std=fc_std)\n+            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+        elif isinstance(module, MLCDVisionTransformer):\n+            factor = self.config.initializer_factor\n+            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n+            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Linear) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+\n+MLCD_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`MLCDVisionConfig`]):\n+            Model configuration class with all the parameters of the vision encoder. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare MLCD vision encoder outputting raw hidden-states without any specific head on top.\",\n+    MLCD_START_DOCSTRING,\n+)\n+class MLCDVisionModel(MLCDPreTrainedModel):\n+    config_class = MLCDVisionConfig\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\"MLCDEncoderLayer\"]\n+\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__(config)\n+        self.vision_model = MLCDVisionTransformer(config)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_model.embeddings.patch_embedding\n+\n+    @add_start_docstrings_to_model_forward(MLCD_VISION_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> import requests\n+        >>> from PIL import Image\n+        >>> from transformers import AutoProcessor, MLCDVisionModel\n+        >>> model = MLCDVisionModel.from_pretrained(\"DeepGlint-AI/mlcd-vit-bigG-patch14-448\")\n+        >>> processor = AutoProcessor.from_pretrained(\"DeepGlint-AI/mlcd-vit-bigG-patch14-448\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs, output_attentions=True)\n+\n+        >>> features = outputs.last_hidden_state\n+        >>> print(f\"Extracted features shape: {features.shape}\")\n+        >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n+        >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n+        ```\"\"\"\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        return self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+\n+__all__ = [\"MLCDPreTrainedModel\", \"MLCDVisionModel\"]"
        },
        {
            "sha": "c8de2a31cb4fdbef61a5d85e9d2a728ff729fc21",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "added",
            "additions": 596,
            "deletions": 0,
            "changes": 596,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -0,0 +1,596 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Callable, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    BaseModelOutputWithPooling,\n+)\n+from ...modeling_utils import (\n+    ALL_ATTENTION_FUNCTIONS,\n+    PreTrainedModel,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+)\n+from ..clip.modeling_clip import (\n+    CLIPMLP,\n+    CLIPAttention,\n+    CLIPEncoder,\n+    CLIPEncoderLayer,\n+    CLIPVisionEmbeddings,\n+    CLIPVisionModel,\n+    CLIPVisionTransformer,\n+)\n+from ..llama.modeling_llama import eager_attention_forward\n+from ..qwen2_vl.modeling_qwen2_vl import (\n+    VisionRotaryEmbedding,\n+    apply_rotary_pos_emb_vision,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class MLCDVisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MLCDVisionModel`]. It is used to instantiate a MLCD\n+    vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the vision encoder of the MLCD\n+    [DeepGlint-AI/mlcd-vit-bigG-patch14-336](https://huggingface.co/DeepGlint-AI/mlcd-vit-bigG-patch14-336) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1664):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 8192):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        projection_dim (`int`, *optional*, defaults to 1024):\n+            Dimensionality of text and vision projection layers.\n+        num_hidden_layers (`int`, *optional*, defaults to 48):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        image_size (`int`, *optional*, defaults to 336):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_factor (`float`, *optional*, defaults to 1.0):\n+            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n+            testing).\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MLCDVisionConfig, MLCDVisionModel\n+\n+    >>> # Initializing a MLCDVisionConfig with DeepGlint-AI/mlcd-vit-bigG-patch14-336 style configuration\n+    >>> configuration = MLCDVisionConfig()\n+\n+    >>> # Initializing a MLCDVisionModel (with random weights) from the DeepGlint-AI/mlcd-vit-bigG-patch14-336 style configuration\n+    >>> model = MLCDVisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"mlcd_vision_model\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1664,\n+        intermediate_size=8192,\n+        num_hidden_layers=48,\n+        num_attention_heads=16,\n+        num_key_value_groups=1,\n+        num_channels=3,\n+        image_size=336,\n+        patch_size=14,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-5,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        initializer_factor=1.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_groups = num_key_value_groups\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.image_size = image_size\n+        self.initializer_range = initializer_range\n+        self.initializer_factor = initializer_factor\n+        self.attention_dropout = attention_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.hidden_act = hidden_act\n+\n+\n+class MLCDMLP(CLIPMLP):\n+    pass\n+\n+\n+class MLCDRotaryEmbedding(VisionRotaryEmbedding):\n+    def forward(self, num_patches_height: int, num_patches_width: int) -> torch.Tensor:\n+        \"\"\"\n+        Calculate the Rotary Position Embedding (RoPE) for MLCDVisionModel based on the grid size.\n+\n+        Args:\n+            num_patches_height (int): Number of patches in the height dimension.\n+            num_patches_width (int): Number of patches in the width dimension.\n+\n+        Returns:\n+            torch.Tensor: Rotary positional embeddings for the given grid size.\n+        \"\"\"\n+        # Generate position IDs for height and width dimensions\n+        hpos_ids = (\n+            torch.arange(num_patches_height, device=self.inv_freq.device).unsqueeze(1).expand(-1, num_patches_width)\n+        )\n+        wpos_ids = (\n+            torch.arange(num_patches_width, device=self.inv_freq.device).unsqueeze(0).expand(num_patches_height, -1)\n+        )\n+\n+        # Flatten and stack the position IDs\n+        pos_ids = torch.stack([hpos_ids.flatten(), wpos_ids.flatten()], dim=-1)\n+\n+        # Generate the full rotary positional embeddings for the maximum grid size\n+        max_grid_size = max(num_patches_height, num_patches_width)\n+        seq = torch.arange(max_grid_size, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        rotary_pos_emb_full = torch.outer(seq, self.inv_freq)\n+\n+        # Select and flatten the embeddings based on the position IDs\n+        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n+\n+        return rotary_pos_emb\n+\n+\n+class MLCDVisionEmbeddings(CLIPVisionEmbeddings):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__(config)\n+        del self.position_embedding\n+\n+    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:\n+        batch_size = pixel_values.shape[0]\n+        target_dtype = self.patch_embedding.weight.dtype\n+        # patch_embeds -> shape = [batch, width, grid, grid]\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))\n+        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n+\n+        class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n+        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n+\n+        return embeddings\n+\n+\n+class MLCDAttention(CLIPAttention):\n+    \"\"\"Multi-headed attention with RoPE. Refer to papers:\n+    - Attention is all you need:\n+        https://arxiv.org/abs/1706.03762\n+    - RoFormer: Enhanced Transformer with Rotary Position Embedding:\n+        https://arxiv.org/abs/2104.09864\n+    \"\"\"\n+\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__(config)\n+        self.num_key_value_groups = config.num_key_value_groups\n+        self.is_causal = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        batch_size, seq_length = hidden_states.shape[:-1]\n+\n+        # Each of shape: [batch_size, seq_length, num_heads, head_dim]\n+        query_states = self.q_proj(hidden_states).reshape((batch_size, seq_length, self.num_heads, self.head_dim))\n+        key_states = self.k_proj(hidden_states).reshape((batch_size, seq_length, self.num_heads, self.head_dim))\n+        value_states = self.v_proj(hidden_states).reshape((batch_size, seq_length, self.num_heads, self.head_dim))\n+\n+        # Apply positional embeddings\n+        cos = position_embeddings[0].unsqueeze(0).float()\n+        sin = position_embeddings[1].unsqueeze(0).float()\n+        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n+\n+        # Each of shape: [batch_size, num_heads, seq_length, head_dim]\n+        query_states = query_states.permute(0, 2, 1, 3).contiguous()\n+        key_states = key_states.permute(0, 2, 1, 3).contiguous()\n+        value_states = value_states.permute(0, 2, 1, 3).contiguous()\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=self.scale,\n+            is_causal=self.is_causal,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.permute(1, 0, 2, 3).contiguous()  # [seq_length, batch_size, num_heads, head_dim]\n+        attn_output = attn_output.view(seq_length, batch_size, -1)  # [seq_length, batch_size, embedding_dim]\n+        attn_output = self.out_proj(attn_output)\n+        attn_output = attn_output.permute(1, 0, 2).contiguous()  # [batch_size, seq_length, embedding_dim]\n+        return attn_output, attn_weights\n+\n+\n+class MLCDEncoderLayer(CLIPEncoderLayer):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__(config)\n+        self.self_attn = MLCDAttention(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.FloatTensor]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`):\n+                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n+                Represents the hidden states from the previous layer or the input embeddings.\n+            position_embeddings (`Tuple[torch.Tensor, torch.Tensor]`):\n+                A tuple of two tensors, each of shape `(batch, seq_len, embed_dim)`.\n+                Represents absolute positional embeddings for the query and key in the attention mechanism.\n+            attention_mask (`torch.FloatTensor`):\n+                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n+            output_attentions (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class MLCDEncoder(CLIPEncoder):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`MLCDEncoderLayer`].\n+\n+    Args:\n+        config: MLCDVisionConfig\n+    \"\"\"\n+\n+    def __init__(self, config: MLCDVisionConfig):\n+        \"\"\"Overwrite dummy `MLCDConfig` to `MLCDVisionConfig`.\"\"\"\n+        super().__init__(config)\n+\n+    def forward(\n+        self,\n+        inputs_embeds: torch.FloatTensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            position_embeddings (`Tuple[torch.Tensor, torch.Tensor]`):\n+                A tuple of two tensors, each of shape `(batch, seq_len, embed_dim)`.\n+                Represents absolute positional embeddings for the query and key in the attention mechanism.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        hidden_states = inputs_embeds\n+        for idx, encoder_layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    position_embeddings,\n+                    attention_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states=hidden_states,\n+                    position_embeddings=position_embeddings,\n+                    attention_mask=attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n+        )\n+\n+\n+MLCD_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+class MLCDVisionTransformer(CLIPVisionTransformer):\n+    def __init__(self, config: MLCDVisionConfig):\n+        super().__init__(config)\n+        self.vision_rotary_embedding = MLCDRotaryEmbedding(config.hidden_size // config.num_attention_heads // 2)\n+        self.class_pos_emb = nn.Parameter(torch.randn(1, config.hidden_size // config.num_attention_heads // 2))\n+\n+    @add_start_docstrings_to_model_forward(MLCD_VISION_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        num_patches_height = pixel_values.shape[-2] // self.config.patch_size\n+        num_patches_width = pixel_values.shape[-1] // self.config.patch_size\n+        rotary_pos_emb = self.vision_rotary_embedding(num_patches_height, num_patches_width)\n+        rotary_pos_emb = rotary_pos_emb.to(self.class_pos_emb.device)\n+        rotary_pos_emb = torch.cat([self.class_pos_emb, rotary_pos_emb], dim=0)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+        position_embeddings = (emb.cos(), emb.sin())\n+\n+        hidden_states = self.embeddings(pixel_values)\n+        hidden_states = self.pre_layrnorm(hidden_states)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            position_embeddings=position_embeddings,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        if not return_dict:\n+            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+MLCD_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`MLCDVisionConfig`]):\n+            Model configuration class with all the parameters of the vision encoder. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+class MLCDPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = MLCDVisionConfig\n+    base_model_prefix = \"mlcd\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_factor\n+        if isinstance(module, MLCDVisionEmbeddings):\n+            factor = self.config.initializer_factor\n+            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+        elif isinstance(module, MLCDAttention):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            out_proj_std = (module.embed_dim**-0.5) * factor\n+            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+        elif isinstance(module, MLCDMLP):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n+            nn.init.normal_(module.fc1.weight, std=fc_std)\n+            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+        elif isinstance(module, MLCDVisionTransformer):\n+            factor = self.config.initializer_factor\n+            pos_emb_std = (module.config.hidden_size // module.config.num_attention_heads // 2) ** -0.5 * factor\n+            nn.init.normal_(module.class_pos_emb, mean=0.0, std=pos_emb_std)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Linear) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+\n+@add_start_docstrings(\n+    \"The bare MLCD vision encoder outputting raw hidden-states without any specific head on top.\",\n+    MLCD_START_DOCSTRING,\n+)\n+class MLCDVisionModel(CLIPVisionModel):\n+    @add_start_docstrings_to_model_forward(MLCD_VISION_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        ```python\n+        >>> import requests\n+        >>> from PIL import Image\n+        >>> from transformers import AutoProcessor, MLCDVisionModel\n+        >>> model = MLCDVisionModel.from_pretrained(\"DeepGlint-AI/mlcd-vit-bigG-patch14-448\")\n+        >>> processor = AutoProcessor.from_pretrained(\"DeepGlint-AI/mlcd-vit-bigG-patch14-448\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs, output_attentions=True)\n+\n+        >>> features = outputs.last_hidden_state\n+        >>> print(f\"Extracted features shape: {features.shape}\")\n+        >>> print(f\"Number of attention layers: {len(outputs.attentions)}\")\n+        >>> print(f\"Attention shape: {outputs.attentions[0].shape}\")\n+        ```\"\"\"\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        return self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+\n+__all__ = [\n+    \"MLCDVisionConfig\",\n+    \"MLCDPreTrainedModel\",\n+    \"MLCDVisionModel\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/mlcd/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/tests%2Fmodels%2Fmlcd%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/tests%2Fmodels%2Fmlcd%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmlcd%2F__init__.py?ref=6f7ea1cf006414231610906e7710a84281960c24"
        },
        {
            "sha": "9f864ebaf23476385f6f8ad1af961625e540881b",
            "filename": "tests/models/mlcd/test_modeling_mlcd.py",
            "status": "added",
            "additions": 221,
            "deletions": 0,
            "changes": 221,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -0,0 +1,221 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch MLCD model.\"\"\"\n+\n+import unittest\n+\n+import requests\n+from PIL import Image\n+\n+from transformers import (\n+    AutoProcessor,\n+    MLCDVisionConfig,\n+    MLCDVisionModel,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class MLCDVisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=12,\n+        image_size=30,\n+        patch_size=2,\n+        num_channels=3,\n+        is_training=True,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        dropout=0.1,\n+        attention_dropout=0.1,\n+        initializer_range=0.02,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.scope = scope\n+\n+        # in MLCD, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + 1\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        return MLCDVisionConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            initializer_range=self.initializer_range,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = MLCDVisionModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n+        image_size = (self.image_size, self.image_size)\n+        patch_size = (self.patch_size, self.patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n+        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class MLCDVisionModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `MLCDVisionModel`.\n+    \"\"\"\n+\n+    all_model_classes = (MLCDVisionModel,) if is_torch_available() else ()\n+    test_pruning = False\n+    test_head_masking = False\n+    test_torchscript = False\n+    test_resize_embeddings = False\n+    test_torch_exportable = True\n+\n+    def setUp(self):\n+        self.model_tester = MLCDVisionModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=MLCDVisionConfig, has_text_modality=False)\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (torch.nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, torch.nn.Linear))\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad and \"class_pos_emb\" not in name:\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+\n+@require_torch\n+class MLCDVisionModelIntegrationTest(unittest.TestCase):\n+    @slow\n+    def test_inference(self):\n+        model_name = \"DeepGlint-AI/mlcd-vit-bigG-patch14-448\"\n+        model = MLCDVisionModel.from_pretrained(model_name).to(torch_device)\n+        processor = AutoProcessor.from_pretrained(model_name)\n+\n+        # process single image\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+        inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        # move inputs to the same device as the model\n+        inputs = {k: v.to(torch_device) for k, v in inputs.items()}\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs, output_attentions=True)\n+\n+        last_hidden_state = outputs.last_hidden_state\n+        last_attention = outputs.attentions[-1]\n+\n+        # verify the shapes of last_hidden_state and last_attention\n+        self.assertEqual(\n+            last_hidden_state.shape,\n+            torch.Size([1, 1025, 1664]),\n+        )\n+        self.assertEqual(\n+            last_attention.shape,\n+            torch.Size([1, 16, 1025, 1025]),\n+        )\n+\n+        # verify the values of last_hidden_state and last_attention\n+        # fmt: off\n+        expected_partial_5x5_last_hidden_state = torch.tensor(\n+            [\n+                [-0.8978, -0.1181,  0.4769,  0.4761, -0.5779],\n+                [ 0.2640, -2.6150,  0.4853,  0.5743, -1.1003],\n+                [ 0.3314, -0.3328, -0.4305, -0.1874, -0.7701],\n+                [-1.5174, -1.0238, -1.1854,  0.1749, -0.8786],\n+                [ 0.2323, -0.8346, -0.9680, -0.2951,  0.0867],\n+            ]\n+        ).to(torch_device)\n+\n+        expected_partial_5x5_last_attention = torch.tensor(\n+            [\n+                [2.0930e-01, 6.3073e-05, 1.4717e-03, 2.6881e-05, 3.0513e-03],\n+                [1.5828e-04, 2.1056e-03, 4.6784e-04, 1.8276e-03, 5.3233e-04],\n+                [5.7824e-04, 1.1446e-03, 1.3854e-03, 1.1775e-03, 1.2750e-03],\n+                [9.6343e-05, 1.6365e-03, 2.9066e-04, 3.1089e-03, 2.0607e-04],\n+                [6.2688e-04, 1.1656e-03, 1.5030e-03, 8.2819e-04, 2.6992e-03],\n+            ]\n+        ).to(torch_device)\n+        # fmt: on\n+\n+        torch.testing.assert_close(\n+            last_hidden_state[0, :5, :5], expected_partial_5x5_last_hidden_state, rtol=1e-3, atol=1e-3\n+        )\n+        torch.testing.assert_close(\n+            last_attention[0, 0, :5, :5], expected_partial_5x5_last_attention, rtol=1e-4, atol=1e-4\n+        )"
        },
        {
            "sha": "09f21e0e2f4c0d5cd12c9348d3d3912c69745c05",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6f7ea1cf006414231610906e7710a84281960c24/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6f7ea1cf006414231610906e7710a84281960c24/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=6f7ea1cf006414231610906e7710a84281960c24",
            "patch": "@@ -383,6 +383,7 @@\n     \"MegatronBertConfig\",\n     \"MegatronBertForPreTraining\",\n     \"MegatronBertModel\",\n+    \"MLCDVisionConfig\",\n     \"MobileBertConfig\",\n     \"MobileBertModel\",\n     \"MobileBertTokenizerFast\","
        }
    ],
    "stats": {
        "total": 2066,
        "additions": 2066,
        "deletions": 0
    }
}