{
    "author": "OnTheThirdDay",
    "message": "Fix post process function called in the instance segmentation example of mask2former (#34588)\n\n* Fix post process function called in the instance segmentation example of mask2former\r\n\r\n* fix description and additional notes for post_process_instance_segmentation of maskformers\r\n\r\n* remove white space in maskformers post_process_instance_segmentation doc\r\n\r\n* change image.size[::-1] to height and width for clarity in segmentation examples",
    "sha": "427b62ed1a4649539988225b841d158187ab4850",
    "files": [
        {
            "sha": "6a1545e123297c2e2a189dc9fb8e1abfdf4dd609",
            "filename": "docs/source/en/model_doc/rt_detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/427b62ed1a4649539988225b841d158187ab4850/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/427b62ed1a4649539988225b841d158187ab4850/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md?ref=427b62ed1a4649539988225b841d158187ab4850",
            "patch": "@@ -57,7 +57,7 @@ Initially, an image is processed using a pre-trained convolutional neural networ\n >>> with torch.no_grad():\n ...     outputs = model(**inputs)\n \n->>> results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([image.size[::-1]]), threshold=0.3)\n+>>> results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3)\n \n >>> for result in results:\n ...     for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):"
        },
        {
            "sha": "339d7591523de763f81262b98c68eeff113a8815",
            "filename": "examples/pytorch/instance-segmentation/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/427b62ed1a4649539988225b841d158187ab4850/examples%2Fpytorch%2Finstance-segmentation%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/427b62ed1a4649539988225b841d158187ab4850/examples%2Fpytorch%2Finstance-segmentation%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Finstance-segmentation%2FREADME.md?ref=427b62ed1a4649539988225b841d158187ab4850",
            "patch": "@@ -148,7 +148,7 @@ with torch.no_grad():\n     outputs = model(**inputs)\n \n # Post-process outputs\n-outputs = image_processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])\n+outputs = image_processor.post_process_instance_segmentation(outputs, target_sizes=[(image.height, image.width)])\n \n print(\"Mask shape: \", outputs[0][\"segmentation\"].shape)\n print(\"Mask values: \", outputs[0][\"segmentation\"].unique())"
        },
        {
            "sha": "555ee6e956709f7c7e752eefe7b2bfb4ca5c194b",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=427b62ed1a4649539988225b841d158187ab4850",
            "patch": "@@ -1034,7 +1034,8 @@ def post_process_instance_segmentation(\n     ) -> List[Dict]:\n         \"\"\"\n         Converts the output of [`Mask2FormerForUniversalSegmentationOutput`] into instance segmentation predictions.\n-        Only supports PyTorch.\n+        Only supports PyTorch. If instances could overlap, set either return_coco_annotation or return_binary_maps\n+        to `True` to get the correct segmentation result.\n \n         Args:\n             outputs ([`Mask2FormerForUniversalSegmentation`]):\n@@ -1056,9 +1057,10 @@ def post_process_instance_segmentation(\n                 (one per detected instance).\n         Returns:\n             `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n-            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id` or\n+            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id`, or\n               `List[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to\n-              `True`. Set to `None` if no mask if found above `threshold`.\n+              `True`, or a tensor of shape `(num_instances, height, width)` if return_binary_maps is set to `True`.\n+              Set to `None` if no mask if found above `threshold`.\n             - **segments_info** -- A dictionary that contains additional information on each segment.\n                 - **id** -- An integer representing the `segment_id`.\n                 - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`."
        },
        {
            "sha": "e91d03575451020f5f73bc835da1bac3c7aca4c9",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=427b62ed1a4649539988225b841d158187ab4850",
            "patch": "@@ -2428,8 +2428,8 @@ def forward(\n         >>> masks_queries_logits = outputs.masks_queries_logits\n \n         >>> # Perform post-processing to get instance segmentation map\n-        >>> pred_instance_map = image_processor.post_process_semantic_segmentation(\n-        ...     outputs, target_sizes=[image.size[::-1]]\n+        >>> pred_instance_map = image_processor.post_process_instance_segmentation(\n+        ...     outputs, target_sizes=[(image.height, image.width)]\n         ... )[0]\n         >>> print(pred_instance_map.shape)\n         torch.Size([480, 640])\n@@ -2462,7 +2462,7 @@ def forward(\n \n         >>> # Perform post-processing to get semantic segmentation map\n         >>> pred_semantic_map = image_processor.post_process_semantic_segmentation(\n-        ...     outputs, target_sizes=[image.size[::-1]]\n+        ...     outputs, target_sizes=[(image.height, image.width)]\n         ... )[0]\n         >>> print(pred_semantic_map.shape)\n         torch.Size([512, 683])\n@@ -2496,7 +2496,7 @@ def forward(\n \n         >>> # Perform post-processing to get panoptic segmentation map\n         >>> pred_panoptic_map = image_processor.post_process_panoptic_segmentation(\n-        ...     outputs, target_sizes=[image.size[::-1]]\n+        ...     outputs, target_sizes=[(image.height, image.width)]\n         ... )[0][\"segmentation\"]\n         >>> print(pred_panoptic_map.shape)\n         torch.Size([338, 676])"
        },
        {
            "sha": "f4eb1bb56f4eb9b04eb69ea53a94b79d3f7dce52",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=427b62ed1a4649539988225b841d158187ab4850",
            "patch": "@@ -1080,7 +1080,8 @@ def post_process_instance_segmentation(\n     ) -> List[Dict]:\n         \"\"\"\n         Converts the output of [`MaskFormerForInstanceSegmentationOutput`] into instance segmentation predictions. Only\n-        supports PyTorch.\n+        supports PyTorch. If instances could overlap, set either return_coco_annotation or return_binary_maps\n+        to `True` to get the correct segmentation result.\n \n         Args:\n             outputs ([`MaskFormerForInstanceSegmentation`]):\n@@ -1102,9 +1103,10 @@ def post_process_instance_segmentation(\n                 (one per detected instance).\n         Returns:\n             `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n-            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id` or\n+            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id`, or\n               `List[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to\n-              `True`. Set to `None` if no mask if found above `threshold`.\n+              `True`, or a tensor of shape `(num_instances, height, width)` if return_binary_maps is set to `True`.\n+              Set to `None` if no mask if found above `threshold`.\n             - **segments_info** -- A dictionary that contains additional information on each segment.\n                 - **id** -- An integer representing the `segment_id`.\n                 - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`."
        },
        {
            "sha": "a8398ec9725b3069de9b0477e55edb93e879b384",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=427b62ed1a4649539988225b841d158187ab4850",
            "patch": "@@ -1780,7 +1780,7 @@ def forward(\n \n         >>> # you can pass them to image_processor for postprocessing\n         >>> predicted_semantic_map = image_processor.post_process_semantic_segmentation(\n-        ...     outputs, target_sizes=[image.size[::-1]]\n+        ...     outputs, target_sizes=[(image.height, image.width)]\n         ... )[0]\n \n         >>> # we refer to the demo notebooks for visualization (see \"Resources\" section in the MaskFormer docs)\n@@ -1810,7 +1810,7 @@ def forward(\n         >>> masks_queries_logits = outputs.masks_queries_logits\n \n         >>> # you can pass them to image_processor for postprocessing\n-        >>> result = image_processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n+        >>> result = image_processor.post_process_panoptic_segmentation(outputs, target_sizes=[(image.height, image.width)])[0]\n \n         >>> # we refer to the demo notebooks for visualization (see \"Resources\" section in the MaskFormer docs)\n         >>> predicted_panoptic_map = result[\"segmentation\"]"
        },
        {
            "sha": "e237467c242bbce94c217664fef0e7bbd05ca7c6",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=427b62ed1a4649539988225b841d158187ab4850",
            "patch": "@@ -3161,7 +3161,7 @@ def forward(\n \n         >>> # you can pass them to processor for semantic postprocessing\n         >>> predicted_semantic_map = processor.post_process_semantic_segmentation(\n-        ...     outputs, target_sizes=[image.size[::-1]]\n+        ...     outputs, target_sizes=[(image.height, image.width)]\n         ... )[0]\n         >>> f\"ðŸ‘‰ Semantic Predictions Shape: {list(predicted_semantic_map.shape)}\"\n         'ðŸ‘‰ Semantic Predictions Shape: [512, 683]'\n@@ -3178,7 +3178,7 @@ def forward(\n \n         >>> # you can pass them to processor for instance postprocessing\n         >>> predicted_instance_map = processor.post_process_instance_segmentation(\n-        ...     outputs, target_sizes=[image.size[::-1]]\n+        ...     outputs, target_sizes=[(image.height, image.width)]\n         ... )[0][\"segmentation\"]\n         >>> f\"ðŸ‘‰ Instance Predictions Shape: {list(predicted_instance_map.shape)}\"\n         'ðŸ‘‰ Instance Predictions Shape: [512, 683]'\n@@ -3195,7 +3195,7 @@ def forward(\n \n         >>> # you can pass them to processor for panoptic postprocessing\n         >>> predicted_panoptic_map = processor.post_process_panoptic_segmentation(\n-        ...     outputs, target_sizes=[image.size[::-1]]\n+        ...     outputs, target_sizes=[(image.height, image.width)]\n         ... )[0][\"segmentation\"]\n         >>> f\"ðŸ‘‰ Panoptic Predictions Shape: {list(predicted_panoptic_map.shape)}\"\n         'ðŸ‘‰ Panoptic Predictions Shape: [512, 683]'"
        },
        {
            "sha": "c0f1f24a31781f3923b14d1a12a87e6b03f1bae5",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/427b62ed1a4649539988225b841d158187ab4850/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=427b62ed1a4649539988225b841d158187ab4850",
            "patch": "@@ -962,7 +962,7 @@ def forward(\n \n         >>> inputs = image_processor(images=image_input, prompt_images=image_prompt, prompt_masks=mask_prompt, return_tensors=\"pt\")\n         >>> outputs = model(**inputs)\n-        >>> result = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image_input.size[::-1]])[0]\n+        >>> result = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[(image_input.height, image_input.width)])[0]\n         >>> print(list(result.shape))\n         [170, 297]\n         ```"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 22,
        "deletions": 18
    }
}