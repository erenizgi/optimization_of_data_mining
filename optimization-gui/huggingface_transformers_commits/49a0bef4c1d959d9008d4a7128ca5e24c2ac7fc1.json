{
    "author": "jiqing-feng",
    "message": "enable low-precision pipeline (#31625)\n\n* enable low-precision pipeline\r\n\r\n* fix parameter for ASR\r\n\r\n* reformat\r\n\r\n* fix asr bug\r\n\r\n* fix bug for zero-shot\r\n\r\n* add dtype check\r\n\r\n* rm useless comments\r\n\r\n* add np.float16 check\r\n\r\n* Update src/transformers/pipelines/image_classification.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* Update src/transformers/pipelines/token_classification.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* fix comments\r\n\r\n* fix asr check\r\n\r\n* make fixup\r\n\r\n* No more need for is_torch_available()\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\r\nCo-authored-by: Matt <rocketknight1@gmail.com>",
    "sha": "49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1",
    "files": [
        {
            "sha": "9b82b67820c51b5f1ba49d2147babb6c0fb7793f",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1",
            "patch": "@@ -565,7 +565,10 @@ def postprocess(\n         key = \"logits\" if self.type == \"ctc_with_lm\" else \"tokens\"\n         stride = None\n         for outputs in model_outputs:\n-            items = outputs[key].numpy()\n+            if self.framework == \"pt\" and outputs[key].dtype in (torch.bfloat16, torch.float16):\n+                items = outputs[key].to(torch.float32).numpy()\n+            else:\n+                items = outputs[key].numpy()\n             stride = outputs.get(\"stride\", None)\n             if stride is not None and self.type in {\"ctc\", \"ctc_with_lm\"}:\n                 total_n, left, right = stride"
        },
        {
            "sha": "9256f238148476b4d923c84f884156b4564c93a7",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1",
            "patch": "@@ -19,6 +19,8 @@\n \n     from ..models.auto.modeling_tf_auto import TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES\n if is_torch_available():\n+    import torch\n+\n     from ..models.auto.modeling_auto import MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES\n \n \n@@ -299,7 +301,11 @@ def postprocess(self, all_outputs, aggregation_strategy=AggregationStrategy.NONE\n             ignore_labels = [\"O\"]\n         all_entities = []\n         for model_outputs in all_outputs:\n-            logits = model_outputs[\"logits\"][0].numpy()\n+            if self.framework == \"pt\" and model_outputs[\"logits\"][0].dtype in (torch.bfloat16, torch.float16):\n+                logits = model_outputs[\"logits\"][0].to(torch.float32).numpy()\n+            else:\n+                logits = model_outputs[\"logits\"][0].numpy()\n+\n             sentence = all_outputs[0][\"sentence\"]\n             input_ids = model_outputs[\"input_ids\"][0]\n             offset_mapping = ("
        },
        {
            "sha": "e0608acfeb8a5451f984719d3dd1daef180153c7",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1",
            "patch": "@@ -2143,7 +2143,7 @@ def nested_simplify(obj, decimals=3):\n         return nested_simplify(obj.numpy().tolist())\n     elif isinstance(obj, float):\n         return round(obj, decimals)\n-    elif isinstance(obj, (np.int32, np.float32)):\n+    elif isinstance(obj, (np.int32, np.float32, np.float16)):\n         return nested_simplify(obj.item(), decimals)\n     else:\n         raise Exception(f\"Not supported: {type(obj)}\")"
        },
        {
            "sha": "c12292fc3370d383fd7ccc6054be4713bb9c2dcf",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 42,
            "deletions": 0,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1",
            "patch": "@@ -167,6 +167,48 @@ def test_small_model_pt(self):\n         ):\n             _ = speech_recognizer(waveform, return_timestamps=\"char\")\n \n+    @require_torch\n+    def test_small_model_pt_fp16(self):\n+        speech_recognizer = pipeline(\n+            task=\"automatic-speech-recognition\",\n+            model=\"facebook/s2t-small-mustc-en-fr-st\",\n+            tokenizer=\"facebook/s2t-small-mustc-en-fr-st\",\n+            framework=\"pt\",\n+            torch_dtype=torch.float16,\n+        )\n+        waveform = np.tile(np.arange(1000, dtype=np.float32), 34)\n+        output = speech_recognizer(waveform)\n+        self.assertEqual(output, {\"text\": \"(Applaudissements)\"})\n+        output = speech_recognizer(waveform, chunk_length_s=10)\n+        self.assertEqual(output, {\"text\": \"(Applaudissements)\"})\n+\n+        # Non CTC models cannot use return_timestamps\n+        with self.assertRaisesRegex(\n+            ValueError, \"^We cannot return_timestamps yet on non-CTC models apart from Whisper!$\"\n+        ):\n+            _ = speech_recognizer(waveform, return_timestamps=\"char\")\n+\n+    @require_torch\n+    def test_small_model_pt_bf16(self):\n+        speech_recognizer = pipeline(\n+            task=\"automatic-speech-recognition\",\n+            model=\"facebook/s2t-small-mustc-en-fr-st\",\n+            tokenizer=\"facebook/s2t-small-mustc-en-fr-st\",\n+            framework=\"pt\",\n+            torch_dtype=torch.bfloat16,\n+        )\n+        waveform = np.tile(np.arange(1000, dtype=np.float32), 34)\n+        output = speech_recognizer(waveform)\n+        self.assertEqual(output, {\"text\": \"(Applaudissements)\"})\n+        output = speech_recognizer(waveform, chunk_length_s=10)\n+        self.assertEqual(output, {\"text\": \"(Applaudissements)\"})\n+\n+        # Non CTC models cannot use return_timestamps\n+        with self.assertRaisesRegex(\n+            ValueError, \"^We cannot return_timestamps yet on non-CTC models apart from Whisper!$\"\n+        ):\n+            _ = speech_recognizer(waveform, return_timestamps=\"char\")\n+\n     @slow\n     @require_torch_accelerator\n     def test_whisper_fp16(self):"
        },
        {
            "sha": "5e804bec199ab01e6e29f20f95f1b70de7bf53b1",
            "filename": "tests/pipelines/test_pipelines_token_classification.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_token_classification.py?ref=49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1",
            "patch": "@@ -27,6 +27,7 @@\n from transformers.pipelines import AggregationStrategy, TokenClassificationArgumentHandler\n from transformers.testing_utils import (\n     is_pipeline_test,\n+    is_torch_available,\n     nested_simplify,\n     require_tf,\n     require_torch,\n@@ -38,6 +39,10 @@\n from .test_pipelines_common import ANY\n \n \n+if is_torch_available():\n+    import torch\n+\n+\n VALID_INPUTS = [\"A simple string\", [\"list of strings\", \"A simple string that is quite a bit longer\"]]\n \n # These 2 model types require different inputs than those of the usual text models.\n@@ -841,6 +846,36 @@ def test_small_model_pt(self):\n             ],\n         )\n \n+    @require_torch\n+    def test_small_model_pt_fp16(self):\n+        model_name = \"hf-internal-testing/tiny-bert-for-token-classification\"\n+        token_classifier = pipeline(\n+            task=\"token-classification\", model=model_name, framework=\"pt\", torch_dtype=torch.float16\n+        )\n+        outputs = token_classifier(\"This is a test !\")\n+        self.assertEqual(\n+            nested_simplify(outputs),\n+            [\n+                {\"entity\": \"I-MISC\", \"score\": 0.115, \"index\": 1, \"word\": \"this\", \"start\": 0, \"end\": 4},\n+                {\"entity\": \"I-MISC\", \"score\": 0.115, \"index\": 2, \"word\": \"is\", \"start\": 5, \"end\": 7},\n+            ],\n+        )\n+\n+    @require_torch\n+    def test_small_model_pt_bf16(self):\n+        model_name = \"hf-internal-testing/tiny-bert-for-token-classification\"\n+        token_classifier = pipeline(\n+            task=\"token-classification\", model=model_name, framework=\"pt\", torch_dtype=torch.bfloat16\n+        )\n+        outputs = token_classifier(\"This is a test !\")\n+        self.assertEqual(\n+            nested_simplify(outputs),\n+            [\n+                {\"entity\": \"I-MISC\", \"score\": 0.115, \"index\": 1, \"word\": \"this\", \"start\": 0, \"end\": 4},\n+                {\"entity\": \"I-MISC\", \"score\": 0.115, \"index\": 2, \"word\": \"is\", \"start\": 5, \"end\": 7},\n+            ],\n+        )\n+\n     @require_torch\n     def test_pt_ignore_subwords_slow_tokenizer_raises(self):\n         model_name = \"sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english\""
        },
        {
            "sha": "7c437b0a418da24c6ce0ad7ca9fb43d5f48f3252",
            "filename": "tests/pipelines/test_pipelines_zero_shot.py",
            "status": "modified",
            "additions": 54,
            "deletions": 1,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py?ref=49a0bef4c1d959d9008d4a7128ca5e24c2ac7fc1",
            "patch": "@@ -21,11 +21,22 @@\n     ZeroShotClassificationPipeline,\n     pipeline,\n )\n-from transformers.testing_utils import is_pipeline_test, nested_simplify, require_tf, require_torch, slow\n+from transformers.testing_utils import (\n+    is_pipeline_test,\n+    is_torch_available,\n+    nested_simplify,\n+    require_tf,\n+    require_torch,\n+    slow,\n+)\n \n from .test_pipelines_common import ANY\n \n \n+if is_torch_available():\n+    import torch\n+\n+\n # These 2 model types require different inputs than those of the usual text models.\n _TO_SKIP = {\"LayoutLMv2Config\", \"LayoutLMv3Config\"}\n \n@@ -176,6 +187,48 @@ def test_small_model_pt(self):\n             },\n         )\n \n+    @require_torch\n+    def test_small_model_pt_fp16(self):\n+        zero_shot_classifier = pipeline(\n+            \"zero-shot-classification\",\n+            model=\"sshleifer/tiny-distilbert-base-cased-distilled-squad\",\n+            framework=\"pt\",\n+            torch_dtype=torch.float16,\n+        )\n+        outputs = zero_shot_classifier(\n+            \"Who are you voting for in 2020?\", candidate_labels=[\"politics\", \"public health\", \"science\"]\n+        )\n+\n+        self.assertEqual(\n+            nested_simplify(outputs),\n+            {\n+                \"sequence\": \"Who are you voting for in 2020?\",\n+                \"labels\": [\"science\", \"public health\", \"politics\"],\n+                \"scores\": [0.333, 0.333, 0.333],\n+            },\n+        )\n+\n+    @require_torch\n+    def test_small_model_pt_bf16(self):\n+        zero_shot_classifier = pipeline(\n+            \"zero-shot-classification\",\n+            model=\"sshleifer/tiny-distilbert-base-cased-distilled-squad\",\n+            framework=\"pt\",\n+            torch_dtype=torch.bfloat16,\n+        )\n+        outputs = zero_shot_classifier(\n+            \"Who are you voting for in 2020?\", candidate_labels=[\"politics\", \"public health\", \"science\"]\n+        )\n+\n+        self.assertEqual(\n+            nested_simplify(outputs),\n+            {\n+                \"sequence\": \"Who are you voting for in 2020?\",\n+                \"labels\": [\"science\", \"public health\", \"politics\"],\n+                \"scores\": [0.333, 0.333, 0.333],\n+            },\n+        )\n+\n     @require_tf\n     def test_small_model_tf(self):\n         zero_shot_classifier = pipeline("
        }
    ],
    "stats": {
        "total": 147,
        "additions": 143,
        "deletions": 4
    }
}