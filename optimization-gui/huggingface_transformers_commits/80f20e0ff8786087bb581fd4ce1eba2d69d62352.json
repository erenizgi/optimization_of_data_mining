{
    "author": "notkisk",
    "message": "[Qwen3-next] Fix dimension mismatch in torch_chunk_gated_delta_rule and torch_recurrent_gated_delta_rule (#40963) (#41036)\n\n* fix mismatched dims for qwen3 next\n\n* propagate changes\n\n* chore: renamed tot_heads to total_sequence_length\n\n* Apply suggestion from @vasqu\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* minor fix to modular qwen3 next file\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "80f20e0ff8786087bb581fd4ce1eba2d69d62352",
    "files": [
        {
            "sha": "e15e3435f732076de2426422742744e5bbfaa85c",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/80f20e0ff8786087bb581fd4ce1eba2d69d62352/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80f20e0ff8786087bb581fd4ce1eba2d69d62352/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=80f20e0ff8786087bb581fd4ce1eba2d69d62352",
            "patch": "@@ -458,15 +458,15 @@ def torch_chunk_gated_delta_rule(\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]\n \n-    batch_size, sequence_length, num_heads, k_head_dim = key.shape\n+    batch_size, num_heads, sequence_length, k_head_dim = key.shape\n     v_head_dim = value.shape[-1]\n-    pad_size = (chunk_size - num_heads % chunk_size) % chunk_size\n+    pad_size = (chunk_size - sequence_length % chunk_size) % chunk_size\n     query = F.pad(query, (0, 0, 0, pad_size))\n     key = F.pad(key, (0, 0, 0, pad_size))\n     value = F.pad(value, (0, 0, 0, pad_size))\n     beta = F.pad(beta, (0, pad_size))\n     g = F.pad(g, (0, pad_size))\n-    tot_heads = num_heads + pad_size\n+    total_sequence_length = sequence_length + pad_size\n     scale = 1 / (query.shape[-1] ** 0.5)\n     query = query * scale\n \n@@ -491,15 +491,15 @@ def torch_chunk_gated_delta_rule(\n     value = attn @ v_beta\n     k_cumdecay = attn @ (k_beta * g.exp().unsqueeze(-1))\n     last_recurrent_state = (\n-        torch.zeros(batch_size, sequence_length, k_head_dim, v_head_dim).to(value)\n+        torch.zeros(batch_size, num_heads, k_head_dim, v_head_dim).to(value)\n         if initial_state is None\n         else initial_state.to(value)\n     )\n     core_attn_out = torch.zeros_like(value)\n     mask = torch.triu(torch.ones(chunk_size, chunk_size, dtype=torch.bool, device=query.device), diagonal=1)\n \n     # for each chunk\n-    for i in range(0, tot_heads // chunk_size):\n+    for i in range(0, total_sequence_length // chunk_size):\n         q_i, k_i, v_i = query[:, :, i], key[:, :, i], value[:, :, i]\n         attn = (q_i @ k_i.transpose(-1, -2) * decay_mask[:, :, i]).masked_fill_(mask, 0)\n         v_prime = (k_cumdecay[:, :, i]) @ last_recurrent_state\n@@ -514,7 +514,7 @@ def torch_chunk_gated_delta_rule(\n     if not output_final_state:\n         last_recurrent_state = None\n     core_attn_out = core_attn_out.reshape(core_attn_out.shape[0], core_attn_out.shape[1], -1, core_attn_out.shape[-1])\n-    core_attn_out = core_attn_out[:, :, :num_heads]\n+    core_attn_out = core_attn_out[:, :, :sequence_length]\n     core_attn_out = core_attn_out.transpose(1, 2).contiguous().to(initial_dtype)\n     return core_attn_out, last_recurrent_state\n \n@@ -530,19 +530,19 @@ def torch_recurrent_gated_delta_rule(\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]\n \n-    batch_size, sequence_length, num_heads, k_head_dim = key.shape\n+    batch_size, num_heads, sequence_length, k_head_dim = key.shape\n     v_head_dim = value.shape[-1]\n     scale = 1 / (query.shape[-1] ** 0.5)\n     query = query * scale\n \n-    core_attn_out = torch.zeros(batch_size, sequence_length, num_heads, v_head_dim).to(value)\n+    core_attn_out = torch.zeros(batch_size, num_heads, sequence_length, v_head_dim).to(value)\n     last_recurrent_state = (\n-        torch.zeros(batch_size, sequence_length, k_head_dim, v_head_dim).to(value)\n+        torch.zeros(batch_size, num_heads, k_head_dim, v_head_dim).to(value)\n         if initial_state is None\n         else initial_state.to(value)\n     )\n \n-    for i in range(num_heads):\n+    for i in range(sequence_length):\n         q_t = query[:, :, i]\n         k_t = key[:, :, i]\n         v_t = value[:, :, i]"
        },
        {
            "sha": "6d4b6a5e04a31462c46c2f3e3ad1abea1a7ded5a",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/80f20e0ff8786087bb581fd4ce1eba2d69d62352/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80f20e0ff8786087bb581fd4ce1eba2d69d62352/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=80f20e0ff8786087bb581fd4ce1eba2d69d62352",
            "patch": "@@ -293,15 +293,15 @@ def torch_chunk_gated_delta_rule(\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]\n \n-    batch_size, sequence_length, num_heads, k_head_dim = key.shape\n+    batch_size, num_heads, sequence_length, k_head_dim = key.shape\n     v_head_dim = value.shape[-1]\n-    pad_size = (chunk_size - num_heads % chunk_size) % chunk_size\n+    pad_size = (chunk_size - sequence_length % chunk_size) % chunk_size\n     query = F.pad(query, (0, 0, 0, pad_size))\n     key = F.pad(key, (0, 0, 0, pad_size))\n     value = F.pad(value, (0, 0, 0, pad_size))\n     beta = F.pad(beta, (0, pad_size))\n     g = F.pad(g, (0, pad_size))\n-    tot_heads = num_heads + pad_size\n+    total_sequence_length = sequence_length + pad_size\n     scale = 1 / (query.shape[-1] ** 0.5)\n     query = query * scale\n \n@@ -326,15 +326,15 @@ def torch_chunk_gated_delta_rule(\n     value = attn @ v_beta\n     k_cumdecay = attn @ (k_beta * g.exp().unsqueeze(-1))\n     last_recurrent_state = (\n-        torch.zeros(batch_size, sequence_length, k_head_dim, v_head_dim).to(value)\n+        torch.zeros(batch_size, num_heads, k_head_dim, v_head_dim).to(value)\n         if initial_state is None\n         else initial_state.to(value)\n     )\n     core_attn_out = torch.zeros_like(value)\n     mask = torch.triu(torch.ones(chunk_size, chunk_size, dtype=torch.bool, device=query.device), diagonal=1)\n \n     # for each chunk\n-    for i in range(0, tot_heads // chunk_size):\n+    for i in range(0, total_sequence_length // chunk_size):\n         q_i, k_i, v_i = query[:, :, i], key[:, :, i], value[:, :, i]\n         attn = (q_i @ k_i.transpose(-1, -2) * decay_mask[:, :, i]).masked_fill_(mask, 0)\n         v_prime = (k_cumdecay[:, :, i]) @ last_recurrent_state\n@@ -349,7 +349,7 @@ def torch_chunk_gated_delta_rule(\n     if not output_final_state:\n         last_recurrent_state = None\n     core_attn_out = core_attn_out.reshape(core_attn_out.shape[0], core_attn_out.shape[1], -1, core_attn_out.shape[-1])\n-    core_attn_out = core_attn_out[:, :, :num_heads]\n+    core_attn_out = core_attn_out[:, :, :sequence_length]\n     core_attn_out = core_attn_out.transpose(1, 2).contiguous().to(initial_dtype)\n     return core_attn_out, last_recurrent_state\n \n@@ -365,19 +365,19 @@ def torch_recurrent_gated_delta_rule(\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]\n \n-    batch_size, sequence_length, num_heads, k_head_dim = key.shape\n+    batch_size, num_heads, sequence_length, k_head_dim = key.shape\n     v_head_dim = value.shape[-1]\n     scale = 1 / (query.shape[-1] ** 0.5)\n     query = query * scale\n \n-    core_attn_out = torch.zeros(batch_size, sequence_length, num_heads, v_head_dim).to(value)\n+    core_attn_out = torch.zeros(batch_size, num_heads, sequence_length, v_head_dim).to(value)\n     last_recurrent_state = (\n-        torch.zeros(batch_size, sequence_length, k_head_dim, v_head_dim).to(value)\n+        torch.zeros(batch_size, num_heads, k_head_dim, v_head_dim).to(value)\n         if initial_state is None\n         else initial_state.to(value)\n     )\n \n-    for i in range(num_heads):\n+    for i in range(sequence_length):\n         q_t = query[:, :, i]\n         k_t = key[:, :, i]\n         v_t = value[:, :, i]"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 20,
        "deletions": 20
    }
}