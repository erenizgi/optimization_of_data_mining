{
    "author": "gante",
    "message": "[generate] update docstring of `SequenceBiasLogitsProcessor` (#35699)\n\n* fix docstring\r\n\r\n* space",
    "sha": "88b95e6179ad330b65f402daaf70565f1a89a0e3",
    "files": [
        {
            "sha": "7bc43e5664e639de7c817258fbd55bce158db263",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 26,
            "deletions": 24,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/88b95e6179ad330b65f402daaf70565f1a89a0e3/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/88b95e6179ad330b65f402daaf70565f1a89a0e3/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=88b95e6179ad330b65f402daaf70565f1a89a0e3",
            "patch": "@@ -1040,10 +1040,9 @@ class SequenceBiasLogitsProcessor(LogitsProcessor):\n \n     <Tip>\n \n-    In order to get the token ids of the sequences that you want to bias, make sure to set `add_prefix_space=True` when\n-    initializing the tokenizer, and use `tokenizer(bad_words, add_special_tokens=False).input_ids`. The\n-    `add_prefix_space` argument is only supported for some slow tokenizers, as fast tokenizers' prefixing behaviours\n-    come from `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).\n+    At a token-level, biasing a word is different from biasing a word with a space before it. If you want to bias\n+    \"foo\" mid-sentence, you'll likely want to add a prefix space and bias \" foo\" instead. Check the tokenizer section\n+    of our NLP course to find out why: https://huggingface.co/learn/nlp-course/chapter2/4?fw=pt\n \n     </Tip>\n \n@@ -1060,37 +1059,40 @@ class SequenceBiasLogitsProcessor(LogitsProcessor):\n     ```python\n     >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n \n-    >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n     >>> inputs = tokenizer([\"The full name of Donald is Donald\"], return_tensors=\"pt\")\n \n-    >>> summary_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4)\n+    >>> summary_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, do_sample=False)\n     >>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])\n-    The full name of Donald is Donald J. Trump Jr\n-\n-    >>> # Now let's control generation through a bias. Please note that the tokenizer is initialized differently!\n-    >>> tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", add_prefix_space=True)\n-\n+    The full name of Donald is Donald John Trump Sr.\n \n     >>> def get_tokens(word):\n-    ...     return tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0]\n+    ...     return tokenizer([word], add_special_tokens=False).input_ids[0]\n \n-\n-    >>> # If we add a negative bias without beam search, it may become \"stuck\" in a prefix without good continuations\n-    >>> sequence_bias = [get_tokens(\"Trump\"), -10.0]\n-    >>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, sequence_bias=sequence_bias)\n+    >>> # IMPORTANT: Remember our tip about adding spaces before words to bias them correctly.\n+    >>> sequence_bias = [[get_tokens(\"Trump\"), -10.0],]  # will fail to apply bias\n+    >>> biased_ids = model.generate(\n+    ...     inputs[\"input_ids\"], max_new_tokens=4, do_sample=False, sequence_bias=sequence_bias\n+    ... )\n     >>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n-    The full name of Donald is Donald J. Donald,\n+    The full name of Donald is Donald John Trump Sr.\n \n-    >>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n+    >>> sequence_bias = [[get_tokens(\" Trump\"), -10.0],]  # will work\n+    >>> biased_ids = model.generate(\n+    ...     inputs[\"input_ids\"], max_new_tokens=4, do_sample=False, sequence_bias=sequence_bias\n+    ... )\n     >>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n-    The full name of Donald is Donald Rumsfeld,\n+    The full name of Donald is Donald John Harper. He\n \n-    >>> # We can also add a positive bias to nudge the model towards specific tokens or continuations\n-    >>> sequence_bias = [get_tokens(\"Donald Duck\"), 10.0]\n-    >>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n+    >>> # We can also add a positive bias to nudge the model towards specific tokens or continuations. This technique\n+    >>> # is also more effective when paired up with beam search.\n+    >>> sequence_bias = [[get_tokens(\" Donald Duck\"), 10.0],]\n+    >>> biased_ids = model.generate(\n+    ...     inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, do_sample=False, sequence_bias=sequence_bias\n+    ... )\n     >>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n-    The full name of Donald is Donald Duck.\n+    The full name of Donald is Donald Duck. He is\n     ```\n     \"\"\"\n "
        }
    ],
    "stats": {
        "total": 50,
        "additions": 26,
        "deletions": 24
    }
}