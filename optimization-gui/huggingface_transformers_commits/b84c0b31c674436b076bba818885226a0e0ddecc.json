{
    "author": "Rocketknight1",
    "message": "Remove references to AutoModelForVision2Seq (#41513)\n\n* Since Vision2Seq is deprecated, remove it from pipelines and docstrings\n\n* Catch some more references",
    "sha": "b84c0b31c674436b076bba818885226a0e0ddecc",
    "files": [
        {
            "sha": "a830a8d4703188dbb9f48892c9172c114729e11d",
            "filename": "docs/source/en/model_doc/donut.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b84c0b31c674436b076bba818885226a0e0ddecc/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b84c0b31c674436b076bba818885226a0e0ddecc/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md?ref=b84c0b31c674436b076bba818885226a0e0ddecc",
            "patch": "@@ -61,10 +61,10 @@ pipeline(image=image, question=\"What time is the coffee break?\")\n # pip install datasets\n import torch\n from datasets import load_dataset\n-from transformers import AutoProcessor, AutoModelForVision2Seq\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n \n processor = AutoProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n-model = AutoModelForVision2Seq.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n+model = AutoModelForImageTextToText.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n \n dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n image = dataset[0][\"image\"]\n@@ -92,11 +92,11 @@ The example below uses [torchao](../quantization/torchao) to only quantize the w\n # pip install datasets torchao\n import torch\n from datasets import load_dataset\n-from transformers import TorchAoConfig, AutoProcessor, AutoModelForVision2Seq\n+from transformers import TorchAoConfig, AutoProcessor, AutoModelForImageTextToText\n \n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n processor = AutoProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n-model = AutoModelForVision2Seq.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\", quantization_config=quantization_config)\n+model = AutoModelForImageTextToText.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\", quantization_config=quantization_config)\n \n dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n image = dataset[0][\"image\"]"
        },
        {
            "sha": "1671f2d42e2ff7369fee50a300f71e85ab0334ba",
            "filename": "docs/source/en/model_doc/ovis2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b84c0b31c674436b076bba818885226a0e0ddecc/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b84c0b31c674436b076bba818885226a0e0ddecc/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md?ref=b84c0b31c674436b076bba818885226a0e0ddecc",
            "patch": "@@ -39,12 +39,12 @@ import torch\n from torchvision import io\n from typing import Dict\n from transformers.image_utils import load_images, load_video\n-from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor\n+from transformers import AutoModelForImageTextToText, AutoTokenizer, AutoProcessor\n from accelerate import Accelerator\n \n device = Accelerator().device\n \n-model = AutoModelForVision2Seq.from_pretrained(\n+model = AutoModelForImageTextToText.from_pretrained(\n     \"thisisiron/Ovis2-2B-hf\",\n     dtype=torch.bfloat16,\n ).eval().to(device)"
        },
        {
            "sha": "348206703265b6e375a64638a7c065cdb9e93d5f",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b84c0b31c674436b076bba818885226a0e0ddecc/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b84c0b31c674436b076bba818885226a0e0ddecc/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=b84c0b31c674436b076bba818885226a0e0ddecc",
            "patch": "@@ -1089,7 +1089,7 @@ def forward(\n         >>> from PIL import Image\n         >>> from io import BytesIO\n \n-        >>> from transformers import AutoProcessor, AutoModelForVision2Seq\n+        >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n         >>> from transformers.image_utils import load_image\n \n         >>> # Note that passing the image urls (instead of the actual pil images) to the processor is also possible\n@@ -1098,7 +1098,7 @@ def forward(\n         >>> image3 = load_image(\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\")\n \n         >>> processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b-base\")\n-        >>> model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceM4/idefics2-8b-base\", device_map=\"auto\")\n+        >>> model = AutoModelForImageTextToText.from_pretrained(\"HuggingFaceM4/idefics2-8b-base\", device_map=\"auto\")\n \n         >>> BAD_WORDS_IDS = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n         >>> EOS_WORDS_IDS = [processor.tokenizer.eos_token_id]"
        },
        {
            "sha": "998e8c7da96a4a0e1a69c88fc8f0c06dfd271c6a",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b84c0b31c674436b076bba818885226a0e0ddecc/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b84c0b31c674436b076bba818885226a0e0ddecc/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=b84c0b31c674436b076bba818885226a0e0ddecc",
            "patch": "@@ -855,7 +855,7 @@ def forward(\n         >>> from PIL import Image\n         >>> from io import BytesIO\n \n-        >>> from transformers import AutoProcessor, AutoModelForVision2Seq\n+        >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n         >>> from transformers.image_utils import load_image\n \n         >>> # Note that passing the image urls (instead of the actual pil images) to the processor is also possible\n@@ -864,7 +864,7 @@ def forward(\n         >>> image3 = load_image(\"https://cdn.britannica.com/68/170868-050-8DDE8263/Golden-Gate-Bridge-San-Francisco.jpg\")\n \n         >>> processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/Idefics3-8B-Llama3\")\n-        >>> model = AutoModelForVision2Seq.from_pretrained(\"HuggingFaceM4/Idefics3-8B-Llama3\", dtype=torch.bfloat16, device_map=\"auto\")\n+        >>> model = AutoModelForImageTextToText.from_pretrained(\"HuggingFaceM4/Idefics3-8B-Llama3\", dtype=torch.bfloat16, device_map=\"auto\")\n \n         >>> # Create inputs\n         >>> messages = ["
        },
        {
            "sha": "0f57e78e7a9146fa30f4954c63dab75e8b78e7a0",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b84c0b31c674436b076bba818885226a0e0ddecc/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b84c0b31c674436b076bba818885226a0e0ddecc/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=b84c0b31c674436b076bba818885226a0e0ddecc",
            "patch": "@@ -118,7 +118,6 @@\n         AutoModelForTextToWaveform,\n         AutoModelForTokenClassification,\n         AutoModelForVideoClassification,\n-        AutoModelForVision2Seq,\n         AutoModelForVisualQuestionAnswering,\n         AutoModelForZeroShotImageClassification,\n         AutoModelForZeroShotObjectDetection,\n@@ -277,7 +276,7 @@\n     },\n     \"image-to-text\": {\n         \"impl\": ImageToTextPipeline,\n-        \"pt\": (AutoModelForVision2Seq,) if is_torch_available() else (),\n+        \"pt\": (AutoModelForImageTextToText,) if is_torch_available() else (),\n         \"default\": {\"model\": (\"ydshieh/vit-gpt2-coco-en\", \"5bebf1e\")},\n         \"type\": \"multimodal\",\n     },"
        },
        {
            "sha": "f1e45b53941194916df557a0fd4546882fb0b7fd",
            "filename": "src/transformers/pipelines/image_to_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b84c0b31c674436b076bba818885226a0e0ddecc/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b84c0b31c674436b076bba818885226a0e0ddecc/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_text.py?ref=b84c0b31c674436b076bba818885226a0e0ddecc",
            "patch": "@@ -34,15 +34,15 @@\n if is_torch_available():\n     import torch\n \n-    from ..models.auto.modeling_auto import MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES\n+    from ..models.auto.modeling_auto import MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES\n \n logger = logging.get_logger(__name__)\n \n \n @add_end_docstrings(build_pipeline_init_args(has_tokenizer=True, has_image_processor=True))\n class ImageToTextPipeline(Pipeline):\n     \"\"\"\n-    Image To Text pipeline using a `AutoModelForVision2Seq`. This pipeline predicts a caption for a given image.\n+    Image To Text pipeline using a `AutoModelForImageTextToText`. This pipeline predicts a caption for a given image.\n \n     Unless the model you're using explicitly sets these generation parameters in its configuration files\n     (`generation_config.json`), the following default values will be used:\n@@ -80,7 +80,7 @@ class ImageToTextPipeline(Pipeline):\n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         requires_backends(self, \"vision\")\n-        self.check_model_type(MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES)\n+        self.check_model_type(MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES)\n \n     def _sanitize_parameters(self, max_new_tokens=None, generate_kwargs=None, prompt=None, timeout=None):\n         forward_params = {}"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 14,
        "deletions": 15
    }
}