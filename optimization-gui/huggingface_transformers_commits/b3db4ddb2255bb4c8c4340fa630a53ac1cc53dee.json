{
    "author": "yao-matrix",
    "message": "enable mamba2 integration cases on xpu (#38006)\n\n* enable mamba2 integration cases on XPU\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>",
    "sha": "b3db4ddb2255bb4c8c4340fa630a53ac1cc53dee",
    "files": [
        {
            "sha": "6d9b98ccedbe43f28d47626066046e830a98c5ec",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 8,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3db4ddb2255bb4c8c4340fa630a53ac1cc53dee/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3db4ddb2255bb4c8c4340fa630a53ac1cc53dee/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=b3db4ddb2255bb4c8c4340fa630a53ac1cc53dee",
            "patch": "@@ -18,7 +18,14 @@\n from parameterized import parameterized\n \n from transformers import AutoTokenizer, Mamba2Config, is_torch_available\n-from transformers.testing_utils import require_read_token, require_torch, require_torch_gpu, slow, torch_device\n+from transformers.testing_utils import (\n+    Expectations,\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -357,12 +364,18 @@ def test_simple_generate(self, device):\n \n         out = model.generate(input_ids, do_sample=False, use_cache=True, max_new_tokens=30)\n         output_sentence = tokenizer.decode(out[0])\n-        ground_truth_sentence = \"\"\"<s>[INST]Write a hello world program in C++.[/INST] Sure, here is a simple \"Hello, World!\" program in C++:\\n\\n```cpp\\n#include <iostream>\\n\\n\"\"\"\n+        ground_truth_sentences = Expectations(\n+            {\n+                (\"xpu\", 3): \"\"\"<s>[INST]Write a hello world program in C++.[/INST] Sure, here is a simple \"Hello, World!\" program written in C++:\\n\\n```cpp\\n#include <iostream>\\n\"\"\",\n+                (\"cuda\", 7): \"\"\"<s>[INST]Write a hello world program in C++.[/INST] Sure, here is a simple \"Hello, World!\" program in C++:\\n\\n```cpp\\n#include <iostream>\\n\\n\"\"\",\n+            }\n+        )  # fmt: skip\n+        ground_truth_sentence = ground_truth_sentences.get_expectation()\n         self.assertEqual(output_sentence, ground_truth_sentence)\n \n     @require_read_token\n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_batched_equivalence_with_cache(self):\n         \"\"\"\n         Verifies that batched generation matches individual generation.\n@@ -393,7 +406,7 @@ def test_batched_equivalence_with_cache(self):\n \n     @require_read_token\n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_batched_equivalence_without_cache(self):\n         \"\"\"\n         Verifies that batched generation matches individual generation without cache.\n@@ -423,7 +436,7 @@ def test_batched_equivalence_without_cache(self):\n             self.assertEqual(individual_output[:100], batched_output[index_gen][:100])\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_mamba2_mixer_train_vs_eval_equivalence(self):\n         # Based on https://github.com/sustcsonglin/flash-linear-attention/issues/63\n         # Credit to zhixuan-lin\n@@ -433,10 +446,10 @@ def test_mamba2_mixer_train_vs_eval_equivalence(self):\n         config = Mamba2Config(num_heads=24, head_dim=64, hidden_size=768, expand=2, n_groups=1)\n \n         torch.manual_seed(42)\n-        with torch.amp.autocast(device_type=\"cuda\", dtype=dtype):\n+        with torch.amp.autocast(device_type=torch_device, dtype=dtype):\n             with torch.no_grad():\n-                mixer = Mamba2Mixer(config, layer_idx=0).to(\"cuda\")\n-                hidden_states = torch.rand(size=(B, T, D), dtype=dtype, device=\"cuda\")\n+                mixer = Mamba2Mixer(config, layer_idx=0).to(torch_device)\n+                hidden_states = torch.rand(size=(B, T, D), dtype=dtype, device=torch_device)\n \n                 mixer.train()\n                 out_train = mixer(hidden_states)"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 21,
        "deletions": 8
    }
}