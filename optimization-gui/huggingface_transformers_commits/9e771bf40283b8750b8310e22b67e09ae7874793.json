{
    "author": "capemox",
    "message": "Add support for seed in `DataCollatorForLanguageModeling` (#36497)\n\nAdd support for `seed` in `DataCollatorForLanguageModeling`. Also wrote tests for verifying behaviour.",
    "sha": "9e771bf40283b8750b8310e22b67e09ae7874793",
    "files": [
        {
            "sha": "dce699136535a141c2761ce62b03a16054fa90fe",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 115,
            "deletions": 22,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e771bf40283b8750b8310e22b67e09ae7874793/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e771bf40283b8750b8310e22b67e09ae7874793/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=9e771bf40283b8750b8310e22b67e09ae7874793",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import multiprocessing as mp\n import random\n import warnings\n from collections.abc import Mapping\n@@ -787,6 +788,8 @@ class DataCollatorForLanguageModeling(DataCollatorMixin):\n             If set, will pad the sequence to a multiple of the provided value.\n         return_tensors (`str`):\n             The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n+        seed (`int`, *optional*):\n+            The seed to use for the random number generator for masking. If not provided, the global RNG will be used.\n \n     <Tip>\n \n@@ -827,6 +830,7 @@ class DataCollatorForLanguageModeling(DataCollatorMixin):\n     pad_to_multiple_of: Optional[int] = None\n     tf_experimental_compile: bool = False\n     return_tensors: str = \"pt\"\n+    seed: Optional[int] = None\n \n     def __post_init__(self):\n         if self.mlm and self.tokenizer.mask_token is None:\n@@ -852,12 +856,57 @@ def __post_init__(self):\n \n             self.tf_mask_tokens = tf.function(self.tf_mask_tokens, jit_compile=True)\n \n+        self.generator = None\n+\n+    def get_generator(self, seed):\n+        if self.return_tensors == \"pt\":\n+            import torch\n+\n+            return torch.Generator().manual_seed(seed)\n+        elif self.return_tensors == \"tf\":\n+            import tensorflow as tf\n+\n+            return tf.random.Generator.from_seed(seed)\n+        else:\n+            import numpy as np\n+\n+            return np.random.default_rng(seed)\n+\n+    def create_rng(self):\n+        if mp.current_process().name == \"MainProcess\":\n+            # If we are in the main process, we create a generator object with the seed\n+            self.generator = self.get_generator(self.seed)\n+        else:\n+            # If we are in a worker process (i.e using multiprocessing), we need to set a unique seed for each\n+            # worker's generator, generated as the main seed + the worker's ID.\n+            # (https://pytorch.org/docs/stable/data.html#randomness-in-multi-process-data-loading)\n+            # Only PyTorch DataLoader allows us to access the worker ID, and so we check for this.\n+            # For other frameworks, we will throw an error.\n+            import torch\n+\n+            worker_info = torch.utils.data.get_worker_info()\n+            if worker_info is None:\n+                error_string = (\n+                    \"Worker process information is not available for seeding the generator. This may be because\",\n+                    \"you are using multiprocessing without using a PyTorch DataLoader. The `seed` parameter can\",\n+                    \"only be used when using multiprocessing with a PyTorch DataLoader. Please either use a\",\n+                    \"single process or use a PyTorch DataLoader with multiple workers.\",\n+                )\n+                raise ValueError(error_string)\n+\n+            self.generator = self.get_generator(self.seed + worker_info.id)\n+\n     @staticmethod\n-    def tf_bernoulli(shape, probability):\n+    def tf_bernoulli(shape, probability, generator=None):\n         import tensorflow as tf\n \n         prob_matrix = tf.fill(shape, probability)\n-        return tf.cast(prob_matrix - tf.random.uniform(shape, 0, 1) >= 0, tf.bool)\n+        # if generator exists, use it to generate the random numbers\n+        # otherwise, use the global RNG\n+        if generator:\n+            return tf.cast(prob_matrix - generator.uniform(shape, 0, 1) >= 0, tf.bool)\n+        else:\n+            return tf.cast(prob_matrix - tf.random.uniform(shape, 0, 1) >= 0, tf.bool)\n \n     def tf_mask_tokens(\n         self, inputs: Any, vocab_size, mask_token_id, special_tokens_mask: Optional[Any] = None\n@@ -872,12 +921,12 @@ def tf_mask_tokens(\n         input_shape = tf.shape(inputs)\n         # 1 for a special token, 0 for a normal token in the special tokens mask\n         # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n-        masked_indices = self.tf_bernoulli(input_shape, self.mlm_probability) & ~special_tokens_mask\n+        masked_indices = self.tf_bernoulli(input_shape, self.mlm_probability, self.generator) & ~special_tokens_mask\n         # Replace unmasked indices with -100 in the labels since we only compute loss on masked tokens\n         labels = tf.where(masked_indices, inputs, -100)\n \n         # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = self.tf_bernoulli(input_shape, self.mask_replace_prob) & masked_indices\n+        indices_replaced = self.tf_bernoulli(input_shape, self.mask_replace_prob, self.generator) & masked_indices\n \n         inputs = tf.where(indices_replaced, mask_token_id, inputs)\n \n@@ -891,9 +940,15 @@ def tf_mask_tokens(\n         random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n         # random_replace_prob% of the time, we replace masked input tokens with random word\n         indices_random = (\n-            self.tf_bernoulli(input_shape, random_replace_prob_scaled) & masked_indices & ~indices_replaced\n+            self.tf_bernoulli(input_shape, random_replace_prob_scaled, self.generator)\n+            & masked_indices\n+            & ~indices_replaced\n         )\n-        random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)\n+\n+        if self.generator:\n+            random_words = self.generator.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)\n+        else:\n+            random_words = tf.random.uniform(input_shape, maxval=vocab_size, dtype=inputs.dtype)\n \n         inputs = tf.where(indices_random, random_words, inputs)\n \n@@ -903,6 +958,11 @@ def tf_mask_tokens(\n     def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n         import tensorflow as tf\n \n+        if self.seed and self.generator is None:\n+            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n+            # If no seed supplied, we will use the global RNG\n+            self.create_rng()\n+\n         # Handle dict or lists with proper padding and conversion to tensor.\n         if isinstance(examples[0], Mapping):\n             batch = pad_without_fast_tokenizer_warning(\n@@ -943,6 +1003,12 @@ def tf_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict\n \n     def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n         # Handle dict or lists with proper padding and conversion to tensor.\n+\n+        if self.seed and self.generator is None:\n+            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n+            # If no seed supplied, we will use the global RNG\n+            self.create_rng()\n+\n         if isinstance(examples[0], Mapping):\n             batch = pad_without_fast_tokenizer_warning(\n                 self.tokenizer, examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of\n@@ -983,11 +1049,14 @@ def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n             special_tokens_mask = special_tokens_mask.bool()\n \n         probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n-        masked_indices = torch.bernoulli(probability_matrix).bool()\n+        masked_indices = torch.bernoulli(probability_matrix, generator=self.generator).bool()\n         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n \n         # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = torch.bernoulli(torch.full(labels.shape, self.mask_replace_prob)).bool() & masked_indices\n+        indices_replaced = (\n+            torch.bernoulli(torch.full(labels.shape, self.mask_replace_prob), generator=self.generator).bool()\n+            & masked_indices\n+        )\n         inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n \n         if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n@@ -1001,18 +1070,24 @@ def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n \n         # random_replace_prob% of the time, we replace masked input tokens with random word\n         indices_random = (\n-            torch.bernoulli(torch.full(labels.shape, random_replace_prob_scaled)).bool()\n+            torch.bernoulli(torch.full(labels.shape, random_replace_prob_scaled), generator=self.generator).bool()\n             & masked_indices\n             & ~indices_replaced\n         )\n-        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n+        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long, generator=self.generator)\n         inputs[indices_random] = random_words[indices_random]\n \n         # The rest of the time ((1-random_replace_prob-mask_replace_prob)% of the time) we keep the masked input tokens unchanged\n         return inputs, labels\n \n     def numpy_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n         # Handle dict or lists with proper padding and conversion to tensor.\n+\n+        if self.seed and self.generator is None:\n+            # If we have a seed, we need to create a generator object. Subsequent calls to this function will use the same generator.\n+            # If no seed supplied, we will use the global RNG\n+            self.create_rng()\n+\n         if isinstance(examples[0], Mapping):\n             batch = pad_without_fast_tokenizer_warning(\n                 self.tokenizer, examples, return_tensors=\"np\", pad_to_multiple_of=self.pad_to_multiple_of\n@@ -1052,13 +1127,21 @@ def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n \n         probability_matrix[special_tokens_mask] = 0\n         # Numpy doesn't have bernoulli, so we use a binomial with 1 trial\n-        masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n+        if self.generator:\n+            masked_indices = self.generator.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n+        else:\n+            masked_indices = np.random.binomial(1, probability_matrix, size=probability_matrix.shape).astype(bool)\n         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n \n         # mask_replace_prob% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = (\n-            np.random.binomial(1, self.mask_replace_prob, size=labels.shape).astype(bool) & masked_indices\n-        )\n+        if self.generator:\n+            indices_replaced = (\n+                self.generator.binomial(1, self.mask_replace_prob, size=labels.shape).astype(bool) & masked_indices\n+            )\n+        else:\n+            indices_replaced = (\n+                np.random.binomial(1, self.mask_replace_prob, size=labels.shape).astype(bool) & masked_indices\n+            )\n         inputs[indices_replaced] = self.tokenizer.mask_token_id\n \n         if self.mask_replace_prob == 1 or self.random_replace_prob == 0:\n@@ -1069,14 +1152,24 @@ def numpy_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = No\n         # mask_replace_prob = 0.8 and random_replace_prob = 0.1,\n         # then random_replace_prob_scaled = 0.1 / 0.2 = 0.5\n         random_replace_prob_scaled = self.random_replace_prob / remaining_prob\n-        indices_random = (\n-            np.random.binomial(1, random_replace_prob_scaled, size=labels.shape).astype(bool)\n-            & masked_indices\n-            & ~indices_replaced\n-        )\n-        random_words = np.random.randint(\n-            low=0, high=len(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64\n-        )\n+        if self.generator:\n+            indices_random = (\n+                self.generator.binomial(1, random_replace_prob_scaled, size=labels.shape).astype(bool)\n+                & masked_indices\n+                & ~indices_replaced\n+            )\n+            random_words = self.generator.integers(\n+                low=0, high=len(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64\n+            )\n+        else:\n+            indices_random = (\n+                np.random.binomial(1, random_replace_prob_scaled, size=labels.shape).astype(bool)\n+                & masked_indices\n+                & ~indices_replaced\n+            )\n+            random_words = np.random.randint(\n+                low=0, high=len(self.tokenizer), size=np.count_nonzero(indices_random), dtype=np.int64\n+            )\n         inputs[indices_random] = random_words\n \n         # The rest of the time (10% of the time) we keep the masked input tokens unchanged"
        },
        {
            "sha": "ca88b3c79c3e256a005828923af851eb02e560fc",
            "filename": "tests/trainer/test_data_collator.py",
            "status": "modified",
            "additions": 133,
            "deletions": 0,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e771bf40283b8750b8310e22b67e09ae7874793/tests%2Ftrainer%2Ftest_data_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e771bf40283b8750b8310e22b67e09ae7874793/tests%2Ftrainer%2Ftest_data_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_data_collator.py?ref=9e771bf40283b8750b8310e22b67e09ae7874793",
            "patch": "@@ -350,6 +350,86 @@ def test_data_collator_for_language_modeling(self):\n         pad_features = [list(range(5)), list(range(10))]\n         self._test_no_pad_and_pad(no_pad_features, pad_features)\n \n+    def test_data_collator_for_language_modeling_with_seed(self):\n+        tokenizer = BertTokenizer(self.vocab_file)\n+        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n+\n+        # check if seed is respected between two different DataCollatorForLanguageModeling instances\n+        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=42)\n+        batch_1 = data_collator(features)\n+        self.assertEqual(batch_1[\"input_ids\"].shape, torch.Size((2, 1000)))\n+        self.assertEqual(batch_1[\"labels\"].shape, torch.Size((2, 1000)))\n+\n+        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=42)\n+        batch_2 = data_collator(features)\n+        self.assertEqual(batch_2[\"input_ids\"].shape, torch.Size((2, 1000)))\n+        self.assertEqual(batch_2[\"labels\"].shape, torch.Size((2, 1000)))\n+\n+        self.assertTrue(torch.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n+        self.assertTrue(torch.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n+\n+        # check if seed is respected in multiple workers situation\n+        features = [{\"input_ids\": list(range(1000))} for _ in range(10)]\n+        dataloader = torch.utils.data.DataLoader(\n+            features,\n+            batch_size=2,\n+            num_workers=2,\n+            generator=torch.Generator().manual_seed(42),\n+            collate_fn=DataCollatorForLanguageModeling(tokenizer, seed=42),\n+        )\n+\n+        batch_3_input_ids = []\n+        batch_3_labels = []\n+        for batch in dataloader:\n+            batch_3_input_ids.append(batch[\"input_ids\"])\n+            batch_3_labels.append(batch[\"labels\"])\n+\n+        batch_3_input_ids = torch.stack(batch_3_input_ids)\n+        batch_3_labels = torch.stack(batch_3_labels)\n+        self.assertEqual(batch_3_input_ids.shape, torch.Size((5, 2, 1000)))\n+        self.assertEqual(batch_3_labels.shape, torch.Size((5, 2, 1000)))\n+\n+        dataloader = torch.utils.data.DataLoader(\n+            features,\n+            batch_size=2,\n+            num_workers=2,\n+            collate_fn=DataCollatorForLanguageModeling(tokenizer, seed=42),\n+        )\n+\n+        batch_4_input_ids = []\n+        batch_4_labels = []\n+        for batch in dataloader:\n+            batch_4_input_ids.append(batch[\"input_ids\"])\n+            batch_4_labels.append(batch[\"labels\"])\n+        batch_4_input_ids = torch.stack(batch_4_input_ids)\n+        batch_4_labels = torch.stack(batch_4_labels)\n+        self.assertEqual(batch_4_input_ids.shape, torch.Size((5, 2, 1000)))\n+        self.assertEqual(batch_4_labels.shape, torch.Size((5, 2, 1000)))\n+\n+        self.assertTrue(torch.all(batch_3_input_ids == batch_4_input_ids))\n+        self.assertTrue(torch.all(batch_3_labels == batch_4_labels))\n+\n+        # try with different seed\n+        dataloader = torch.utils.data.DataLoader(\n+            features,\n+            batch_size=2,\n+            num_workers=2,\n+            collate_fn=DataCollatorForLanguageModeling(tokenizer, seed=43),\n+        )\n+\n+        batch_5_input_ids = []\n+        batch_5_labels = []\n+        for batch in dataloader:\n+            batch_5_input_ids.append(batch[\"input_ids\"])\n+            batch_5_labels.append(batch[\"labels\"])\n+        batch_5_input_ids = torch.stack(batch_5_input_ids)\n+        batch_5_labels = torch.stack(batch_5_labels)\n+        self.assertEqual(batch_5_input_ids.shape, torch.Size((5, 2, 1000)))\n+        self.assertEqual(batch_5_labels.shape, torch.Size((5, 2, 1000)))\n+\n+        self.assertFalse(torch.all(batch_3_input_ids == batch_5_input_ids))\n+        self.assertFalse(torch.all(batch_3_labels == batch_5_labels))\n+\n     def test_data_collator_for_whole_word_mask(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n         data_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"pt\")\n@@ -1077,6 +1157,33 @@ def test_data_collator_for_language_modeling(self):\n         pad_features = [list(range(5)), list(range(10))]\n         self._test_no_pad_and_pad(no_pad_features, pad_features)\n \n+    def test_data_collator_for_language_modeling_with_seed(self):\n+        tokenizer = BertTokenizer(self.vocab_file)\n+        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n+\n+        # check if seed is respected between two different DataCollatorForLanguageModeling instances\n+        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=42, return_tensors=\"tf\")\n+        batch_1 = data_collator(features)\n+        self.assertEqual(batch_1[\"input_ids\"].shape.as_list(), [2, 1000])\n+        self.assertEqual(batch_1[\"labels\"].shape.as_list(), [2, 1000])\n+\n+        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=42, return_tensors=\"tf\")\n+        batch_2 = data_collator(features)\n+        self.assertEqual(batch_2[\"input_ids\"].shape.as_list(), [2, 1000])\n+        self.assertEqual(batch_2[\"labels\"].shape.as_list(), [2, 1000])\n+\n+        self.assertTrue(np.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n+        self.assertTrue(np.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n+\n+        # try with different seed\n+        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=43, return_tensors=\"tf\")\n+        batch_3 = data_collator(features)\n+        self.assertEqual(batch_3[\"input_ids\"].shape.as_list(), [2, 1000])\n+        self.assertEqual(batch_3[\"labels\"].shape.as_list(), [2, 1000])\n+\n+        self.assertFalse(np.all(batch_1[\"input_ids\"] == batch_3[\"input_ids\"]))\n+        self.assertFalse(np.all(batch_1[\"labels\"] == batch_3[\"labels\"]))\n+\n     def test_data_collator_for_whole_word_mask(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n         data_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"tf\")\n@@ -1772,6 +1879,32 @@ def test_data_collator_for_language_modeling(self):\n         pad_features = [list(range(5)), list(range(10))]\n         self._test_no_pad_and_pad(no_pad_features, pad_features)\n \n+    def test_data_collator_for_language_modeling_with_seed(self):\n+        tokenizer = BertTokenizer(self.vocab_file)\n+        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n+\n+        # check if seed is respected between two different DataCollatorForLanguageModeling instances\n+        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=42, return_tensors=\"np\")\n+        batch_1 = data_collator(features)\n+        self.assertEqual(batch_1[\"input_ids\"].shape, (2, 1000))\n+        self.assertEqual(batch_1[\"labels\"].shape, (2, 1000))\n+\n+        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=42, return_tensors=\"np\")\n+        batch_2 = data_collator(features)\n+        self.assertEqual(batch_2[\"input_ids\"].shape, (2, 1000))\n+        self.assertEqual(batch_2[\"labels\"].shape, (2, 1000))\n+\n+        self.assertTrue(np.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n+        self.assertTrue(np.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n+\n+        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=43, return_tensors=\"np\")\n+        batch_3 = data_collator(features)\n+        self.assertEqual(batch_3[\"input_ids\"].shape, (2, 1000))\n+        self.assertEqual(batch_3[\"labels\"].shape, (2, 1000))\n+\n+        self.assertFalse(np.all(batch_1[\"input_ids\"] == batch_3[\"input_ids\"]))\n+        self.assertFalse(np.all(batch_1[\"labels\"] == batch_3[\"labels\"]))\n+\n     def test_data_collator_for_whole_word_mask(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n         data_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"np\")"
        }
    ],
    "stats": {
        "total": 270,
        "additions": 248,
        "deletions": 22
    }
}