{
    "author": "cyyever",
    "message": "Fix documentation issues (#41726)\n\nFix more documentation issues\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "2cf8f833b08efecfce3a2d2f690d43f821eec7b6",
    "files": [
        {
            "sha": "ce54640e6cdf85e51a56d0411d2aacbf978e0e8e",
            "filename": "docs/source/en/executorch.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fexecutorch.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fexecutorch.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fexecutorch.md?ref=2cf8f833b08efecfce3a2d2f690d43f821eec7b6",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n [ExecuTorch](https://pytorch.org/executorch/stable/index.html) runs PyTorch models on mobile and edge devices. Export your Transformers models to the ExecuTorch format with [Optimum ExecuTorch](https://github.com/huggingface/optimum-executorch) with the command below.\n \n-```\n+```bash\n optimum-cli export executorch \\\n     --model \"HuggingFaceTB/SmolLM2-135M-Instruct\" \\\n     --task \"text-generation\" \\"
        },
        {
            "sha": "49e5efc3a4b788c64c4b0ce9fb95f7eb02669204",
            "filename": "docs/source/en/internal/rope_utils.md",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Finternal%2Frope_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Finternal%2Frope_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Frope_utils.md?ref=2cf8f833b08efecfce3a2d2f690d43f821eec7b6",
            "patch": "@@ -18,7 +18,6 @@ rendered properly in your Markdown viewer.\n \n This page explains how the Rotary Embedding is computed and applied in Transformers and what types of RoPE are supported.\n \n-\n ## Overview\n \n Rotary Position Embeddings are a technique used to inject positional information into attention mechanisms without relying on explicit position encodings.  \n@@ -35,11 +34,9 @@ The Transformers library provides a flexible and extensible implementation of va\n | `\"longrope\"` | [LongRoPE](https://github.com/microsoft/LongRoPE) scaling as in Phi-2 model series. |\n | `\"llama3\"` | RoPE scaling as in Llama3.1. |\n \n+## Configuration in Model Configs\n \n-# Configuration in Model Configs\n-\n-To enable and customize rotary embeddings, add a `rope_parameters` field to your model’s configuration file (`config.json`). This field controls the RoPE behavior across model layers. Note that each RoPE variant defines its own set of expected keys and missing keys will raise an error. See the example below which creates a llama config with default RoPE parameters: \n-\n+To enable and customize rotary embeddings, add a `rope_parameters` field to your model’s configuration file (`config.json`). This field controls the RoPE behavior across model layers. Note that each RoPE variant defines its own set of expected keys and missing keys will raise an error. See the example below which creates a llama config with default RoPE parameters:\n \n ```python\n from transformers import LlamaConfig\n@@ -62,7 +59,6 @@ config.rope_parameters = {\n \n Some models such as Gemma-3 use different layer types with different attention mechanisms, i.e. \"full attention\" in some blocks and \"sliding-window attention\" in others. Transformers supports specifying distinct RoPE parameters per layer type for these models. In this case, `rope_parameters` should be a nested dictionary, where top-level keys correspond to `config.layer_types` and values are per-type RoPE parameters. During model initialization, each decoder layer will automatically look up the matching RoPE configuration based on its declared layer type.\n \n-\n ```python\n from transformers import Gemma3Config\n \n@@ -81,9 +77,7 @@ config.rope_parameters = {\n }\n ```\n \n-# Utilities\n+## Utilities\n \n [[autodoc]] RopeParameters\n     - __call__\n-\n-"
        },
        {
            "sha": "6d73735de5fd0c1b2b92afa450ceb084fcbb2ee1",
            "filename": "docs/source/en/main_classes/data_collator.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md?ref=2cf8f833b08efecfce3a2d2f690d43f821eec7b6",
            "patch": "@@ -67,6 +67,6 @@ Examples of use can be found in the [example scripts](../examples) or [example n\n \n [[autodoc]] data.data_collator.DataCollatorWithFlattening\n \n-# DataCollatorForMultipleChoice\n+## DataCollatorForMultipleChoice\n \n [[autodoc]] data.data_collator.DataCollatorForMultipleChoice"
        },
        {
            "sha": "3f16bfbfeda5858cba94f41a74815393eb2ef076",
            "filename": "docs/source/en/main_classes/tokenizer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md?ref=2cf8f833b08efecfce3a2d2f690d43f821eec7b6",
            "patch": "@@ -50,14 +50,14 @@ several advanced alignment methods which can be used to map between the original\n token space (e.g., getting the index of the token comprising a given character or the span of characters corresponding\n to a given token).\n \n-# Multimodal Tokenizer\n+## Multimodal Tokenizer\n \n Apart from that each tokenizer can be a \"multimodal\" tokenizer which means that the tokenizer will hold all relevant special tokens\n as part of tokenizer attributes for easier access. For example, if the tokenizer is loaded from a vision-language model like LLaVA, you will\n be able to access `tokenizer.image_token_id` to obtain the special image token used as a placeholder.\n \n To enable extra special tokens for any type of tokenizer, you have to add the following lines and save the tokenizer. Extra special tokens do not\n-have to be modality related and can ne anything that the model often needs access to. In the below code, tokenizer at `output_dir` will have direct access\n+have to be modality related and can be anything that the model often needs access to. In the below code, tokenizer at `output_dir` will have direct access\n to three more special tokens.  \n \n ```python"
        },
        {
            "sha": "725ec41660f691bb8fa68e3f63f7eb0a1a014af5",
            "filename": "docs/source/en/model_doc/fastspeech2_conformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md?ref=2cf8f833b08efecfce3a2d2f690d43f821eec7b6",
            "patch": "@@ -31,7 +31,7 @@ This model was contributed by [Connor Henderson](https://huggingface.co/connor-h\n \n FastSpeech2's general structure with a Mel-spectrogram decoder was implemented, and the traditional transformer blocks were replaced with conformer blocks as done in the ESPnet library.\n \n-#### FastSpeech2 Model Architecture\n+### FastSpeech2 Model Architecture\n \n ![FastSpeech2 Model Architecture](https://www.microsoft.com/en-us/research/uploads/prod/2021/04/fastspeech2-1.png)\n "
        },
        {
            "sha": "2d329bda6146d2a395e3279b6ef2f574d67ac7d7",
            "filename": "docs/source/en/model_doc/gemma3n.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md?ref=2cf8f833b08efecfce3a2d2f690d43f821eec7b6",
            "patch": "@@ -33,7 +33,7 @@ this model, including [Alternating Updates][altup] (AltUp), [Learned Augmented R\n [MatFormer][matformer], Per-Layer Embeddings (PLE), [Activation Sparsity with Statistical Top-k][spark-transformer], and KV cache sharing. The language model uses\n a similar attention pattern to [Gemma 3](./gemma3) with alternating 4 local sliding window self-attention layers for\n every global self-attention layer with a maximum context length of 32k tokens. Gemma 3n introduces\n-[MobileNet v5][mobilenetv5] as the vision encoder, using a default resolution of 768x768 pixels, and adds a newly\n+MobileNet v5 as the vision encoder, using a default resolution of 768x768 pixels, and adds a newly\n trained audio encoder based on the [Universal Speech Model][usm] (USM) architecture.\n \n The instruction-tuned variant was post-trained with knowledge distillation and reinforcement learning."
        },
        {
            "sha": "afb1f5d14b2d6d0948d60f3d33aa6fc1beab99ef",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2cf8f833b08efecfce3a2d2f690d43f821eec7b6/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=2cf8f833b08efecfce3a2d2f690d43f821eec7b6",
            "patch": "@@ -54,7 +54,7 @@ model.set_output_embeddings(resized_embeddings)\n \n ## Usage Example\n \n-#### Instruct model\n+### Instruct model\n \n ```python\n import torch\n@@ -80,7 +80,7 @@ output = model.generate(**inputs, max_new_tokens=25)\n print(processor.decode(output[0]))\n ```\n \n-#### Base model\n+### Base model\n \n ```python\n import requests"
        }
    ],
    "stats": {
        "total": 28,
        "additions": 11,
        "deletions": 17
    }
}