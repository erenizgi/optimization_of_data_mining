{
    "author": "beep-bebop",
    "message": "Customise the separator used for splicing in DataCollatorWithFlattening (#33114)\n\n* Customising the separator used for splicing in DataCollatorWithFlattening\r\n\r\n* update DataCollatorWithFlattening docs\r\n\r\n---------\r\n\r\nCo-authored-by: weifangyuan <i.weifangyuan@yuewen.com>",
    "sha": "5c84682f16402bfa184a14b821cb324eab4e756f",
    "files": [
        {
            "sha": "7f982c49cf13ea1781b4a262e9ae7a28c4615ae7",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c84682f16402bfa184a14b821cb324eab4e756f/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c84682f16402bfa184a14b821cb324eab4e756f/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=5c84682f16402bfa184a14b821cb324eab4e756f",
            "patch": "@@ -1619,30 +1619,34 @@ class DataCollatorWithFlattening(DefaultDataCollator):\n     Data collator used for padding free approach. Does the following:\n \n     - concatate the entire mini batch into single long sequence [1, total_tokens]\n+    - uses `separator_id` to separate sequences within the concatenated `labels`, default value is -100\n     - no padding will be added, returns `input_ids`, `labels` and `position_ids`\n     \"\"\"\n \n-    def __init__(self, *args, return_position_ids=True, **kwargs):\n+    def __init__(self, *args, return_position_ids=True, separator_id=-100, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.return_position_ids = return_position_ids\n+        self.separator_id = separator_id\n         warnings.warn(\n             \"Using `DataCollatorWithFlattening` will flatten the entire mini batch into single long sequence.\"\n             \"Make sure your attention computation is able to handle it!\"\n         )\n \n-    def __call__(self, features, return_tensors=None):\n+    def __call__(self, features, return_tensors=None, separator_id=None):\n         if return_tensors is None:\n             return_tensors = self.return_tensors\n+        if separator_id is None:\n+            separator_id = self.separator_id\n         is_labels_provided = \"labels\" in features[0]\n         ret = {\"input_ids\": [], \"labels\": []}\n         if self.return_position_ids:\n             ret.update({\"position_ids\": []})\n         for idx in range(0, len(features)):\n             ret[\"input_ids\"] += features[idx][\"input_ids\"]\n             if is_labels_provided:\n-                ret[\"labels\"] += [-100] + features[idx][\"labels\"][1:]\n+                ret[\"labels\"] += [separator_id] + features[idx][\"labels\"][1:]\n             else:\n-                ret[\"labels\"] += [-100] + features[idx][\"input_ids\"][1:]\n+                ret[\"labels\"] += [separator_id] + features[idx][\"input_ids\"][1:]\n             if self.return_position_ids:\n                 ret[\"position_ids\"] += list(range(len(features[idx][\"input_ids\"])))\n         return default_data_collator([ret], return_tensors)"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 8,
        "deletions": 4
    }
}