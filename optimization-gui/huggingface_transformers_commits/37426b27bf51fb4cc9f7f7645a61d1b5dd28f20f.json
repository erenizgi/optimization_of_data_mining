{
    "author": "zucchini-nlp",
    "message": "Fix a typo in MoE models (#42835)\n\nfix",
    "sha": "37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f",
    "files": [
        {
            "sha": "b878938f43316fe6a52a9aa395c6e0c7b27c4f65",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f",
            "patch": "@@ -157,7 +157,7 @@ def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_local_experts\n         self.hidden_dim = config.hidden_size\n-        self.intermediate_dim = config.intermediate_size\n+        self.intermediate_dim = config.moe_intermediate_size\n         self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n         self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n         self.act_fn = ACT2FN[config.hidden_act]"
        },
        {
            "sha": "3d2617dbf903bbfb11d99edf7985e3de757865aa",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f",
            "patch": "@@ -107,6 +107,7 @@ class DeepseekV3NaiveMoe(MixtralExperts):\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_experts = config.num_local_experts\n+        self.intermediate_dim = config.moe_intermediate_size\n \n \n class DeepseekV3MoE(nn.Module):"
        },
        {
            "sha": "7b261909a193585a640a4e2fc2e588e2681feed6",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f",
            "patch": "@@ -315,7 +315,7 @@ def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_local_experts\n         self.hidden_dim = config.hidden_size\n-        self.intermediate_dim = config.intermediate_size\n+        self.intermediate_dim = config.moe_intermediate_size\n         self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n         self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n         self.act_fn = ACT2FN[config.hidden_act]"
        },
        {
            "sha": "d56d08622064f39cfcb692e3498d4c06132697b3",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f",
            "patch": "@@ -339,7 +339,7 @@ def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_local_experts\n         self.hidden_dim = config.hidden_size\n-        self.intermediate_dim = config.intermediate_size\n+        self.intermediate_dim = config.moe_intermediate_size\n         self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n         self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n         self.act_fn = ACT2FN[config.hidden_act]"
        },
        {
            "sha": "40d80b4e35e375b149a25a1abb688e52ab323501",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=37426b27bf51fb4cc9f7f7645a61d1b5dd28f20f",
            "patch": "@@ -402,7 +402,7 @@ def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_local_experts\n         self.hidden_dim = config.hidden_size\n-        self.intermediate_dim = config.intermediate_size\n+        self.intermediate_dim = config.moe_intermediate_size\n         self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, 2 * self.intermediate_dim, self.hidden_dim))\n         self.down_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim, self.intermediate_dim))\n         self.act_fn = ACT2FN[config.hidden_act]"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}