{
    "author": "MekkCyber",
    "message": "Llama Kernel integration (#37092)\n\n* initial commit\n\n* style\n\n* update\n\n* change approach attention\n\n* clean up\n\n* fix import\n\n* update\n\n* update\n\n* fix style\n\n* change method\n\n* attention\n\n* add mlp back\n\n* change name\n\n* update name\n\n* fix copies\n\n* fix config\n\n* fix",
    "sha": "0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
    "files": [
        {
            "sha": "ba41b2c0a8b2ba5d90e2632dfca381df071dcccd",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -31,7 +31,20 @@\n                 repo_id=\"kernels-community/deformable-detr\",\n                 layer_name=\"MultiScaleDeformableAttention\",\n             )\n-        }\n+        },\n+        \"RMSNorm\": {\n+            \"cuda\": LayerRepository(\n+                repo_id=\"kernels-community/triton-layer-norm\",\n+                layer_name=\"LlamaRMSNorm\",\n+                revision=\"pure-layer-test\",\n+            )\n+        },\n+        \"MLP\": {\n+            \"cuda\": LayerRepository(\n+                repo_id=\"medmekk/triton-llama-mlp\",\n+                layer_name=\"TritonLlamaMLP\",\n+            )\n+        },\n     }\n \n     register_kernel_mapping(_KERNEL_MAPPING)"
        },
        {
            "sha": "28b27cf1146bc61b218e9fd9fbc5efa49ca5679a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 37,
            "deletions": 1,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -102,6 +102,7 @@\n     is_accelerate_available,\n     is_bitsandbytes_available,\n     is_flash_attn_2_available,\n+    is_kernels_available,\n     is_offline_mode,\n     is_optimum_available,\n     is_peft_available,\n@@ -157,6 +158,9 @@\n if is_deepspeed_available():\n     import deepspeed\n \n+if is_kernels_available():\n+    from kernels import get_kernel\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -2024,6 +2028,35 @@ def _autoset_attn_implementation(\n                     ' We recommend to just use `attn_implementation=\"flash_attention_2\"` when loading the model.'\n                 )\n \n+            if isinstance(config._attn_implementation, str) and re.match(\n+                r\"^[^/:]+/[^/:]+:[^/:]+$\", config._attn_implementation\n+            ):\n+                if not is_kernels_available():\n+                    raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n+\n+                # Extract repo_id and kernel_name from the string\n+                repo_id, kernel_name = config._attn_implementation.split(\":\")\n+                kernel_name = kernel_name.strip()\n+                repo_id = repo_id.strip()\n+\n+                try:\n+                    kernel = get_kernel(repo_id)\n+                    ALL_ATTENTION_FUNCTIONS.register(\n+                        f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name)\n+                    )\n+                    config._attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n+                except FileNotFoundError as e:\n+                    logger.warning(\n+                        f\"Could not find a kernel repository '{repo_id}' compatible with your devicein the hub: {e}. Using eager attention implementation instead.\"\n+                    )\n+                    config._attn_implementation = \"eager\"\n+                except AttributeError:\n+                    raise ValueError(\n+                        \"the kernel function name or class specified in the attn_implementation argument is not valid. \\\n+                                     Please check the documentation for the correct format, \\\n+                                     and check that the kernel exports the class and the function correctly.\"\n+                    )\n+\n             if (\n                 not isinstance(config._attn_implementation, dict)\n                 and config._attn_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n@@ -4299,7 +4332,10 @@ def from_pretrained(\n         config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.\n         if not getattr(config, \"_attn_implementation_autoset\", False):\n             config = cls._autoset_attn_implementation(\n-                config, use_flash_attention_2=use_flash_attention_2, torch_dtype=torch_dtype, device_map=device_map\n+                config,\n+                use_flash_attention_2=use_flash_attention_2,\n+                torch_dtype=torch_dtype,\n+                device_map=device_map,\n             )\n \n         with ContextManagers(model_init_context):"
        },
        {
            "sha": "19997e2a9bb49d56212028e286c518c89eb744f7",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n@@ -61,6 +62,7 @@\n _CONFIG_FOR_DOC = \"AriaTextConfig\"\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class AriaTextRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -226,6 +228,7 @@ def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tens\n         return out\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class AriaSharedExpertsMLP(nn.Module):\n     \"\"\"\n     Shared Expert MLP for shared experts.\n@@ -563,6 +566,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -623,7 +627,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "7c084d37ed6f51db4bd5588a32b416857bfdbcb8",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -34,6 +34,7 @@\n \n from ...cache_utils import Cache  # we need __iter__ and __len__ of pkv\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -295,6 +296,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -880,6 +882,7 @@ def forward(\n         return self.torch_forward(hidden_states, cache_params, cache_position, attention_mask)\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class BambaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -896,6 +899,7 @@ def forward(self, x):\n         return down_proj\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class BambaRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\""
        },
        {
            "sha": "2083bb63ab75e545b816f61087b6eb49c3504af7",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -36,6 +36,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -117,6 +118,7 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class CohereMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "e419379969d3bf29627bac9ad0444e075c90497a",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -28,6 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -267,6 +268,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class Cohere2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "e7ad5c5cf0b6550678022a1842c7cd568494d040",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -15,6 +15,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -44,6 +45,7 @@\n _CONFIG_FOR_DOC = \"DeepseekV3Config\"\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class DeepseekV3RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -481,7 +483,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "e8031be755417b77a76eeb1332287fdbc0c409c8",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -31,6 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import (\n     FlashAttentionKwargs,\n@@ -73,6 +74,7 @@\n _CONFIG_FOR_DOC = \"DiffLlamaConfig\"\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class DiffLlamaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -498,6 +500,7 @@ def forward(\n         return attn_output, None\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class DiffLlamaRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -549,7 +552,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "341944a236758b121262cd53af86338697d57061",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -31,6 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -62,6 +63,7 @@\n _CONFIG_FOR_DOC = \"Emu3Config\"\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Emu3RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -82,6 +84,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class Emu3MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -221,6 +224,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once("
        },
        {
            "sha": "99e65dbae925280a614cc3c469ad820e27545754",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -84,6 +85,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class GemmaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -257,6 +259,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -306,7 +309,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "c7040de011ba0ef8878d94bf1d9256585123d271",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -28,6 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -77,6 +78,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class Gemma2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "23f28281a1de6e63ff767e2f912c4b544c9b115a",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -31,6 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -106,6 +107,7 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale.to(self.weight.dtype)\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class Gemma3MLP(nn.Module):\n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__()"
        },
        {
            "sha": "f2acc45c6625b4fed7b731e0e8bde1e9358351d3",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -28,6 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -219,6 +220,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -244,6 +246,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class GlmRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -322,7 +325,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "c30ced71973c8682e6a48c14787ab095a35ff4d6",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -28,6 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -277,6 +278,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -305,6 +307,7 @@ def forward(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Glm4RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\""
        },
        {
            "sha": "4160a658f83adb2f666f97a91664e80170143d00",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -28,6 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -180,6 +181,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -205,6 +207,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class GraniteRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -225,6 +228,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class GraniteMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "599c8a5667f4a7c471475e7279c0f09ce59d057d",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -117,6 +118,7 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class HeliumMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -260,6 +262,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -309,7 +312,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "1a598f21a35b904cb7576f61c6aa6edfd9d19fe3",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -59,13 +59,16 @@\n \n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n+from ...integrations import use_kernel_forward_from_hub\n+\n \n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"meta-llama/Llama-2-7b-hf\"\n _CONFIG_FOR_DOC = \"LlamaConfig\"\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class LlamaRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -157,6 +160,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class LlamaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -262,6 +266,7 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n+\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n                 logger.warning_once(\n@@ -311,7 +316,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "72a9c88eac84f2f1b14065a523280a991baa0fd2",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -13,6 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -44,6 +45,7 @@\n _CONFIG_FOR_DOC = \"MistralConfig\"\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class MistralMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -200,6 +202,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class MistralRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -242,7 +245,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "6d60202db46b685cfc2f1a5dd59b67bd556a0f87",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -43,6 +44,7 @@\n _CONFIG_FOR_DOC = \"Mistral3Config\"\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Mistral3RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\""
        },
        {
            "sha": "cb4e0a9c367dcdaac80880e15d58ed2496759ab7",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -34,6 +34,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -152,6 +153,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return final_hidden_states, router_logits\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class MixtralRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\""
        },
        {
            "sha": "6e470f7bf78b927c8c0e2e313edcd6b78698e98b",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -381,7 +381,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "8b015057efd05ac024b221b9ca0844ae8fcb41e6",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -14,6 +14,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -57,6 +58,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         )\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class OlmoMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -253,7 +255,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "c99ba1f0dea219b02cd49233fb2d6d8037044de8",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -13,6 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -42,6 +43,7 @@\n _CONFIG_FOR_DOC = \"Olmo2Config\"\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Olmo2RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -216,6 +218,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class Olmo2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        },
        {
            "sha": "0b0d7b626b3ed924af5a2c9ad33ef1d5e6bea4bd",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -229,6 +230,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Phi3RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\""
        },
        {
            "sha": "68a9ae55365bf19869a5e0c1104251714bd756e7",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -33,6 +33,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -1281,6 +1282,7 @@ def forward(\n         return audio_embeds\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Phi4MultimodalRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\""
        },
        {
            "sha": "661e3181d77b6b5d4bca4cab6fc3d48ea19a68c3",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -13,6 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -44,6 +45,7 @@\n _CONFIG_FOR_DOC = \"Qwen2Config\"\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class Qwen2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -208,6 +210,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Qwen2RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -255,7 +258,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "dbf31b6d52100f218dd1bee9fc423028a60a8390",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -28,6 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -59,6 +60,7 @@\n _CONFIG_FOR_DOC = \"Qwen3Config\"\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Qwen3RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -79,6 +81,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+@use_kernel_forward_from_hub(\"MLP\")\n class Qwen3MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -282,7 +285,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "e56b121b3ae30334bb147f33f84e25878668f21a",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -289,6 +290,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return final_hidden_states, router_logits\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Qwen3MoeRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\""
        },
        {
            "sha": "4ca5ecf636922d7bf32426281d8b6f32680e4cf1",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -246,7 +246,6 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention"
        },
        {
            "sha": "221bb39c8487411d7248e94129075e5dc4a0d9ba",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -156,6 +156,7 @@\n     is_jumanpp_available,\n     is_kenlm_available,\n     is_keras_nlp_available,\n+    is_kernels_available,\n     is_levenshtein_available,\n     is_librosa_available,\n     is_liger_kernel_available,"
        },
        {
            "sha": "75c88dd019e9d2c923c75bcfd241d330bf519bbb",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0ea1151222b0ba4bf8e509e5e7ae73b57359d296/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=0ea1151222b0ba4bf8e509e5e7ae73b57359d296",
            "patch": "@@ -216,6 +216,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _triton_available = _is_package_available(\"triton\")\n _spqr_available = _is_package_available(\"spqr_quant\")\n _rich_available = _is_package_available(\"rich\")\n+_kernels_available = _is_package_available(\"kernels\")\n \n _torch_version = \"N/A\"\n _torch_available = False\n@@ -329,6 +330,10 @@ def is_kenlm_available():\n     return _kenlm_available\n \n \n+def is_kernels_available():\n+    return _kernels_available\n+\n+\n def is_cv2_available():\n     return _cv2_available\n "
        }
    ],
    "stats": {
        "total": 142,
        "additions": 127,
        "deletions": 15
    }
}