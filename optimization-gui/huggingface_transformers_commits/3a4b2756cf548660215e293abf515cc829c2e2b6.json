{
    "author": "AceHunterr",
    "message": "docs: Update TrOCR model card to new format (#40240)\n\n* docs: Update TrOCR model card to new format\n\n* Updated Sugegestions",
    "sha": "3a4b2756cf548660215e293abf515cc829c2e2b6",
    "files": [
        {
            "sha": "6346977dafa16df6e633dc2dfe21ae25303c7356",
            "filename": "docs/source/en/model_doc/trocr.md",
            "status": "modified",
            "additions": 68,
            "deletions": 67,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a4b2756cf548660215e293abf515cc829c2e2b6/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrocr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a4b2756cf548660215e293abf515cc829c2e2b6/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrocr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrocr.md?ref=3a4b2756cf548660215e293abf515cc829c2e2b6",
            "patch": "@@ -14,104 +14,105 @@ rendered properly in your Markdown viewer.\n specific language governing permissions and limitations under the License. -->\n *This model was released on 2021-09-21 and added to Hugging Face Transformers on 2021-10-13.*\n \n-# TrOCR\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n \n-## Overview\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+           <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n \n-The TrOCR model was proposed in [TrOCR: Transformer-based Optical Character Recognition with Pre-trained\n-Models](https://huggingface.co/papers/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\n-Zhoujun Li, Furu Wei. TrOCR consists of an image Transformer encoder and an autoregressive text Transformer decoder to\n-perform [optical character recognition (OCR)](https://en.wikipedia.org/wiki/Optical_character_recognition).\n+# TrOCR\n \n-The abstract from the paper is the following:\n+[TrOCR](https://huggingface.co/papers/2109.10282) is a text recognition model for both image understanding and text generation. It doesn't require separate models for image processing or character generation. TrOCR is a simple single end-to-end system that uses a transformer to handle visual understanding and text generation.\n \n-*Text recognition is a long-standing research problem for document digitalization. Existing approaches for text recognition\n-are usually built based on CNN for image understanding and RNN for char-level text generation. In addition, another language\n-model is usually needed to improve the overall accuracy as a post-processing step. In this paper, we propose an end-to-end\n-text recognition approach with pre-trained image Transformer and text Transformer models, namely TrOCR, which leverages the\n-Transformer architecture for both image understanding and wordpiece-level text generation. The TrOCR model is simple but\n-effective, and can be pre-trained with large-scale synthetic data and fine-tuned with human-labeled datasets. Experiments\n-show that the TrOCR model outperforms the current state-of-the-art models on both printed and handwritten text recognition\n-tasks.*\n+You can find all the original TrOCR checkpoints under the [Microsoft](https://huggingface.co/microsoft/models?search=trocr) organization.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/trocr_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n-\n <small> TrOCR architecture. Taken from the <a href=\"https://huggingface.co/papers/2109.10282\">original paper</a>. </small>\n \n-Please refer to the [`VisionEncoderDecoder`] class on how to use this model.\n \n-This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n-[here](https://github.com/microsoft/unilm/tree/6f60612e7cc86a2a1ae85c47231507a587ab4e01/trocr).\n+> [!TIP]\n+> This model was contributed by [nielsr](https://huggingface.co/nielsr).\n+>\n+> Click on the TrOCR models in the right sidebar for more examples of how to apply TrOCR to different image and text tasks.\n \n-## Usage tips\n \n-- The quickest way to get started with TrOCR is by checking the [tutorial\n-  notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/TrOCR), which show how to use the model\n-  at inference time as well as fine-tuning on custom data.\n-- TrOCR is pre-trained in 2 stages before being fine-tuned on downstream datasets. It achieves state-of-the-art results\n-  on both printed (e.g. the [SROIE dataset](https://paperswithcode.com/dataset/sroie) and handwritten (e.g. the [IAM\n-  Handwriting dataset](https://fki.tic.heia-fr.ch/databases/iam-handwriting-database>) text recognition tasks. For more\n-  information, see the [official models](https://huggingface.co/models?other=trocr>).\n-- [Fine‚Äëtune TrOCR on your own OCR dataset](https://github.com/Ashutosh-4485/trocr-custom-fine-tune.git).\n-- TrOCR is always used within the [VisionEncoderDecoder](vision-encoder-decoder) framework.\n+The example below demonstrates how to perform optical character recognition (OCR) with the [`AutoModel`] class.\n \n-## Resources\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n \n-A list of official Hugging Face and community (indicated by üåé) resources to help you get started with TrOCR. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+```python\n+from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n+import requests\n+from PIL import Image\n \n-<PipelineTag pipeline=\"text-classification\"/>\n+processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n+model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n \n-- A blog post on [Accelerating Document AI](https://huggingface.co/blog/document-ai) with TrOCR.\n-- A blog post on how to [Document AI](https://github.com/philschmid/document-ai-transformers) with TrOCR.\n-- A notebook on how to [finetune TrOCR on IAM Handwriting Database using Seq2SeqTrainer](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb).\n-- A notebook on [inference with TrOCR](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Inference_with_TrOCR_%2B_Gradio_demo.ipynb) and Gradio demo.\n-- A notebook on [finetune TrOCR on the IAM Handwriting Database](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb) using native PyTorch.\n-- A notebook on [evaluating TrOCR on the IAM test set](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Evaluating_TrOCR_base_handwritten_on_the_IAM_test_set.ipynb).\n+# load image from the IAM dataset\n+url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n \n-<PipelineTag pipeline=\"text-generation\"/>\n+pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n+generated_ids = model.generate(pixel_values)\n \n-- [Casual language modeling](https://huggingface.co/docs/transformers/tasks/language_modeling) task guide.\n+generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+print(generated_text)\n+```\n \n-‚ö°Ô∏è Inference\n+</hfoption>\n+</hfoptions>\n \n-- An interactive-demo on [TrOCR handwritten character recognition](https://huggingface.co/spaces/nielsr/TrOCR-handwritten).\n+## Quantization\n \n-## Inference\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-TrOCR's [`VisionEncoderDecoder`] model accepts images as input and makes use of\n-[`~generation.GenerationMixin.generate`] to autoregressively generate text given the input image.\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the weights to 8-bits.\n \n-The [`ViTImageProcessor`/`DeiTImageProcessor`] class is responsible for preprocessing the input image and\n-[`RobertaTokenizer`/`XLMRobertaTokenizer`] decodes the generated target tokens to the target string. The\n-[`TrOCRProcessor`] wraps [`ViTImageProcessor`/`DeiTImageProcessor`] and [`RobertaTokenizer`/`XLMRobertaTokenizer`]\n-into a single instance to both extract the input features and decode the predicted token ids.\n+```python\n+# pip install bitsandbytes accelerate\n+from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BitsandBytesConfig\n+import requests\n+from PIL import Image\n \n-- Step-by-step Optical Character Recognition (OCR)\n+# Set up the quantization configuration\n+quantization_config = BitsandBytesConfig(load_in_8bit=True)\n \n-``` py\n->>> from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n->>> import requests\n->>> from PIL import Image\n+# Use a large checkpoint for a more noticeable impact\n+processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")\n+model = VisionEncoderDecoderModel.from_pretrained(\n+    \"microsoft/trocr-large-handwritten\",\n+    quantization_config=quantization_config\n+)\n \n->>> processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n->>> model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n+# load image from the IAM dataset\n+url = \"[https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg](https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg)\"\n+image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n \n->>> # load image from the IAM dataset\n->>> url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n+generated_ids = model.generate(pixel_values)\n \n->>> pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n->>> generated_ids = model.generate(pixel_values)\n-\n->>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n+print(generated_text)\n ```\n \n-See the [model hub](https://huggingface.co/models?filter=trocr) to look for TrOCR checkpoints.\n+## Notes\n+\n+- TrOCR wraps [`ViTImageProcessor`]/[`DeiTImageProcessor`] and [`RobertaTokenizer`]/[`XLMRobertaTokenizer`] into a single instance of [`TrOCRProcessor`] to handle images and text.\n+- TrOCR is always used within the [VisionEncoderDecoder](vision-encoder-decoder) framework.\n+\n+## Resources\n+\n+- A blog post on [Accelerating Document AI](https://huggingface.co/blog/document-ai) with TrOCR.\n+- A blog post on how to [Document AI](https://github.com/philschmid/document-ai-transformers) with TrOCR.\n+- A notebook on how to [finetune TrOCR on IAM Handwriting Database using Seq2SeqTrainer](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb).\n+- An interactive-demo on [TrOCR handwritten character recognition](https://huggingface.co/spaces/nielsr/TrOCR-handwritten).\n+- A notebook on [inference with TrOCR](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Inference_with_TrOCR_%2B_Gradio_demo.ipynb) and Gradio demo.\n+- A notebook on [evaluating TrOCR on the IAM test set](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Evaluating_TrOCR_base_handwritten_on_the_IAM_test_set.ipynb).\n+\n \n ## TrOCRConfig\n "
        }
    ],
    "stats": {
        "total": 135,
        "additions": 68,
        "deletions": 67
    }
}