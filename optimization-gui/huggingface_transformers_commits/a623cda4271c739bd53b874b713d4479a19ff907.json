{
    "author": "MekkCyber",
    "message": "[kernels]Â Add Tests & CI for kernels (#41765)\n\n* first commit\n\n* add tests\n\n* add kernel config\n\n* add more tests\n\n* add ci\n\n* small fix\n\n* change branch name\n\n* update tests\n\n* nit\n\n* change test name\n\n* revert jobs\n\n* addressing review\n\n* reenable all jobs\n\n* address second review",
    "sha": "a623cda4271c739bd53b874b713d4479a19ff907",
    "files": [
        {
            "sha": "65ba9e3ce32f9f148bd26c4d9bcbe4ea726a0c0d",
            "filename": ".github/workflows/self-scheduled-caller.yml",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a623cda4271c739bd53b874b713d4479a19ff907/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a623cda4271c739bd53b874b713d4479a19ff907/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-caller.yml?ref=a623cda4271c739bd53b874b713d4479a19ff907",
            "patch": "@@ -118,3 +118,15 @@ jobs:\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n       commit_sha: ${{ github.sha }}\n     secrets: inherit\n+\n+  kernels-ci:\n+    name: Kernels CI\n+    uses: ./.github/workflows/self-scheduled.yml\n+    with:\n+      job: run_kernels_gpu\n+      slack_report_channel: \"#transformers-ci-daily-kernels\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: Daily CI\n+      report_repo_id: hf-internal-testing/transformers_daily_ci\n+      commit_sha: ${{ github.sha }}\n+    secrets: inherit\n\\ No newline at end of file"
        },
        {
            "sha": "ccf6aa962915d79f4696d9bbd136875f23ec356b",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/a623cda4271c739bd53b874b713d4479a19ff907/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a623cda4271c739bd53b874b713d4479a19ff907/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=a623cda4271c739bd53b874b713d4479a19ff907",
            "patch": "@@ -475,6 +475,70 @@ jobs:\n           name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\n           path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports\n \n+  run_kernels_gpu:\n+    if: ${{ inputs.job == 'run_kernels_gpu' }}\n+    name: Kernel tests\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [aws-g5-4xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n+    container:\n+      image: ${{ inputs.docker }}\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+    steps:\n+      - name: Update clone\n+        working-directory: /transformers\n+        run: git fetch && git checkout ${{ inputs.commit_sha || github.sha }}\n+\n+      - name: Reinstall transformers in edit mode\n+        working-directory: /transformers\n+        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .[testing]\n+  \n+      - name: Install kernels\n+        working-directory: /transformers\n+        run: python3 -m pip install -U kernels\n+  \n+      - name: NVIDIA-SMI\n+        run: nvidia-smi\n+\n+      - name: Environment\n+        working-directory: /transformers\n+        run: python3 utils/print_env.py\n+\n+      - name: Show installed libraries and their versions\n+        working-directory: /transformers\n+        run: pip freeze\n+\n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g5-4xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+    \n+      - name: Run kernel tests on GPU\n+        working-directory: /transformers\n+        run: |\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_kernels_gpu_test_reports tests/kernels/test_kernels.py\n+\n+      - name: Failure short reports\n+        if: ${{ failure() }}\n+        continue-on-error: true\n+        run: cat /transformers/reports/${{ env.machine_type }}_run_kernels_gpu_test_reports/failures_short.txt\n+\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_kernels_gpu_test_reports\"\n+        if: ${{ always() }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: ${{ env.machine_type }}_run_kernels_gpu_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_kernels_gpu_test_reports\n+\n   run_extract_warnings:\n     # Let's only do this for the job `run_models_gpu` to simplify the (already complex) logic.\n     if: ${{ always() && inputs.job == 'run_models_gpu' }}\n@@ -527,6 +591,7 @@ jobs:\n       run_examples_gpu,\n       run_torch_cuda_extensions_gpu,\n       run_quantization_torch_gpu,\n+      run_kernels_gpu,\n       run_extract_warnings\n     ]\n     if: always() && !cancelled()"
        },
        {
            "sha": "a64e156bacf40959b93691f95771d380b40b1beb",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a623cda4271c739bd53b874b713d4479a19ff907/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a623cda4271c739bd53b874b713d4479a19ff907/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=a623cda4271c739bd53b874b713d4479a19ff907",
            "patch": "@@ -51,10 +51,13 @@\n             )\n         },\n         \"RMSNorm\": {\n-            \"cuda\": LayerRepository(\n-                repo_id=\"kernels-community/liger_kernels\",\n-                layer_name=\"LigerRMSNorm\",\n-            ),\n+            \"cuda\": {\n+                Mode.INFERENCE: LayerRepository(\n+                    repo_id=\"kernels-community/liger_kernels\",\n+                    layer_name=\"LigerRMSNorm\",\n+                    # revision=\"pure-layer-test\",\n+                ),\n+            },\n             \"rocm\": {\n                 Mode.INFERENCE: LayerRepository(\n                     repo_id=\"kernels-community/liger_kernels\","
        },
        {
            "sha": "6311629ac4f2f644fede76160eb242c27c72188b",
            "filename": "tests/kernels/test_kernels.py",
            "status": "added",
            "additions": 403,
            "deletions": 0,
            "changes": 403,
            "blob_url": "https://github.com/huggingface/transformers/blob/a623cda4271c739bd53b874b713d4479a19ff907/tests%2Fkernels%2Ftest_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a623cda4271c739bd53b874b713d4479a19ff907/tests%2Fkernels%2Ftest_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fkernels%2Ftest_kernels.py?ref=a623cda4271c739bd53b874b713d4479a19ff907",
            "patch": "@@ -0,0 +1,403 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# Run the test: CUDA_VISIBLE_DEVICES=0 RUN_SLOW=1 pytest -sv tests/kernels/test_kernels.py\n+\n+\n+import copy\n+import types\n+from unittest.mock import patch\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, KernelConfig\n+from transformers.integrations.hub_kernels import (\n+    _HUB_KERNEL_MAPPING,\n+    _KERNEL_MODULE_MAPPING,\n+    is_kernel,\n+    lazy_load_kernel,\n+    load_and_register_attn_kernel,\n+)\n+from transformers.masking_utils import ALL_MASK_ATTENTION_FUNCTIONS\n+from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    cleanup,\n+    require_kernels,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils.import_utils import is_kernels_available\n+\n+\n+if is_kernels_available():\n+    import kernels as kernels_pkg\n+    from kernels import Device, Mode, kernelize\n+\n+\n+@require_kernels\n+@slow\n+class TestHubKernels(TestCasePlus):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n+        cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_id)\n+        cls.model_kernelized = AutoModelForCausalLM.from_pretrained(\n+            cls.model_id, use_kernels=True, device_map=torch_device\n+        )\n+        cls.model_not_kernelized = AutoModelForCausalLM.from_pretrained(\n+            cls.model_id, use_kernels=False, device_map=torch_device\n+        )\n+        cls.input = \"Hello\"\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        for attr in [\n+            \"model_kernelized\",\n+            \"model_not_kernelized\",\n+            \"tokenizer\",\n+        ]:\n+            if hasattr(cls, attr):\n+                try:\n+                    delattr(cls, attr)\n+                except Exception:\n+                    pass\n+\n+        # Clear any temporary kernel module cache entries populated by tests\n+        try:\n+            keys_to_remove = [\n+                k for k, v in list(_KERNEL_MODULE_MAPPING.items()) if v is None or isinstance(v, types.ModuleType)\n+            ]\n+            for k in keys_to_remove:\n+                _KERNEL_MODULE_MAPPING.pop(k, None)\n+        except Exception:\n+            pass\n+\n+    def tearDown(self):\n+        # Free accelerator memory/cache and trigger GC\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @require_torch_accelerator\n+    def test_forward(self):\n+        tokenized_input = self.tokenizer(self.input, return_tensors=\"pt\").input_ids.to(self.model_kernelized.device)\n+        output_ = self.model_kernelized.generate(tokenized_input, max_new_tokens=10, do_sample=False)\n+        output = self.tokenizer.decode(output_[0], skip_special_tokens=True)\n+\n+        self.EXPECTED_OUTPUT = set()\n+        self.EXPECTED_OUTPUT.add(\"Hello, I'm looking for a reliable and trustworthy online\")\n+\n+        self.assertTrue(output in self.EXPECTED_OUTPUT)\n+\n+    def test_getter_use_kernels(self):\n+        self.assertTrue(self.model_kernelized.use_kernels)\n+        self.assertFalse(self.model_not_kernelized.use_kernels)\n+\n+    def assert_kernelized_forward_is_different(self, kernelized_model, not_kernelized_model):\n+        \"\"\"\n+        Iterate over modules and check if the forward method is different between\n+        the kernelized and not kernelized models. Break on first difference, else continue.\n+        Finally, assert that at least one forward is different.\n+        \"\"\"\n+        found_difference = False\n+        for (name1, module1), (name2, module2) in zip(\n+            kernelized_model.named_modules(), not_kernelized_model.named_modules()\n+        ):\n+            # Only compare modules with the same name\n+            if name1 != name2:\n+                continue\n+            # Check if both modules have a 'forward' attribute\n+            if hasattr(module1, \"forward\") and hasattr(module2, \"forward\"):\n+                # Compare the code objects of the forward methods\n+                code1 = getattr(module1.forward, \"__code__\", None)\n+                code2 = getattr(module2.forward, \"__code__\", None)\n+                if code1 is not None and code2 is not None:\n+                    if code1 is not code2:\n+                        found_difference = True\n+                        break\n+        self.assertTrue(\n+            found_difference,\n+            \"No module's forward method was different between kernelized and not kernelized models.\",\n+        )\n+\n+    def assert_kernelized_forward_is_the_same(self, model_1, model_2):\n+        \"\"\"\n+        Iterate over modules and check if the forward method is the same between\n+        the kernelized and not kernelized models. Break on first difference, else continue.\n+        Finally, assert that at least one forward is the same.\n+        \"\"\"\n+        no_difference = True\n+        for (name1, module1), (name2, module2) in zip(model_1.named_modules(), model_2.named_modules()):\n+            # Only compare modules with the same name\n+            if name1 != name2:\n+                continue\n+            # Check if both modules have a 'forward' attribute\n+            if hasattr(module1, \"forward\") and hasattr(module2, \"forward\"):\n+                # Compare the code objects of the forward methods\n+                code1 = getattr(module1.forward, \"__code__\", None)\n+                code2 = getattr(module2.forward, \"__code__\", None)\n+                if code1 is not None and code2 is not None:\n+                    if code1 != code2:\n+                        no_difference = False\n+                        break\n+        self.assertTrue(\n+            no_difference,\n+            \"All module's forward methods were the same between the two models\",\n+        )\n+\n+    def test_kernelize(self):\n+        model = copy.deepcopy(self.model_not_kernelized)\n+        kernelize(model, mode=Mode.INFERENCE, device=Device(type=model.device.type))  # type: ignore[arg-type]\n+        self.assert_kernelized_forward_is_different(model, self.model_not_kernelized)\n+        self.assert_kernelized_forward_is_the_same(model, self.model_kernelized)\n+        del model\n+\n+    def test_setter_use_kernels(self):\n+        model = copy.deepcopy(self.model_not_kernelized)\n+        model.use_kernels = True\n+        self.assertTrue(model.use_kernels)\n+        self.assert_kernelized_forward_is_different(model, self.model_not_kernelized)\n+        self.assert_kernelized_forward_is_the_same(model, self.model_kernelized)\n+        del model\n+\n+    def test_unkernelize(self):\n+        model = copy.deepcopy(self.model_kernelized)\n+\n+        with self.assertLogs(\"transformers.modeling_utils\", level=\"WARNING\") as cm:\n+            model.use_kernels = False\n+\n+        self.assertTrue(\n+            any(\n+                \"Disabling kernels at runtime is a no-op as there is no 'unkernelize' routine; keeping current kernels active.\"\n+                in msg\n+                for msg in cm.output\n+            )\n+        )\n+\n+        self.assertFalse(model.use_kernels)\n+        del model\n+\n+    def test_kernels_mapping(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm\": \"kernels-community/layer_norm:LlamaRMSNorm\"})\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+        )\n+\n+        EXPECTED_OUTPUT = set()\n+        EXPECTED_OUTPUT.add(\"Hello, I'm looking for a reliable and trustworthy online\")\n+\n+        tokenized_input = self.tokenizer(self.input, return_tensors=\"pt\").input_ids.to(model.device)\n+        output = model.generate(tokenized_input, max_new_tokens=10, do_sample=False)\n+        output = self.tokenizer.decode(output[0], skip_special_tokens=True)\n+        self.assertTrue(output in EXPECTED_OUTPUT)\n+\n+        del model\n+\n+    def test_faulty_kernel_mapping_layer_name(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm1\": \"kernels-community/layer_norm:LlamaRMSNorm\"})\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(\n+                \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+            )\n+\n+    def test_faulty_kernel_mapping_type(self):\n+        kernel_config = KernelConfig(kernel_mapping={\"RMSNorm\": 1})\n+        with self.assertRaises(ValueError):\n+            _ = AutoModelForCausalLM.from_pretrained(\n+                \"unsloth/Llama-3.2-1B-Instruct\", use_kernels=True, device_map=torch_device, kernel_config=kernel_config\n+            )\n+\n+\n+@require_kernels\n+class TestKernelUtilities(TestCasePlus):\n+    def test_is_kernel_regex(self):\n+        valid = [\n+            \"org/model\",\n+            \"org/model@main\",\n+            \"org/model:my_func\",\n+            \"org/model@v1.2.3:my_func\",\n+            \"flash|org/model@rev:fn\",\n+        ]\n+        invalid = [\n+            \"org//model\",\n+            \"org/model:too:many\",\n+            \"org/model@rev:fn:extra\",\n+            \"/org/model\",\n+            \"org:model\",\n+        ]\n+        for s in valid:\n+            self.assertTrue(is_kernel(s.split(\"|\")[-1]))\n+        for s in invalid:\n+            self.assertFalse(is_kernel(s))\n+\n+    def test_lazy_load_kernel_success_and_cache(self):\n+        sentinel = types.SimpleNamespace(name=\"sentinel\")\n+\n+        original_get_kernel = getattr(kernels_pkg, \"get_kernel\")\n+        try:\n+\n+            def fake_get_kernel(repo_id, revision=None, version=None):\n+                self.assertIn(repo_id, {\"kernels-community/causal-conv1d\"})\n+                return sentinel\n+\n+            setattr(kernels_pkg, \"get_kernel\", fake_get_kernel)\n+            _KERNEL_MODULE_MAPPING.pop(\"causal-conv1d\", None)\n+\n+            mod1 = lazy_load_kernel(\"causal-conv1d\")\n+            self.assertIs(mod1, sentinel)\n+            mod2 = lazy_load_kernel(\"causal-conv1d\")\n+            self.assertIs(mod2, sentinel)\n+        finally:\n+            setattr(kernels_pkg, \"get_kernel\", original_get_kernel)\n+            # Ensure cache is cleared to avoid holding onto module references across tests\n+            _KERNEL_MODULE_MAPPING.pop(\"causal-conv1d\", None)\n+\n+    def test_lazy_load_kernel_unknown(self):\n+        name = \"unknown-kernel-name\"\n+        _KERNEL_MODULE_MAPPING.pop(name, None)\n+        mod = lazy_load_kernel(name)\n+        self.assertIsNone(mod)\n+        self.assertIn(name, _KERNEL_MODULE_MAPPING)\n+        # Cleanup cache entry to avoid growth across tests\n+        _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+    def test_lazy_load_kernel_version(self):\n+        HUB = _HUB_KERNEL_MAPPING\n+        name = \"causal-conv1d\"\n+        version_spec = \">=0.0.4,<0.1.0\"\n+        original_get_kernel = getattr(kernels_pkg, \"get_kernel\")\n+        original_entry = HUB.get(name, None)\n+\n+        # Use a real ModuleType so caching short-circuits on the second call\n+        sentinel_mod = types.ModuleType(\"sentinel_kernel_module\")\n+        call_count = {\"n\": 0}\n+\n+        try:\n+            # Inject dict-style mapping with repo_id and version\n+            HUB[name] = {\"repo_id\": \"kernels-community/causal-conv1d\", \"version\": version_spec}  # type: ignore[assignment]\n+            _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+            def fake_get_kernel(repo_id, revision=None, version=None, user_agent=None):\n+                call_count[\"n\"] += 1\n+                self.assertEqual(repo_id, \"kernels-community/causal-conv1d\")\n+                self.assertIsNone(revision, \"revision must not be set when version is provided\")\n+                self.assertEqual(version, version_spec)\n+                return sentinel_mod\n+\n+            # Patch kernels.get_kernel so lazy_load_kernel picks it up on import\n+            setattr(kernels_pkg, \"get_kernel\", fake_get_kernel)\n+\n+            # Act\n+            mod1 = lazy_load_kernel(name)\n+            mod2 = lazy_load_kernel(name)\n+\n+            # Assert\n+            self.assertIs(mod1, sentinel_mod)\n+            self.assertIs(mod2, sentinel_mod)\n+            self.assertEqual(call_count[\"n\"], 1, \"second call should hit the cache\")\n+        finally:\n+            # Restore patched function and mapping to avoid side effects\n+            setattr(kernels_pkg, \"get_kernel\", original_get_kernel)\n+            if original_entry is None:\n+                HUB.pop(name, None)\n+            else:\n+                HUB[name] = original_entry\n+            _KERNEL_MODULE_MAPPING.pop(name, None)\n+\n+\n+@require_kernels\n+class TestAttentionKernelRegistration(TestCasePlus):\n+    def test_load_and_register_flash_attn_like_kernel(self):\n+        kernel_obj = types.SimpleNamespace(flash_attn_varlen_func=lambda *a, **k: None)\n+\n+        with (\n+            patch(\"transformers.integrations.hub_kernels.get_kernel\", return_value=kernel_obj),\n+            patch(\"transformers.integrations.hub_kernels.lazy_import_flash_attention\", return_value=None),\n+        ):\n+            attn_impl = \"org/model\"\n+            load_and_register_attn_kernel(attn_impl)\n+            self.assertIn(attn_impl, ALL_ATTENTION_FUNCTIONS.valid_keys())\n+            # Cleanup registration to avoid leaking functions across tests\n+            try:\n+                ALL_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+            try:\n+                ALL_MASK_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+\n+    def test_load_and_register_named_function_kernel(self):\n+        def my_attention(*args, **kwargs):\n+            return None\n+\n+        kernel_obj = types.SimpleNamespace(my_func=my_attention)\n+        with patch(\"transformers.integrations.hub_kernels.get_kernel\", return_value=kernel_obj):\n+            attn_impl = \"org/model:my_func\"\n+            load_and_register_attn_kernel(attn_impl)\n+            self.assertIn(attn_impl, ALL_ATTENTION_FUNCTIONS.valid_keys())\n+            # Cleanup registration to avoid leaking functions across tests\n+            try:\n+                ALL_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+            try:\n+                ALL_MASK_ATTENTION_FUNCTIONS.pop(attn_impl, None)\n+            except Exception:\n+                pass\n+\n+\n+@require_kernels\n+class TestUseKernelsLifecycle(TestCasePlus):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model_id = \"unsloth/Llama-3.2-1B-Instruct\"\n+        cls.model = AutoModelForCausalLM.from_pretrained(cls.model_id, use_kernels=False, device_map=torch_device)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        # Delete large objects to drop references early\n+        if hasattr(cls, \"model\"):\n+            try:\n+                del cls.model\n+            except Exception:\n+                pass\n+\n+    def tearDown(self):\n+        # Free accelerator memory/cache and trigger GC\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_setting_use_kernels_twice_does_not_rekernelize(self):\n+        call_count = {\"n\": 0}\n+\n+        def spy_kernelize(*args, **kwargs):\n+            call_count[\"n\"] += 1\n+\n+        with patch.object(kernels_pkg, \"kernelize\", side_effect=spy_kernelize):\n+            self.model.use_kernels = True\n+            self.assertTrue(self.model.use_kernels)\n+            self.assertEqual(call_count[\"n\"], 1)\n+            self.model.use_kernels = True\n+            self.assertEqual(call_count[\"n\"], 1)\n+\n+    def test_train_eval_calls_kernelize_with_correct_mode(self):\n+        last_modes = []\n+\n+        def spy_kernelize(model, device=None, mode=None):\n+            last_modes.append(mode)\n+\n+        with patch.object(kernels_pkg, \"kernelize\", side_effect=spy_kernelize):\n+            self.model.use_kernels = True\n+            self.model.train(True)\n+            self.assertTrue(any(m == Mode.TRAINING for m in last_modes))\n+            self.model.eval()\n+            self.assertTrue(any(m == Mode.INFERENCE for m in last_modes))"
        },
        {
            "sha": "ca734646b5c2b4ace8e02eed0d953cd8857f3328",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a623cda4271c739bd53b874b713d4479a19ff907/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a623cda4271c739bd53b874b713d4479a19ff907/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=a623cda4271c739bd53b874b713d4479a19ff907",
            "patch": "@@ -40,6 +40,7 @@\n     \"run_examples_gpu\": \"Examples directory\",\n     \"run_torch_cuda_extensions_gpu\": \"DeepSpeed\",\n     \"run_quantization_torch_gpu\": \"Quantization\",\n+    \"run_kernels_gpu\": \"Kernels\",\n }\n \n # The values are used as the file names where to save the corresponding CI job results.\n@@ -50,6 +51,7 @@\n     \"Examples directory\": \"example\",\n     \"DeepSpeed\": \"deepspeed\",\n     \"Quantization\": \"quantization\",\n+    \"Kernels\": \"kernels\",\n }\n \n NON_MODEL_TEST_MODULES = [\n@@ -65,6 +67,7 @@\n     \"utils\",\n     \"fsdp\",\n     \"quantization\",\n+    \"kernels\",\n ]\n \n \n@@ -1301,6 +1304,7 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n         \"PyTorch pipelines\": \"run_pipelines_torch_gpu_test_reports\",\n         \"Examples directory\": \"run_examples_gpu_test_reports\",\n         \"DeepSpeed\": \"run_torch_cuda_extensions_gpu_test_reports\",\n+        \"Kernels\": \"run_kernels_gpu_test_reports\",\n     }\n \n     if ci_event in [\"push\", \"Nightly CI\"] or ci_event.startswith(\"Past CI\"):"
        }
    ],
    "stats": {
        "total": 495,
        "additions": 491,
        "deletions": 4
    }
}