{
    "author": "cyyever",
    "message": "Improve typing in TrainingArgument (#36944)\n\n* Better error message in TrainingArgument typing checks\n\n* Better typing\n\n* Small fixes\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "ae3e4e2d97d7342a626ce5bd36e19fdcc07d4d35",
    "files": [
        {
            "sha": "04a4972e0a7c6c461cc6127bae61a059813db882",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae3e4e2d97d7342a626ce5bd36e19fdcc07d4d35/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae3e4e2d97d7342a626ce5bd36e19fdcc07d4d35/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=ae3e4e2d97d7342a626ce5bd36e19fdcc07d4d35",
            "patch": "@@ -907,7 +907,7 @@ class TrainingArguments:\n         default=\"linear\",\n         metadata={\"help\": \"The scheduler type to use.\"},\n     )\n-    lr_scheduler_kwargs: Optional[Union[dict, str]] = field(\n+    lr_scheduler_kwargs: Optional[Union[dict[str, Any], str]] = field(\n         default_factory=dict,\n         metadata={\n             \"help\": (\n@@ -1230,11 +1230,11 @@ class TrainingArguments:\n             )\n         },\n     )\n-    fsdp_config: Optional[Union[dict, str]] = field(\n+    fsdp_config: Optional[Union[dict[str, Any], str]] = field(\n         default=None,\n         metadata={\n             \"help\": (\n-                \"Config to be used with FSDP (Pytorch Fully Sharded  Data Parallel). The value is either a \"\n+                \"Config to be used with FSDP (Pytorch Fully Sharded Data Parallel). The value is either a \"\n                 \"fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.\"\n             )\n         },\n@@ -1366,7 +1366,7 @@ class TrainingArguments:\n             \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n         },\n     )\n-    gradient_checkpointing_kwargs: Optional[Union[dict, str]] = field(\n+    gradient_checkpointing_kwargs: Optional[Union[dict[str, Any], str]] = field(\n         default=None,\n         metadata={\n             \"help\": \"Gradient checkpointing key word arguments such as `use_reentrant`. Will be passed to `torch.utils.checkpoint.checkpoint` through `model.gradient_checkpointing_enable`.\"\n@@ -1451,7 +1451,7 @@ class TrainingArguments:\n             )\n         },\n     )\n-    ddp_timeout: Optional[int] = field(\n+    ddp_timeout: int = field(\n         default=1800,\n         metadata={\n             \"help\": \"Overrides the default timeout for distributed training (value should be given in seconds).\"\n@@ -1667,7 +1667,7 @@ def __post_init__(self):\n         ) and self.metric_for_best_model is None:\n             self.metric_for_best_model = \"loss\"\n         if self.greater_is_better is None and self.metric_for_best_model is not None:\n-            self.greater_is_better = not (self.metric_for_best_model.endswith(\"loss\"))\n+            self.greater_is_better = not self.metric_for_best_model.endswith(\"loss\")\n         if self.run_name is None:\n             self.run_name = self.output_dir\n         if self.framework == \"pt\" and is_torch_available():\n@@ -2140,7 +2140,7 @@ def _setup_devices(self) -> \"torch.device\":\n                     f\"Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n                 )\n         # We delay the init of `PartialState` to the end for clarity\n-        accelerator_state_kwargs = {\"enabled\": True, \"use_configured_state\": False}\n+        accelerator_state_kwargs: dict[str, Any] = {\"enabled\": True, \"use_configured_state\": False}\n         if isinstance(self.accelerator_config, AcceleratorConfig):\n             accelerator_state_kwargs[\"use_configured_state\"] = self.accelerator_config.pop(\n                 \"use_configured_state\", False"
        },
        {
            "sha": "dbcc5b006609d7104ef087577eede067da56b576",
            "filename": "tests/utils/test_hf_argparser.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae3e4e2d97d7342a626ce5bd36e19fdcc07d4d35/tests%2Futils%2Ftest_hf_argparser.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae3e4e2d97d7342a626ce5bd36e19fdcc07d4d35/tests%2Futils%2Ftest_hf_argparser.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_hf_argparser.py?ref=ae3e4e2d97d7342a626ce5bd36e19fdcc07d4d35",
            "patch": "@@ -446,7 +446,7 @@ def test_valid_dict_annotation(self):\n         self.assertEqual(\n             len(raw_dict_fields),\n             0,\n-            \"Found invalid raw `dict` types in the `TrainingArgument` typings. \"\n+            f\"Found invalid raw `dict` types in the `TrainingArgument` typings, which are {raw_dict_fields}. \"\n             \"This leads to issues with the CLI. Please turn this into `typing.Optional[dict,str]`\",\n         )\n "
        }
    ],
    "stats": {
        "total": 16,
        "additions": 8,
        "deletions": 8
    }
}