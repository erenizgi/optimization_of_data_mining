{
    "author": "jiqing-feng",
    "message": "fix check inputs for text2text pipeline (#41556)\n\nfix check inputs\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "143acfe2ceb06b2bc39c2c59412949cfc4c8ed08",
    "files": [
        {
            "sha": "afaf1e080b9f08f6e91192864185d26646482eda",
            "filename": "src/transformers/pipelines/text2text_generation.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/143acfe2ceb06b2bc39c2c59412949cfc4c8ed08/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/143acfe2ceb06b2bc39c2c59412949cfc4c8ed08/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext2text_generation.py?ref=143acfe2ceb06b2bc39c2c59412949cfc4c8ed08",
            "patch": "@@ -194,17 +194,13 @@ def preprocess(self, inputs, truncation=TruncationStrategy.DO_NOT_TRUNCATE, **kw\n \n     def _forward(self, model_inputs, **generate_kwargs):\n         in_b, input_length = model_inputs[\"input_ids\"].shape\n-\n-        self.check_inputs(\n-            input_length,\n-            generate_kwargs.get(\"min_length\", self.generation_config.min_length),\n-            generate_kwargs.get(\"max_new_tokens\", self.generation_config.max_new_tokens),\n-        )\n-\n         # User-defined `generation_config` passed to the pipeline call take precedence\n         if \"generation_config\" not in generate_kwargs:\n             generate_kwargs[\"generation_config\"] = self.generation_config\n \n+        min_length = generate_kwargs.get(\"min_length\", generate_kwargs[\"generation_config\"].min_length)\n+        max_new_tokens = generate_kwargs.get(\"max_new_tokens\", generate_kwargs[\"generation_config\"].max_new_tokens)\n+        self.check_inputs(input_length, min_length, max_new_tokens)\n         output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n         out_b = output_ids.shape[0]\n         output_ids = output_ids.reshape(in_b, out_b // in_b, *output_ids.shape[1:])"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 3,
        "deletions": 7
    }
}