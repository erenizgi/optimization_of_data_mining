{
    "author": "tugsbayasgalan",
    "message": "Support tracable dynamicKVcache (#36311)\n\n* Support tracable dynamicKVcache\n\n* Fix lint\n\n* More fine grained test\n\n* Lint\n\n* Update\n\n* Update\n\n* Fix up\n\n* Apply suggestions from code review\n\n* Update src/transformers/cache_utils.py\n\n* Update tests/utils/test_cache_utils.py\n\n* Apply suggestions from code review\n\n* Update\n\n* Change error message\n\n* Rename\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\n* Apply suggestions from code review\n\n---------\n\nCo-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "f39f4960f30e3eadd6d948e4dcb2da32eda253b5",
    "files": [
        {
            "sha": "d873a17d36976a35ffd6aa3fcf0c35cc16eb27d7",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39f4960f30e3eadd6d948e4dcb2da32eda253b5/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39f4960f30e3eadd6d948e4dcb2da32eda253b5/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=f39f4960f30e3eadd6d948e4dcb2da32eda253b5",
            "patch": "@@ -8,6 +8,8 @@\n import torch\n from packaging import version\n \n+from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_6\n+\n from .configuration_utils import PretrainedConfig\n from .utils import is_hqq_available, is_optimum_quanto_available, is_torch_greater_or_equal, logging\n \n@@ -535,6 +537,64 @@ def batch_select_indices(self, indices: torch.Tensor):\n             self.value_cache[layer_idx] = self.value_cache[layer_idx][indices, ...]\n \n \n+def _flatten_dynamic_cache(\n+    dynamic_cache: DynamicCache,\n+):\n+    \"\"\"Flattens DynamicCache into flat list of tensors for `torch.export.export` to consume\"\"\"\n+    if not isinstance(dynamic_cache, DynamicCache):\n+        raise RuntimeError(\"This pytree flattening function should only be applied to DynamicCache\")\n+\n+    if not is_torch_greater_or_equal_than_2_6:\n+        logger.warning_once(\n+            \"DynamicCache + torch.export is tested on torch 2.6.0+ and may not work on earlier versions.\"\n+        )\n+\n+    # NOTE it seems _seen_tokens is deprecated, so probably doesn't need tracking\n+    dictionary = {\n+        \"key_cache\": getattr(dynamic_cache, \"key_cache\"),\n+        \"value_cache\": getattr(dynamic_cache, \"value_cache\"),\n+    }\n+    return torch.utils._pytree._dict_flatten(dictionary)\n+\n+\n+def _flatten_with_keys_dynamic_cache(dynamic_cache: DynamicCache):\n+    dictionary = {\n+        \"key_cache\": getattr(dynamic_cache, \"key_cache\"),\n+        \"value_cache\": getattr(dynamic_cache, \"value_cache\"),\n+    }\n+    return torch.utils._pytree._dict_flatten_with_keys(dictionary)\n+\n+\n+def _unflatten_dynamic_cache(\n+    values,\n+    context: torch.utils._pytree.Context,\n+):\n+    dictionary = torch.utils._pytree._dict_unflatten(values, context)\n+    cache = DynamicCache()\n+    for k, v in dictionary.items():\n+        setattr(cache, k, v)\n+    return cache\n+\n+\n+def _flatten_dynamic_cache_for_fx(cache, spec):\n+    dictionary = {\n+        \"key_cache\": getattr(cache, \"key_cache\"),\n+        \"value_cache\": getattr(cache, \"value_cache\"),\n+    }\n+    return torch.utils._pytree.tree_flatten(dictionary)[0]\n+\n+\n+torch.utils._pytree.register_pytree_node(\n+    DynamicCache,\n+    _flatten_dynamic_cache,\n+    _unflatten_dynamic_cache,\n+    serialized_type_name=f\"{DynamicCache.__module__}.{DynamicCache.__name__}\",\n+    flatten_with_keys_fn=_flatten_with_keys_dynamic_cache,\n+)\n+# TODO (tmanlaibaatar) This won't be needed in torch 2.7.\n+torch.fx._pytree.register_pytree_flatten_spec(DynamicCache, _flatten_dynamic_cache_for_fx)\n+\n+\n class OffloadedCache(DynamicCache):\n     \"\"\"\n     A drop-in replacement for DynamicCache that conserves accelerator(GPU, XPU) memory at the expense of more CPU memory."
        },
        {
            "sha": "d16d07f597f4a7139066ad2209412d0048c17830",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39f4960f30e3eadd6d948e4dcb2da32eda253b5/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39f4960f30e3eadd6d948e4dcb2da32eda253b5/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=f39f4960f30e3eadd6d948e4dcb2da32eda253b5",
            "patch": "@@ -31,6 +31,7 @@\n \n parsed_torch_version_base = version.parse(version.parse(torch.__version__).base_version)\n \n+is_torch_greater_or_equal_than_2_6 = parsed_torch_version_base >= version.parse(\"2.6\")\n is_torch_greater_or_equal_than_2_4 = parsed_torch_version_base >= version.parse(\"2.4\")\n is_torch_greater_or_equal_than_2_3 = parsed_torch_version_base >= version.parse(\"2.3\")\n is_torch_greater_or_equal_than_2_2 = parsed_torch_version_base >= version.parse(\"2.2\")\n@@ -46,10 +47,7 @@\n \n if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n     from torch.distributed.tensor import Replicate\n-    from torch.distributed.tensor.parallel import (\n-        ColwiseParallel,\n-        RowwiseParallel,\n-    )\n+    from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\n \n \n def softmax_backward_data(parent, grad_output, output, dim, self):"
        },
        {
            "sha": "fd5b720d90017c947e37242f606372f292cc48d2",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39f4960f30e3eadd6d948e4dcb2da32eda253b5/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39f4960f30e3eadd6d948e4dcb2da32eda253b5/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=f39f4960f30e3eadd6d948e4dcb2da32eda253b5",
            "patch": "@@ -174,6 +174,60 @@ def _random_kvs(config):\n         self.assertTrue(cached_keys.shape == (1, 1, 10, 128))\n         self.assertTrue(cached_values.shape == (1, 1, 10, 128))\n \n+    def test_dynamic_cache_exportability(self):\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+        model = model.eval()\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+        prompt = \"What is the best way to debug python script?\"\n+        inputs = tokenizer(prompt, return_tensors=\"pt\")\n+        attention_mask = inputs.attention_mask\n+        input_ids = inputs.input_ids\n+\n+        past_key_values = DynamicCache()\n+        ep = torch.export.export(\n+            model,\n+            (),\n+            {\n+                \"input_ids\": input_ids,\n+                \"attention_mask\": attention_mask,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": True,\n+            },\n+            strict=False,\n+        )\n+        res = ep.module()(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            use_cache=True,\n+        )\n+        self.assertTrue(len(res.past_key_values.key_cache) == model.config.num_hidden_layers)\n+        self.assertEqual(2 * model.config.num_hidden_layers + 1, len(ep.graph_signature.output_specs))\n+        self.assertEqual(\n+            3,\n+            len(\n+                [\n+                    x\n+                    for x in ep.graph_signature.input_specs\n+                    if x.kind == torch.export.graph_signature.InputKind.USER_INPUT\n+                ]\n+            ),\n+        )\n+\n+        past_key_values_eager = DynamicCache()\n+        res_eager = model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values_eager,\n+            use_cache=True,\n+        )\n+        self.assertTrue(torch.allclose(res.logits, res_eager.logits))\n+        for k1, k2 in zip(res.past_key_values.key_cache, res_eager.past_key_values.key_cache):\n+            self.assertTrue(torch.allclose(k1, k2))\n+\n+        for v1, v2 in zip(res.past_key_values.value_cache, res_eager.past_key_values.value_cache):\n+            self.assertTrue(torch.allclose(v1, v2))\n+\n     @slow\n     @require_read_token\n     def test_static_cache_exportability(self):"
        }
    ],
    "stats": {
        "total": 120,
        "additions": 116,
        "deletions": 4
    }
}