{
    "author": "jerryzh168",
    "message": "[torchao] Add regex support for ModuleFqnToConfig (#41242)\n\n* Add regex support for ModuleFqnToConfig\n\nSummary:\nSimilar to https://github.com/pytorch/ao/pull/3084 we added regex support\nin transformers so people can use regex to quantize the models.\n\nSee https://github.com/pytorch/ao/pull/3084 for docs and precedence of different\nconfigurations\n\nUploaded model: https://huggingface.co/torchao-testing/opt-125m-ModuleFqnToConfig-v1-regex-0.14.0.dev\n\nTest Plan:\npytest tests/quantization/torchao_integration/test_torchao.py -k test_module_fqn_to_config_regex\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\n* Apply style fixes\n\n* add assert for\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "2166e26cb1999d1fc6555bd4c1974c4464d1ea57",
    "files": [
        {
            "sha": "dae7123999d6746091bf085d50b3d93c74affac3",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 114,
            "deletions": 1,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/2166e26cb1999d1fc6555bd4c1974c4464d1ea57/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2166e26cb1999d1fc6555bd4c1974c4464d1ea57/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=2166e26cb1999d1fc6555bd4c1974c4464d1ea57",
            "patch": "@@ -445,7 +445,7 @@ output_text = tokenizer.batch_decode(\n print(output_text)\n ```\n \n-#### 2. Quantizing different layers with different quantization configs\n+#### 2. Quantizing different layers with different quantization configs (no regex)\n \n ```py\n import torch\n@@ -484,6 +484,119 @@ output_text = tokenizer.batch_decode(\n print(output_text)\n ```\n \n+#### 3. Quantizing different layers with different quantization configs (with regex)\n+We can also use regex to specify the config for all modules that has `module_fqn` that\n+matches the regex, all regex should start with `re:`, for example `re:layers\\..*\\.gate_proj` will\n+match all layers like `layers.0.gate_proj`. See [here](https://github.com/pytorch/ao/blob/2fe0ca0899c730c528efdbec8886feaa38879f39/torchao/quantization/quant_api.py#L2392) for docs.\n+\n+```py\n+import logging\n+\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n+\n+# Configure logging to see warnings and debug information\n+logging.basicConfig(\n+    level=logging.INFO, format=\"%(name)s - %(levelname)s - %(message)s\"\n+)\n+\n+# Enable specific loggers that might contain the serialization warnings\n+logging.getLogger(\"transformers\").setLevel(logging.INFO)\n+logging.getLogger(\"torchao\").setLevel(logging.INFO)\n+logging.getLogger(\"safetensors\").setLevel(logging.INFO)\n+logging.getLogger(\"huggingface_hub\").setLevel(logging.INFO)\n+\n+model_id = \"facebook/opt-125m\"\n+\n+from torchao.quantization import (\n+    Float8DynamicActivationFloat8WeightConfig,\n+    Int4WeightOnlyConfig,\n+    IntxWeightOnlyConfig,\n+    PerRow,\n+    PerAxis,\n+    ModuleFqnToConfig,\n+    Float8Tensor,\n+    Int4TilePackedTo4dTensor,\n+    IntxUnpackedToInt8Tensor,\n+)\n+\n+float8dyn = Float8DynamicActivationFloat8WeightConfig(granularity=PerRow())\n+int4wo = Int4WeightOnlyConfig(int4_packing_format=\"tile_packed_to_4d\")\n+intxwo = IntxWeightOnlyConfig(weight_dtype=torch.int8, granularity=PerAxis(0))\n+\n+qconfig_dict = {\n+    # highest priority\n+    \"model.decoder.layers.3.self_attn.q_proj\": int4wo,\n+    \"model.decoder.layers.3.self_attn.k_proj\": int4wo,\n+    \"model.decoder.layers.3.self_attn.v_proj\": int4wo,\n+    # vllm\n+    \"model.decoder.layers.3.self_attn.qkv_proj\": int4wo,\n+\n+    \"re:model\\.decoder\\.layers\\..+\\.self_attn\\.q_proj\": float8dyn,\n+    \"re:model\\.decoder\\.layers\\..+\\.self_attn\\.k_proj\": float8dyn,\n+    \"re:model\\.decoder\\.layers\\..+\\.self_attn\\.v_proj\": float8dyn,\n+    # this should not take effect and we'll fallback to _default\n+    # since no full mach (missing `j` in the end)\n+    \"re:model\\.decoder\\.layers\\..+\\.self_attn\\.out_pro\": float8dyn,\n+    # vllm\n+    \"re:model\\.decoder\\.layers\\..+\\.self_attn\\.qkv_proj\": float8dyn,\n+\n+    \"_default\": intxwo,\n+}\n+quant_config = ModuleFqnToConfig(qconfig_dict)\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+quantized_model = AutoModelForCausalLM.from_pretrained(\n+    model_id,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+    quantization_config=quantization_config,\n+)\n+print(\"quantized model:\", quantized_model)\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+for i in range(12):\n+    if i == 3:\n+        assert isinstance(quantized_model.model.decoder.layers[i].self_attn.q_proj.weight, Int4TilePackedTo4dTensor)\n+        assert isinstance(quantized_model.model.decoder.layers[i].self_attn.k_proj.weight, Int4TilePackedTo4dTensor)\n+        assert isinstance(quantized_model.model.decoder.layers[i].self_attn.v_proj.weight, Int4TilePackedTo4dTensor)\n+    else:\n+        assert isinstance(quantized_model.model.decoder.layers[i].self_attn.q_proj.weight, Float8Tensor)\n+        assert isinstance(quantized_model.model.decoder.layers[i].self_attn.k_proj.weight, Float8Tensor)\n+        assert isinstance(quantized_model.model.decoder.layers[i].self_attn.v_proj.weight, Float8Tensor)\n+    assert isinstance(quantized_model.model.decoder.layers[i].self_attn.out_proj.weight, IntxUnpackedToInt8Tensor)\n+\n+# Manual Testing\n+prompt = \"What are we having for dinner?\"\n+print(\"Prompt:\", prompt)\n+inputs = tokenizer(\n+    prompt,\n+    return_tensors=\"pt\",\n+).to(\"cuda\")\n+# setting temperature to 0 to make sure result deterministic\n+generated_ids = quantized_model.generate(**inputs, max_new_tokens=128, temperature=0)\n+\n+correct_output_text = tokenizer.batch_decode(\n+    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+print(\"Response:\", correct_output_text[0][len(prompt) :])\n+\n+\n+# Load model from saved checkpoint\n+reloaded_model = AutoModelForCausalLM.from_pretrained(\n+    save_to,\n+    device_map=\"cuda:0\",\n+    torch_dtype=torch.bfloat16,\n+    # quantization_config=quantization_config,\n+)\n+\n+generated_ids = reloaded_model.generate(**inputs, max_new_tokens=128, temperature=0)\n+output_text = tokenizer.batch_decode(\n+    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+print(\"Response:\", output_text[0][len(prompt) :])\n+\n+assert(correct_output_text == output_text)\n+```\n+\n ### Autoquant\n \n If you want to automatically choose a quantization type for quantizable layers (`nn.Linear`) you can use the [autoquant](https://pytorch.org/ao/stable/generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant) API."
        },
        {
            "sha": "44f0be43da807386c4d97d5dbf9bb71438a5b66e",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/2166e26cb1999d1fc6555bd4c1974c4464d1ea57/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2166e26cb1999d1fc6555bd4c1974c4464d1ea57/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=2166e26cb1999d1fc6555bd4c1974c4464d1ea57",
            "patch": "@@ -328,9 +328,21 @@ def create_quantized_param(\n                     module_fqn, _ = param_name.rsplit(\".\", 1)\n                     c = None\n                     if module_fqn in config.module_fqn_to_config:\n+                        assert not module_fqn.startswith(\"re:\"), (\n+                            \"module fqn should not start with`re:`, which is used for specifying regex\"\n+                        )\n                         c = config.module_fqn_to_config[module_fqn]\n                     else:\n-                        c = config.module_fqn_to_config.get(\"_default\", None)\n+                        for maybe_module_fqn_pattern in config.module_fqn_to_config:\n+                            if not maybe_module_fqn_pattern.startswith(\"re:\"):\n+                                continue\n+                            elif re.fullmatch(maybe_module_fqn_pattern[3:], module_fqn):\n+                                # we'll apply the config for first fully matched pattern\n+                                c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n+                                break\n+                        else:\n+                            c = config.module_fqn_to_config.get(\"_default\", None)\n+\n                     if c is not None:\n                         # filter_fn: not filtering out any modules\n                         quantize_(module, c, filter_fn=lambda x, fqn: True)"
        },
        {
            "sha": "896e999d7666683a9f02f8e49e249c542a1cd155",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 95,
            "deletions": 0,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/2166e26cb1999d1fc6555bd4c1974c4464d1ea57/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2166e26cb1999d1fc6555bd4c1974c4464d1ea57/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=2166e26cb1999d1fc6555bd4c1974c4464d1ea57",
            "patch": "@@ -46,6 +46,8 @@\n         TensorCoreTiledLayout,\n     )\n     from torchao.quantization import (\n+        Float8Tensor,\n+        Float8WeightOnlyConfig,\n         Int8WeightOnlyConfig,\n         IntxWeightOnlyConfig,\n         MappingType,\n@@ -277,6 +279,99 @@ def test_per_module_config_skip(self):\n         ]\n         self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n \n+    @require_torchao_version_greater_or_equal(\"0.13.0\")\n+    def test_module_fqn_to_config_regex_basic(self):\n+        linear_config = Int8WeightOnlyConfig()\n+        config = ModuleFqnToConfig({\"_default\": linear_config, r\"re:model\\.layers\\..+\\.self_attn\\.q_proj\": None})\n+        quant_config = TorchAoConfig(quant_type=config)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=self.device,\n+            quantization_config=quant_config,\n+        )\n+        # making sure `model.layers.0.self_attn.q_proj` is skipped\n+        self.assertTrue(not isinstance(quantized_model.model.layers[0].self_attn.q_proj.weight, AffineQuantizedTensor))\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n+    @require_torchao_version_greater_or_equal(\"0.13.0\")\n+    def test_module_fqn_to_config_regex_fullmatch(self):\n+        \"\"\"Testing that we will only match the fqns that fully\n+        matches the regex\n+        \"\"\"\n+        linear1_config = Int8WeightOnlyConfig()\n+        linear2_config = Float8WeightOnlyConfig()\n+        # intentially removing `j` after `q_proj` so it's not a full match\n+        config = ModuleFqnToConfig(\n+            {\n+                r\"re:model\\.layers\\.+\\.self_attn\\.q_pro\": linear1_config,\n+                \"model.layers.3.self_attn.q_proj\": linear2_config,\n+            }\n+        )\n+        quant_config = TorchAoConfig(quant_type=config)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=self.device,\n+            quantization_config=quant_config,\n+        )\n+        # highest precedence is fully specified module fqn\n+        self.assertTrue(isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, Float8Tensor))\n+        # because regex `model\\.layers\\.+*\\.self_attn\\.q_pro` didin't fully match `model.layers.1.self_attn.q_proj` (missing last `j`)\n+        # this layer is not expected to be quantized to int8\n+        self.assertTrue(not isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n+    @require_torchao_version_greater_or_equal(\"0.13.0\")\n+    def test_module_fqn_to_config_regex_precedence(self):\n+        linear1_config = Int8WeightOnlyConfig()\n+        linear2_config = Float8WeightOnlyConfig()\n+        config = ModuleFqnToConfig(\n+            {\n+                r\"re:model\\.layers\\..+\\.self_attn\\.q_proj\": None,\n+                \"model.layers.3.self_attn.q_proj\": linear2_config,\n+                \"_default\": linear1_config,\n+            }\n+        )\n+        quant_config = TorchAoConfig(quant_type=config)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=self.device,\n+            quantization_config=quant_config,\n+        )\n+        # highest precedence is fully specified module fqn\n+        self.assertTrue(isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, Float8Tensor))\n+        # second precedence: regex\n+        self.assertTrue(not isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n+        # last precedence: _default\n+        self.assertTrue(isinstance(quantized_model.model.layers[1].self_attn.k_proj.weight, AffineQuantizedTensor))\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n \n @require_torch_accelerator\n class TorchAoAcceleratorTest(TorchAoTest):"
        }
    ],
    "stats": {
        "total": 224,
        "additions": 222,
        "deletions": 2
    }
}