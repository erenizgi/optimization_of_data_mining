{
    "author": "SunMarc",
    "message": "Remove deprecated code (#41616)\n\nremove\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "72fd67929b9a877cb2934985629d80b158f37689",
    "files": [
        {
            "sha": "227c877e8ad05c9268d245ea091adce5c4d98b2f",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/72fd67929b9a877cb2934985629d80b158f37689/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72fd67929b9a877cb2934985629d80b158f37689/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=72fd67929b9a877cb2934985629d80b158f37689",
            "patch": "@@ -79,19 +79,6 @@ def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n                 f\"pass `pre_quantized=True` while knowing what you are doing.\"\n             )\n \n-    def update_torch_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        \"\"\"\n-        Deprecared in favor of `update_dtype`!\n-\n-        Args:\n-            dtype (`torch.dtype`):\n-                The input dtype that is passed in `from_pretrained`\n-        \"\"\"\n-        logger.warning_once(\n-            \"`update_torch_dtype` is deprecated in favor of `update_dtype`! It will be removed in version v4.57\"\n-        )\n-        return self.update_dtype(dtype)\n-\n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         \"\"\"\n         Some quantization methods require to explicitly set the dtype of the model to a"
        },
        {
            "sha": "0c11802280cc60872ceb0c222337b2900ce0dc75",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/72fd67929b9a877cb2934985629d80b158f37689/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72fd67929b9a877cb2934985629d80b158f37689/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=72fd67929b9a877cb2934985629d80b158f37689",
            "patch": "@@ -327,7 +327,6 @@ class Trainer:\n             Processing class used to process the data. If provided, will be used to automatically process the inputs\n             for the model, and it will be saved along the model to make it easier to rerun an interrupted training or\n             reuse the fine-tuned model.\n-            This supersedes the `tokenizer` argument, which is now deprecated.\n         model_init (`Callable[[], PreTrainedModel]`, *optional*):\n             A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n             from a new instance of the model as given by this function."
        },
        {
            "sha": "1f4352b48518372b01f53bfeaefa7b294c3813ad",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/72fd67929b9a877cb2934985629d80b158f37689/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72fd67929b9a877cb2934985629d80b158f37689/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=72fd67929b9a877cb2934985629d80b158f37689",
            "patch": "@@ -308,8 +308,6 @@ class TrainerCallback:\n             The object that is returned to the [`Trainer`] and can be used to make some decisions.\n         model ([`PreTrainedModel`] or `torch.nn.Module`):\n             The model being trained.\n-        tokenizer ([`PreTrainedTokenizer`]):\n-            The tokenizer used for encoding the data. This is deprecated in favour of `processing_class`.\n         processing_class ([`PreTrainedTokenizer` or `BaseImageProcessor` or `ProcessorMixin` or `FeatureExtractionMixin`]):\n             The processing class used for encoding the data. Can be a tokenizer, a processor, an image processor or a feature extractor.\n         optimizer (`torch.optim.Optimizer`):"
        },
        {
            "sha": "359b9b24c5a6104a88f329df3021d2bca5812f24",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/72fd67929b9a877cb2934985629d80b158f37689/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72fd67929b9a877cb2934985629d80b158f37689/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=72fd67929b9a877cb2934985629d80b158f37689",
            "patch": "@@ -316,12 +316,6 @@ def __init__(\n                 \"A valid HQQ version (>=0.2.1) is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`.\"\n             )\n \n-        for deprecated_key in [\"quant_zero\", \"quant_scale\", \"offload_meta\"]:\n-            if deprecated_key in kwargs:\n-                logger.info(\n-                    deprecated_key + \" is deprecated. This parameter will be ignored in quantization settings.\"\n-                )\n-\n         if axis is None:\n             axis = 1\n             logger.info(\"Setting axis=1 as faster backends such as TorchAO or BitBlas are only compatible with it.\")\n@@ -773,11 +767,6 @@ def post_init(self):\n             raise ValueError(\"damp_percent must between 0 and 1.\")\n         if self.dataset is not None:\n             if isinstance(self.dataset, str):\n-                if self.dataset in [\"ptb\", \"ptb-new\"]:\n-                    raise ValueError(\n-                        f\"\"\"{self.dataset} dataset was deprecated. You can only choose between\n-                        ['wikitext2','c4','c4-new']\"\"\"\n-                    )\n                 if self.dataset not in [\"wikitext2\", \"c4\", \"c4-new\"]:\n                     raise ValueError(\n                         f\"\"\"You have entered a string value for dataset. You can only choose between"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 0,
        "deletions": 27
    }
}