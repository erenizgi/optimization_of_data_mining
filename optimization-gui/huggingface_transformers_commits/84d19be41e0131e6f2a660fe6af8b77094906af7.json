{
    "author": "qubvel",
    "message": "Apply GradientCheckpointingLayer to the whole repo (#38913)\n\n* first batch (4)\n\n* align\n\n* altclip\n\n* beit\n\n* bert\n\n* yolos\n\n* dino, pvt_v2\n\n* bark, bart, bert_generation\n\n* big_bird, biogpt\n\n* blnderbot, bloom\n\n* bridgetower\n\n* camambert, canine, chameleon\n\n* chinese clip, clap, clip\n\n* codegen, conditional detr, convbert\n\n* dab_detr, data2vec\n\n* dbrx, deberta\n\n* deberta, decicion_tranformer, deformable_detr\n\n* deit, deta, mctct\n\n* detr, dinov2, distilbert\n\n* donut, dpt, electra\n\n* ernie, esm, falcon\n\n* flava, fnet, falcon_mamba\n\n* focalnet, git, gpt2\n\n* gpt - bigcode, neo, neox\n\n* gptj, groupvit\n\n* idefics2, idefics3\n\n* ijepa, imagegpt, internvl\n\n* jetmoe, kosmos2, layoutlm\n\n* layoutlm2-3, led\n\n* lilt, longformer, longt5, luke\n\n* m2m, mamba1-2\n\n* marian, markuplm, mask2former\n\n* maskformer\n\n* mbart, megatron_bert, mimi\n\n* mixtral, mlcd\n\n* mobilevit1-2, modernbert\n\n* moshi, mpt, mra\n\n* mt5, musicgen\n\n* mvp, nemotron\n\n* nllb_moe\n\n* nystromformer, omdet_turbo\n\n* opt, owlvit, owlv2\n\n* pegasus, pegasus_x, presimmon\n\n* phimoe, pix2struct, pixtral\n\n* plbart, pop2piano, prophetnet\n\n* qwen2*\n\n* qwen2, qwen3 moe,  rec gemma\n\n* rembert\n\n* roberta\n\n* roberta prelayernorm\n\n* roc_bert, roformer, rwkv\n\n* sam, sam_hq\n\n* seggpt, smolvlm, speech_to_text\n\n* splinter, stablelm, swin\n\n* swin2sr, switch_transformer, t5, table_transformer\n\n* tapas, time_series_tranformer, timesformer\n\n* trocr, tvp, umt5\n\n* videomae, vilt, visual_bert\n\n* vit, vit_mae, vit_msn\n\n* vitpose_backbone, vits, vivit\n\n* whisper. x_clip, xglm\n\n* xlm_roberta, xmod\n\n* yoso\n\n* zamba\n\n* vitdet, wav2vec2, wav2vec2_bert\n\n* unispeech, wav2vec_conformer\n\n* wavlm\n\n* speecht5\n\n* swinv2\n\n* sew / _d\n\n* seamless_mt4 / _v2\n\n* deprecated models update\n\n* bros\n\n* gemma2, gemma3\n\n* got, hiera, hubert, llama4, mllama, oneformer, phi, olmoe, informer\n\n* fixup\n\n* Add use_cache=False and past_key_value=None to  GradientCheckpointingLayer\n\n* fixup\n\n* fix prophetnet\n\n* fix bigbird_pegasus\n\n* fix blenderbot\n\n* fix mbart\n\n* fix mvp\n\n* fix zamba2\n\n* fix bart\n\n* fix blenderbot_small\n\n* fix codegen\n\n* Update gradient checkpointing layer to support more past_key_values arg names\n\n* fix data2vec vision\n\n* fix deformable_detr\n\n* fix gptj\n\n* fix led\n\n* fix m2m_100\n\n* add comment\n\n* fix nnlb_moe\n\n* Fix pegasus_x\n\n* fix plbart\n\n* udop\n\n* fix-copies: beit, wav2vec2\n\n* fix gpt_bigcode\n\n* fixup\n\n* fix t5\n\n* fix switch_transformers\n\n* fix longt5\n\n* fix mt5\n\n* update tapas\n\n* fix blip2\n\n* update blip\n\n* fix musicgen\n\n* fix gpt2, trocr\n\n* fix copies\n\n* !!! Revert zamba, mllama\n\n* update autoformer\n\n* update bros\n\n* update args / kwargs for BERT and copies\n\n* 2nd round of updates\n\n* update conditional detr\n\n* Pass encoder_hidden_states as positional arg\n\n* Update to pass encoder_decoder_position_bias as positional arg\n\n* fixup\n\n* biogpt modular\n\n* modular gemma2\n\n* modular gemma3\n\n* modular gpt_neox\n\n* modular informer\n\n* modular internvl\n\n* modular mixtral\n\n* modular mlcd\n\n* modular modernbert\n\n* modular phi\n\n* modular qwen2_5_omni\n\n* modular qwen2_5_vl\n\n* modular sam_hq\n\n* modular sew\n\n* wav2vec2_bert\n\n* modular wav2vec2_conformer\n\n* modular wavlm\n\n* fixup\n\n* Update by modular instructblipvideo\n\n* modular data2vec_audio\n\n* nit modular mistral\n\n* apply modular minimax\n\n* fix modular moonshine\n\n* revert zamba2\n\n* fix mask2former\n\n* refactor idefics",
    "sha": "84d19be41e0131e6f2a660fe6af8b77094906af7",
    "files": [
        {
            "sha": "5179cfa6571e05de5cfe86900e5b4dd56e14a1e8",
            "filename": "src/transformers/modeling_layers.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodeling_layers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodeling_layers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_layers.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -16,6 +16,11 @@\n \n import torch.nn as nn\n \n+from transformers.utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n \n class GradientCheckpointingLayer(nn.Module):\n     \"\"\"Base class for layers with gradient checkpointing.\n@@ -44,5 +49,35 @@ class GradientCheckpointingLayer(nn.Module):\n \n     def __call__(self, *args, **kwargs):\n         if self.gradient_checkpointing and self.training:\n+            do_warn = False\n+            layer_name = self.__class__.__name__\n+            message = f\"Caching is incompatible with gradient checkpointing in {layer_name}. Setting\"\n+\n+            if \"use_cache\" in kwargs and kwargs[\"use_cache\"]:\n+                kwargs[\"use_cache\"] = False\n+                message += \" `use_cache=False`,\"\n+                do_warn = True\n+\n+            # different names for the same thing in different layers\n+            if \"past_key_value\" in kwargs and kwargs[\"past_key_value\"] is not None:\n+                kwargs[\"past_key_value\"] = None\n+                message += \" `past_key_value=None`,\"\n+                do_warn = True\n+\n+            if \"past_key_values\" in kwargs and kwargs[\"past_key_values\"] is not None:\n+                kwargs[\"past_key_values\"] = None\n+                message += \" `past_key_values=None`,\"\n+                do_warn = True\n+\n+            if \"layer_past\" in kwargs and kwargs[\"layer_past\"] is not None:\n+                kwargs[\"layer_past\"] = None\n+                message += \" `layer_past=None`,\"\n+                do_warn = True\n+\n+            # warn if anything was changed\n+            if do_warn:\n+                message = message.rstrip(\",\") + \".\"\n+                logger.warning(message)\n+\n             return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)\n         return super().__call__(*args, **kwargs)"
        },
        {
            "sha": "6ff99d6a4918c80d3498324709bd77acddfed6a8",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithNoAttention,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -827,7 +828,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->AlignText\n-class AlignTextLayer(nn.Module):\n+class AlignTextLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -953,27 +954,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "41d4c595c8d1b04cf04d3bdf43f9d1b117395048",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 18,
            "deletions": 38,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n import torch.utils.checkpoint\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -418,7 +419,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaLayer with Roberta->AltRoberta\n-class AltRobertaLayer(nn.Module):\n+class AltRobertaLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -544,27 +545,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:\n@@ -732,7 +721,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class AltCLIPEncoderLayer(nn.Module):\n+class AltCLIPEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: AltCLIPConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -848,21 +837,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "602de3ff72b5cfef95456d93d2aa5e2688170080",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, SequenceClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -282,7 +283,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->AST,VIT->AST\n-class ASTLayer(nn.Module):\n+class ASTLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: ASTConfig) -> None:\n@@ -349,16 +350,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n-\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "be8c8c621a1cbce35a56d63218d16909e61335fe",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 26,
            "deletions": 49,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput, SampleTSPredictionOutput, Seq2SeqTSPredictionOutput\n from ...modeling_utils import PreTrainedModel\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n@@ -670,7 +671,7 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-class AutoformerEncoderLayer(nn.Module):\n+class AutoformerEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: AutoformerConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -744,7 +745,7 @@ def forward(\n         return outputs\n \n \n-class AutoformerDecoderLayer(nn.Module):\n+class AutoformerDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: AutoformerConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1042,21 +1043,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1186,6 +1178,12 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+            )\n+            use_cache = False\n+\n         input_shape = inputs_embeds.size()[:-1]\n \n         # expand encoder attention mask\n@@ -1228,38 +1226,17 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                if use_cache:\n-                    logger.warning(\n-                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                    )\n-                    use_cache = False\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             (hidden_states, residual_trend) = layer_outputs[0]\n             trend = trend + residual_trend\n "
        },
        {
            "sha": "8ace5221c082daaddb7bfa85b05c865f9e353e03",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -31,6 +31,7 @@\n )\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import CausalLMOutputWithPast, MaskedLMOutput\n from ...modeling_utils import PreTrainedModel, get_parameter_device\n from ...utils import (\n@@ -309,7 +310,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class BarkBlock(nn.Module):\n+class BarkBlock(GradientCheckpointingLayer):\n     def __init__(self, config, is_causal=False):\n         super().__init__()\n \n@@ -606,25 +607,14 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    None,\n-                    attention_mask,\n-                    head_mask[i],\n-                    use_cache,\n-                    output_attentions,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states,\n-                    past_key_values=past_layer_key_values,\n-                    attention_mask=attention_mask,\n-                    head_mask=head_mask[i],\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                past_key_values=past_layer_key_values,\n+                attention_mask=attention_mask,\n+                head_mask=head_mask[i],\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = outputs[0]\n "
        },
        {
            "sha": "994bf9d85dca8a4dc8283c208421462f37434c05",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 21,
            "deletions": 46,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -33,6 +33,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -270,7 +271,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class BartEncoderLayer(nn.Module):\n+class BartEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -341,7 +342,7 @@ def forward(\n         return outputs\n \n \n-class BartDecoderLayer(nn.Module):\n+class BartDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -875,21 +876,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1137,35 +1129,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "347471fc7f7af81a91a1538f4baf9b74849706d5",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 11,
            "deletions": 21,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BackboneOutput,\n     BaseModelOutput,\n@@ -497,7 +498,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class BeitLayer(nn.Module):\n+class BeitLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None, drop_path_rate: float = 0.0) -> None:\n@@ -525,7 +526,7 @@ def forward(\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[tuple[int]] = None,\n+        resolution: Optional[tuple[int, int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in BEiT, layernorm is applied before self-attention\n@@ -695,25 +696,14 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    relative_position_bias,\n-                    interpolate_pos_encoding,\n-                    resolution,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    relative_position_bias,\n-                    interpolate_pos_encoding,\n-                    resolution,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                head_mask=layer_head_mask,\n+                output_attentions=output_attentions,\n+                relative_position_bias=relative_position_bias,\n+                interpolate_pos_encoding=interpolate_pos_encoding,\n+                resolution=resolution,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "e508a98614af426d4ef05f9623c89062eeefaf4c",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -522,7 +523,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class BertLayer(nn.Module):\n+class BertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -647,27 +648,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "bd65e88ae855fd592482e25f9fba9d76bbd31406",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n@@ -275,7 +276,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->BertGeneration\n-class BertGenerationLayer(nn.Module):\n+class BertGenerationLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -401,27 +402,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "a7b3c0647033004572f0c1877df6058297901c7a",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 15,
            "deletions": 30,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -1419,7 +1420,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class BigBirdLayer(nn.Module):\n+class BigBirdLayer(GradientCheckpointingLayer):\n     def __init__(self, config, seed=None):\n         super().__init__()\n         self.config = config\n@@ -1593,35 +1594,19 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    band_mask,\n-                    from_mask,\n-                    to_mask,\n-                    blocked_encoder_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    band_mask,\n-                    from_mask,\n-                    to_mask,\n-                    blocked_encoder_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                band_mask,\n+                from_mask,\n+                to_mask,\n+                blocked_encoder_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "465b94e13bee5f7d5d3103e0543d9ae4b3c5e6e1",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 26,
            "deletions": 56,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -32,6 +32,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1333,7 +1334,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class BigBirdPegasusEncoderLayer(nn.Module):\n+class BigBirdPegasusEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BigBirdPegasusConfig, seed=None):\n         super().__init__()\n         self.attention_type = config.attention_type\n@@ -1420,7 +1421,7 @@ def set_attention_type(self, value: str):\n         self.self_attn.set_attention_type(value)\n \n \n-class BigBirdPegasusDecoderLayer(nn.Module):\n+class BigBirdPegasusDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BigBirdPegasusConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1947,31 +1948,17 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        band_mask,\n-                        from_mask,\n-                        to_mask,\n-                        blocked_encoder_mask,\n-                        blocked_encoder_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        band_mask=band_mask,\n-                        from_mask=from_mask,\n-                        to_mask=to_mask,\n-                        from_blocked_mask=blocked_encoder_mask,\n-                        to_blocked_mask=blocked_encoder_mask,\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    band_mask=band_mask,\n+                    from_mask=from_mask,\n+                    to_mask=to_mask,\n+                    from_blocked_mask=blocked_encoder_mask,\n+                    to_blocked_mask=blocked_encoder_mask,\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -2297,35 +2284,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "8a0c43eafd3f4cce785e6d845b8efcc1cca91456",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -20,7 +20,6 @@\n # limitations under the License.\n \n import math\n-from functools import partial\n from typing import Callable, Optional, Union\n \n import torch\n@@ -32,6 +31,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -248,7 +248,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class BioGptDecoderLayer(nn.Module):\n+class BioGptDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -646,30 +646,17 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    position_ids,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_ids=position_ids,\n-                    cache_position=cache_position,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_ids=position_ids,\n+                cache_position=cache_position,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "938b1c9d8bebed9d5dc26a4b666938d01bb395ac",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 11,
            "deletions": 25,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch BioGPT model.\"\"\"\n \n import math\n-from functools import partial\n from typing import Optional, Union\n \n import torch\n@@ -473,30 +472,17 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    position_ids,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_ids=position_ids,\n-                    cache_position=cache_position,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_ids=position_ids,\n+                cache_position=cache_position,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "7821e1c7b4fb0a83ec48db4927bc8ace01998871",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 21,
            "deletions": 46,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -34,6 +34,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -270,7 +271,7 @@ def forward(\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Blenderbot, MBART->BLENDERBOT\n-class BlenderbotEncoderLayer(nn.Module):\n+class BlenderbotEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BlenderbotConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -339,7 +340,7 @@ def forward(\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Blenderbot, MBART->BLENDERBOT\n-class BlenderbotDecoderLayer(nn.Module):\n+class BlenderbotDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BlenderbotConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -825,21 +826,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1090,35 +1082,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                causal_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "550e512219299dc984f1c51b6b39df39aaad816e",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 21,
            "deletions": 46,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -32,6 +32,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -254,7 +255,7 @@ def forward(\n \n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL\n-class BlenderbotSmallEncoderLayer(nn.Module):\n+class BlenderbotSmallEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -326,7 +327,7 @@ def forward(\n \n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->BlenderbotSmall, BART->BLENDERBOT_SMALL\n-class BlenderbotSmallDecoderLayer(nn.Module):\n+class BlenderbotSmallDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -812,21 +813,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1073,35 +1065,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                causal_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "e43b79595ca3a1c09ea639270cc4407c58a101bb",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -552,7 +552,7 @@ def forward(\n \n             layer_outputs = encoder_layer(\n                 hidden_states,\n-                attention_mask,\n+                attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n             )\n "
        },
        {
            "sha": "04b20b7513a3031b7f828130aaa5efbaf6e8d499",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -531,7 +531,7 @@ def forward(\n \n             layer_outputs = encoder_layer(\n                 hidden_states,\n-                attention_mask,\n+                attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n             )\n \n@@ -992,11 +992,11 @@ def forward(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                past_key_value,\n-                output_attentions,\n-                query_length,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                query_length=query_length,\n             )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "414426844513e03f6b6925ac7ada27293f9204b2",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -366,7 +367,7 @@ def forward(self, hidden_states: torch.Tensor, residual: torch.Tensor) -> torch.\n         return output\n \n \n-class BloomBlock(nn.Module):\n+class BloomBlock(GradientCheckpointingLayer):\n     def __init__(self, config: BloomConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -605,29 +606,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    alibi,\n-                    causal_mask,\n-                    past_key_values,\n-                    head_mask[i],\n-                    use_cache,\n-                    output_attentions,\n-                    cache_position,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states,\n-                    layer_past=past_key_values,\n-                    attention_mask=causal_mask,\n-                    head_mask=head_mask[i],\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    alibi=alibi,\n-                    cache_position=cache_position,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                layer_past=past_key_values,\n+                attention_mask=causal_mask,\n+                head_mask=head_mask[i],\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                alibi=alibi,\n+                cache_position=cache_position,\n+            )\n \n             hidden_states = outputs[0]\n             if use_cache:"
        },
        {
            "sha": "bb9ac6c1bd377fd3051a46101e4e46274719be52",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN, QuickGELUActivation\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -662,7 +663,7 @@ def feed_forward_chunk(self, attention_output):\n         return layer_output\n \n \n-class BridgeTowerTextLayer(nn.Module):\n+class BridgeTowerTextLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -788,27 +789,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "aab9d62d6461a4d80acaad062c1e57b33f6d602f",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 12,
            "deletions": 29,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -428,7 +429,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class BrosLayer(nn.Module):\n+class BrosLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -550,34 +551,16 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n-                if use_cache:\n-                    logger.warning(\n-                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n-                        \"`use_cache=False`...\"\n-                    )\n-                    use_cache = False\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    bbox_pos_emb,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states=hidden_states,\n-                    bbox_pos_emb=bbox_pos_emb,\n-                    attention_mask=attention_mask,\n-                    head_mask=layer_head_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                bbox_pos_emb,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "3dff8f3b2cf01ea6fdc20570a8f0716ce7711a25",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN, gelu\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -478,7 +479,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaLayer with Roberta->Camembert\n-class CamembertLayer(nn.Module):\n+class CamembertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -604,27 +605,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "a5f5552b78e7d2540bd143a2258a3f17023e35d3",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     ModelOutput,\n@@ -672,7 +673,7 @@ def forward(self, hidden_states: tuple[torch.FloatTensor], input_tensor: torch.F\n         return hidden_states\n \n \n-class CanineLayer(nn.Module):\n+class CanineLayer(GradientCheckpointingLayer):\n     def __init__(\n         self,\n         config,\n@@ -779,16 +780,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n             if output_attentions:"
        },
        {
            "sha": "0aaf197dbe8f8de25f164c1689d68227549650f4",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 13,
            "deletions": 24,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -383,7 +384,7 @@ def forward(\n \n \n # copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with Llama->Chameleon, LLAMA->CHAMELEON\n-class ChameleonDecoderLayer(nn.Module):\n+class ChameleonDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ChameleonConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -458,7 +459,7 @@ def forward(\n         return outputs\n \n \n-class ChameleonSwinDecoderLayer(nn.Module):\n+class ChameleonSwinDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ChameleonConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1011,28 +1012,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "8d676fc5013966fcceaafba2a002ec3ba81a194a",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 16,
            "deletions": 34,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -577,7 +578,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->ChineseCLIPText\n-class ChineseCLIPTextLayer(nn.Module):\n+class ChineseCLIPTextLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -663,7 +664,7 @@ def feed_forward_chunk(self, attention_output):\n         return layer_output\n \n \n-class ChineseCLIPVisionLayer(nn.Module):\n+class ChineseCLIPVisionLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ChineseCLIPConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -816,27 +817,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:\n@@ -920,17 +909,10 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "973ae11fd37f8b52ae8d81a42380d9790ef12414",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 15,
            "deletions": 31,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPooling,\n@@ -691,7 +692,7 @@ def forward(\n \n \n # Copied from transformers.models.swin.modeling_swin.SwinStage with Swin->ClapAudio\n-class ClapAudioStage(nn.Module):\n+class ClapAudioStage(GradientCheckpointingLayer):\n     def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, downsample):\n         super().__init__()\n         self.config = config\n@@ -928,14 +929,9 @@ def forward(\n \n             input_dimensions = self.input_resolutions[i]\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__, hidden_states, input_dimensions, layer_head_mask, output_attentions\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -1355,7 +1351,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->ClapText\n-class ClapTextLayer(nn.Module):\n+class ClapTextLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -1481,27 +1477,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "3e8a898b35df1c3b1cf8670623bf889008d6a890",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n@@ -393,7 +394,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class CLIPEncoderLayer(nn.Module):\n+class CLIPEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Union[CLIPVisionConfig, CLIPTextConfig]):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -575,21 +576,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "cff0471c8137d3bfbbd699dbc00cdd971527978f",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 8,
            "deletions": 17,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n@@ -374,7 +375,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->CLIPSeg\n-class CLIPSegEncoderLayer(nn.Module):\n+class CLIPSegEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: CLIPSegConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -539,22 +540,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n-\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "d00528b14069c57a699c772645a6377e6f1713a3",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -245,7 +246,7 @@ def forward(self, hidden_states: Optional[torch.FloatTensor]) -> torch.FloatTens\n \n \n # Copied from transformers.models.gptj.modeling_gptj.GPTJBlock with GPTJ->CodeGen\n-class CodeGenBlock(nn.Module):\n+class CodeGenBlock(GradientCheckpointingLayer):\n     # Ignore copy\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n@@ -437,29 +438,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    None,\n-                    causal_mask,\n-                    position_ids,\n-                    head_mask[i],\n-                    use_cache,\n-                    output_attentions,\n-                    cache_position,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states=hidden_states,\n-                    layer_past=past_key_values,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    head_mask=head_mask[i],\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    cache_position=cache_position,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                layer_past=past_key_values,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                head_mask=head_mask[i],\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n+            )\n \n             hidden_states = outputs[0]\n             if use_cache is True:"
        },
        {
            "sha": "2042817a2107b8e7b48c7e0ae218d0b34b739a18",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 14,
            "deletions": 26,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithCrossAttentions, Seq2SeqModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, is_timm_available, logging, requires_backends\n@@ -827,7 +828,7 @@ def forward(\n         return outputs\n \n \n-class ConditionalDetrDecoderLayer(nn.Module):\n+class ConditionalDetrDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ConditionalDetrConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1297,31 +1298,18 @@ def forward(\n                 pos_transformation = self.query_scale(hidden_states)\n             # apply transformation\n             query_sine_embed = query_sine_embed_before_transformation * pos_transformation\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    None,\n-                    object_queries,\n-                    query_position_embeddings,\n-                    query_sine_embed,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    None,\n-                    None,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=None,\n-                    object_queries=object_queries,\n-                    query_position_embeddings=query_position_embeddings,\n-                    query_sine_embed=query_sine_embed,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n-                    is_first=(idx == 0),\n-                )\n+\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                None,  # attention_mask\n+                object_queries,\n+                query_position_embeddings,\n+                query_sine_embed,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+                is_first=(idx == 0),\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "bdac1fecc1c3c2814ee41695135577c5ced2c534",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n     MaskedLMOutput,\n@@ -532,7 +533,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class ConvBertLayer(nn.Module):\n+class ConvBertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -620,25 +621,14 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                output_attentions,\n+            )\n             hidden_states = layer_outputs[0]\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)"
        },
        {
            "sha": "47b67f4f7c93f0fa772117a028ef281870916268",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 19,
            "deletions": 40,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithCrossAttentions, Seq2SeqModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -702,7 +703,7 @@ def forward(self, hidden_states: torch.Tensor):\n \n \n # Modified from transformers.models.detr.modeling_detr.DetrEncoderLayer with DetrEncoderLayer->DabDetrEncoderLayer,DetrConfig->DabDetrConfig\n-class DabDetrEncoderLayer(nn.Module):\n+class DabDetrEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DabDetrConfig):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -764,7 +765,7 @@ def forward(\n \n \n # Modified from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoderLayer with ConditionalDetr->DabDetr\n-class DabDetrDecoderLayer(nn.Module):\n+class DabDetrDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DabDetrConfig, is_first: bool = False):\n         super().__init__()\n         self.self_attn = DabDetrDecoderLayerSelfAttention(config)\n@@ -976,21 +977,12 @@ def forward(\n             # we add object_queries * pos_scaler as extra input to the encoder_layer\n             scaled_object_queries = object_queries * pos_scales\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    scaled_object_queries,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    object_queries=scaled_object_queries,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                object_queries=scaled_object_queries,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -1138,29 +1130,16 @@ def forward(\n                 reference_anchor_size[..., 1] / obj_center[..., 3]\n             ).unsqueeze(-1)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    None,\n-                    object_queries,\n-                    query_pos,\n-                    query_sine_embed,\n-                    encoder_hidden_states,\n-                    memory_key_padding_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=None,\n-                    object_queries=object_queries,\n-                    query_position_embeddings=query_pos,\n-                    query_sine_embed=query_sine_embed,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=memory_key_padding_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                None,  # attention_mask\n+                object_queries,\n+                query_pos,\n+                query_sine_embed,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=memory_key_padding_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             # iter update\n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "e4ddac37541a9d8aba757b440df2180cdc2027b9",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 7,
            "deletions": 20,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -33,6 +33,7 @@\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -51,7 +52,7 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-class Data2VecAudioConvLayer(nn.Module):\n+class Data2VecAudioConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -155,13 +156,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n \n@@ -357,7 +352,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class Data2VecAudioEncoderLayer(nn.Module):\n+class Data2VecAudioEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = Data2VecAudioAttention(\n@@ -441,17 +436,9 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "97bca6d0d69eade4c2e48c5e89be51840f1b28db",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN, gelu\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -375,7 +376,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Data2VecText\n-class Data2VecTextLayer(nn.Module):\n+class Data2VecTextLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -501,27 +502,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "381d354e3e954139d6c960b7d190b3ebfae1c807",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 11,
            "deletions": 21,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -497,7 +498,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.beit.modeling_beit.BeitLayer with Beit->Data2VecVision,BEiT->Data2VecVision\n-class Data2VecVisionLayer(nn.Module):\n+class Data2VecVisionLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(\n@@ -527,7 +528,7 @@ def forward(\n         output_attentions: bool = False,\n         relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[tuple[int]] = None,\n+        resolution: Optional[tuple[int, int]] = None,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n         self_attention_outputs = self.attention(\n             self.layernorm_before(hidden_states),  # in Data2VecVision, layernorm is applied before self-attention\n@@ -699,25 +700,14 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    relative_position_bias,\n-                    interpolate_pos_encoding,\n-                    resolution,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    relative_position_bias,\n-                    interpolate_pos_encoding,\n-                    resolution,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                head_mask=layer_head_mask,\n+                output_attentions=output_attentions,\n+                relative_position_bias=relative_position_bias,\n+                interpolate_pos_encoding=interpolate_pos_encoding,\n+                resolution=resolution,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "94a4d3e080affe92e1298b64e7c67bafaa494aa2",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -20,6 +20,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import Wav2Vec2BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ..wav2vec2.modeling_wav2vec2 import (\n@@ -38,7 +39,7 @@\n from .configuration_data2vec_audio import Data2VecAudioConfig\n \n \n-class Data2VecAudioConvLayer(nn.Module):\n+class Data2VecAudioConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1"
        },
        {
            "sha": "e53ea75f26cea1a57eb3f665d6720e6ca1ad262b",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n@@ -724,7 +725,7 @@ def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         return out, weights\n \n \n-class DbrxBlock(nn.Module):\n+class DbrxBlock(GradientCheckpointingLayer):\n     def __init__(self, config: DbrxConfig, block_idx: int):\n         super().__init__()\n         self.hidden_size = config.d_model\n@@ -947,29 +948,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                block_outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    output_router_logits,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                block_outputs = block(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            block_outputs = block(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n \n             hidden_states = block_outputs[0]\n "
        },
        {
            "sha": "c6dd97736c5795cef8fb7bfa2502075f00ef6127",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n@@ -492,7 +493,7 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n-class DebertaLayer(nn.Module):\n+class DebertaLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = DebertaAttention(config)\n@@ -580,25 +581,14 @@ def forward(\n \n         rel_embeddings = self.get_rel_embedding()\n         for i, layer_module in enumerate(self.layer):\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states, att_m = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    next_kv,\n-                    attention_mask,\n-                    query_states,\n-                    relative_pos,\n-                    rel_embeddings,\n-                    output_attentions,\n-                )\n-            else:\n-                hidden_states, att_m = layer_module(\n-                    next_kv,\n-                    attention_mask,\n-                    query_states=query_states,\n-                    relative_pos=relative_pos,\n-                    rel_embeddings=rel_embeddings,\n-                    output_attentions=output_attentions,\n-                )\n+            hidden_states, att_m = layer_module(\n+                next_kv,\n+                attention_mask,\n+                query_states=query_states,\n+                relative_pos=relative_pos,\n+                rel_embeddings=rel_embeddings,\n+                output_attentions=output_attentions,\n+            )\n \n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)"
        },
        {
            "sha": "9089fe1f6504a2d3df16205db9f92df1c9268b14",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, LayerNorm, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n@@ -418,7 +419,7 @@ def forward(self, hidden_states, input_tensor):\n \n \n # Copied from transformers.models.deberta.modeling_deberta.DebertaLayer with Deberta->DebertaV2\n-class DebertaV2Layer(nn.Module):\n+class DebertaV2Layer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = DebertaV2Attention(config)\n@@ -655,25 +656,14 @@ def forward(\n         next_kv = hidden_states\n         rel_embeddings = self.get_rel_embedding()\n         for i, layer_module in enumerate(self.layer):\n-            if self.gradient_checkpointing and self.training:\n-                output_states, attn_weights = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    next_kv,\n-                    attention_mask,\n-                    query_states,\n-                    relative_pos,\n-                    rel_embeddings,\n-                    output_attentions,\n-                )\n-            else:\n-                output_states, attn_weights = layer_module(\n-                    next_kv,\n-                    attention_mask,\n-                    query_states=query_states,\n-                    relative_pos=relative_pos,\n-                    rel_embeddings=rel_embeddings,\n-                    output_attentions=output_attentions,\n-                )\n+            output_states, attn_weights = layer_module(\n+                next_kv,\n+                attention_mask,\n+                query_states=query_states,\n+                relative_pos=relative_pos,\n+                rel_embeddings=rel_embeddings,\n+                output_attentions=output_attentions,\n+            )\n \n             if output_attentions:\n                 all_attentions = all_attentions + (attn_weights,)"
        },
        {
            "sha": "9184b11bfe69fb4065b19ed3d34c231ac853b3c0",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n@@ -360,7 +361,7 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n \n \n # Copied from transformers.models.gpt2.modeling_gpt2.GPT2Block with GPT2->DecisionTransformerGPT2\n-class DecisionTransformerGPT2Block(nn.Module):\n+class DecisionTransformerGPT2Block(GradientCheckpointingLayer):\n     # Ignore copy\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n@@ -654,31 +655,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    None,\n-                    None,\n-                    attention_mask,\n-                    head_mask[i],\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    use_cache,\n-                    output_attentions,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states,\n-                    past_key_value=past_key_values,\n-                    cache_position=cache_position,\n-                    attention_mask=attention_mask,\n-                    head_mask=head_mask[i],\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                past_key_values if not (self.gradient_checkpointing and self.training) else None,\n+                cache_position,\n+                attention_mask,\n+                head_mask[i],\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = outputs[0]\n "
        },
        {
            "sha": "26298a0f6b213ab473275b76fcae845fa2056ff8",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 24,
            "deletions": 50,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import meshgrid\n@@ -759,7 +760,7 @@ def forward(\n         return attn_output, attn_weights_reshaped\n \n \n-class DeformableDetrEncoderLayer(nn.Module):\n+class DeformableDetrEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DeformableDetrConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -848,7 +849,7 @@ def forward(\n         return outputs\n \n \n-class DeformableDetrDecoderLayer(nn.Module):\n+class DeformableDetrDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DeformableDetrConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1126,29 +1127,16 @@ def forward(\n         for i, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    position_embeddings,\n-                    reference_points,\n-                    spatial_shapes,\n-                    spatial_shapes_list,\n-                    level_start_index,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    position_embeddings=position_embeddings,\n-                    reference_points=reference_points,\n-                    spatial_shapes=spatial_shapes,\n-                    spatial_shapes_list=spatial_shapes_list,\n-                    level_start_index=level_start_index,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                position_embeddings=position_embeddings,\n+                reference_points=reference_points,\n+                spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n+                level_start_index=level_start_index,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -1273,31 +1261,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    position_embeddings,\n-                    reference_points_input,\n-                    spatial_shapes,\n-                    spatial_shapes_list,\n-                    level_start_index,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    position_embeddings=position_embeddings,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    reference_points=reference_points_input,\n-                    spatial_shapes=spatial_shapes,\n-                    spatial_shapes_list=spatial_shapes_list,\n-                    level_start_index=level_start_index,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                reference_points_input,\n+                spatial_shapes,\n+                spatial_shapes_list,\n+                level_start_index,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "4250c1180bc6b4b91faf2ed9cb65c1df37e24984",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -347,7 +348,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->DeiT,VIT->DEIT\n-class DeiTLayer(nn.Module):\n+class DeiTLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: DeiTConfig) -> None:\n@@ -414,15 +415,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "64912b512ff90e6bf2f7fb664a8929380ba99751",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -39,6 +39,7 @@\n     replace_return_docstrings,\n )\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import meshgrid\n@@ -909,7 +910,7 @@ def forward(\n         return outputs\n \n \n-class DetaDecoderLayer(nn.Module):\n+class DetaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DetaConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1341,29 +1342,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    position_embeddings,\n-                    reference_points_input,\n-                    spatial_shapes,\n-                    level_start_index,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    position_embeddings=position_embeddings,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    reference_points=reference_points_input,\n-                    spatial_shapes=spatial_shapes,\n-                    level_start_index=level_start_index,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                encoder_hidden_states=encoder_hidden_states,\n+                reference_points=reference_points_input,\n+                spatial_shapes=spatial_shapes,\n+                level_start_index=level_start_index,\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "7bc835cf13df37a32a7ba4b72140318fe1555dc9",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 7,
            "deletions": 15,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ....integrations.deepspeed import is_deepspeed_zero3_enabled\n from ....integrations.fsdp import is_fsdp_managed_module\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput, CausalLMOutput\n from ....modeling_utils import (\n     PreTrainedModel,\n@@ -377,7 +378,7 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n-class MCTCTLayer(nn.Module):\n+class MCTCTLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MCTCTConfig):\n         super().__init__()\n \n@@ -591,20 +592,11 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states=hidden_states,\n-                        attention_mask=attention_mask,\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states=hidden_states,\n+                    attention_mask=attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "2ef4a560952e3c82ca2cb0c117975f520f9ab11d",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -438,7 +439,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class NezhaLayer(nn.Module):\n+class NezhaLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -563,27 +564,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "848f3f971e05a20eea6f66a79e4931d83ea636ec",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -29,6 +29,7 @@\n \n from ....activations import ACT2FN\n from ....modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ....modeling_utils import PreTrainedModel\n from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n@@ -339,7 +340,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class OpenLlamaDecoderLayer(nn.Module):\n+class OpenLlamaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: OpenLlamaConfig):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -631,25 +632,14 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    position_ids,\n-                    None,\n-                    output_attentions,\n-                    None,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "df3fce3b52053020bdd4f6bb196daefc03a36291",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 27,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -452,7 +453,7 @@ def forward(self, hidden_states, input_tensor):\n \n \n # Based on transformers.models.bert.modeling_bert.BertLayer with Bert -> QDQBert\n-class QDQBertLayer(nn.Module):\n+class QDQBertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.seq_len_dim = 1\n@@ -568,32 +569,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                if use_cache:\n-                    logger.warning_once(\n-                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                    )\n-                    use_cache = False\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "e88a75bd1bf270402ffde3eff28c556b907d3fe8",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ....activations import ACT2FN\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -447,7 +448,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class RealmLayer(nn.Module):\n+class RealmLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -572,27 +573,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "0599c3b592f184b49c031513350e52659badad4e",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n \n from ....activations import ACT2FN\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ....modeling_utils import PreTrainedModel\n from ....utils import add_start_docstrings, logging, replace_return_docstrings\n@@ -263,7 +264,7 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-class Speech2Text2DecoderLayer(nn.Module):\n+class Speech2Text2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Speech2Text2Config):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -612,31 +613,17 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "fdfbecf7fe2926954849707eb66c36e6d40a8541",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_utils import PreTrainedModel\n from ....utils import (\n     ModelOutput,\n@@ -346,7 +347,7 @@ def forward(\n         return outputs\n \n \n-class Block(nn.Module):\n+class Block(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.ln1 = nn.LayerNorm(config.n_embd)\n@@ -540,16 +541,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    layer_past,\n-                    use_cache,\n-                    output_attentions,\n-                )\n-            else:\n-                outputs = block(hidden_states, layer_past, use_cache, output_attentions)\n+            outputs = block(hidden_states, layer_past, use_cache, output_attentions)\n \n             hidden_states = outputs[0]\n             if use_cache is True:"
        },
        {
            "sha": "5280248c59e1272010f86deb288efbd331046aa4",
            "filename": "src/transformers/models/deprecated/tvlt/modeling_tvlt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 20,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput, SequenceClassifierOutput\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -483,7 +484,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class TvltLayer(nn.Module):\n+class TvltLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config):\n@@ -546,16 +547,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n \n@@ -853,15 +845,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    None,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n+            layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "03bcc24beb6b007f1bbdd3a12fe4ed03d151bd6e",
            "filename": "src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -390,7 +391,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n }\n \n \n-class ViTHybridLayer(nn.Module):\n+class ViTHybridLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: ViTHybridConfig) -> None:\n@@ -457,15 +458,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "57f2c2610e821755a760fa812e956f0c711bddb8",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 24,
            "deletions": 52,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import LayerNorm\n \n from ....activations import ACT2FN\n+from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput\n from ....modeling_utils import PreTrainedModel\n from ....utils import (\n@@ -1090,7 +1091,7 @@ def get_predict_relative_pos_embeddings(\n         return predict_relative_pos_embeddings\n \n \n-class XLMProphetNetEncoderLayer(nn.Module):\n+class XLMProphetNetEncoderLayer(GradientCheckpointingLayer):\n     \"\"\"\n     Encoder block for XLMProphetnet\n     \"\"\"\n@@ -1133,7 +1134,7 @@ def forward(\n         return outputs\n \n \n-class XLMProphetNetDecoderLayer(nn.Module):\n+class XLMProphetNetDecoderLayer(GradientCheckpointingLayer):\n     \"\"\"\n     Decoder block for XLMProphetnet\n     \"\"\"\n@@ -1320,21 +1321,12 @@ def forward(\n             if output_hidden_states:\n                 encoder_hidden_states = encoder_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    extended_attention_mask,\n-                    (head_mask[idx] if head_mask is not None else None),\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask=extended_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask=extended_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -1554,41 +1546,21 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    extended_attention_mask,\n-                    encoder_hidden_states,\n-                    extended_encoder_attention_mask,\n-                    (head_mask[idx] if head_mask is not None else None),\n-                    (cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                    extended_predict_attention_mask,\n-                    main_relative_position_buckets,\n-                    predict_relative_position_buckets,\n-                    position_ids,\n-                    None,\n-                    use_cache,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=extended_attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attn_mask=extended_encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    extended_predict_attention_mask=extended_predict_attention_mask,\n-                    main_relative_position_buckets=main_relative_position_buckets,\n-                    predict_relative_position_buckets=predict_relative_position_buckets,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_value,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=extended_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                encoder_attn_mask=extended_encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                extended_predict_attention_mask=extended_predict_attention_mask,\n+                main_relative_position_buckets=main_relative_position_buckets,\n+                predict_relative_position_buckets=predict_relative_position_buckets,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "e52ab48cdf26fa458f7a07a6a55754227e6311da",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 11,
            "deletions": 20,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithCrossAttentions, Seq2SeqModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -677,7 +678,7 @@ def forward(\n         return outputs\n \n \n-class DetrDecoderLayer(nn.Module):\n+class DetrDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DetrConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1045,25 +1046,15 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    combined_attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    None,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=combined_attention_mask,\n-                    object_queries=object_queries,\n-                    query_position_embeddings=query_position_embeddings,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                combined_attention_mask,\n+                object_queries,\n+                query_position_embeddings,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "7b023242a1cf51d848343214bddd1f120b1064a1",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -382,7 +383,7 @@ def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n         return self.weights_out(hidden)\n \n \n-class Dinov2Layer(nn.Module):\n+class Dinov2Layer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n \n     def __init__(self, config: Dinov2Config) -> None:\n@@ -458,15 +459,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "adbb34c2fd45e4e032b0b9efb069841a03e67e86",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -399,7 +400,7 @@ def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n         return self.weights_out(hidden)\n \n \n-class Dinov2WithRegistersLayer(nn.Module):\n+class Dinov2WithRegistersLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n \n     def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n@@ -476,15 +477,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "30ccc04c1f5c3f4454d45fdaafde5b6f82300dd4",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -31,6 +31,7 @@\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n@@ -441,7 +442,7 @@ def ff_chunk(self, input: torch.Tensor) -> torch.Tensor:\n }\n \n \n-class TransformerBlock(nn.Module):\n+class TransformerBlock(GradientCheckpointingLayer):\n     def __init__(self, config: PretrainedConfig):\n         super().__init__()\n \n@@ -537,21 +538,12 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_state,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_state,\n-                    attn_mask,\n-                    head_mask[i],\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_state,\n-                    attn_mask,\n-                    head_mask[i],\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_state,\n+                attn_mask,\n+                head_mask[i],\n+                output_attentions,\n+            )\n \n             hidden_state = layer_outputs[-1]\n "
        },
        {
            "sha": "603acec77829a10a0f2b767333740f2184240aea",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n@@ -706,7 +707,7 @@ def forward(\n \n \n # Copied from transformers.models.swin.modeling_swin.SwinStage with Swin->DonutSwin\n-class DonutSwinStage(nn.Module):\n+class DonutSwinStage(GradientCheckpointingLayer):\n     def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, downsample):\n         super().__init__()\n         self.config = config\n@@ -816,19 +817,9 @@ def forward(\n         for i, layer_module in enumerate(self.layers):\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    input_dimensions,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    always_partition,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n+            )\n \n             hidden_states = layer_outputs[0]\n             hidden_states_before_downsampling = layer_outputs[1]"
        },
        {
            "sha": "82ff615afc25d55dd22dc96a3900839710db3180",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -29,6 +29,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, DepthEstimatorOutput, SemanticSegmenterOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -469,7 +470,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # copied from transformers.models.vit.modeling_vit.ViTLayer with ViTConfig->DPTConfig, ViTAttention->DPTViTAttention, ViTIntermediate->DPTViTIntermediate, ViTOutput->DPTViTOutput\n-class DPTViTLayer(nn.Module):\n+class DPTViTLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: DPTConfig) -> None:\n@@ -536,15 +537,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "81eb2d894d61c01204667d9c49ad02505174af3c",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN, get_activation\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -436,7 +437,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Electra\n-class ElectraLayer(nn.Module):\n+class ElectraLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -562,27 +563,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "dda93fb81c9d0ce042920843758d876c6867441f",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -361,7 +362,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Ernie\n-class ErnieLayer(nn.Module):\n+class ErnieLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -487,27 +488,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "953a024a82345620ed4a1e5de673445e878e0484",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -599,7 +600,7 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n-class EsmLayer(nn.Module):\n+class EsmLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -725,27 +726,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "d59245679226ffafb2f19a1a60bd664ed5dd98b6",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 14,
            "deletions": 28,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n     AttentionMaskConverter,\n )\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -556,7 +557,7 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n }\n \n \n-class FalconDecoderLayer(nn.Module):\n+class FalconDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: FalconConfig, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -836,33 +837,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    alibi,\n-                    causal_mask,\n-                    position_ids,\n-                    head_mask[i],\n-                    past_key_values,\n-                    use_cache,\n-                    output_attentions,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states,\n-                    layer_past=past_key_values,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    head_mask=head_mask[i],\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    alibi=alibi,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                layer_past=past_key_values,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                head_mask=head_mask[i],\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                alibi=alibi,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n \n             hidden_states = outputs[0]\n             if use_cache is True:"
        },
        {
            "sha": "2df27e390ea6f5c184c0cf388bee33cee26acf1c",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import MambaCache\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available, is_mambapy_available\n@@ -405,7 +406,7 @@ def forward(self, hidden_states):\n \n \n # Copied from transformers.models.mamba.modeling_mamba.MambaBlock with Mamba->FalconMamba,FalconMambaCache->MambaCache\n-class FalconMambaBlock(nn.Module):\n+class FalconMambaBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.config = config\n@@ -620,17 +621,12 @@ def forward(\n         hidden_states = inputs_embeds\n         all_hidden_states = () if output_hidden_states else None\n         for mixer_block in self.layers:\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    mixer_block.__call__, hidden_states, cache_params, cache_position, attention_mask\n-                )\n-            else:\n-                hidden_states = mixer_block(\n-                    hidden_states,\n-                    cache_params=cache_params,\n-                    cache_position=cache_position,\n-                    attention_mask=attention_mask,\n-                )\n+            hidden_states = mixer_block(\n+                hidden_states,\n+                cache_params=cache_params,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n \n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)"
        },
        {
            "sha": "1d1ce04c3563d4485e4d17cb0af775a9801d0adc",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n@@ -577,7 +578,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class FlavaLayer(nn.Module):\n+class FlavaLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: FlavaPossibleConfigs) -> None:\n@@ -648,16 +649,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "fed31339da7cc8a6f3850fad883588cd1230addd",
            "filename": "src/transformers/models/fnet/modeling_fnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -31,6 +31,7 @@\n     from scipy import linalg\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -235,7 +236,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class FNetLayer(nn.Module):\n+class FNetLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -276,10 +277,7 @@ def forward(self, hidden_states, output_hidden_states=False, return_dict=True):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(layer_module.__call__, hidden_states)\n-            else:\n-                layer_outputs = layer_module(hidden_states)\n+            layer_outputs = layer_module(hidden_states)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "47fa9d4f2eb7f3b4a7131f1070d45a66b12d1d6f",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n@@ -455,7 +456,7 @@ def forward(self, hidden_state, input_dimensions):\n         return hidden_state\n \n \n-class FocalNetStage(nn.Module):\n+class FocalNetStage(GradientCheckpointingLayer):\n     def __init__(self, config, index, input_resolution):\n         super().__init__()\n \n@@ -560,14 +561,7 @@ def forward(\n             all_reshaped_hidden_states += (reshaped_hidden_state,)\n \n         for i, stage_module in enumerate(self.stages):\n-            if self.gradient_checkpointing and self.training:\n-                stage_outputs = self._gradient_checkpointing_func(\n-                    stage_module.__call__,\n-                    hidden_states,\n-                    input_dimensions,\n-                )\n-            else:\n-                stage_outputs = stage_module(hidden_states, input_dimensions)\n+            stage_outputs = stage_module(hidden_states, input_dimensions)\n \n             hidden_states = stage_outputs[0]\n             hidden_states_before_downsampling = stage_outputs[1]"
        },
        {
            "sha": "7008538c7ab0ca0ad2a62d5c3afc58f661209438",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from functools import partial\n from typing import Callable, Optional, Union\n \n import torch\n@@ -30,6 +29,7 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -238,7 +238,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Gemma2DecoderLayer(nn.Module):\n+class Gemma2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -466,30 +466,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    position_embeddings,\n-                    causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    position_embeddings=position_embeddings,\n-                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "b317936c77667b33723fefd5bca8e62417d18dab",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -13,7 +13,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from functools import partial\n from typing import Callable, Optional, Union\n \n import torch\n@@ -25,6 +24,7 @@\n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n@@ -303,7 +303,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Gemma2DecoderLayer(nn.Module):\n+class Gemma2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -449,30 +449,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    position_embeddings,\n-                    causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    position_embeddings=position_embeddings,\n-                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "bed1d5310af635d4e98e4599837f836e13c42836",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 14,
            "deletions": 28,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -22,7 +22,6 @@\n import copy\n from collections.abc import Callable\n from dataclasses import dataclass\n-from functools import partial\n from typing import Optional, Union\n \n import torch\n@@ -34,6 +33,7 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -364,7 +364,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Gemma3DecoderLayer(nn.Module):\n+class Gemma3DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n@@ -581,32 +581,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    position_embeddings_global,\n-                    position_embeddings_local,\n-                    causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    position_embeddings_global=position_embeddings_global,\n-                    position_embeddings_local=position_embeddings_local,\n-                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings_global=position_embeddings_global,\n+                position_embeddings_local=position_embeddings_local,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "bc1db4b50a458d66dffaead0bd349422b64a59e9",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 14,
            "deletions": 28,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -16,7 +16,6 @@\n import copy\n from collections.abc import Callable\n from dataclasses import dataclass\n-from functools import partial\n from typing import Any, Optional, Union\n \n import torch\n@@ -27,6 +26,7 @@\n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n@@ -443,7 +443,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Gemma3DecoderLayer(nn.Module):\n+class Gemma3DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n@@ -632,32 +632,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    position_embeddings_global,\n-                    position_embeddings_local,\n-                    causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    position_embeddings_global=position_embeddings_global,\n-                    position_embeddings_local=position_embeddings_local,\n-                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings_global=position_embeddings_global,\n+                position_embeddings_local=position_embeddings_local,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "8058c542e9d2eb2bb8e711e2954e543deb520e82",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 17,
            "deletions": 35,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPast,\n@@ -343,7 +344,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class GitLayer(nn.Module):\n+class GitLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -441,24 +442,14 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    pixel_values_present,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                past_key_values,\n+                output_attentions,\n+                pixel_values_present,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:\n@@ -723,7 +714,7 @@ def forward(\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->GitVision\n-class GitVisionEncoderLayer(nn.Module):\n+class GitVisionEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: GitVisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -840,21 +831,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "12a1d58151fadaa2d3632e08e61a4bbb1976e4d7",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -31,6 +31,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n@@ -192,7 +193,7 @@ def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch\n         return outputs\n \n \n-class GotOcr2VisionLayer(nn.Module):\n+class GotOcr2VisionLayer(GradientCheckpointingLayer):\n     def __init__(self, config, window_size):\n         super().__init__()\n         self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -463,13 +464,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n+            layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "b88290343b67bd8015da958d9845a2064b2cda47",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 27,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -368,7 +369,7 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n         return hidden_states\n \n \n-class GPT2Block(nn.Module):\n+class GPT2Block(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -922,32 +923,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    past_key_values,\n-                    cache_position,\n-                    causal_mask,\n-                    head_mask[i],\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    use_cache,\n-                    output_attentions,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states,\n-                    past_key_value=past_key_values,\n-                    cache_position=cache_position,\n-                    attention_mask=causal_mask,\n-                    head_mask=head_mask[i],\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    **kwargs,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                past_key_values if not (self.gradient_checkpointing and self.training) else None,\n+                cache_position,\n+                causal_mask,\n+                head_mask[i],\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                **kwargs,\n+            )\n \n             hidden_states = outputs[0]\n "
        },
        {
            "sha": "725ddbabf7afe53c7719a5737e1216f7bb689f91",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 18,
            "deletions": 24,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -558,7 +559,7 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n }\n \n \n-class GPTBigCodeBlock(nn.Module):\n+class GPTBigCodeBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -759,6 +760,12 @@ def forward(\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+            )\n+            use_cache = False\n+\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -891,29 +898,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    None,\n-                    attention_mask,\n-                    head_mask[i],\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    use_cache,\n-                    output_attentions,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states,\n-                    layer_past=layer_past,\n-                    attention_mask=attention_mask,\n-                    head_mask=head_mask[i],\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                layer_past,\n+                attention_mask,\n+                head_mask[i],\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = outputs[0]\n             if use_cache:"
        },
        {
            "sha": "25542ac3129eae954c98657d2d9942b34b2cbbac",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -431,7 +432,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class GPTNeoBlock(nn.Module):\n+class GPTNeoBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -635,27 +636,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    None,\n-                    causal_mask,\n-                    head_mask[i],\n-                    use_cache,\n-                    output_attentions,\n-                    cache_position,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states,\n-                    layer_past=past_key_values,\n-                    attention_mask=causal_mask,\n-                    head_mask=head_mask[i],\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    cache_position=cache_position,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                layer_past=past_key_values,\n+                attention_mask=causal_mask,\n+                head_mask=head_mask[i],\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n+            )\n \n             hidden_states = outputs[0]\n             if use_cache:"
        },
        {
            "sha": "d3c5141371b243a3dd39743999659a19006c9f07",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 14,
            "deletions": 27,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -14,6 +14,7 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -190,7 +191,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class GPTNeoXLayer(nn.Module):\n+class GPTNeoXLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.use_parallel_residual = config.use_parallel_residual\n@@ -415,32 +416,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    head_mask[i],\n-                    use_cache,\n-                    past_key_values,\n-                    output_attentions,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                outputs = layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    head_mask=head_mask[i],\n-                    layer_past=past_key_values,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            outputs = layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                head_mask=head_mask[i],\n+                layer_past=past_key_values,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n             hidden_states = outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "fde2677b4e208a139c1459c54a2cb4841bac99eb",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 14,
            "deletions": 27,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -9,6 +9,7 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -177,7 +178,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class GPTNeoXLayer(nn.Module):\n+class GPTNeoXLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.use_parallel_residual = config.use_parallel_residual\n@@ -362,32 +363,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    head_mask[i],\n-                    use_cache,\n-                    past_key_values,\n-                    output_attentions,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                outputs = layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    head_mask=head_mask[i],\n-                    layer_past=past_key_values,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            outputs = layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                head_mask=head_mask[i],\n+                layer_past=past_key_values,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n             hidden_states = outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "a8504db42c824a2661725a957f1c518b18c0d14f",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -434,7 +435,7 @@ def forward(self, hidden_states: Optional[torch.FloatTensor]) -> torch.FloatTens\n         return hidden_states\n \n \n-class GPTJBlock(nn.Module):\n+class GPTJBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         inner_dim = config.n_inner if config.n_inner is not None else 4 * config.n_embd\n@@ -738,29 +739,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    None,\n-                    causal_mask,\n-                    position_ids,\n-                    head_mask[i],\n-                    use_cache,\n-                    output_attentions,\n-                    cache_position,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states=hidden_states,\n-                    layer_past=past_key_values,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    head_mask=head_mask[i],\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    cache_position=cache_position,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                layer_past=past_key_values,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                head_mask=head_mask[i],\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n+            )\n \n             hidden_states = outputs[0]\n             if use_cache is True:"
        },
        {
            "sha": "362d170ffa8130137a691d4e449ab9e494ae2205",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n@@ -692,7 +693,7 @@ def forward(\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->GroupViT\n-class GroupViTEncoderLayer(nn.Module):\n+class GroupViTEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: GroupViTConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -906,21 +907,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "e086b432ba50b0a89817d1b2e1571de44dfd272f",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BackboneOutput,\n     BaseModelOutput,\n@@ -540,7 +541,7 @@ def forward(\n         return (hidden_states, attn_weights)\n \n \n-class HieraStage(nn.Module):\n+class HieraStage(GradientCheckpointingLayer):\n     def __init__(\n         self,\n         config,\n@@ -734,12 +735,7 @@ def forward(\n         for i, stage_module in enumerate(self.stages):\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    stage_module.__call__, hidden_states, layer_head_mask, output_attentions\n-                )\n-            else:\n-                layer_outputs = stage_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = stage_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "0fab4184bfe36da4da889cef511b757a336cba02",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 34,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -32,6 +32,7 @@\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -107,7 +108,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class HubertNoLayerNormConvLayer(nn.Module):\n+class HubertNoLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -128,7 +129,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class HubertLayerNormConvLayer(nn.Module):\n+class HubertLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -155,7 +156,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class HubertGroupNormConvLayer(nn.Module):\n+class HubertGroupNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -212,13 +213,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n \n@@ -417,7 +412,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class HubertEncoderLayer(nn.Module):\n+class HubertEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = HubertAttention(\n@@ -501,17 +496,9 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:\n@@ -579,7 +566,7 @@ def forward(self, hidden_states: torch.FloatTensor):\n         return hidden_states\n \n \n-class HubertEncoderLayerStableLayerNorm(nn.Module):\n+class HubertEncoderLayerStableLayerNorm(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = HubertAttention(\n@@ -675,17 +662,9 @@ def forward(\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "a5a868072a6ab0182844134a5ef4d65a775ed154",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 19,
            "deletions": 81,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -32,6 +32,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PretrainedConfig, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -668,7 +669,7 @@ def forward(\n \n \n # this was adapted from LlamaDecoderLayer\n-class IdeficsDecoderLayer(nn.Module):\n+class IdeficsDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -749,7 +750,7 @@ def forward(\n         return outputs\n \n \n-class IdeficsGatedCrossAttentionLayer(nn.Module):\n+class IdeficsGatedCrossAttentionLayer(GradientCheckpointingLayer):\n     def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1185,95 +1186,32 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            def vblock(\n-                main_block,\n-                hidden_states,\n-                attention_mask,\n-                position_ids,\n-                past_key_value,\n-                image_hidden_states,\n-                image_attention_mask,\n-                cross_attention_gate,\n-                output_attentions,\n-                use_cache,\n-                layer_idx,\n-                cross_layer_interval,\n-                gated_cross_attn_layers,\n-                cache_position,\n-            ):\n-                # TODO(ls): Add cross attention values to respective lists\n-                if layer_idx % cross_layer_interval == 0:\n-                    xblock = gated_cross_attn_layers[layer_idx // cross_layer_interval]\n-                    outputs = xblock(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        image_hidden_states=image_hidden_states,\n-                        image_attention_mask=image_attention_mask,\n-                        cross_attention_gate=cross_attention_gate,\n-                        output_attentions=output_attentions,\n-                        use_cache=use_cache,\n-                        past_key_value=None,  # not implemented\n-                        **kwargs,\n-                    )\n-                    hidden_states = outputs[0]\n-\n-                layer_outputs = main_block(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    **kwargs,\n-                )\n-\n-                return layer_outputs\n-\n-            if self.gradient_checkpointing and self.training:\n-                past_key_values = None\n-                if use_cache:\n-                    logger.warning_once(\n-                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                    )\n-                    use_cache = False\n-\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    vblock,\n-                    decoder_layer,\n+            # TODO(ls): Add cross attention values to respective lists\n+            if idx % self.cross_layer_interval == 0:\n+                cross_attn_block = self.gated_cross_attn_layers[idx // self.cross_layer_interval]\n+                outputs = cross_attn_block(\n                     hidden_states,\n                     attention_mask,\n-                    position_ids,\n-                    past_key_values,\n                     image_hidden_states,\n-                    image_attention_mask,\n-                    cross_attention_gate,\n-                    output_attentions,\n-                    use_cache,\n-                    idx,\n-                    self.cross_layer_interval,\n-                    self.gated_cross_attn_layers,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = vblock(\n-                    decoder_layer,\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    image_hidden_states=image_hidden_states,\n                     image_attention_mask=image_attention_mask,\n                     cross_attention_gate=cross_attention_gate,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n-                    layer_idx=idx,\n-                    cross_layer_interval=self.cross_layer_interval,\n-                    gated_cross_attn_layers=self.gated_cross_attn_layers,\n-                    cache_position=cache_position,\n+                    past_key_value=None,  # not implemented\n                     **kwargs,\n                 )\n+                hidden_states = outputs[0]\n \n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "d75d61545ec2b061a967d5c03810e5a8beaedabd",
            "filename": "src/transformers/models/idefics/vision.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...utils import (\n@@ -283,7 +284,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->IdeficsVision\n-class IdeficsVisionEncoderLayer(nn.Module):\n+class IdeficsVisionEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: IdeficsVisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -400,21 +401,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "1f3f96de63031f4823fe9e06956cc136aeaf9fa3",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 14,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -339,7 +340,7 @@ def forward(self, hidden_state):\n         return hidden_state[:, 0]\n \n \n-class Idefics2EncoderLayer(nn.Module):\n+class Idefics2EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Idefics2VisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -448,19 +449,11 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "56750bc5298c033a4937be490248a2828cdcd2fe",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 14,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -300,7 +301,7 @@ def forward(self, x):\n \n \n # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2EncoderLayer with Idefics2->Idefics3\n-class Idefics3EncoderLayer(nn.Module):\n+class Idefics3EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Idefics3VisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -409,19 +410,11 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "5568b4ebcc8bbcedb43f9b7fe68b732a6243e7d9",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -12,6 +12,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -357,7 +358,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class IJepaLayer(nn.Module):\n+class IJepaLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: IJepaConfig) -> None:\n@@ -423,15 +424,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "d076a193162c866201852e07cb4455afc67641ef",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -401,7 +402,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class ImageGPTBlock(nn.Module):\n+class ImageGPTBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -719,29 +720,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    None,\n-                    attention_mask,\n-                    head_mask[i],\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    use_cache,\n-                    output_attentions,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states,\n-                    layer_past=layer_past,\n-                    attention_mask=attention_mask,\n-                    head_mask=head_mask[i],\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                layer_past,\n+                attention_mask,\n+                head_mask[i],\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = outputs[0]\n             if use_cache is True:"
        },
        {
            "sha": "9718e8fb736e82ccea4f4f1aa6643164664638bd",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 25,
            "deletions": 53,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -34,6 +34,7 @@\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -744,7 +745,7 @@ def forward(\n \n \n # source: https://github.com/zhouhaoyi/Informer2020/blob/main/models/encoder.py\n-class InformerConvLayer(nn.Module):\n+class InformerConvLayer(GradientCheckpointingLayer):\n     def __init__(self, c_in):\n         super().__init__()\n         self.downConv = nn.Conv1d(\n@@ -767,7 +768,7 @@ def forward(self, x):\n         return x\n \n \n-class InformerEncoderLayer(nn.Module):\n+class InformerEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: InformerConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -845,7 +846,7 @@ def forward(\n         return outputs\n \n \n-class InformerDecoderLayer(nn.Module):\n+class InformerDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: InformerConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1086,27 +1087,15 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                    if conv_layer is not None:\n-                        output = self._gradient_checkpointing_func(conv_layer, layer_outputs[0])\n-                        layer_outputs = (output,) + layer_outputs[1:]\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n-                    if conv_layer is not None:\n-                        output = conv_layer(layer_outputs[0])\n-                        layer_outputs = (output,) + layer_outputs[1:]\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n+                if conv_layer is not None:\n+                    output = conv_layer(layer_outputs[0])\n+                    layer_outputs = (output,) + layer_outputs[1:]\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1299,35 +1288,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "3d46275bdc81bf1e47b921669a1f64a789708bfc",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n     _prepare_4d_causal_attention_mask,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n )\n@@ -433,7 +434,7 @@ def forward(\n \n \n # source: https://github.com/zhouhaoyi/Informer2020/blob/main/models/encoder.py\n-class InformerConvLayer(nn.Module):\n+class InformerConvLayer(GradientCheckpointingLayer):\n     def __init__(self, c_in):\n         super().__init__()\n         self.downConv = nn.Conv1d(\n@@ -610,27 +611,15 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                    if conv_layer is not None:\n-                        output = self._gradient_checkpointing_func(conv_layer, layer_outputs[0])\n-                        layer_outputs = (output,) + layer_outputs[1:]\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n-                    if conv_layer is not None:\n-                        output = conv_layer(layer_outputs[0])\n-                        layer_outputs = (output,) + layer_outputs[1:]\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n+                if conv_layer is not None:\n+                    output = conv_layer(layer_outputs[0])\n+                    layer_outputs = (output,) + layer_outputs[1:]\n \n                 hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "bf2c76cf9e59ab017bba49c2d740a20d98e35fbd",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -427,7 +427,7 @@ def forward(\n \n             layer_outputs = encoder_layer(\n                 hidden_states,\n-                attention_mask,\n+                attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n             )\n \n@@ -889,11 +889,11 @@ def forward(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                past_key_value,\n-                output_attentions,\n-                query_length,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                query_length=query_length,\n             )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "ee9cffd4f2e81ea1ad2553a0cd8baa521b7c8bfb",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -356,7 +356,7 @@ def forward(\n \n             layer_outputs = encoder_layer(\n                 hidden_states,\n-                attention_mask,\n+                attention_mask=attention_mask,\n                 output_attentions=output_attentions,\n             )\n \n@@ -750,11 +750,11 @@ def forward(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n-                encoder_hidden_states,\n-                encoder_attention_mask,\n-                past_key_value,\n-                output_attentions,\n-                query_length,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                query_length=query_length,\n             )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "65b2952b39d39f555f53c6746de8a75de2e6378e",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -31,6 +31,7 @@\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -383,7 +384,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n NORM2FN = {\"layer_norm\": nn.LayerNorm, \"rms_norm\": InternVLVisionRMSNorm}\n \n \n-class InternVLVisionLayer(nn.Module):\n+class InternVLVisionLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: InternVLVisionConfig) -> None:\n@@ -452,12 +453,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__, hidden_states, output_attentions\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, output_attentions)\n+            layer_outputs = layer_module(hidden_states, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "a71b9fbdad81ab5fcbe387014d7451e27d02806c",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -334,7 +335,7 @@ class InternVLVisionMLP(CLIPMLP):\n NORM2FN = {\"layer_norm\": nn.LayerNorm, \"rms_norm\": InternVLVisionRMSNorm}\n \n \n-class InternVLVisionLayer(nn.Module):\n+class InternVLVisionLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: InternVLVisionConfig) -> None:\n@@ -403,12 +404,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__, hidden_states, output_attentions\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, output_attentions)\n+            layer_outputs = layer_module(hidden_states, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "ce17baf328171f91e0a5e740b126c588f16a1c98",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 23,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n@@ -763,7 +764,7 @@ def forward(\n }\n \n \n-class JetMoeBlock(nn.Module):\n+class JetMoeBlock(GradientCheckpointingLayer):\n     def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n         \"\"\"\n         Initialize the JetMoeBlock module.\n@@ -967,28 +968,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    position_ids,\n-                    past_key_values,\n-                    causal_mask,\n-                    output_attentions,\n-                    output_router_logits,\n-                    use_cache,\n-                    use_reentrant=False,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                use_cache=use_cache,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "a78ad47f2dd412b2ae087702bacae9c08de75aad",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 45,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -404,7 +405,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->Kosmos2Vision\n-class Kosmos2VisionEncoderLayer(nn.Module):\n+class Kosmos2VisionEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Kosmos2VisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -521,21 +522,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -840,7 +832,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class Kosmos2TextBlock(nn.Module):\n+class Kosmos2TextBlock(GradientCheckpointingLayer):\n     def __init__(self, config: Kosmos2TextConfig):\n         super().__init__()\n         self.embed_dim = config.embed_dim\n@@ -1138,34 +1130,18 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                **kwargs,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "372e4b89e071790c01afc0a075505f542c70c0c2",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -358,7 +359,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->LayoutLM\n-class LayoutLMLayer(nn.Module):\n+class LayoutLMLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -484,27 +485,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "fdaa37b9e50e5d015811a10c2239d55f5969fd9f",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -261,7 +262,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class LayoutLMv2Layer(nn.Module):\n+class LayoutLMv2Layer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -436,25 +437,14 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    rel_pos=rel_pos,\n-                    rel_2d_pos=rel_2d_pos,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    rel_pos=rel_pos,\n-                    rel_2d_pos=rel_2d_pos,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                output_attentions,\n+                rel_pos=rel_pos,\n+                rel_2d_pos=rel_2d_pos,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if output_attentions:"
        },
        {
            "sha": "1b6398a382d4faf39d2bca4982b64b39da39ea69",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     QuestionAnsweringModelOutput,\n@@ -358,7 +359,7 @@ def forward(\n \n \n # Copied from transformers.models.layoutlmv2.modeling_layoutlmv2.LayoutLMv2Layer with LayoutLMv2->LayoutLMv3\n-class LayoutLMv3Layer(nn.Module):\n+class LayoutLMv3Layer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -514,25 +515,14 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    rel_pos,\n-                    rel_2d_pos,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    rel_pos=rel_pos,\n-                    rel_2d_pos=rel_2d_pos,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                output_attentions,\n+                rel_pos=rel_pos,\n+                rel_2d_pos=rel_2d_pos,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if output_attentions:"
        },
        {
            "sha": "ad095cbcd472105c455b83159b29ae372e8a9e32",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 23,
            "deletions": 50,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n@@ -900,7 +901,7 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-class LEDEncoderLayer(nn.Module):\n+class LEDEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: LEDConfig, layer_id: int):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -962,7 +963,7 @@ def forward(\n         return (hidden_states,) + attn_outputs[1:]\n \n \n-class LEDDecoderLayer(nn.Module):\n+class LEDDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: LEDConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1680,27 +1681,15 @@ def forward(\n             if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n                 layer_outputs = (None, None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        head_mask[idx] if head_mask is not None else None,\n-                        is_index_masked,\n-                        is_index_global_attn,\n-                        is_global_attn,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        is_index_masked=is_index_masked,\n-                        is_index_global_attn=is_index_global_attn,\n-                        is_global_attn=is_global_attn,\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    is_index_masked=is_index_masked,\n+                    is_index_global_attn=is_index_global_attn,\n+                    is_global_attn=is_global_attn,\n+                    output_attentions=output_attentions,\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if output_attentions:\n@@ -1943,33 +1932,17 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    combined_attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=combined_attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                combined_attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "91664c32facbf8edf369103108bfc41718923775",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -419,7 +420,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class LiltLayer(nn.Module):\n+class LiltLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -506,23 +507,13 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layout_inputs,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    layout_inputs,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                layout_inputs,\n+                attention_mask,\n+                layer_head_mask,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0][0]\n             layout_inputs = layer_outputs[0][1]"
        },
        {
            "sha": "fb546ad681629a526785667ff458b8066748045c",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 21,
            "deletions": 42,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -29,6 +29,7 @@\n from ...integrations.hub_kernels import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_chunked_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -360,7 +361,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Llama4TextDecoderLayer(nn.Module):\n+class Llama4TextDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -571,31 +572,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    False,  # output_router_logits is False\n-                    use_cache,\n-                    cache_position,\n-                    freq_cis,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=freq_cis,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=freq_cis,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -930,7 +917,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class Llama4VisionEncoderLayer(nn.Module):\n+class Llama4VisionEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Llama4VisionConfig):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1033,21 +1020,13 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    freqs_ci,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_state=hidden_states,\n-                    attention_mask=attention_mask,\n-                    output_attentions=output_attentions,\n-                    freqs_ci=freqs_ci,\n-                )\n+\n+            layer_outputs = encoder_layer(\n+                hidden_state=hidden_states,\n+                attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+                freqs_ci=freqs_ci,\n+            )\n \n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[1],)"
        },
        {
            "sha": "c6b16492c8d13ad512f81ce688089a1acc057c6f",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n@@ -1205,7 +1206,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class LongformerLayer(nn.Module):\n+class LongformerLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.attention = LongformerAttention(config, layer_id)\n@@ -1284,27 +1285,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    is_index_masked,\n-                    is_index_global_attn,\n-                    is_global_attn,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    layer_head_mask=head_mask[idx] if head_mask is not None else None,\n-                    is_index_masked=is_index_masked,\n-                    is_index_global_attn=is_index_global_attn,\n-                    is_global_attn=is_global_attn,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                layer_head_mask=head_mask[idx] if head_mask is not None else None,\n+                is_index_masked=is_index_masked,\n+                is_index_global_attn=is_index_global_attn,\n+                is_global_attn=is_global_attn,\n+                output_attentions=output_attentions,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "081869ec8fc5c23a0765d68d4ebce8381a1eee94",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 17,
            "deletions": 34,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -1143,7 +1144,7 @@ def forward(\n         return outputs\n \n \n-class LongT5Block(nn.Module):\n+class LongT5Block(GradientCheckpointingLayer):\n     def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n@@ -1501,39 +1502,21 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.forward,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_bias,\n-                    encoder_hidden_states,\n-                    encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias,\n-                    layer_head_mask,\n-                    cross_attn_layer_head_mask,\n-                    None,  # past_key_value is always None with gradient checkpointing\n-                    use_cache,\n-                    output_attentions,\n-                    return_dict,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_bias=position_bias,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n-                    layer_head_mask=layer_head_mask,\n-                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_values,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    return_dict=return_dict,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                causal_mask,\n+                position_bias,\n+                encoder_hidden_states,\n+                encoder_extended_attention_mask,\n+                encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n+                layer_head_mask=layer_head_mask,\n+                cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                return_dict=return_dict,\n+                cache_position=cache_position,\n+            )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)"
        },
        {
            "sha": "af01cf77be4f70f7f6402d2736230b9ddedc905f",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n@@ -695,7 +696,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class LukeLayer(nn.Module):\n+class LukeLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -774,23 +775,13 @@ def forward(\n                 all_entity_hidden_states = all_entity_hidden_states + (entity_hidden_states,)\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    word_hidden_states,\n-                    entity_hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    word_hidden_states,\n-                    entity_hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                word_hidden_states,\n+                entity_hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                output_attentions,\n+            )\n \n             word_hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "7d5a73667ee4e64e9160b60bef503a1e9c4a8b5f",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 23,
            "deletions": 46,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -34,6 +34,7 @@\n from ...modeling_flash_attention_utils import (\n     FlashAttentionKwargs,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -335,7 +336,7 @@ def forward(\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->M2M100, MBART->M2M100\n-class M2M100EncoderLayer(nn.Module):\n+class M2M100EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: M2M100Config):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -404,7 +405,7 @@ def forward(\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->M2M100, MBART->M2M100\n-class M2M100DecoderLayer(nn.Module):\n+class M2M100DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: M2M100Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -883,21 +884,12 @@ def forward(\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n \n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1142,35 +1134,20 @@ def forward(\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n \n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        decoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        encoder_hidden_states,\n-                        encoder_attention_mask,\n-                        head_mask[idx] if head_mask is not None else None,\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                        None,\n-                        output_attentions,\n-                        use_cache,\n-                        cache_position,\n-                    )\n-                else:\n-                    layer_outputs = decoder_layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        encoder_hidden_states=encoder_hidden_states,\n-                        encoder_attention_mask=encoder_attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        cross_attn_layer_head_mask=(\n-                            cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                        ),\n-                        past_key_value=past_key_values,\n-                        output_attentions=output_attentions,\n-                        use_cache=use_cache,\n-                        cache_position=cache_position,\n-                    )\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                    encoder_attention_mask=encoder_attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    cross_attn_layer_head_mask=(\n+                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n+                    ),\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                )\n \n                 hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "d771494486f22f49ec12ce71cdeea9f893be660b",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import MambaCache\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -343,7 +344,7 @@ def extra_repr(self):\n         return f\"{self.weight.shape[0]}, eps={self.variance_epsilon}\"\n \n \n-class MambaBlock(nn.Module):\n+class MambaBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.config = config\n@@ -561,17 +562,12 @@ def forward(\n         hidden_states = inputs_embeds\n         all_hidden_states = () if output_hidden_states else None\n         for mixer_block in self.layers:\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    mixer_block.__call__, hidden_states, cache_params, cache_position, attention_mask\n-                )\n-            else:\n-                hidden_states = mixer_block(\n-                    hidden_states,\n-                    cache_params=cache_params,\n-                    cache_position=cache_position,\n-                    attention_mask=attention_mask,\n-                )\n+            hidden_states = mixer_block(\n+                hidden_states,\n+                cache_params=cache_params,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n \n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)"
        },
        {
            "sha": "7dd6ecc92d42921585951348981fe3b668fd86d1",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -682,7 +683,7 @@ def forward(self, hidden_states):\n         return self.weight * hidden_states.to(input_dtype)\n \n \n-class Mamba2Block(nn.Module):\n+class Mamba2Block(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.config = config\n@@ -901,17 +902,12 @@ def forward(\n         hidden_states = inputs_embeds\n         all_hidden_states = () if output_hidden_states else None\n         for mixer_block in self.layers:\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    mixer_block.__call__, hidden_states, cache_params, cache_position, attention_mask\n-                )\n-            else:\n-                hidden_states = mixer_block(\n-                    hidden_states,\n-                    cache_params=cache_params,\n-                    cache_position=cache_position,\n-                    attention_mask=attention_mask,\n-                )\n+            hidden_states = mixer_block(\n+                hidden_states,\n+                cache_params=cache_params,\n+                cache_position=cache_position,\n+                attention_mask=attention_mask,\n+            )\n \n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)"
        },
        {
            "sha": "7319671b485eda5b2174fadd16a205682c16a246",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 21,
            "deletions": 46,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -33,6 +33,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -270,7 +271,7 @@ def forward(\n \n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->Marian, BART->MARIAN\n-class MarianEncoderLayer(nn.Module):\n+class MarianEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MarianConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -342,7 +343,7 @@ def forward(\n \n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->Marian, BART->MARIAN\n-class MarianDecoderLayer(nn.Module):\n+class MarianDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MarianConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -831,21 +832,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1087,35 +1079,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                causal_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "4a34c85b3dbc77db1ab220ecd2cc1a9c4b8c9eb8",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -518,7 +519,7 @@ def forward(\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->MarkupLM\n-class MarkupLMLayer(nn.Module):\n+class MarkupLMLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -644,27 +645,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "5ab37ea53581fa5e2ef8cb934414e647148ce598",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 26,
            "deletions": 36,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...file_utils import ModelOutput, is_scipy_available, requires_backends\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_accelerate_available, logging\n@@ -1535,7 +1536,7 @@ def forward(\n         return attn_output, attn_weights_reshaped\n \n \n-class Mask2FormerMaskedAttentionDecoderLayer(nn.Module):\n+class Mask2FormerMaskedAttentionDecoderLayer(GradientCheckpointingLayer):\n     \"\"\"\n     The Mask2FormerMaskedAttentionDecoderLayer is made up of self-attention, cross (masked) attention as well as FFN\n     blocks. The cross attention block used as part of `Mask2FormerMaskedAttentionDecoderLayer` is actually a `masked\n@@ -1858,46 +1859,35 @@ def forward(\n             if self.training and (dropout_probability < self.layerdrop):\n                 continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    None,\n-                    None,\n-                    output_attentions,\n-                )\n+            level_index = idx % self.num_feature_levels\n \n-            else:\n-                level_index = idx % self.num_feature_levels\n-\n-                where = (attention_mask.sum(-1) != attention_mask.shape[-1]).to(attention_mask.dtype)\n-                # Multiply the attention mask instead of indexing to avoid issue in torch.export.\n-                attention_mask = attention_mask * where.unsqueeze(-1)\n-\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    level_index=level_index,\n-                    position_embeddings=multi_stage_positional_embeddings,\n-                    query_position_embeddings=query_position_embeddings,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            where = (attention_mask.sum(-1) != attention_mask.shape[-1]).to(attention_mask.dtype)\n+            # Multiply the attention mask instead of indexing to avoid issue in torch.export.\n+            attention_mask = attention_mask * where.unsqueeze(-1)\n \n-                intermediate_hidden_states = self.layernorm(layer_outputs[0])\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                level_index,\n+                None,  # attention_mask\n+                multi_stage_positional_embeddings,\n+                query_position_embeddings,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n-                predicted_mask, attention_mask = self.mask_predictor(\n-                    intermediate_hidden_states,\n-                    pixel_embeddings,\n-                    feature_size_list[(idx + 1) % self.num_feature_levels],\n-                )\n+            intermediate_hidden_states = self.layernorm(layer_outputs[0])\n+\n+            predicted_mask, attention_mask = self.mask_predictor(\n+                intermediate_hidden_states,\n+                pixel_embeddings,\n+                feature_size_list[(idx + 1) % self.num_feature_levels],\n+            )\n \n-                intermediate_mask_predictions += (predicted_mask,)\n+            intermediate_mask_predictions += (predicted_mask,)\n \n-                # add intermediate hidden states with layer norm applied which will be used for predicting class logits\n-                intermediate += (intermediate_hidden_states,)\n+            # add intermediate hidden states with layer norm applied which will be used for predicting class logits\n+            intermediate += (intermediate_hidden_states,)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "18d36427d921898af1081ee1f72595104e491e6d",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 21,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -529,7 +530,7 @@ def forward(\n \n \n # Copied from transformers.models.detr.modeling_detr.DetrDecoderLayer\n-class DetrDecoderLayer(nn.Module):\n+class DetrDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DetrConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -742,26 +743,15 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    None,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    None,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=None,\n-                    object_queries=object_queries,\n-                    query_position_embeddings=query_position_embeddings,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                None,  # attention_mask\n+                object_queries,\n+                query_position_embeddings,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "68c291f1b108fa61c63cfe1af29a493a5f0dc61f",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 9,
            "deletions": 16,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...file_utils import ModelOutput\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n@@ -629,7 +630,7 @@ def forward(self, hidden_states, input_dimensions, head_mask=None, output_attent\n         return outputs\n \n \n-class MaskFormerSwinStage(nn.Module):\n+class MaskFormerSwinStage(GradientCheckpointingLayer):\n     # Copied from transformers.models.swin.modeling_swin.SwinStage.__init__ with Swin->MaskFormerSwin\n     def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, downsample):\n         super().__init__()\n@@ -729,21 +730,13 @@ def forward(\n         for i, layer_module in enumerate(self.layers):\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_hidden_states, output_dimensions, layer_all_hidden_states = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_hidden_states, output_dimensions, layer_all_hidden_states = layer_module(\n-                    hidden_states,\n-                    input_dimensions,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    output_hidden_states,\n-                )\n+            layer_hidden_states, output_dimensions, layer_all_hidden_states = layer_module(\n+                hidden_states,\n+                input_dimensions,\n+                layer_head_mask,\n+                output_attentions,\n+                output_hidden_states,\n+            )\n \n             input_dimensions = (output_dimensions[-2], output_dimensions[-1])\n             all_input_dimensions += (input_dimensions,)"
        },
        {
            "sha": "2585d91a3e3e4f3dc2eef0f7ce44c952dbc7bd05",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 21,
            "deletions": 46,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -34,6 +34,7 @@\n from ...modeling_flash_attention_utils import (\n     FlashAttentionKwargs,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -279,7 +280,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class MBartEncoderLayer(nn.Module):\n+class MBartEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MBartConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -347,7 +348,7 @@ def forward(\n         return outputs\n \n \n-class MBartDecoderLayer(nn.Module):\n+class MBartDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MBartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -866,21 +867,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1130,35 +1122,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                causal_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "941d62b586963cef57bc53bd1834ca189165a8db",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -405,7 +406,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Based on transformers.models.bert.modeling_bert.BertLayer. Added LayerNorm.\n-class MegatronBertLayer(nn.Module):\n+class MegatronBertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -535,27 +536,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             # Because we moved the layer-norm at the end of the hidden layer, we have non-normali-\n             # zed data here. If that's really needed, we must apply LN to match Transformer's BERT."
        },
        {
            "sha": "3bea10553625abb0242a5fc390808306e2655a80",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n@@ -804,7 +805,7 @@ def forward(\n }\n \n \n-class MimiTransformerLayer(nn.Module):\n+class MimiTransformerLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MimiConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1019,27 +1020,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "34e0b507f446ddbeb02c153ec622f2a9c7a40d79",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -32,6 +32,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     MoeCausalLMOutputWithPast,\n@@ -485,7 +486,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return final_hidden_states, router_logits\n \n \n-class MiniMaxDecoderLayer(nn.Module):\n+class MiniMaxDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MiniMaxConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size"
        },
        {
            "sha": "8f82b59e5e47a06d3de33c77ebcbd2024d6417d2",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 14,
            "deletions": 28,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,7 +24,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from functools import partial\n from typing import Callable, Optional, Union\n \n import torch\n@@ -37,6 +36,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     MoeCausalLMOutputWithPast,\n@@ -295,7 +295,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class MixtralDecoderLayer(nn.Module):\n+class MixtralDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MixtralConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -535,32 +535,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    output_router_logits,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "c4e4a429666387da19cbdc70622eee666d547ad2",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 14,
            "deletions": 28,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -19,7 +19,6 @@\n # limitations under the License.\n \"\"\"PyTorch Mixtral model.\"\"\"\n \n-from functools import partial\n from typing import Optional, Union\n \n import torch\n@@ -31,6 +30,7 @@\n from ...cache_utils import DynamicCache\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import LossKwargs, logging\n@@ -226,7 +226,7 @@ class MixtralAttention(MistralAttention):\n     pass\n \n \n-class MixtralDecoderLayer(nn.Module):\n+class MixtralDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MixtralConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -386,32 +386,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    output_router_logits,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "26a12cab8bacc79320a3dd39351ab7f1b0394514",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -299,7 +300,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class MLCDEncoderLayer(nn.Module):\n+class MLCDEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MLCDVisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -416,21 +417,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    position_embeddings,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states=hidden_states,\n-                    position_embeddings=position_embeddings,\n-                    attention_mask=attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states=hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "412d34daa5f716fcfe639c38c2440c7bce5d8ad1",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "modified",
            "additions": 6,
            "deletions": 15,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -356,21 +356,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    position_embeddings,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states=hidden_states,\n-                    position_embeddings=position_embeddings,\n-                    attention_mask=attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states=hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "1b483fe958c074be9d8150c08fabd4019e2e4e11",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithNoAttention,\n     BaseModelOutputWithPoolingAndNoAttention,\n@@ -350,7 +351,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class MobileViTLayer(nn.Module):\n+class MobileViTLayer(GradientCheckpointingLayer):\n     \"\"\"\n     MobileViT block: https://huggingface.co/papers/2110.02178\n     \"\"\"\n@@ -603,13 +604,7 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n \n         for i, layer_module in enumerate(self.layer):\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = layer_module(hidden_states)\n+            hidden_states = layer_module(hidden_states)\n \n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)"
        },
        {
            "sha": "a52aedca7cf61df753a722e3cfb8b2f6131b248d",
            "filename": "src/transformers/models/mobilevitv2/modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithNoAttention,\n     BaseModelOutputWithPoolingAndNoAttention,\n@@ -351,7 +352,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class MobileViTV2Layer(nn.Module):\n+class MobileViTV2Layer(GradientCheckpointingLayer):\n     \"\"\"\n     MobileViTV2 layer: https://huggingface.co/papers/2206.02680\n     \"\"\"\n@@ -556,13 +557,7 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n \n         for i, layer_module in enumerate(self.layer):\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = layer_module(hidden_states)\n+            hidden_states = layer_module(hidden_states)\n \n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)"
        },
        {
            "sha": "9089a8d342539e1f7b6910dcac66952a0733e1cf",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n@@ -508,7 +509,7 @@ def forward(\n         return (hidden_states,) + attn_outputs[1:]  # add attentions if outputted\n \n \n-class ModernBertEncoderLayer(nn.Module):\n+class ModernBertEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n         super().__init__()\n         self.config = config\n@@ -864,27 +865,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    sliding_window_mask,\n-                    position_ids,\n-                    cu_seqlens,\n-                    max_seqlen,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    sliding_window_mask=sliding_window_mask,\n-                    position_ids=position_ids,\n-                    cu_seqlens=cu_seqlens,\n-                    max_seqlen=max_seqlen,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                sliding_window_mask=sliding_window_mask,\n+                position_ids=position_ids,\n+                cu_seqlens=cu_seqlens,\n+                max_seqlen=max_seqlen,\n+                output_attentions=output_attentions,\n+            )\n             hidden_states = layer_outputs[0]\n             if output_attentions and len(layer_outputs) > 1:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)"
        },
        {
            "sha": "bafbb3bf7d77353a51b8ce75e5f549aea4044ede",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PretrainedConfig\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n@@ -710,7 +711,7 @@ def forward(\n         return (hidden_states,) + attn_outputs[1:]  # add attentions if outputted\n \n \n-class ModernBertEncoderLayer(nn.Module):\n+class ModernBertEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n         super().__init__()\n         self.config = config\n@@ -994,27 +995,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    sliding_window_mask,\n-                    position_ids,\n-                    cu_seqlens,\n-                    max_seqlen,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    sliding_window_mask=sliding_window_mask,\n-                    position_ids=position_ids,\n-                    cu_seqlens=cu_seqlens,\n-                    max_seqlen=max_seqlen,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                sliding_window_mask=sliding_window_mask,\n+                position_ids=position_ids,\n+                cu_seqlens=cu_seqlens,\n+                max_seqlen=max_seqlen,\n+                output_attentions=output_attentions,\n+            )\n             hidden_states = layer_outputs[0]\n             if output_attentions and len(layer_outputs) > 1:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)"
        },
        {
            "sha": "2909fb386fb5ec1a587e9d9f175a05d47d38865a",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -781,9 +781,9 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                causal_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,"
        },
        {
            "sha": "500231f3b48b087f0a90da5712fb22e797bf1a00",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -787,9 +787,9 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                causal_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,"
        },
        {
            "sha": "d397dc0d923bfc6ddef195290e8e8fe39aefadb4",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 20,
            "deletions": 43,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n from ...generation import GenerationConfig, GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput, Seq2SeqLMOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n@@ -738,7 +739,7 @@ def forward(\n }\n \n \n-class MoshiDecoderLayer(nn.Module):\n+class MoshiDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MoshiConfig, layer_idx: int, use_flexible_linear: bool, use_rope=True):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1026,27 +1027,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -1343,27 +1332,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "b3005728e7b9b5f281f6619f41ee5cf6b11c1a22",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -160,7 +161,7 @@ def forward(self, hidden_states: torch.Tensor, residual: torch.Tensor) -> torch.\n         return output\n \n \n-class MptBlock(nn.Module):\n+class MptBlock(GradientCheckpointingLayer):\n     def __init__(self, config: MptConfig):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -388,25 +389,14 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                outputs = self._gradient_checkpointing_func(\n-                    block.__call__,\n-                    hidden_states,\n-                    alibi,\n-                    causal_mask,\n-                    layer_past,\n-                    use_cache,\n-                    output_attentions,\n-                )\n-            else:\n-                outputs = block(\n-                    hidden_states,\n-                    layer_past=layer_past,\n-                    attention_mask=causal_mask,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    position_bias=alibi,\n-                )\n+            outputs = block(\n+                hidden_states,\n+                layer_past=layer_past,\n+                attention_mask=causal_mask,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                position_bias=alibi,\n+            )\n \n             hidden_states = outputs[0]\n             if use_cache is True:"
        },
        {
            "sha": "a7fd783d848d3a383fa8c902667d6583a0271a9d",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch.utils.cpp_extension import load\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n     MaskedLMOutput,\n@@ -688,7 +689,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class MraLayer(nn.Module):\n+class MraLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -738,14 +739,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask)\n+            layer_outputs = layer_module(hidden_states, attention_mask)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "5584b2ee8255a16009ffe2280bda6d122d21919a",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 17,
            "deletions": 34,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -523,7 +524,7 @@ def forward(\n \n \n # Copied from transformers.models.t5.modeling_t5.T5Block with T5->MT5\n-class MT5Block(nn.Module):\n+class MT5Block(GradientCheckpointingLayer):\n     def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n@@ -1088,39 +1089,21 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.forward,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_bias,\n-                    encoder_hidden_states,\n-                    encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias,\n-                    layer_head_mask,\n-                    cross_attn_layer_head_mask,\n-                    None,  # past_key_value is always None with gradient checkpointing\n-                    use_cache,\n-                    output_attentions,\n-                    return_dict,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_bias=position_bias,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n-                    layer_head_mask=layer_head_mask,\n-                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_values,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    return_dict=return_dict,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                causal_mask,\n+                position_bias,\n+                encoder_hidden_states,\n+                encoder_extended_attention_mask,\n+                encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n+                layer_head_mask=layer_head_mask,\n+                cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                return_dict=return_dict,\n+                cache_position=cache_position,\n+            )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)"
        },
        {
            "sha": "54fd0b31bb3b0aee334c172f28c43dfbaa09a580",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 13,
            "deletions": 28,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -43,6 +43,7 @@\n from ...modeling_flash_attention_utils import (\n     FlashAttentionKwargs,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -304,7 +305,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class MusicgenDecoderLayer(nn.Module):\n+class MusicgenDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MusicgenDecoderConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -619,33 +620,17 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.forward,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "441c0e862b28104e353cf6c323665b7a5be7e1b4",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -41,6 +41,7 @@\n from ...modeling_flash_attention_utils import (\n     FlashAttentionKwargs,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -320,7 +321,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class MusicgenMelodyDecoderLayer(nn.Module):\n+class MusicgenMelodyDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MusicgenMelodyDecoderConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -596,25 +597,14 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.forward,\n-                    hidden_states,\n-                    attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "9e5136d27ad02e603536a882457650f4098f6d0f",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 23,
            "deletions": 50,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -29,6 +29,7 @@\n     _prepare_4d_attention_mask,\n     _prepare_4d_causal_attention_mask,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -244,7 +245,7 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-class MvpEncoderLayer(nn.Module):\n+class MvpEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MvpConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -316,7 +317,7 @@ def forward(\n         return outputs\n \n \n-class MvpDecoderLayer(nn.Module):\n+class MvpDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: MvpConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -682,23 +683,13 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        (self_attn_prompt[idx] if self.use_prompt else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        self_attn_prompt=(self_attn_prompt[idx] if self.use_prompt else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    self_attn_prompt=(self_attn_prompt[idx] if self.use_prompt else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -935,37 +926,19 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    self_attn_prompt[idx] if self.use_prompt else None,\n-                    cross_attn_prompt[idx] if self.use_prompt else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    self_attn_prompt=(self_attn_prompt[idx] if self.use_prompt else None),\n-                    cross_attn_prompt=(cross_attn_prompt[idx] if self.use_prompt else None),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                self_attn_prompt=(self_attn_prompt[idx] if self.use_prompt else None),\n+                cross_attn_prompt=(cross_attn_prompt[idx] if self.use_prompt else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "6fba76b55d39d597368f5dad1b0433a5fde259d4",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -498,7 +499,7 @@ def forward(\n \n # copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n # no longer copied after attention refactors\n-class NemotronDecoderLayer(nn.Module):\n+class NemotronDecoderLayer(GradientCheckpointingLayer):\n     # Ignore copy\n     def __init__(self, config: NemotronConfig, layer_idx: int):\n         super().__init__()\n@@ -703,29 +704,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "f498cf743fcccbccd455cea9468502fdef6d4e09",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 22,
            "deletions": 49,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -32,6 +32,7 @@\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     MoEModelOutput,\n     MoEModelOutputWithPastAndCrossAttentions,\n@@ -625,7 +626,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class NllbMoeEncoderLayer(nn.Module):\n+class NllbMoeEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -707,7 +708,7 @@ def forward(\n         return outputs\n \n \n-class NllbMoeDecoderLayer(nn.Module):\n+class NllbMoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1018,22 +1019,13 @@ def forward(\n             if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n                 layer_outputs = (None, None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                        output_router_logits=output_router_logits,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                    output_router_logits=output_router_logits,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1296,37 +1288,18 @@ def forward(\n                 past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    if use_cache:\n-                        logger.warning_once(\n-                            \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                        )\n-                        use_cache = False\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        decoder_layer.forward,\n-                        hidden_states,\n-                        attention_mask,\n-                        encoder_hidden_states,\n-                        encoder_attention_mask,\n-                        layer_head_mask,\n-                        cross_attn_layer_head_mask,\n-                        None,  # past_key_value is always None with gradient checkpointing\n-                        use_cache,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = decoder_layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        encoder_hidden_states=encoder_hidden_states,\n-                        encoder_attention_mask=encoder_attention_mask,\n-                        layer_head_mask=layer_head_mask,\n-                        cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                        past_key_value=past_key_value,\n-                        use_cache=use_cache,\n-                        output_attentions=output_attentions,\n-                        output_router_logits=output_router_logits,\n-                    )\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                    encoder_attention_mask=encoder_attention_mask,\n+                    layer_head_mask=layer_head_mask,\n+                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n+                    past_key_value=past_key_value,\n+                    use_cache=use_cache,\n+                    output_attentions=output_attentions,\n+                    output_router_logits=output_router_logits,\n+                )\n \n                 hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "f5b940157ded172f716d7c4a97394349069f20a5",
            "filename": "src/transformers/models/nystromformer/modeling_nystromformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     MaskedLMOutput,\n@@ -311,7 +312,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class NystromformerLayer(nn.Module):\n+class NystromformerLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -363,15 +364,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n             if output_attentions:"
        },
        {
            "sha": "61732ab1c2591fb0bab096a5ee6566a8bf23d0a0",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n@@ -611,7 +612,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return final_hidden_states, router_logits\n \n \n-class OlmoeDecoderLayer(nn.Module):\n+class OlmoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: OlmoeConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -828,31 +829,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    output_router_logits,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "1007be135ed79db1f5e9ca4d5fcf57ccd8a38e80",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 15,
            "deletions": 32,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -31,6 +31,7 @@\n )\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n from ...utils.backbone_utils import load_backbone\n@@ -879,7 +880,7 @@ def forward(self, x):\n         return x\n \n \n-class OmDetTurboDeformableTransformerDecoderLayer(nn.Module):\n+class OmDetTurboDeformableTransformerDecoderLayer(GradientCheckpointingLayer):\n     \"\"\"\n     A single layer of the Deformable Transformer Decoder.\n     \"\"\"\n@@ -1376,37 +1377,19 @@ def forward(\n         last_refined_bbox = None\n         reference_points = reference_points.sigmoid()\n         for i, layer in enumerate(self.layers):\n-            if self.gradient_checkpointing and self.training:\n-                predicted_class_features, task_features, self_attention, cross_attention = (\n-                    self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        predicted_class_features,\n-                        task_features,\n-                        reference_points,\n-                        vision_features,\n-                        vision_shapes,\n-                        vision_shapes_list,\n-                        level_start_index=level_start_index,\n-                        attention_mask=attention_mask,\n-                        query_position=self.query_position_head(reference_points),\n-                        output_attentions=output_attentions,\n-                        output_hidden_states=output_hidden_states,\n-                    )\n-                )\n-            else:\n-                predicted_class_features, task_features, self_attention, cross_attention = layer(\n-                    predicted_class_features,\n-                    task_features,\n-                    reference_points,\n-                    vision_features,\n-                    vision_shapes,\n-                    vision_shapes_list,\n-                    level_start_index=level_start_index,\n-                    attention_mask=attention_mask,\n-                    query_position=self.query_position_head(reference_points),\n-                    output_attentions=output_attentions,\n-                    output_hidden_states=output_hidden_states,\n-                )\n+            predicted_class_features, task_features, self_attention, cross_attention = layer(\n+                predicted_class_features,\n+                task_features,\n+                reference_points,\n+                vision_features,\n+                vision_shapes,\n+                vision_shapes_list,\n+                level_start_index=level_start_index,\n+                attention_mask=attention_mask,\n+                query_position=self.query_position_head(reference_points),\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+            )\n             if output_attentions:\n                 all_self_attns = all_self_attns + (self_attention,)\n                 all_cross_attns = all_cross_attns + (cross_attention,)"
        },
        {
            "sha": "99dde2fabba836b608881949cfdff826dc95d2eb",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.cuda.amp import autocast\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -2563,7 +2564,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class OneFormerTextTransformerLayer(nn.Module):\n+class OneFormerTextTransformerLayer(GradientCheckpointingLayer):\n     def __init__(self, width: int, heads: int, attn_mask: torch.Tensor, layer_norm_eps=1e-05):\n         super().__init__()\n         self.self_attn = nn.MultiheadAttention(width, heads)\n@@ -2617,10 +2618,7 @@ def __init__(\n \n     def forward(self, hidden_states: torch.Tensor):\n         for layer in self.layers:\n-            if self.use_checkpoint:\n-                hidden_states = self._gradient_checkpointing_func(layer, hidden_states)\n-            else:\n-                hidden_states = layer(hidden_states)\n+            hidden_states = layer(hidden_states)\n         return hidden_states\n \n "
        },
        {
            "sha": "d18378ba8e9439637acc9185f6546a0b7b4af3b6",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 13,
            "deletions": 25,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -200,7 +201,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class OPTDecoderLayer(nn.Module):\n+class OPTDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: OPTConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -668,30 +669,17 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    position_ids,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "ee4fb714f5496cbbcb048b36a5771acf9ae09abb",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, is_vision_available, logging, torch_int\n@@ -497,7 +498,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->Owlv2\n-class Owlv2EncoderLayer(nn.Module):\n+class Owlv2EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Owlv2Config):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -655,21 +656,12 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "4e269cd465993d05a888919752dfa661161e352f",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, is_vision_available, logging, torch_int\n@@ -485,7 +486,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->OwlViT\n-class OwlViTEncoderLayer(nn.Module):\n+class OwlViTEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: OwlViTConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -641,21 +642,12 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "2ffb53ee9e018b1ccd0c535570fd372982484360",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 21,
            "deletions": 46,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -33,6 +33,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -269,7 +270,7 @@ def forward(\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Pegasus, MBART->PEGASUS\n-class PegasusEncoderLayer(nn.Module):\n+class PegasusEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PegasusConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -338,7 +339,7 @@ def forward(\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Pegasus, MBART->PEGASUS\n-class PegasusDecoderLayer(nn.Module):\n+class PegasusDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PegasusConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -845,21 +846,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1135,35 +1127,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                causal_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "842e365fb8272759ec9a28d01bf147b9eefcb890",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 19,
            "deletions": 40,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -33,6 +33,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -528,7 +529,7 @@ def compute_local_attention_representations(\n         return attn_output, attn_probs\n \n \n-class PegasusXEncoderLayer(nn.Module):\n+class PegasusXEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, stagger_blocks_this_layer: bool, config: PegasusXConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -643,7 +644,7 @@ def unpad_local_tokens(cls, padded_hidden_states, block_size):\n         return padded_hidden_states[:, pad_size:-pad_size, :]\n \n \n-class PegasusXDecoderLayer(nn.Module):\n+class PegasusXDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PegasusXConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1148,21 +1149,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        global_hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        global_hidden_states,\n-                        attention_mask,\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    global_hidden_states,\n+                    attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n                 global_hidden_states = layer_outputs[1]\n@@ -1388,29 +1380,16 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                causal_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "f2bbef331a0879bf9249569211e4d0493375730b",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 13,
            "deletions": 25,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -299,7 +300,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class PersimmonDecoderLayer(nn.Module):\n+class PersimmonDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PersimmonConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -517,30 +518,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "95164a5f5dbd55e1328912ac76d07f41200bf7bb",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_phi.py file directly. One of our CI enforces this.\n #                \n-from functools import partial\n from typing import Callable, Optional, Union\n \n import torch\n@@ -15,6 +14,7 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -206,7 +206,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class PhiDecoderLayer(nn.Module):\n+class PhiDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PhiConfig, layer_idx: int):\n         super().__init__()\n         self.self_attn = PhiAttention(config, layer_idx=layer_idx)\n@@ -410,30 +410,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "46a367bbdb101245aedb052c843a5355cb56d910",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -1,4 +1,3 @@\n-from functools import partial\n from typing import Callable, Optional\n \n import torch\n@@ -7,6 +6,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n )\n@@ -118,7 +118,7 @@ class PhiMLP(CLIPMLP):\n     pass\n \n \n-class PhiDecoderLayer(nn.Module):\n+class PhiDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PhiConfig, layer_idx: int):\n         super().__init__()\n         self.self_attn = PhiAttention(config, layer_idx=layer_idx)\n@@ -261,30 +261,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "af4e823a98d8c334ca00cd89595b81aa5511657a",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n from ...modeling_flash_attention_utils import is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n@@ -795,7 +796,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return final_hidden_states, router_logits\n \n \n-class PhimoeDecoderLayer(nn.Module):\n+class PhimoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PhimoeConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -1018,31 +1019,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    output_router_logits,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "6b90ae80d7c764a999fe41afa88c3a764d8e94bf",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 24,
            "deletions": 48,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -259,7 +260,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class Pix2StructVisionLayer(nn.Module):\n+class Pix2StructVisionLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Pix2StructConfig) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -327,16 +328,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n \n@@ -925,7 +917,7 @@ def forward(\n         return outputs\n \n \n-class Pix2StructTextBlock(nn.Module):\n+class Pix2StructTextBlock(GradientCheckpointingLayer):\n     def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n \n@@ -1148,6 +1140,12 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+            )\n+            use_cache = False\n+\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -1241,42 +1239,20 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                if use_cache:\n-                    logger.warning(\n-                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                    )\n-                    use_cache = False\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.forward,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_bias,\n-                    encoder_hidden_states,\n-                    encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias,\n-                    layer_head_mask,\n-                    cross_attn_layer_head_mask,\n-                    None,  # past_key_value is always None with gradient checkpointing\n-                    use_cache,\n-                    output_attentions,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_bias=position_bias,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n-                    layer_head_mask=layer_head_mask,\n-                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_values,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                causal_mask,\n+                position_bias,\n+                encoder_hidden_states,\n+                encoder_extended_attention_mask,\n+                encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n+                layer_head_mask=layer_head_mask,\n+                cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n+            )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)"
        },
        {
            "sha": "f1d5ab06d54be92a9e5e777ee23dfd94d54cc41b",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 17,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_rope_utils import dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -272,7 +273,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class PixtralAttentionLayer(nn.Module):\n+class PixtralAttentionLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention_norm = PixtralRMSNorm(config.hidden_size, eps=1e-5)\n@@ -374,22 +375,13 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    position_embeddings,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    position_embeddings=position_embeddings,\n-                    output_attentions=output_attentions,\n-                    **kwargs,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                position_embeddings=position_embeddings,\n+                output_attentions=output_attentions,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "327b70b5ec73276419b0a740796967c1d92f6639",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 21,
            "deletions": 46,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -36,6 +36,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -465,7 +466,7 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-class PLBartEncoderLayer(nn.Module):\n+class PLBartEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PLBartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -683,21 +684,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -714,7 +706,7 @@ def forward(\n         )\n \n \n-class PLBartDecoderLayer(nn.Module):\n+class PLBartDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PLBartConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -1064,35 +1056,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "5c4285afe728f19c451f33a875f97d7ca15a2e62",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 16,
            "deletions": 32,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -467,7 +468,7 @@ def forward(\n \n \n # Copied from transformers.models.t5.modeling_t5.T5Block with T5->Pop2Piano,t5->pop2piano\n-class Pop2PianoBlock(nn.Module):\n+class Pop2PianoBlock(GradientCheckpointingLayer):\n     def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n@@ -814,37 +815,20 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.forward,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_bias,\n-                    encoder_hidden_states,\n-                    encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias,\n-                    layer_head_mask,\n-                    cross_attn_layer_head_mask,\n-                    None,  # past_key_value is always None with gradient checkpointing\n-                    use_cache,\n-                    output_attentions,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_bias=position_bias,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n-                    layer_head_mask=layer_head_mask,\n-                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_values,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                causal_mask,\n+                position_bias,\n+                encoder_hidden_states,\n+                encoder_extended_attention_mask,\n+                encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n+                layer_head_mask=layer_head_mask,\n+                cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n+            )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)"
        },
        {
            "sha": "eb0b2e59471aa808acea46c63645a5339e240523",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 24,
            "deletions": 52,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n@@ -956,7 +957,7 @@ def get_predict_relative_pos_embeddings(\n         return predict_relative_pos_embeddings\n \n \n-class ProphetNetEncoderLayer(nn.Module):\n+class ProphetNetEncoderLayer(GradientCheckpointingLayer):\n     \"\"\"\n     Encoder block for Prophetnet\n     \"\"\"\n@@ -999,7 +1000,7 @@ def forward(\n         return outputs\n \n \n-class ProphetNetDecoderLayer(nn.Module):\n+class ProphetNetDecoderLayer(GradientCheckpointingLayer):\n     \"\"\"\n     Decoder block for Prophetnet\n     \"\"\"\n@@ -1183,21 +1184,12 @@ def forward(\n             if output_hidden_states:\n                 encoder_hidden_states = encoder_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    extended_attention_mask,\n-                    (head_mask[idx] if head_mask is not None else None),\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask=extended_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask=extended_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -1395,41 +1387,21 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    extended_attention_mask,\n-                    encoder_hidden_states,\n-                    extended_encoder_attention_mask,\n-                    (head_mask[idx] if head_mask is not None else None),\n-                    (cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n-                    extended_predict_attention_mask,\n-                    main_relative_position_buckets,\n-                    predict_relative_position_buckets,\n-                    position_ids,\n-                    None,\n-                    use_cache,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=extended_attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attn_mask=extended_encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    extended_predict_attention_mask=extended_predict_attention_mask,\n-                    main_relative_position_buckets=main_relative_position_buckets,\n-                    predict_relative_position_buckets=predict_relative_position_buckets,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_value,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                extended_attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attn_mask=extended_encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                extended_predict_attention_mask=extended_predict_attention_mask,\n+                main_relative_position_buckets=main_relative_position_buckets,\n+                predict_relative_position_buckets=predict_relative_position_buckets,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "b357cb5970a45c72d1c99243fc28aeec76e14132",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -300,7 +301,7 @@ def forward(self, hidden_states: torch.Tensor, height: int, width: int, output_a\n         return outputs\n \n \n-class PvtV2EncoderLayer(nn.Module):\n+class PvtV2EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PvtV2Config, layer_idx: int):\n         super().__init__()\n         self.patch_embedding = PvtV2OverlapPatchEmbeddings(\n@@ -367,10 +368,7 @@ def forward(\n         batch_size = pixel_values.shape[0]\n         hidden_states = pixel_values\n         for idx, layer in enumerate(self.layers):\n-            if self.gradient_checkpointing and self.training:\n-                layer_output = self._gradient_checkpointing_func(layer.__call__, hidden_states, output_attentions)\n-            else:\n-                layer_output = layer(hidden_states, output_attentions)\n+            layer_output = layer(hidden_states, output_attentions)\n             outputs, height, width = layer_output\n             hidden_states = outputs[0]\n             if output_attentions:"
        },
        {
            "sha": "e7f362c1f4a8b5fb3bf1a3e3115a0ca3797b947a",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 31,
            "deletions": 73,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -727,7 +727,7 @@ def forward(\n }\n \n \n-class Qwen2_5OmniAudioEncoderLayer(nn.Module):\n+class Qwen2_5OmniAudioEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2_5OmniAudioEncoderConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -889,19 +889,8 @@ def forward(\n             )\n         ).to(torch.int32)\n \n-        for idx, encoder_layer in enumerate(self.layers):\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    cu_seqlens,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    cu_seqlens,\n-                )\n-\n+        for encoder_layer in self.layers:\n+            layer_outputs = encoder_layer(hidden_states, cu_seqlens)\n             hidden_states = layer_outputs[0]\n \n         hidden_states_list = hidden_states.split(aftercnn_lens.tolist(), dim=0)\n@@ -1107,7 +1096,7 @@ def forward(self, hidden_state):\n }\n \n \n-class Qwen2_5OmniVisionBlock(nn.Module):\n+class Qwen2_5OmniVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig) -> None:\n         super().__init__()\n         self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n@@ -1324,16 +1313,11 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens_now, rotary_pos_emb\n-                )\n-            else:\n-                hidden_states = blk(\n-                    hidden_states,\n-                    cu_seqlens=cu_seqlens_now,\n-                    rotary_pos_emb=rotary_pos_emb,\n-                )\n+            hidden_states = blk(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens_now,\n+                rotary_pos_emb=rotary_pos_emb,\n+            )\n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)\n         hidden_states = hidden_states[reverse_indices, :]\n@@ -1760,30 +1744,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -2331,30 +2302,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "0d55daf14c15fec1440e7eee3e79912b16296148",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 7,
            "deletions": 23,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -1900,19 +1900,8 @@ def forward(\n             )\n         ).to(torch.int32)\n \n-        for idx, encoder_layer in enumerate(self.layers):\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    cu_seqlens,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    cu_seqlens,\n-                )\n-\n+        for encoder_layer in self.layers:\n+            layer_outputs = encoder_layer(hidden_states, cu_seqlens)\n             hidden_states = layer_outputs[0]\n \n         hidden_states_list = hidden_states.split(aftercnn_lens.tolist(), dim=0)\n@@ -2166,16 +2155,11 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens_now, rotary_pos_emb\n-                )\n-            else:\n-                hidden_states = blk(\n-                    hidden_states,\n-                    cu_seqlens=cu_seqlens_now,\n-                    rotary_pos_emb=rotary_pos_emb,\n-                )\n+            hidden_states = blk(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens_now,\n+                rotary_pos_emb=rotary_pos_emb,\n+            )\n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)\n         hidden_states = hidden_states[reverse_indices, :]"
        },
        {
            "sha": "75a5b79f8b5020bf0c8a0f89a2943bd8787e0901",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 31,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -314,7 +314,7 @@ def forward(\n }\n \n \n-class Qwen2_5_VLVisionBlock(nn.Module):\n+class Qwen2_5_VLVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         super().__init__()\n         self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n@@ -516,12 +516,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens_now, None, position_embeddings\n-                )\n-            else:\n-                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n+            hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n \n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)\n@@ -991,30 +986,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "a6dedefa019544a99eb30b2eb3232225c7b30fee",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -50,6 +50,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...modeling_flash_attention_utils import is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...processing_utils import MultiModalData, ProcessingKwargs, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_torchdynamo_compiling, logging\n@@ -205,7 +206,7 @@ class Qwen2_5_VLVisionSdpaAttention(VisionSdpaAttention):\n }\n \n \n-class Qwen2_5_VLVisionBlock(nn.Module):\n+class Qwen2_5_VLVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         super().__init__()\n         self.norm1 = Qwen2RMSNorm(config.hidden_size, eps=1e-6)\n@@ -395,12 +396,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens_now, None, position_embeddings\n-                )\n-            else:\n-                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n+            hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens_now, position_embeddings=position_embeddings)\n \n         hidden_states = self.merger(hidden_states)\n         reverse_indices = torch.argsort(window_index)"
        },
        {
            "sha": "8e331e1bd0bd98467723750eadabd858b5f74595",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -200,7 +201,7 @@ def forward(\n \n \n # Copied from transformers.models.whisper.modeling_whisper.WhisperEncoderLayer with Whisper->Qwen2Audio, WHISPER->QWEN2AUDIO\n-class Qwen2AudioEncoderLayer(nn.Module):\n+class Qwen2AudioEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2AudioConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -436,21 +437,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "243bd12baca06a0d4a1b5d55480ed8a7e802bded",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -32,6 +32,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n@@ -639,7 +640,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return final_hidden_states, router_logits\n \n \n-class Qwen2MoeDecoderLayer(nn.Module):\n+class Qwen2MoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -865,31 +866,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    output_router_logits,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "32b9a416600d21d1c0a1e1e3e7c909c8cfbb4dce",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 31,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -439,7 +439,7 @@ def forward(\n }\n \n \n-class Qwen2VLVisionBlock(nn.Module):\n+class Qwen2VLVisionBlock(GradientCheckpointingLayer):\n     def __init__(self, config, attn_implementation: str = \"sdpa\") -> None:\n         super().__init__()\n         self.norm1 = LayerNorm(config.embed_dim, eps=1e-6)\n@@ -837,12 +837,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n \n         for blk in self.blocks:\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    blk.__call__, hidden_states, cu_seqlens, None, position_embeddings\n-                )\n-            else:\n-                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, position_embeddings=position_embeddings)\n+            hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, position_embeddings=position_embeddings)\n \n         return self.merger(hidden_states)\n \n@@ -959,30 +954,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "1f74f5e55893ddddc30592a89ca7657586d28af5",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 28,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from functools import partial\n from typing import Callable, Optional, Union\n \n import torch\n@@ -32,6 +31,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     MoeCausalLMOutputWithPast,\n@@ -286,7 +286,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class Qwen3MoeDecoderLayer(nn.Module):\n+class Qwen3MoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n@@ -541,32 +541,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    output_router_logits,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **flash_attn_kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "4b82c490cc916563bcda30847087b442a52482c4",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithNoAttention, CausalLMOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -467,7 +468,7 @@ def forward(self, hidden_states):\n         return self.down_proj(gate * self.up_proj(hidden_states))\n \n \n-class RecurrentGemmaDecoderLayer(nn.Module):\n+class RecurrentGemmaDecoderLayer(GradientCheckpointingLayer):\n     \"\"\"Griffin and Hawk's residual block.\"\"\"\n \n     def __init__(self, config, layer_idx):\n@@ -644,12 +645,7 @@ def forward(\n         for i, residual_block in enumerate(self.layers):\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    residual_block.__call__, hidden_states, position_ids, causal_mask, cache_position, use_cache\n-                )\n-            else:\n-                hidden_states = residual_block(hidden_states, position_ids, causal_mask, cache_position, use_cache)\n+            hidden_states = residual_block(hidden_states, position_ids, causal_mask, cache_position, use_cache)\n \n         hidden_states = self.final_norm(hidden_states)\n "
        },
        {
            "sha": "774fc46ef7fc13164eb4b1f235ea6534cb81d785",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -399,7 +400,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class RemBertLayer(nn.Module):\n+class RemBertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -528,27 +529,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "ecf3a6cc53148a98624c5ce81173480f5a7d717e",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN, gelu\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -477,7 +478,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\n-class RobertaLayer(nn.Module):\n+class RobertaLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -603,27 +604,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "e8636281e3996b062d5964016f72f11c89c22200",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN, gelu\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -365,7 +366,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->RobertaPreLayerNorm\n-class RobertaPreLayerNormLayer(nn.Module):\n+class RobertaPreLayerNormLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -491,27 +492,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "3985e86e0b36c0f9323cd0a2f149b62252ae7da4",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -488,7 +489,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->RoCBert\n-class RoCBertLayer(nn.Module):\n+class RoCBertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -614,27 +615,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "8439fed19cfcf127878bddf9107bfc637c00170b",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN, get_activation\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -423,7 +424,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class RoFormerLayer(nn.Module):\n+class RoFormerLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -558,29 +559,16 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    sinusoidal_pos,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    sinusoidal_pos,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                sinusoidal_pos,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "8f04539810267036deb3cf6e70c9d19e7e14dfc3",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n \n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n@@ -344,7 +345,7 @@ def forward(self, hidden, state=None):\n         return receptance * value, state\n \n \n-class RwkvBlock(nn.Module):\n+class RwkvBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id):\n         super().__init__()\n         self.config = config\n@@ -604,14 +605,9 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n         for idx, block in enumerate(self.blocks):\n-            if self.gradient_checkpointing and self.training:\n-                hidden_states, state, attentions = self._gradient_checkpointing_func(\n-                    block.__call__, hidden_states, state, use_cache, output_attentions\n-                )\n-            else:\n-                hidden_states, state, attentions = block(\n-                    hidden_states, state=state, use_cache=use_cache, output_attentions=output_attentions\n-                )\n+            hidden_states, state, attentions = block(\n+                hidden_states, state=state, use_cache=use_cache, output_attentions=output_attentions\n+            )\n \n             if (\n                 self.layers_are_rescaled"
        },
        {
            "sha": "31cdec6d5f7edd6bfe9296a00f0db627273d9c0f",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch import Tensor, nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -969,7 +970,7 @@ def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch\n }\n \n \n-class SamVisionLayer(nn.Module):\n+class SamVisionLayer(GradientCheckpointingLayer):\n     def __init__(self, config, window_size):\n         super().__init__()\n         self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -1145,13 +1146,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n+            layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "982bbbb47e07894ddeb3ed77f187ab69029dc061",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -29,6 +29,7 @@\n from torch import Tensor, nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n@@ -364,7 +365,7 @@ def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch\n }\n \n \n-class SamHQVisionLayer(nn.Module):\n+class SamHQVisionLayer(GradientCheckpointingLayer):\n     def __init__(self, config, window_size):\n         super().__init__()\n         self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -539,18 +540,10 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n         intermediate_embeddings = []\n \n-        for i, layer_module in enumerate(self.layers):\n+        for layer_module in self.layers:\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n-\n+            layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n             hidden_states = layer_outputs[0]\n \n             # Collect embeddings from non-windowed blocks"
        },
        {
            "sha": "55f475880cabf02b23343455df19b5c1666aecc3",
            "filename": "src/transformers/models/sam_hq/modular_sam_hq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -151,18 +151,10 @@ def forward(\n         all_self_attentions = () if output_attentions else None\n         intermediate_embeddings = []\n \n-        for i, layer_module in enumerate(self.layers):\n+        for layer_module in self.layers:\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n-\n+            layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n             hidden_states = layer_outputs[0]\n \n             # Collect embeddings from non-windowed blocks"
        },
        {
            "sha": "65feeb2d2226d2aac6887f485c3a61957f69cba9",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 25,
            "deletions": 54,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -32,6 +32,7 @@\n     _prepare_4d_attention_mask,\n     _prepare_4d_causal_attention_mask,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -610,7 +611,7 @@ def _apply_relative_embeddings(self, query, key, relative_position_embeddings):\n         return scores\n \n \n-class SeamlessM4TConformerEncoderLayer(nn.Module):\n+class SeamlessM4TConformerEncoderLayer(GradientCheckpointingLayer):\n     \"\"\"Conformer block based on https://huggingface.co/papers/2005.08100.\"\"\"\n \n     # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerEncoderLayer.__init__ with Wav2Vec2->SeamlessM4T, attention_dropout->speech_encoder_dropout, torch.nn->nn\n@@ -743,23 +744,13 @@ def forward(\n             )\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        relative_position_embeddings,\n-                        output_attentions,\n-                        conv_attention_mask,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        relative_position_embeddings=relative_position_embeddings,\n-                        output_attentions=output_attentions,\n-                        conv_attention_mask=conv_attention_mask,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    relative_position_embeddings=relative_position_embeddings,\n+                    output_attentions=output_attentions,\n+                    conv_attention_mask=conv_attention_mask,\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:\n@@ -1173,7 +1164,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class SeamlessM4TEncoderLayer(nn.Module):\n+class SeamlessM4TEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SeamlessM4TConfig, encoder_ffn_dim=None, encoder_attention_heads=None):\n         super().__init__()\n         encoder_ffn_dim = config.encoder_ffn_dim if encoder_ffn_dim is None else encoder_ffn_dim\n@@ -1236,7 +1227,7 @@ def forward(\n         return outputs\n \n \n-class SeamlessM4TDecoderLayer(nn.Module):\n+class SeamlessM4TDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SeamlessM4TConfig, decoder_ffn_dim=None, decoder_attention_heads=None):\n         super().__init__()\n         decoder_ffn_dim = config.decoder_ffn_dim if decoder_ffn_dim is None else decoder_ffn_dim\n@@ -1691,19 +1682,11 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.forward,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1866,27 +1849,15 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "7427f1dfab2d6e3726c9fe9961b2b88c95ef2395",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 31,
            "deletions": 68,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -29,6 +29,7 @@\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -489,7 +490,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class SeamlessM4Tv2ConformerEncoderLayer(nn.Module):\n+class SeamlessM4Tv2ConformerEncoderLayer(GradientCheckpointingLayer):\n     \"\"\"Conformer block based on https://huggingface.co/papers/2005.08100.\"\"\"\n \n     # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerEncoderLayer.__init__ with Wav2Vec2->SeamlessM4Tv2, attention_dropout->speech_encoder_dropout, torch.nn->nn\n@@ -645,21 +646,12 @@ def forward(\n             )\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                        conv_attention_mask,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        output_attentions=output_attentions,\n-                        conv_attention_mask=conv_attention_mask,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    output_attentions=output_attentions,\n+                    conv_attention_mask=conv_attention_mask,\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:\n@@ -1031,7 +1023,7 @@ def forward(self, hidden_states):\n \n \n # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TEncoderLayer with SeamlessM4T->SeamlessM4Tv2\n-class SeamlessM4Tv2EncoderLayer(nn.Module):\n+class SeamlessM4Tv2EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SeamlessM4Tv2Config, encoder_ffn_dim=None, encoder_attention_heads=None):\n         super().__init__()\n         encoder_ffn_dim = config.encoder_ffn_dim if encoder_ffn_dim is None else encoder_ffn_dim\n@@ -1095,7 +1087,7 @@ def forward(\n \n \n # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TDecoderLayer with SeamlessM4T->SeamlessM4Tv2\n-class SeamlessM4Tv2DecoderLayer(nn.Module):\n+class SeamlessM4Tv2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SeamlessM4Tv2Config, decoder_ffn_dim=None, decoder_attention_heads=None):\n         super().__init__()\n         decoder_ffn_dim = config.decoder_ffn_dim if decoder_ffn_dim is None else decoder_ffn_dim\n@@ -1210,7 +1202,7 @@ def forward(\n         return outputs\n \n \n-class SeamlessM4Tv2TextToUnitDecoderLayer(nn.Module):\n+class SeamlessM4Tv2TextToUnitDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SeamlessM4Tv2Config, decoder_ffn_dim=None, decoder_attention_heads=None):\n         super().__init__()\n         decoder_ffn_dim = config.decoder_ffn_dim if decoder_ffn_dim is None else decoder_ffn_dim\n@@ -1760,19 +1752,11 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.forward,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1936,27 +1920,15 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n@@ -2137,21 +2109,12 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    padding_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    padding_mask=padding_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                padding_mask=padding_mask,\n+                output_attentions=output_attentions,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "cf6b6db3f2a6fc0b699092bed159173a1206a93b",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import functional as F\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_seggpt import SegGptConfig\n@@ -395,7 +396,7 @@ def extra_repr(self) -> str:\n         return f\"p={self.drop_prob}\"\n \n \n-class SegGptLayer(nn.Module):\n+class SegGptLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SegGptConfig, drop_path_rate: float) -> None:\n         super().__init__()\n         self.attention = SegGptAttention(config)\n@@ -470,16 +471,7 @@ def forward(\n             # Condition to check if we have the appropriate number of prompts to ensemble\n             ensemble_cond = 2 if self.config.merge_index > i else 1\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    ensemble_cond,\n-                    feature_ensemble,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, ensemble_cond, feature_ensemble, output_attentions)\n+            layer_outputs = layer_module(hidden_states, ensemble_cond, feature_ensemble, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "da4a54b39fc9d4aefdd4dffee9483450909fd04e",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 9,
            "deletions": 22,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -32,6 +32,7 @@\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -42,7 +43,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class SEWNoLayerNormConvLayer(nn.Module):\n+class SEWNoLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -63,7 +64,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class SEWLayerNormConvLayer(nn.Module):\n+class SEWLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -90,7 +91,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class SEWGroupNormConvLayer(nn.Module):\n+class SEWGroupNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -223,13 +224,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n \n@@ -410,7 +405,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class SEWEncoderLayer(nn.Module):\n+class SEWEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = SEWAttention(\n@@ -521,17 +516,9 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "2d56fea3bc6e31003af3943b75d32a4198c9b4ee",
            "filename": "src/transformers/models/sew/modular_sew.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -230,17 +230,9 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "5e00ddcd1f3b9f6e64087c0bdcb2e37ed12f8b27",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 14,
            "deletions": 30,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import softmax_backward_data\n@@ -242,7 +243,7 @@ def get_mask(input, local_context):\n \n \n # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->SEWD\n-class SEWDNoLayerNormConvLayer(nn.Module):\n+class SEWDNoLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -264,7 +265,7 @@ def forward(self, hidden_states):\n \n \n # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->SEWD\n-class SEWDLayerNormConvLayer(nn.Module):\n+class SEWDLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -292,7 +293,7 @@ def forward(self, hidden_states):\n \n \n # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->SEWD\n-class SEWDGroupNormConvLayer(nn.Module):\n+class SEWDGroupNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -429,13 +430,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n \n@@ -930,7 +925,7 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n-class SEWDLayer(nn.Module):\n+class SEWDLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = SEWDAttention(config)\n@@ -1087,25 +1082,14 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (output_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                output_states = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    next_kv,\n-                    attention_mask,\n-                    query_states,\n-                    relative_pos,\n-                    rel_embeddings,\n-                    output_attentions,\n-                )\n-            else:\n-                output_states = layer_module(\n-                    next_kv,\n-                    attention_mask,\n-                    query_states=query_states,\n-                    relative_pos=relative_pos,\n-                    rel_embeddings=rel_embeddings,\n-                    output_attentions=output_attentions,\n-                )\n+            output_states = layer_module(\n+                next_kv,\n+                attention_mask,\n+                query_states=query_states,\n+                relative_pos=relative_pos,\n+                rel_embeddings=rel_embeddings,\n+                output_attentions=output_attentions,\n+            )\n \n             if output_attentions:\n                 output_states, att_m = output_states"
        },
        {
            "sha": "3cf16a992c9f57b8c7085d424ba7debad8ea47ed",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 14,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -239,7 +240,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class SmolVLMEncoderLayer(nn.Module):\n+class SmolVLMEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SmolVLMVisionConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -346,19 +347,11 @@ def forward(\n         for encoder_layer in self.layers:\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "aaff8d90fec42e2df1d341923ed7dde54795e8ac",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 20,
            "deletions": 44,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -328,7 +329,7 @@ def forward(\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Speech2Text, MBART->SPEECH_TO_TEXT\n-class Speech2TextEncoderLayer(nn.Module):\n+class Speech2TextEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Speech2TextConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -398,7 +399,7 @@ def forward(\n \n # copied from transformers.models.mbart.modeling_mbart.MBartDecoderLayer with MBart->Speech2Text, MBART->SPEECH_TO_TEXT\n # TODO: change copy when applying cache class\n-class Speech2TextDecoderLayer(nn.Module):\n+class Speech2TextDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Speech2TextConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -693,21 +694,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -941,33 +933,17 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "9dfb26538289f0900fe9684576ba23efd6d8d53d",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 25,
            "deletions": 56,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -207,7 +208,7 @@ def compute_num_masked_span(input_length):\n \n \n # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->SpeechT5\n-class SpeechT5NoLayerNormConvLayer(nn.Module):\n+class SpeechT5NoLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -229,7 +230,7 @@ def forward(self, hidden_states):\n \n \n # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->SpeechT5\n-class SpeechT5LayerNormConvLayer(nn.Module):\n+class SpeechT5LayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -257,7 +258,7 @@ def forward(self, hidden_states):\n \n \n # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->SpeechT5\n-class SpeechT5GroupNormConvLayer(nn.Module):\n+class SpeechT5GroupNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -487,13 +488,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n \n@@ -1032,7 +1027,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class SpeechT5EncoderLayer(nn.Module):\n+class SpeechT5EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SpeechT5Config):\n         super().__init__()\n         self.attention = SpeechT5Attention(\n@@ -1093,7 +1088,7 @@ def forward(\n         return outputs\n \n \n-class SpeechT5DecoderLayer(nn.Module):\n+class SpeechT5DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: SpeechT5Config):\n         super().__init__()\n         self.self_attn = SpeechT5Attention(\n@@ -1338,23 +1333,13 @@ def forward(\n \n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        position_bias,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        position_bias=position_bias,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    position_bias=position_bias,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:\n@@ -1636,33 +1621,17 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "1d65ec5b954cf7274e6cf30b1ec109b21db38e1a",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, ModelOutput, QuestionAnsweringModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n@@ -330,7 +331,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Splinter\n-class SplinterLayer(nn.Module):\n+class SplinterLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -456,27 +457,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "0dc1d00890ea8bd1ae0490d0be9cea410c6fe3d5",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -31,6 +31,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -519,7 +520,7 @@ def forward(\n }\n \n \n-class StableLmDecoderLayer(nn.Module):\n+class StableLmDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: StableLmConfig, layer_idx: int):\n         super().__init__()\n         self.use_parallel_residual = config.use_parallel_residual\n@@ -744,29 +745,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "c62c2e4fc9507e2cdeaed05b2cde1567a0579d52",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n@@ -739,7 +740,7 @@ def forward(\n         return layer_outputs\n \n \n-class SwinStage(nn.Module):\n+class SwinStage(GradientCheckpointingLayer):\n     def __init__(self, config, dim, input_resolution, depth, num_heads, drop_path, downsample):\n         super().__init__()\n         self.config = config\n@@ -848,19 +849,9 @@ def forward(\n         for i, layer_module in enumerate(self.layers):\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    input_dimensions,\n-                    layer_head_mask,\n-                    output_attentions,\n-                    always_partition,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states, input_dimensions, layer_head_mask, output_attentions, always_partition\n+            )\n \n             hidden_states = layer_outputs[0]\n             hidden_states_before_downsampling = layer_outputs[1]"
        },
        {
            "sha": "ae6e0a6e7952478df126d5c1bc4717d015d90edd",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ImageSuperResolutionOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n@@ -592,7 +593,7 @@ def forward(\n         return layer_outputs\n \n \n-class Swin2SRStage(nn.Module):\n+class Swin2SRStage(GradientCheckpointingLayer):\n     \"\"\"\n     This corresponds to the Residual Swin Transformer Block (RSTB) in the original implementation.\n     \"\"\"\n@@ -705,12 +706,7 @@ def forward(\n         for i, stage_module in enumerate(self.stages):\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    stage_module.__call__, hidden_states, input_dimensions, layer_head_mask, output_attentions\n-                )\n-            else:\n-                layer_outputs = stage_module(hidden_states, input_dimensions, layer_head_mask, output_attentions)\n+            layer_outputs = stage_module(hidden_states, input_dimensions, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n             output_dimensions = layer_outputs[1]"
        },
        {
            "sha": "67657f0111a6fb8f3aad0d2bec1e1f895174fdff",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch import Tensor, nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n@@ -787,7 +788,7 @@ def forward(\n         return layer_outputs\n \n \n-class Swinv2Stage(nn.Module):\n+class Swinv2Stage(GradientCheckpointingLayer):\n     def __init__(\n         self, config, dim, input_resolution, depth, num_heads, drop_path, downsample, pretrained_window_size=0\n     ):\n@@ -902,17 +903,12 @@ def forward(\n         for i, layer_module in enumerate(self.layers):\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__, hidden_states, input_dimensions, layer_head_mask\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    input_dimensions,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                input_dimensions,\n+                layer_head_mask,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             hidden_states_before_downsampling = layer_outputs[1]"
        },
        {
            "sha": "b0273c8a4a330ea2c0b023dea17f9e94e32ed738",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 18,
            "deletions": 36,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     MoEModelOutput,\n     MoEModelOutputWithPastAndCrossAttentions,\n@@ -661,7 +662,7 @@ def forward(\n         return outputs\n \n \n-class SwitchTransformersBlock(nn.Module):\n+class SwitchTransformersBlock(GradientCheckpointingLayer):\n     def __init__(self, config, has_relative_attention_bias=False, is_sparse=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n@@ -1024,41 +1025,22 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.forward,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_bias,\n-                    encoder_hidden_states,\n-                    encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias,\n-                    layer_head_mask,\n-                    cross_attn_layer_head_mask,\n-                    None,  # past_key_value is always None with gradient checkpointing\n-                    use_cache,\n-                    output_attentions,\n-                    output_router_logits,\n-                    return_dict,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_bias=position_bias,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n-                    layer_head_mask=layer_head_mask,\n-                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_values,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    return_dict=return_dict,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                causal_mask,\n+                position_bias,\n+                encoder_hidden_states,\n+                encoder_extended_attention_mask,\n+                encoder_decoder_position_bias,\n+                layer_head_mask=layer_head_mask,\n+                cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                return_dict=return_dict,\n+                cache_position=cache_position,\n+            )\n \n             router_probs = layer_outputs[-1]\n             layer_outputs = layer_outputs[:-1]"
        },
        {
            "sha": "2a1a84b81523019a87fcbaab1d212b7e5f3a9c5a",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 17,
            "deletions": 34,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -645,7 +646,7 @@ def forward(\n         return outputs\n \n \n-class T5Block(nn.Module):\n+class T5Block(GradientCheckpointingLayer):\n     def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n@@ -1101,39 +1102,21 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.forward,\n-                    hidden_states,\n-                    causal_mask,\n-                    position_bias,\n-                    encoder_hidden_states,\n-                    encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias,\n-                    layer_head_mask,\n-                    cross_attn_layer_head_mask,\n-                    None,  # past_key_value is always None with gradient checkpointing\n-                    use_cache,\n-                    output_attentions,\n-                    return_dict,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    position_bias=position_bias,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_extended_attention_mask,\n-                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n-                    layer_head_mask=layer_head_mask,\n-                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_values,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    return_dict=return_dict,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                causal_mask,\n+                position_bias,\n+                encoder_hidden_states,\n+                encoder_extended_attention_mask,\n+                encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n+                layer_head_mask=layer_head_mask,\n+                cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                return_dict=return_dict,\n+                cache_position=cache_position,\n+            )\n \n             # layer_outputs is a tuple with:\n             # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)"
        },
        {
            "sha": "d55a1be0c55a24ef1b0d9a6fcf118de926db8039",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 20,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithCrossAttentions, Seq2SeqModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -618,7 +619,7 @@ def forward(\n         return outputs\n \n \n-class TableTransformerDecoderLayer(nn.Module):\n+class TableTransformerDecoderLayer(GradientCheckpointingLayer):\n     # Copied from transformers.models.detr.modeling_detr.DetrDecoderLayer.__init__ with Detr->TableTransformer\n     def __init__(self, config: TableTransformerConfig):\n         super().__init__()\n@@ -989,25 +990,15 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    combined_attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    None,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=combined_attention_mask,\n-                    object_queries=object_queries,\n-                    query_position_embeddings=query_position_embeddings,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                combined_attention_mask,\n+                object_queries,\n+                query_position_embeddings,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "c2660b6895a091deb6673852b50a39f5211b41cf",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, MaskedLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n@@ -475,7 +476,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class TapasLayer(nn.Module):\n+class TapasLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -591,27 +592,15 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+            )\n             hidden_states = layer_outputs[0]\n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[1],)"
        },
        {
            "sha": "778a0485b4e894acdebd81abdd614e123a386107",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 21,
            "deletions": 46,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -435,7 +436,7 @@ def forward(\n \n \n # Copied from transformers.models.bart.modeling_bart.BartEncoderLayer with Bart->TimeSeriesTransformer, BART->TIME_SERIES_TRANSFORMER\n-class TimeSeriesTransformerEncoderLayer(nn.Module):\n+class TimeSeriesTransformerEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: TimeSeriesTransformerConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -507,7 +508,7 @@ def forward(\n \n \n # Copied from transformers.models.bart.modeling_bart.BartDecoderLayer with Bart->TimeSeriesTransformer, with BART->TIME_SERIES_TRANSFORMER\n-class TimeSeriesTransformerDecoderLayer(nn.Module):\n+class TimeSeriesTransformerDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: TimeSeriesTransformerConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -857,21 +858,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -1066,35 +1058,18 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "191a65f9b1301f0436c1e79c47b8c15393268ac2",
            "filename": "src/transformers/models/timesformer/modeling_timesformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -288,7 +289,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Adapted from https://github.com/facebookresearch/TimeSformer/blob/a5ef29a7b7264baff199a30b3306ac27de901133/timesformer/models/vit.py#L89\n-class TimesformerLayer(nn.Module):\n+class TimesformerLayer(GradientCheckpointingLayer):\n     def __init__(self, config: TimesformerConfig, layer_index: int) -> None:\n         super().__init__()\n \n@@ -432,14 +433,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, output_attentions)\n+            layer_outputs = layer_module(hidden_states, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "fdc0cae068a867cb3f9595e0501a1de880851f0f",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 13,
            "deletions": 28,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -28,6 +28,7 @@\n     _prepare_4d_attention_mask,\n     _prepare_4d_causal_attention_mask,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -288,7 +289,7 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-class TrOCRDecoderLayer(nn.Module):\n+class TrOCRDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: TrOCRConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -643,33 +644,17 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "cd6e88df846ad89c841ed1e05eb673f7e114d148",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import prune_linear_layer\n@@ -455,7 +456,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class TvpEncodeLayer(nn.Module):\n+class TvpEncodeLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = TvpAttention(config)\n@@ -511,16 +512,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    (head_mask[i] if head_mask is not None else None),\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i], output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i], output_attentions)\n \n             hidden_states = layer_outputs[0]\n             if output_attentions:"
        },
        {
            "sha": "c78ee0dc5b88bed38b550b185f1964651753eb90",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -38,6 +38,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n@@ -743,7 +744,7 @@ def forward(\n \n \n # Copied from transformers.models.t5.modeling_t5.T5Block with T5->Udop\n-class UdopBlock(nn.Module):\n+class UdopBlock(GradientCheckpointingLayer):\n     def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n@@ -1295,11 +1296,11 @@ def forward(\n \n             layer_outputs = layer_module(\n                 hidden_states,\n-                attention_mask=causal_mask,\n-                position_bias=position_bias,\n-                encoder_hidden_states=encoder_hidden_states,\n-                encoder_attention_mask=encoder_extended_attention_mask,\n-                encoder_decoder_position_bias=encoder_decoder_position_bias,\n+                causal_mask,\n+                position_bias,\n+                encoder_hidden_states,\n+                encoder_extended_attention_mask,\n+                encoder_decoder_position_bias,  # as a positional argument for gradient checkpointing\n                 layer_head_mask=head_mask[i],\n                 past_key_value=past_key_values,\n                 use_cache=use_cache,"
        },
        {
            "sha": "2b1f650c67890f181387f5f2b041e3e958d75f02",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 15,
            "deletions": 29,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -406,7 +407,7 @@ def forward(\n         return outputs\n \n \n-class UMT5Block(nn.Module):\n+class UMT5Block(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.is_decoder = config.is_decoder\n@@ -765,35 +766,20 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.forward,\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    encoder_extended_attention_mask,\n-                    layer_head_mask,\n-                    cross_attn_layer_head_mask,\n-                    None,  # past_key_value is always None with gradient checkpointing\n-                    use_cache,\n-                    output_attentions,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_extended_attention_mask,\n-                    layer_head_mask=layer_head_mask,\n-                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n-                    past_key_value=past_key_values,\n-                    use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                causal_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_extended_attention_mask,\n+                layer_head_mask=layer_head_mask,\n+                cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                output_attentions=output_attentions,\n+                cache_position=cache_position,\n+            )\n \n-                hidden_states = layer_outputs[0]\n+            hidden_states = layer_outputs[0]\n \n             if use_cache:\n                 next_decoder_cache = layer_outputs[1]"
        },
        {
            "sha": "af8b151ac6b24346b9e6dd57f9e39f1fe9839165",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 13,
            "deletions": 34,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -34,6 +34,7 @@\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -146,7 +147,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class UniSpeechNoLayerNormConvLayer(nn.Module):\n+class UniSpeechNoLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -167,7 +168,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class UniSpeechLayerNormConvLayer(nn.Module):\n+class UniSpeechLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -194,7 +195,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class UniSpeechGroupNormConvLayer(nn.Module):\n+class UniSpeechGroupNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -254,13 +255,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n \n@@ -456,7 +451,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class UniSpeechEncoderLayer(nn.Module):\n+class UniSpeechEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = UniSpeechAttention(\n@@ -540,17 +535,9 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:\n@@ -618,7 +605,7 @@ def forward(self, hidden_states: torch.FloatTensor):\n         return hidden_states\n \n \n-class UniSpeechEncoderLayerStableLayerNorm(nn.Module):\n+class UniSpeechEncoderLayerStableLayerNorm(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = UniSpeechAttention(\n@@ -714,17 +701,9 @@ def forward(\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "c6cc7561aa3acde0b42d64d9ddf80947abb930a0",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 13,
            "deletions": 34,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -34,6 +34,7 @@\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -149,7 +150,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class UniSpeechSatNoLayerNormConvLayer(nn.Module):\n+class UniSpeechSatNoLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -170,7 +171,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class UniSpeechSatLayerNormConvLayer(nn.Module):\n+class UniSpeechSatLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -197,7 +198,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class UniSpeechSatGroupNormConvLayer(nn.Module):\n+class UniSpeechSatGroupNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -257,13 +258,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n \n@@ -459,7 +454,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class UniSpeechSatEncoderLayer(nn.Module):\n+class UniSpeechSatEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = UniSpeechSatAttention(\n@@ -543,17 +538,9 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:\n@@ -621,7 +608,7 @@ def forward(self, hidden_states: torch.FloatTensor):\n         return hidden_states\n \n \n-class UniSpeechSatEncoderLayerStableLayerNorm(nn.Module):\n+class UniSpeechSatEncoderLayerStableLayerNorm(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = UniSpeechSatAttention(\n@@ -717,17 +704,9 @@ def forward(\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "a8278f0b892c2002f513dff8f41a9a8929a7a0af",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 4,
            "deletions": 19,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -389,7 +390,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->VideoMAE,VIT->VIDEOMAE\n-class VideoMAELayer(nn.Module):\n+class VideoMAELayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: VideoMAEConfig) -> None:\n@@ -456,15 +457,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n \n@@ -698,15 +691,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    None,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, head_mask=None, output_attentions=output_attentions)\n+            layer_outputs = layer_module(hidden_states, head_mask=None, output_attentions=output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "d42ac12605cd6f072ece96caeef44d9c9f4acba3",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -456,7 +457,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class ViltLayer(nn.Module):\n+class ViltLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config):\n@@ -519,16 +520,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "34adc5dd74d4545736e47a1ebfdd7b6bd4f77c1d",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss, KLDivLoss, LogSoftmax\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -330,7 +331,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class VisualBertLayer(nn.Module):\n+class VisualBertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -394,16 +395,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n             if output_attentions:"
        },
        {
            "sha": "dbad9ef41f3eb24099c75f290e0913e4c3fc4a8f",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -346,7 +347,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class ViTLayer(nn.Module):\n+class ViTLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: ViTConfig) -> None:\n@@ -412,15 +413,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "32b7151169aeb7d38e4aecf68c7a54526988438f",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 4,
            "deletions": 19,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -25,6 +25,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -531,7 +532,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->ViTMAE,VIT->VITMAE\n-class ViTMAELayer(nn.Module):\n+class ViTMAELayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: ViTMAEConfig) -> None:\n@@ -598,15 +599,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n \n@@ -864,15 +857,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    None,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, head_mask=None, output_attentions=output_attentions)\n+            layer_outputs = layer_module(hidden_states, head_mask=None, output_attentions=output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "11155d2d081c93443596dabd5299c0f6e0c5a71c",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -349,7 +350,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->ViTMSN, VIT->VITMSN\n-class ViTMSNLayer(nn.Module):\n+class ViTMSNLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: ViTMSNConfig) -> None:\n@@ -416,15 +417,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "b74bc1008f70b4a280b8c2b3e001a0f2b2a19771",
            "filename": "src/transformers/models/vitdet/modeling_vitdet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -439,7 +440,7 @@ def window_unpartition(windows, window_size, pad_height_width, height_width):\n     return hidden_state\n \n \n-class VitDetLayer(nn.Module):\n+class VitDetLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n \n     def __init__(\n@@ -560,15 +561,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "fb22d215996cc764bed1761d74d3521f0d7866e8",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -302,7 +303,7 @@ def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n         return hidden_state\n \n \n-class VitPoseBackboneLayer(nn.Module):\n+class VitPoseBackboneLayer(GradientCheckpointingLayer):\n     def __init__(self, config: VitPoseBackboneConfig) -> None:\n         super().__init__()\n         self.num_experts = config.num_experts\n@@ -377,16 +378,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    dataset_index,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, dataset_index, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, dataset_index, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "e202f98070c30a9d9bba02a0c019e4cdb3469a8a",
            "filename": "src/transformers/models/vits/modeling_vits.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -1067,7 +1068,7 @@ def forward(self, hidden_states, padding_mask):\n         return hidden_states\n \n \n-class VitsEncoderLayer(nn.Module):\n+class VitsEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: VitsConfig):\n         super().__init__()\n         self.attention = VitsAttention(config)\n@@ -1145,21 +1146,12 @@ def forward(\n             skip_the_layer = self.training and (dropout_probability < self.layerdrop)\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        padding_mask,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        padding_mask=padding_mask,\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    padding_mask=padding_mask,\n+                    output_attentions=output_attentions,\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "7011552db822ca552f8202ac83084028e3e72f7d",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -22,6 +22,7 @@\n from torch.nn import CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -342,7 +343,7 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n-class VivitLayer(nn.Module):\n+class VivitLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the EncoderBlock class in the scenic/vivit implementation.\"\"\"\n \n     def __init__(self, config):\n@@ -405,15 +406,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "153f067782ca305aa813543bc26bda3a184bed26",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 34,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -33,6 +33,7 @@\n     _prepare_4d_attention_mask_for_sdpa,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -271,7 +272,7 @@ def _sample_negative_indices(\n     return sampled_negative_indices\n \n \n-class Wav2Vec2NoLayerNormConvLayer(nn.Module):\n+class Wav2Vec2NoLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -292,7 +293,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class Wav2Vec2LayerNormConvLayer(nn.Module):\n+class Wav2Vec2LayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -319,7 +320,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class Wav2Vec2GroupNormConvLayer(nn.Module):\n+class Wav2Vec2GroupNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -434,13 +435,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n \n@@ -648,7 +643,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class Wav2Vec2EncoderLayer(nn.Module):\n+class Wav2Vec2EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = Wav2Vec2Attention(\n@@ -684,7 +679,7 @@ def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         return outputs\n \n \n-class Wav2Vec2EncoderLayerStableLayerNorm(nn.Module):\n+class Wav2Vec2EncoderLayerStableLayerNorm(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.attention = Wav2Vec2Attention(\n@@ -778,17 +773,9 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:\n@@ -882,17 +869,9 @@ def forward(\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "e41d7f32ffcb08d559b1c28f351b6629d90c15be",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -17,6 +17,7 @@\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -394,7 +395,7 @@ def _apply_relative_embeddings(self, query, key, relative_position_embeddings):\n         return scores\n \n \n-class Wav2Vec2BertEncoderLayer(nn.Module):\n+class Wav2Vec2BertEncoderLayer(GradientCheckpointingLayer):\n     \"\"\"Conformer block based on https://huggingface.co/papers/2005.08100.\"\"\"\n \n     def __init__(self, config):\n@@ -520,23 +521,13 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        relative_position_embeddings,\n-                        output_attentions,\n-                        conv_attention_mask,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        relative_position_embeddings=relative_position_embeddings,\n-                        output_attentions=output_attentions,\n-                        conv_attention_mask=conv_attention_mask,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    relative_position_embeddings=relative_position_embeddings,\n+                    output_attentions=output_attentions,\n+                    conv_attention_mask=conv_attention_mask,\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "d0f375332b33780f4abeb3309ce1c4e3cb8a0c74",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -9,6 +9,7 @@\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -292,7 +293,7 @@ def forward(\n         return hidden_states, probs\n \n \n-class Wav2Vec2BertEncoderLayer(nn.Module):\n+class Wav2Vec2BertEncoderLayer(GradientCheckpointingLayer):\n     \"\"\"Conformer block based on https://huggingface.co/papers/2005.08100.\"\"\"\n \n     def __init__(self, config):\n@@ -418,23 +419,13 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        relative_position_embeddings,\n-                        output_attentions,\n-                        conv_attention_mask,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        relative_position_embeddings=relative_position_embeddings,\n-                        output_attentions=output_attentions,\n-                        conv_attention_mask=conv_attention_mask,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    relative_position_embeddings=relative_position_embeddings,\n+                    output_attentions=output_attentions,\n+                    conv_attention_mask=conv_attention_mask,\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "d5987ca172b83f043fda3327fbe82713ae7b1835",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 12,
            "deletions": 26,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -17,6 +17,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -216,7 +217,7 @@ def forward(self, hidden_states: torch.Tensor):\n         return relative_position_embeddings\n \n \n-class Wav2Vec2ConformerNoLayerNormConvLayer(nn.Module):\n+class Wav2Vec2ConformerNoLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -237,7 +238,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class Wav2Vec2ConformerLayerNormConvLayer(nn.Module):\n+class Wav2Vec2ConformerLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -264,7 +265,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class Wav2Vec2ConformerGroupNormConvLayer(nn.Module):\n+class Wav2Vec2ConformerGroupNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -324,13 +325,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n \n@@ -582,7 +577,7 @@ def _apply_relative_embeddings(self, query, key, relative_position_embeddings):\n         return scores\n \n \n-class Wav2Vec2ConformerEncoderLayer(nn.Module):\n+class Wav2Vec2ConformerEncoderLayer(GradientCheckpointingLayer):\n     \"\"\"Conformer block based on https://huggingface.co/papers/2005.08100.\"\"\"\n \n     def __init__(self, config):\n@@ -709,21 +704,12 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        relative_position_embeddings,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        relative_position_embeddings=relative_position_embeddings,\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    relative_position_embeddings=relative_position_embeddings,\n+                    output_attentions=output_attentions,\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "3436563c0db8e1912de054d14a70d4abe2df236f",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -8,6 +8,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, Wav2Vec2BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n@@ -384,7 +385,7 @@ def _apply_relative_embeddings(self, query, key, relative_position_embeddings):\n         return scores\n \n \n-class Wav2Vec2ConformerEncoderLayer(nn.Module):\n+class Wav2Vec2ConformerEncoderLayer(GradientCheckpointingLayer):\n     \"\"\"Conformer block based on https://huggingface.co/papers/2005.08100.\"\"\"\n \n     def __init__(self, config):\n@@ -511,21 +512,12 @@ def forward(\n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        relative_position_embeddings,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        relative_position_embeddings=relative_position_embeddings,\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    relative_position_embeddings=relative_position_embeddings,\n+                    output_attentions=output_attentions,\n+                )\n                 hidden_states = layer_outputs[0]\n \n             if skip_the_layer:"
        },
        {
            "sha": "5904f05dcbfb71279083c18952da442593e7bf6c",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 20,
            "deletions": 43,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -17,6 +17,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -294,7 +295,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class WavLMEncoderLayer(nn.Module):\n+class WavLMEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: WavLMConfig, has_relative_position_bias: bool = True):\n         super().__init__()\n         self.attention = WavLMAttention(\n@@ -335,7 +336,7 @@ def forward(self, hidden_states, attention_mask=None, position_bias=None, output\n         return outputs\n \n \n-class WavLMEncoderLayerStableLayerNorm(nn.Module):\n+class WavLMEncoderLayerStableLayerNorm(GradientCheckpointingLayer):\n     def __init__(self, config: WavLMConfig, has_relative_position_bias: bool = True):\n         super().__init__()\n         self.attention = WavLMAttention(\n@@ -418,22 +419,13 @@ def forward(\n             skip_the_layer = self.training and i > 0 and (dropout_probability < self.config.layerdrop)\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        position_bias,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        position_bias=position_bias,\n-                        output_attentions=output_attentions,\n-                        index=i,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    position_bias=position_bias,\n+                    output_attentions=output_attentions,\n+                    index=i,\n+                )\n \n                 hidden_states, position_bias = layer_outputs[:2]\n \n@@ -504,21 +496,12 @@ def forward(\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        position_bias,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        output_attentions=output_attentions,\n-                        position_bias=position_bias,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    output_attentions=output_attentions,\n+                    position_bias=position_bias,\n+                )\n                 hidden_states, position_bias = layer_outputs[:2]\n \n             if skip_the_layer:\n@@ -696,7 +679,7 @@ def _get_feature_vector_attention_mask(\n         return attention_mask\n \n \n-class WavLMNoLayerNormConvLayer(nn.Module):\n+class WavLMNoLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -717,7 +700,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class WavLMLayerNormConvLayer(nn.Module):\n+class WavLMLayerNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -744,7 +727,7 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class WavLMGroupNormConvLayer(nn.Module):\n+class WavLMGroupNormConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n@@ -801,13 +784,7 @@ def forward(self, input_values):\n             hidden_states.requires_grad = True\n \n         for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n+            hidden_states = conv_layer(hidden_states)\n \n         return hidden_states\n "
        },
        {
            "sha": "aac25ff262bbac42a95f1d62b474bc21e8c01753",
            "filename": "src/transformers/models/wavlm/modular_wavlm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 33,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -7,6 +7,7 @@\n \n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, Wav2Vec2BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import logging\n@@ -205,7 +206,7 @@ class WavLMFeedForward(Wav2Vec2FeedForward):\n     pass\n \n \n-class WavLMEncoderLayer(nn.Module):\n+class WavLMEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: WavLMConfig, has_relative_position_bias: bool = True):\n         super().__init__()\n         self.attention = WavLMAttention(\n@@ -246,7 +247,7 @@ def forward(self, hidden_states, attention_mask=None, position_bias=None, output\n         return outputs\n \n \n-class WavLMEncoderLayerStableLayerNorm(nn.Module):\n+class WavLMEncoderLayerStableLayerNorm(GradientCheckpointingLayer):\n     def __init__(self, config: WavLMConfig, has_relative_position_bias: bool = True):\n         super().__init__()\n         self.attention = WavLMAttention(\n@@ -329,22 +330,13 @@ def forward(\n             skip_the_layer = self.training and i > 0 and (dropout_probability < self.config.layerdrop)\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        position_bias,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        position_bias=position_bias,\n-                        output_attentions=output_attentions,\n-                        index=i,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    position_bias=position_bias,\n+                    output_attentions=output_attentions,\n+                    index=i,\n+                )\n \n                 hidden_states, position_bias = layer_outputs[:2]\n \n@@ -415,21 +407,12 @@ def forward(\n             if not skip_the_layer or synced_gpus:\n                 # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        layer.__call__,\n-                        hidden_states,\n-                        attention_mask,\n-                        position_bias,\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = layer(\n-                        hidden_states,\n-                        attention_mask=attention_mask,\n-                        output_attentions=output_attentions,\n-                        position_bias=position_bias,\n-                    )\n+                layer_outputs = layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    output_attentions=output_attentions,\n+                    position_bias=position_bias,\n+                )\n                 hidden_states, position_bias = layer_outputs[:2]\n \n             if skip_the_layer:"
        },
        {
            "sha": "14cbaafe47dc6ec2fa08474621cbb1bd5073c20f",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 20,
            "deletions": 45,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -30,6 +30,7 @@\n from ...modeling_flash_attention_utils import (\n     FlashAttentionKwargs,\n )\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -366,7 +367,7 @@ def forward(\n \n \n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->Whisper, MBART->WHISPER\n-class WhisperEncoderLayer(nn.Module):\n+class WhisperEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: WhisperConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -434,7 +435,7 @@ def forward(\n         return outputs\n \n \n-class WhisperDecoderLayer(nn.Module):\n+class WhisperDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: WhisperConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -713,21 +714,12 @@ def forward(\n             if to_drop:\n                 layer_outputs = (None, None)\n             else:\n-                if self.gradient_checkpointing and self.training:\n-                    layer_outputs = self._gradient_checkpointing_func(\n-                        encoder_layer.__call__,\n-                        hidden_states,\n-                        None,\n-                        (head_mask[idx] if head_mask is not None else None),\n-                        output_attentions,\n-                    )\n-                else:\n-                    layer_outputs = encoder_layer(\n-                        hidden_states,\n-                        None,\n-                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                        output_attentions=output_attentions,\n-                    )\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    None,\n+                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                    output_attentions=output_attentions,\n+                )\n \n                 hidden_states = layer_outputs[0]\n \n@@ -964,34 +956,17 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    causal_mask,\n-                    encoder_hidden_states,\n-                    None,  # encoder attention mask\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,  # past_key_value\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=causal_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_values if use_cache else None,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_values if use_cache else None,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "90f49571946706c24c4d47fdaede504e14ac9896",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 15,
            "deletions": 32,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n@@ -338,7 +339,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoderLayer with AltCLIP->XCLIP\n-class XCLIPEncoderLayer(nn.Module):\n+class XCLIPEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: XCLIPConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -424,7 +425,7 @@ def extra_repr(self) -> str:\n         return f\"p={self.drop_prob}\"\n \n \n-class XCLIPVisionEncoderLayer(nn.Module):\n+class XCLIPVisionEncoderLayer(GradientCheckpointingLayer):\n     \"\"\"\n     This corresponds to the `CrossFramelAttentionBlock` class in the original implementation.\n     \"\"\"\n@@ -625,21 +626,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -842,21 +834,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "65e3e62840464962180d3405800fe2ee43d7a6ed",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 13,
            "deletions": 28,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -253,7 +254,7 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-class XGLMDecoderLayer(nn.Module):\n+class XGLMDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: XGLMConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -547,33 +548,17 @@ def forward(\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    head_mask[idx] if head_mask is not None else None,\n-                    cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None,\n-                    None,\n-                    output_attentions,\n-                    use_cache,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=attention_mask,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    cross_attn_layer_head_mask=(\n-                        cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n-                    ),\n-                    past_key_value=past_key_value,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n+                cross_attn_layer_head_mask=(cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None),\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+            )\n             hidden_states = layer_outputs[0]\n \n             if use_cache:"
        },
        {
            "sha": "f66396b135c54e09b8a70da1c216d6479a112b60",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN, gelu\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -478,7 +479,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaLayer with Roberta->XLMRoberta\n-class XLMRobertaLayer(nn.Module):\n+class XLMRobertaLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -604,27 +605,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "7cbeaadb184c3c7de916a4a7bc5ffb34c1746d1f",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -26,6 +26,7 @@\n from ...activations import ACT2FN, gelu\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -469,7 +470,7 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n-class XLMRobertaXLLayer(nn.Module):\n+class XLMRobertaXLLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -596,27 +597,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "84cf9f6d534940460cabfb25b2d8510ada5ad78f",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 12,
            "deletions": 24,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN, gelu\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -423,7 +424,7 @@ def lang_adapter(self, lang_ids: torch.Tensor, hidden_states: torch.Tensor):\n         return hidden_states\n \n \n-class XmodLayer(nn.Module):\n+class XmodLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -560,29 +561,16 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    lang_ids,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    lang_ids,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                lang_ids,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "c61ff8cb85af529cb2276026b5c68e249a63a324",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n@@ -403,7 +404,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->Yolos,VIT->YOLOS\n-class YolosLayer(nn.Module):\n+class YolosLayer(GradientCheckpointingLayer):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: YolosConfig) -> None:\n@@ -492,15 +493,7 @@ def forward(\n \n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    layer_head_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "da35490c59ed7c39bc5d4d0f7e29f6c331afd759",
            "filename": "src/transformers/models/yoso/modeling_yoso.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84d19be41e0131e6f2a660fe6af8b77094906af7/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py?ref=84d19be41e0131e6f2a660fe6af8b77094906af7",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n     MaskedLMOutput,\n@@ -507,7 +508,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class YosoLayer(nn.Module):\n+class YosoLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -559,15 +560,7 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n+            layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n \n             hidden_states = layer_outputs[0]\n             if output_attentions:"
        }
    ],
    "stats": {
        "total": 7795,
        "additions": 2514,
        "deletions": 5281
    }
}