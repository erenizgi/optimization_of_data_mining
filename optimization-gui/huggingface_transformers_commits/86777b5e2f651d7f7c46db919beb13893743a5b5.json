{
    "author": "jerryzh168",
    "message": "Support `AOPerModuleConfig` and `include_embedding` (#37802)\n\n* Support `AOPerModuleConfig` and include_embedding\n\nSummary:\nThis PR adds support per module configuration for torchao\nAlso added per module quantization examples:\n\n1. Quantizing different layers with different quantization configs\n2. Skip quantization for certain layers\n\nTest Plan:\npython tests/quantization/torchao_integration/test_torchao.py -k test_include_embedding\npython tests/quantization/torchao_integration/test_torchao.py -k test_per_module_config_skip\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\n* format\n\n* format\n\n* inlcude embedding remove input embedding from module not to convert\n\n* more docs\n\n* Update docs/source/en/quantization/torchao.md\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update src/transformers/quantizers/quantizer_torchao.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update src/transformers/quantizers/quantizer_torchao.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "86777b5e2f651d7f7c46db919beb13893743a5b5",
    "files": [
        {
            "sha": "bee2e008b954e88f8cf21d736c6a0a2efe15b39a",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 74,
            "deletions": 4,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/86777b5e2f651d7f7c46db919beb13893743a5b5/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/86777b5e2f651d7f7c46db919beb13893743a5b5/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=86777b5e2f651d7f7c46db919beb13893743a5b5",
            "patch": "@@ -40,6 +40,8 @@ torchao supports the [quantization techniques](https://github.com/pytorch/ao/blo\n - A16W4 Int4 Weight Only Quantization\n - Autoquantization\n \n+torchao also supports module level configuration by specifying a dictionary from fully qualified name of module and its corresponding quantization config. This allows skip quantizing certain layers and using different quantization config for different modules.\n+\n \n Check the table below to see if your hardware is compatible.\n \n@@ -89,7 +91,7 @@ We'll show examples for recommended quantization methods based on hardwares, e.g\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Float8DynamicActivationFloat8WeightConfig\n+from torchao.quantization import Float8DynamicActivationFloat8WeightConfig, Float8WeightOnlyConfig\n \n quant_config = Float8DynamicActivationFloat8WeightConfig()\n # or float8 weight only quantization\n@@ -149,7 +151,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Int8DynamicActivationInt8WeightConfig\n+from torchao.quantization import Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig\n \n quant_config = Int8DynamicActivationInt8WeightConfig()\n # or int8 weight only quantization\n@@ -179,7 +181,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import GemliteUIntXWeightOnlyConfig\n+from torchao.quantization import GemliteUIntXWeightOnlyConfig, Int4WeightOnlyConfig\n \n # For batch size N, we recommend gemlite, which may require autotuning\n # default is 4 bit, 8 bit is also supported by passing `bit_width=8`\n@@ -216,7 +218,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Int8DynamicActivationInt8WeightConfig\n+from torchao.quantization import Int8DynamicActivationInt8WeightConfig, Int8WeightOnlyConfig\n \n quant_config = Int8DynamicActivationInt8WeightConfig()\n # quant_config = Int8WeightOnlyConfig()\n@@ -272,6 +274,74 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n </hfoption>\n </hfoptions>\n \n+### Per Module Quantization\n+#### 1. Skip quantization for certain layers\n+With `AOPerModuleConfig` we can specify a default configuration for all layers while skipping quantization for certain layers.\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n+\n+model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n+\n+from torchao.quantization import Int4WeightOnlyConfig, AOPerModuleConfig\n+config = Int4WeightOnlyConfig(group_size=128)\n+\n+# set default to int4 (for linears), and skip quantizing `model.layers.0.self_attn.q_proj`\n+quant_config = AOPerModuleConfig({\"_default\": config, \"model.layers.0.self_attn.q_proj\": None})\n+quantization_config = TorchAoConfig(quant_type=quant_config)\n+quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n+# lm_head is not quantized and model.layers.0.self_attn.q_proj is not quantized\n+print(\"quantized model:\", quantized_model)\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+# Manual Testing\n+prompt = \"Hey, are you conscious? Can you talk to me?\"\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n+generated_ids = quantized_model.generate(**inputs, max_new_tokens=128)\n+output_text = tokenizer.batch_decode(\n+    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+print(output_text)\n+```\n+\n+#### 2. Quantizing different layers with different quantization configs\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n+\n+model_id = \"facebook/opt-125m\"\n+\n+from torchao.quantization import Int4WeightOnlyConfig, AOPerModuleConfig, Int8DynamicActivationInt4WeightConfig, IntxWeightOnlyConfig, PerAxis, MappingType\n+\n+weight_dtype = torch.int8\n+granularity = PerAxis(0)\n+mapping_type = MappingType.ASYMMETRIC\n+embedding_config = IntxWeightOnlyConfig(\n+    weight_dtype=weight_dtype,\n+    granularity=granularity,\n+    mapping_type=mapping_type,\n+)\n+linear_config = Int8DynamicActivationInt4WeightConfig(group_size=128)\n+quant_config = AOPerModuleConfig({\"_default\": linear_config, \"model.decoder.embed_tokens\": embedding_config, \"model.decoder.embed_positions\": None})\n+# set `include_embedding` to True in order to include embedding in quantization\n+# when `include_embedding` is True, we'll remove input embedding from `modules_not_to_convert` as well\n+quantization_config = TorchAoConfig(quant_type=quant_config, include_embedding=True)\n+quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cpu\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n+print(\"quantized model:\", quantized_model)\n+# make sure embedding is quantized\n+print(\"embed_tokens weight:\", quantized_model.model.decoder.embed_tokens.weight)\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+# Manual Testing\n+prompt = \"Hey, are you conscious? Can you talk to me?\"\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n+generated_ids = quantized_model.generate(**inputs, max_new_tokens=128, cache_implementation=\"static\")\n+output_text = tokenizer.batch_decode(\n+    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n+)\n+print(output_text)\n+```\n+\n ### Autoquant\n \n If you want to automatically choose a quantization type for quantizable layers (`nn.Linear`) you can use the [autoquant](https://pytorch.org/ao/stable/generated/torchao.quantization.autoquant.html#torchao.quantization.autoquant) API."
        },
        {
            "sha": "31d764c3022c1730286c301f3bc3a92d98122d63",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 26,
            "deletions": 2,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/86777b5e2f651d7f7c46db919beb13893743a5b5/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86777b5e2f651d7f7c46db919beb13893743a5b5/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=86777b5e2f651d7f7c46db919beb13893743a5b5",
            "patch": "@@ -185,6 +185,10 @@ def _process_model_before_weight_loading(\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n             model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n         )\n+        if self.quantization_config.include_embedding:\n+            input_emb = model.get_input_embeddings()\n+            input_emb_names = [name for name, module in model.named_modules() if id(module) == id(input_emb)]\n+            self.modules_to_not_convert = [x for x in self.modules_to_not_convert if x not in input_emb_names]\n         return\n \n     def check_quantized_param(\n@@ -206,9 +210,12 @@ def check_quantized_param(\n             # We don't quantize weights that we offload\n             return False\n         else:\n-            # we only quantize the weight of nn.Linear\n+            # we only quantize the weight of nn.Linear and nn.Embedding\n             module, tensor_name = get_module_from_name(model, param_name)\n-            return isinstance(module, torch.nn.Linear) and (tensor_name == \"weight\")\n+            _QUANTIZABLE = [torch.nn.Linear]\n+            if self.quantization_config.include_embedding:\n+                _QUANTIZABLE.append(torch.nn.Embedding)\n+            return isinstance(module, tuple(_QUANTIZABLE)) and (tensor_name == \"weight\")\n \n     def create_quantized_param(\n         self,\n@@ -240,6 +247,23 @@ def create_quantized_param(\n             module._parameters[tensor_name] = torch.nn.Parameter(\n                 param_value, requires_grad=param_value.requires_grad\n             ).to(device=target_device)\n+            # handle AOPerModuleConfig, introduced in torchao 0.11.0+\n+            if self.quantization_config._get_ao_version() > version.Version(\"0.10.0\"):\n+                from torchao.quantization import AOPerModuleConfig\n+\n+                config = self.quantization_config.get_apply_tensor_subclass()\n+                if isinstance(config, AOPerModuleConfig):\n+                    module_fqn, _ = param_name.rsplit(\".\", 1)\n+                    c = None\n+                    if module_fqn in config.module_fqn_to_config:\n+                        c = config.module_fqn_to_config[module_fqn]\n+                    else:\n+                        c = config.module_fqn_to_config.get(\"_default\", None)\n+                    if c is not None:\n+                        # filter_fn: not filtering out any modules\n+                        quantize_(module, c, filter_fn=lambda x, fqn: True)\n+                    return\n+\n             quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n \n     def _process_model_after_weight_loading(self, model, **kwargs):"
        },
        {
            "sha": "aa1f714c46cffd427a6194e37c02977f45422f1d",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/86777b5e2f651d7f7c46db919beb13893743a5b5/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86777b5e2f651d7f7c46db919beb13893743a5b5/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=86777b5e2f651d7f7c46db919beb13893743a5b5",
            "patch": "@@ -1554,6 +1554,7 @@ class TorchAoConfig(QuantizationConfigMixin):\n     quant_type: Union[str, \"AOBaseConfig\"]  # noqa: F821\n     modules_to_not_convert: Optional[List]\n     quant_type_kwargs: Dict[str, Any]\n+    include_embedding: bool\n \n     \"\"\"This is a config class for torchao quantization/sparsity techniques.\n \n@@ -1565,6 +1566,9 @@ class TorchAoConfig(QuantizationConfigMixin):\n         modules_to_not_convert (`list`, *optional*, default to `None`):\n             The list of modules to not quantize, useful for quantizing models that explicitly require to have\n             some modules left in their original precision.\n+        inlcude_embedding (`bool`, default to `False`):\n+            Whether to include embedding in quantization or not, input embedding will be removed from\n+            the module_not_to_convert list as well if this flag is set.\n         kwargs (`Dict[str, Any]`, *optional*):\n             The keyword arguments for the chosen type of quantization, for example, int4_weight_only quantization supports two keyword arguments\n             `group_size` and `inner_k_tiles` currently. More API examples and documentation of arguments can be found in\n@@ -1609,12 +1613,14 @@ def __init__(\n         self,\n         quant_type: Union[str, \"AOBaseConfig\"],  # noqa: F821\n         modules_to_not_convert: Optional[List] = None,\n+        include_embedding: bool = False,\n         **kwargs,\n     ):\n         self.quant_method = QuantizationMethod.TORCHAO\n         self.quant_type = quant_type\n         self.modules_to_not_convert = modules_to_not_convert\n         self.quant_type_kwargs = kwargs.get(\"quant_type_kwargs\", kwargs)\n+        self.include_embedding = include_embedding\n         self.post_init()\n \n     @staticmethod"
        },
        {
            "sha": "61d569a040bf1a7483598926b56a9fc83747f65c",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 61,
            "deletions": 0,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/86777b5e2f651d7f7c46db919beb13893743a5b5/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86777b5e2f651d7f7c46db919beb13893743a5b5/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=86777b5e2f651d7f7c46db919beb13893743a5b5",
            "patch": "@@ -38,6 +38,13 @@\n         AffineQuantizedTensor,\n         TensorCoreTiledLayout,\n     )\n+    from torchao.quantization import (\n+        AOPerModuleConfig,\n+        Int8WeightOnlyConfig,\n+        IntxWeightOnlyConfig,\n+        MappingType,\n+        PerAxis,\n+    )\n     from torchao.quantization.autoquant import AQMixin\n \n     if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\"):\n@@ -193,6 +200,60 @@ def test_int8_dynamic_activation_int8_weight_quant(self):\n         ]\n         self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n \n+    @require_torchao_version_greater_or_equal(\"0.11.0\")\n+    def test_include_embedding(self):\n+        weight_dtype = torch.int8\n+        granularity = PerAxis(0)\n+        mapping_type = MappingType.ASYMMETRIC\n+        embedding_config = IntxWeightOnlyConfig(\n+            weight_dtype=weight_dtype,\n+            granularity=granularity,\n+            mapping_type=mapping_type,\n+        )\n+        config = AOPerModuleConfig({\"_default\": None, \"model.embed_tokens\": embedding_config})\n+        # need set `include_embedding` to True\n+        quant_config = TorchAoConfig(quant_type=config, include_embedding=True)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=self.device,\n+            quantization_config=quant_config,\n+        )\n+        # making sure embedding is quantized\n+        self.assertTrue(isinstance(quantized_model.model.embed_tokens.weight, AffineQuantizedTensor))\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n+    @require_torchao_version_greater_or_equal(\"0.11.0\")\n+    def test_per_module_config_skip(self):\n+        linear_config = Int8WeightOnlyConfig()\n+        config = AOPerModuleConfig({\"_default\": linear_config, \"model.layers.0.self_attn.q_proj\": None})\n+        quant_config = TorchAoConfig(quant_type=config)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=self.device,\n+            quantization_config=quant_config,\n+        )\n+        # making sure `model.layers.0.self_attn.q_proj` is skipped\n+        self.assertTrue(not isinstance(quantized_model.model.layers[0].self_attn.q_proj.weight, AffineQuantizedTensor))\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n \n @require_torch_gpu\n class TorchAoGPUTest(TorchAoTest):"
        }
    ],
    "stats": {
        "total": 173,
        "additions": 167,
        "deletions": 6
    }
}