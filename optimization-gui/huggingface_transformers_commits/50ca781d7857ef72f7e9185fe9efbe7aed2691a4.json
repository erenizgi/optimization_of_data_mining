{
    "author": "Judy-Choi",
    "message": "ğŸŒ [i18n-KO] Translated `code_llama.md` to Korean (#40558)\n\n* docs: ko: code_llama.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* Apply suggestions from code review\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\nCo-authored-by: HyunZ118 <156191095+HyunZ118@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\nCo-authored-by: HyunZ118 <156191095+HyunZ118@users.noreply.github.com>",
    "sha": "50ca781d7857ef72f7e9185fe9efbe7aed2691a4",
    "files": [
        {
            "sha": "b375aaa61e900bdeb5021933a2429f3fbd3ee05b",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/50ca781d7857ef72f7e9185fe9efbe7aed2691a4/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/50ca781d7857ef72f7e9185fe9efbe7aed2691a4/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=50ca781d7857ef72f7e9185fe9efbe7aed2691a4",
            "patch": "@@ -485,7 +485,7 @@\n         title: CANINE\n       - local: model_doc/codegen\n         title: CodeGen\n-      - local: in_translation\n+      - local: model_doc/code_llama\n         title: CodeLlama\n       - local: model_doc/cohere\n         title: Cohere"
        },
        {
            "sha": "079ff7df877b046d87d348dd4968a1c60823ba3e",
            "filename": "docs/source/ko/model_doc/code_llama.md",
            "status": "added",
            "additions": 180,
            "deletions": 0,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/50ca781d7857ef72f7e9185fe9efbe7aed2691a4/docs%2Fsource%2Fko%2Fmodel_doc%2Fcode_llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/50ca781d7857ef72f7e9185fe9efbe7aed2691a4/docs%2Fsource%2Fko%2Fmodel_doc%2Fcode_llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fcode_llama.md?ref=50ca781d7857ef72f7e9185fe9efbe7aed2691a4",
            "patch": "@@ -0,0 +1,180 @@\n+<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*ì´ ëª¨ë¸ì€ 2023ë…„ 8ì›” 24ì¼ì— ê³µê°œë˜ì—ˆìœ¼ë©°, 2023ë…„ 8ì›” 25ì¼ì— Hugging Face Transformersì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.*\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        \">\n+    </div>\n+</div>\n+\n+# CodeLlama[[codellama]]\n+\n+[Code Llama](https://huggingface.co/papers/2308.12950)ëŠ” ì½”ë”© ì‘ì—…ì— íŠ¹í™”ëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ê³„ì—´ë¡œ,  [Llama 2](./llama2)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì½”ë“œ, Python íŠ¹í™”, ëª…ë ¹ì–´(ì§€ì‹œ) ê¸°ë°˜ ë³€í˜• ë“± ë‹¤ì–‘í•œ ë²„ì „ìœ¼ë¡œ ì œê³µë˜ë©°, ëª¨ë‘ 7B, 13B, 34B, 70B ë§¤ê°œë³€ìˆ˜ í¬ê¸°ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Code Llama ëª¨ë¸ì€ ì½”ë“œë¥¼ ìƒì„±í•˜ê³  ì„¤ëª…í•˜ë©°, ì½”ë“œì˜ ëˆ„ë½ëœ ë¶€ë¶„ì„ ì±„ìš¸ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ì¸í•„ë§(infilling)ì´ë¼ê³  í•©ë‹ˆë‹¤. 16K í† í° ê¸¸ì´ë¡œ í›ˆë ¨ë˜ì—ˆì§€ë§Œ, ìµœëŒ€ 100K í† í°ê¹Œì§€ ì•ˆì •ì ìœ¼ë¡œ ìƒì„±í•˜ë©° ê¸´ ì»¨í…ìŠ¤íŠ¸ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+[Code Llama](https://huggingface.co/collections/meta-llama/code-llama-family-661da32d0a9d678b6f55b933) ì»¬ë ‰ì…˜ì—ì„œ ëª¨ë“  ì›ë³¸ Code Llama ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+> [!TIP]\n+> ë‹¤ì–‘í•œ ì½”ë”© ì‘ì—…ì— Code Llamaë¥¼ ì ìš©í•˜ëŠ” ë” ë§ì€ ì˜ˆì‹œë¥¼ ë³´ë ¤ë©´ ì˜¤ë¥¸ìª½ ì‚¬ì´ë“œë°”ì˜ Code Llama ëª¨ë¸ì„ í´ë¦­í•˜ì„¸ìš”.\n+\n+ì•„ë˜ ì˜ˆì‹œëŠ” [`Pipeline`], [`AutoModel`], ê·¸ë¦¬ê³  ëª…ë ¹ì¤„ì—ì„œ ì½”ë“œë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    \"text-generation\",\n+    model=\"meta-llama/CodeLlama-7b-hf\",\n+    torch_dtype=torch.float16,\n+    device_map=0\n+)\n+\n+# ê¸°ë³¸ ì½”ë“œ ìƒì„±\n+result = pipe(\"# Function to calculate the factorial of a number\\ndef factorial(n):\", max_new_tokens=256)\n+print(result[0]['generated_text'])\n+\n+# ì¸í•„ë§\n+infill_result = pipe(\"def remove_non_ascii(s: str) -> str:\\n    \\\"\\\"\\\" <FILL_ME>\\n    return result\", max_new_tokens=200)\n+print(infill_result[0]['generated_text'])\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/CodeLlama-7b-hf\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+\n+# ê¸°ë³¸ ì½”ë“œ ìƒì„±\n+prompt = \"# Function to calculate the factorial of a number\\ndef factorial(n):\"\n+input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n+\n+output = model.generate(\n+    **input_ids,\n+    max_new_tokens=256,\n+    cache_implementation=\"static\"\n+)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+\n+# ì¸í•„ë§\n+infill_prompt = \"def remove_non_ascii(s: str) -> str:\\n    \\\"\\\"\\\" <FILL_ME>\\n    return result\"\n+input_ids = tokenizer(infill_prompt, return_tensors=\"pt\").to(model.device)\n+\n+filled_output = model.generate(**input_ids, max_new_tokens=200)\n+filled_text = tokenizer.decode(filled_output[0], skip_special_tokens=True)\n+print(filled_text)\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"# Function to calculate the factorial of a number\\ndef factorial(n):\" | transformers run --task text-generation --model meta-llama/CodeLlama-7b-hf --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+ì–‘ìí™”ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë” ë‚®ì€ ì •ë°€ë„ë¡œ í‘œí˜„í•˜ì—¬ ëŒ€ê·œëª¨ ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì…ë‹ˆë‹¤. ë” ë§ì€ ì‚¬ìš© ê°€ëŠ¥í•œ ì–‘ìí™” ë°±ì—”ë“œëŠ” [ì–‘ìí™”](../quantization/overview) ê°œìš”ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+ì•„ë˜ ì˜ˆì‹œëŠ” [bitsandbytes](../quantization/bitsandbytes)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ 4ë¹„íŠ¸ë¡œë§Œ ì–‘ìí™”í•©ë‹ˆë‹¤.\n+\n+```py\n+# bitsandbytesë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤.\n+import torch\n+from transformers import AutoModelForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig\n+\n+bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True)\n+tokenizer = CodeLlamaTokenizer.from_pretrained(\"meta-llama/CodeLlama-34b-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\n+   \"meta-llama/CodeLlama-34b-hf\",\n+   torch_dtype=torch.bfloat16,\n+   device_map=\"auto\",\n+   quantization_config=bnb_config\n+)\n+\n+prompt = \"# Write a Python function to check if a string is a palindrome\\ndef is_palindrome(s):\"\n+input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n+\n+output = model.generate(**input_ids, max_new_tokens=200, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+[AttentionMaskVisualizer](https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src/transformers/utils/attention_visualizer.py#L139)ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì´ ì–´ë–¤ í† í°ì— ì£¼ì˜ë¥¼ ê¸°ìš¸ì¼ ìˆ˜ ìˆê³  ê¸°ìš¸ì¼ ìˆ˜ ì—†ëŠ”ì§€ë¥¼ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n+\n+visualizer = AttentionMaskVisualizer(\"meta-llama/CodeLlama-7b-hf\")\n+visualizer(\"\"\"def func(a, b):\n+  return a + b\"\"\")\n+```\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/codellama-attn-mask.png\"/>\n+</div>\n+\n+## ì°¸ê³ ì‚¬í•­[[notes]]\n+\n+- ì¸í•„ë§ ê¸°ëŠ¥ì€ 7B ë° 13B ê¸°ë°˜ ëª¨ë¸ì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, Python, Instruct, 34B ë˜ëŠ” 70B ëª¨ë¸ì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n+- ì½”ë“œë¥¼ ì±„ì›Œ ë„£ê³  ì‹¶ì€ ë¶€ë¶„ì— `<FILL_ME>` í† í°ì„ ì‚¬ìš©í•˜ì„¸ìš”. í† í¬ë‚˜ì´ì €ëŠ” ì´ í† í°ì„ ë¶„í• í•˜ì—¬ [ì›ë³¸ í›ˆë ¨ íŒ¨í„´](https://github.com/facebookresearch/codellama/blob/cb51c14ec761370ba2e2bc351374a79265d0465e/llama/generation.py#L402) ì„ ë”°ë¥´ëŠ” ì…ë ¥ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŠ” ì§ì ‘ íŒ¨í„´ì„ ì¤€ë¹„í•˜ëŠ” ê²ƒë³´ë‹¤ ë” ì•ˆì •ì ì…ë‹ˆë‹¤.\n+    ```py\n+    from transformers import LlamaForCausalLM, CodeLlamaTokenizer\n+\n+    tokenizer = CodeLlamaTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-hf\")\n+    model = LlamaForCausalLM.from_pretrained(\"meta-llama/CodeLlama-7b-hf\")\n+    PROMPT = '''def remove_non_ascii(s: str) -> str:\n+        \"\"\" <FILL_ME>\n+        return result\n+    '''\n+    input_ids = tokenizer(PROMPT, return_tensors=\"pt\")[\"input_ids\"]\n+    generated_ids = model.generate(input_ids, max_new_tokens=128)\n+\n+    filling = tokenizer.batch_decode(generated_ids[:, input_ids.shape[1]:], skip_special_tokens = True)[0]\n+    print(PROMPT.replace(\"<FILL_ME>\", filling))\n+    ```\n+- ì¶”ê°€ í›ˆë ¨ì´ë‚˜ ë¯¸ì„¸ ì¡°ì •ì—ëŠ” `bfloat16`ì„ ì‚¬ìš©í•˜ê³  ì¶”ë¡ ì—ëŠ” `float16`ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n+- `BOS` ë¬¸ìëŠ” ì ‘ë‘ì‚¬ë‚˜ ì ‘ë¯¸ì‚¬ë¥¼ ì¸ì½”ë”©í•  ë•Œ ì¸í•„ë§ ì‘ì—…ì— ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë©°, ê° í”„ë¡¬í”„íŠ¸ì˜ ë§¨ ì•ì—ì„œë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.\n+- í† í¬ë‚˜ì´ì €ëŠ” [SentencePiece](https://github.com/google/sentencepiece)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” byte-pair ì¸ì½”ë”© ëª¨ë¸ì…ë‹ˆë‹¤. ë””ì½”ë”© ê³¼ì •ì—ì„œ ì²« ë²ˆì§¸ í† í°ì´ ë‹¨ì–´ì˜ ì‹œì‘ì¸ ê²½ìš°(ì˜ˆë¥¼ ë“¤ì–´ \"Banana\"), í† í¬ë‚˜ì´ì €ëŠ” ë¬¸ìì—´ì— ì ‘ë‘ì‚¬ ê³µë°±ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n+\n+## CodeLlamaTokenizer\n+\n+[[autodoc]] CodeLlamaTokenizer\n+    - build_inputs_with_special_tokens\n+    - get_special_tokens_mask\n+    - create_token_type_ids_from_sequences\n+    - save_vocabulary\n+\n+## CodeLlamaTokenizerFast\n+\n+[[autodoc]] CodeLlamaTokenizerFast\n+    - build_inputs_with_special_tokens\n+    - get_special_tokens_mask\n+    - create_token_type_ids_from_sequences\n+    - update_post_processor\n+    - save_vocabulary"
        }
    ],
    "stats": {
        "total": 182,
        "additions": 181,
        "deletions": 1
    }
}