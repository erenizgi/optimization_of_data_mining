{
    "author": "thisisiron",
    "message": "Remove unused test_dataset (#34516)",
    "sha": "45b0c7680c0a2d79bda345e04d8461f27602010d",
    "files": [
        {
            "sha": "8353333ef827ede71087802b55ac5ae0f1ba8d3e",
            "filename": "examples/pytorch/contrastive-image-text/run_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/45b0c7680c0a2d79bda345e04d8461f27602010d/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/45b0c7680c0a2d79bda345e04d8461f27602010d/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontrastive-image-text%2Frun_clip.py?ref=45b0c7680c0a2d79bda345e04d8461f27602010d",
            "patch": "@@ -141,10 +141,6 @@ class DataTrainingArguments:\n         default=None,\n         metadata={\"help\": \"An optional input evaluation data file (a jsonlines file).\"},\n     )\n-    test_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input testing data file (a jsonlines file).\"},\n-    )\n     max_seq_length: Optional[int] = field(\n         default=128,\n         metadata={\n@@ -190,9 +186,6 @@ def __post_init__(self):\n             if self.validation_file is not None:\n                 extension = self.validation_file.split(\".\")[-1]\n                 assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n-            if self.test_file is not None:\n-                extension = self.test_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n \n \n dataset_name_mapping = {\n@@ -315,9 +308,6 @@ def main():\n         if data_args.validation_file is not None:\n             data_files[\"validation\"] = data_args.validation_file\n             extension = data_args.validation_file.split(\".\")[-1]\n-        if data_args.test_file is not None:\n-            data_files[\"test\"] = data_args.test_file\n-            extension = data_args.test_file.split(\".\")[-1]\n         dataset = load_dataset(\n             extension,\n             data_files=data_files,\n@@ -387,8 +377,6 @@ def _freeze_params(module):\n         column_names = dataset[\"train\"].column_names\n     elif training_args.do_eval:\n         column_names = dataset[\"validation\"].column_names\n-    elif training_args.do_predict:\n-        column_names = dataset[\"test\"].column_names\n     else:\n         logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n         return\n@@ -490,29 +478,6 @@ def filter_corrupt_images(examples):\n         # Transform images on the fly as doing it on the whole dataset takes too much time.\n         eval_dataset.set_transform(transform_images)\n \n-    if training_args.do_predict:\n-        if \"test\" not in dataset:\n-            raise ValueError(\"--do_predict requires a test dataset\")\n-        test_dataset = dataset[\"test\"]\n-        if data_args.max_eval_samples is not None:\n-            max_eval_samples = min(len(test_dataset), data_args.max_eval_samples)\n-            test_dataset = test_dataset.select(range(max_eval_samples))\n-\n-        test_dataset = test_dataset.filter(\n-            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n-        )\n-        test_dataset = test_dataset.map(\n-            function=tokenize_captions,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=[col for col in column_names if col != image_column],\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on test dataset\",\n-        )\n-\n-        # Transform images on the fly as doing it on the whole dataset takes too much time.\n-        test_dataset.set_transform(transform_images)\n-\n     # 8. Initialize our trainer\n     trainer = Trainer(\n         model=model,"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 0,
        "deletions": 35
    }
}