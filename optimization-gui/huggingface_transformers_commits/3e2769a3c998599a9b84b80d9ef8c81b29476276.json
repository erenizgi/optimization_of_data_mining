{
    "author": "hgt312",
    "message": "Fix DBRX LayerNorm init method (#35177)\n\nfix dbrx layernorm init",
    "sha": "3e2769a3c998599a9b84b80d9ef8c81b29476276",
    "files": [
        {
            "sha": "7d20b766658f23d3b82c4b56e88bda06a117a128",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e2769a3c998599a9b84b80d9ef8c81b29476276/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e2769a3c998599a9b84b80d9ef8c81b29476276/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=3e2769a3c998599a9b84b80d9ef8c81b29476276",
            "patch": "@@ -845,7 +845,7 @@ def _init_weights(self, module: nn.Module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.normal_(mean=0.0, std=std)\n+            module.weight.data.fill_(1.0)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, DbrxExpertGLU):"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}