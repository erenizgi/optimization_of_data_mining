{
    "author": "muellerzr",
    "message": "Fix OS err (#36094)\n\n* Try via local_main_process first\r\n\r\n* try 2",
    "sha": "ba29a439adbe6f371710d0514659127264ae24b3",
    "files": [
        {
            "sha": "5da300a90966fc8d49a817a316d18e69af90d93a",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba29a439adbe6f371710d0514659127264ae24b3/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba29a439adbe6f371710d0514659127264ae24b3/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=ba29a439adbe6f371710d0514659127264ae24b3",
            "patch": "@@ -3153,14 +3153,15 @@ def _save_checkpoint(self, model, trial):\n                         self.state.stateful_callbacks[cb_name] = cb_state\n                 self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))\n \n-            try:\n-                os.renames(output_dir, checkpoint_dir)\n-            except OSError as e:\n-                if e.errno in [errno.ENOTEMPTY, errno.EEXIST]:  # Directory/File already exists\n-                    shutil.rmtree(checkpoint_dir)\n-                    os.renames(output_dir, checkpoint_dir)\n-                else:\n-                    raise\n+                if os.path.exists(output_dir):\n+                    try:\n+                        os.renames(output_dir, checkpoint_dir)\n+                    except OSError as e:\n+                        if e.errno in [errno.ENOTEMPTY, errno.EEXIST]:  # Directory/File already exists\n+                            shutil.rmtree(checkpoint_dir)\n+                            os.renames(output_dir, checkpoint_dir)\n+                        else:\n+                            raise\n \n         if self.args.push_to_hub:\n             self._push_from_checkpoint(checkpoint_dir)"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 9,
        "deletions": 8
    }
}