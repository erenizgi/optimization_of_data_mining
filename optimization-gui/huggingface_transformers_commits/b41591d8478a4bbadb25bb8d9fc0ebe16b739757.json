{
    "author": "MekkCyber",
    "message": "Fix : fix doc fp8 (#36173)\n\n* fix\r\n\r\n* fix",
    "sha": "b41591d8478a4bbadb25bb8d9fc0ebe16b739757",
    "files": [
        {
            "sha": "785e5e88e1283ed01e97e9633f39c907ed409833",
            "filename": "docs/source/en/quantization/finegrained_fp8.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b41591d8478a4bbadb25bb8d9fc0ebe16b739757/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b41591d8478a4bbadb25bb8d9fc0ebe16b739757/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffinegrained_fp8.md?ref=b41591d8478a4bbadb25bb8d9fc0ebe16b739757",
            "patch": "@@ -39,10 +39,10 @@ pip install --upgrade accelerate torch\n By default, the weights are loaded in full precision (torch.float32) regardless of the actual data type the weights are stored in such as torch.float16. Set `torch_dtype=\"auto\"` to load the weights in the data type defined in a model's `config.json` file to automatically load the most memory-optimal data type.\n \n ```py\n-from transformers import FP8Config, AutoModelForCausalLM, AutoTokenizer\n+from transformers import FineGrainedFP8Config, AutoModelForCausalLM, AutoTokenizer\n \n model_name = \"meta-llama/Meta-Llama-3-8B\"\n-quantization_config = FP8Config()\n+quantization_config = FineGrainedFP8Config()\n quantized_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", quantization_config=quantization_config)\n \n tokenizer = AutoTokenizer.from_pretrained(model_name)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}