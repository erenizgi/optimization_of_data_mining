{
    "author": "JustinTong0323",
    "message": "[Fix] Add `original_max_position_embeddings` to YARN rope_scaling optional keys (#36877)\n\n[fix] Update optional keys in _validate_yarn_parameters to include original_max_position_embeddings",
    "sha": "e28be7a6923bc471e3b1cd5f3e664d8f4ea37d9a",
    "files": [
        {
            "sha": "2999e3423f2eb72ae25a231d64885bcb161811e0",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e28be7a6923bc471e3b1cd5f3e664d8f4ea37d9a/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e28be7a6923bc471e3b1cd5f3e664d8f4ea37d9a/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=e28be7a6923bc471e3b1cd5f3e664d8f4ea37d9a",
            "patch": "@@ -425,7 +425,7 @@ def _validate_yarn_parameters(config: PretrainedConfig, ignore_keys: Optional[se\n     rope_scaling = config.rope_scaling\n     rope_type = rope_scaling.get(\"rope_type\", rope_scaling.get(\"type\", None))  # BC: \"rope_type\" was originally \"type\"\n     required_keys = {\"rope_type\", \"factor\"}\n-    optional_keys = {\"attention_factor\", \"beta_fast\", \"beta_slow\"}\n+    optional_keys = {\"attention_factor\", \"beta_fast\", \"beta_slow\", \"original_max_position_embeddings\"}\n     received_keys = set(rope_scaling.keys())\n     _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}