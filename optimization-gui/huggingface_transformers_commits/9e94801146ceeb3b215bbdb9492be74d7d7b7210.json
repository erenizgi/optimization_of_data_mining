{
    "author": "SunMarc",
    "message": "enable/disable compile for quants methods (#36519)\n\n* disable compile for most quants methods\n\n* fix\n\n* Update src/transformers/generation/configuration_utils.py\n\nCo-authored-by: Matthew Douglas <38992547+matthewdouglas@users.noreply.github.com>\n\n* Update tests/quantization/bnb/test_mixed_int8.py\n\nCo-authored-by: Matthew Douglas <38992547+matthewdouglas@users.noreply.github.com>\n\n* Update src/transformers/generation/configuration_utils.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* changes from joao suggestions\n\n---------\n\nCo-authored-by: Matthew Douglas <38992547+matthewdouglas@users.noreply.github.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "9e94801146ceeb3b215bbdb9492be74d7d7b7210",
    "files": [
        {
            "sha": "a6b0a72162fc3b4b7ab02c1d8a25d84eb984b9ba",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=9e94801146ceeb3b215bbdb9492be74d7d7b7210",
            "patch": "@@ -379,8 +379,7 @@ class GenerationConfig(PushToHubMixin):\n             If using a static cache, this controls how `generate` will `compile` the forward pass for performance\n             gains.\n \n-        disable_compile (`bool`, *optional*): Whether to disable the compilation of the forward pass when using 'statis' cache\n-            implementation.\n+        disable_compile (`bool`, *optional*): Whether to disable the automatic compilation of the forward pass. Automatic compilation happens when specific criteria are met, including using a compileable cache. Please open an issue if you find the need to use this flag.\n \n         > Wild card\n "
        },
        {
            "sha": "3761b59da907193c0d4a0dd427be42c407e964c0",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=9e94801146ceeb3b215bbdb9492be74d7d7b7210",
            "patch": "@@ -1613,7 +1613,6 @@ def _prepare_generation_config(\n             model_kwargs = generation_config.update(**kwargs)\n         else:\n             model_kwargs = kwargs\n-\n         return generation_config, model_kwargs\n \n     def _get_initial_cache_position(self, input_ids, model_kwargs):\n@@ -3281,7 +3280,9 @@ def _sample(\n         model_forward = self.__call__\n         if isinstance(model_kwargs.get(\"past_key_values\"), Cache):\n             is_compileable = model_kwargs[\"past_key_values\"].is_compileable and self._supports_static_cache\n-            is_compileable = is_compileable and not self.generation_config.disable_compile\n+            if getattr(self, \"hf_quantizer\", None) is not None:\n+                is_compileable &= self.hf_quantizer.is_compileable\n+            is_compileable = is_compileable and not generation_config.disable_compile\n             if is_compileable and (\n                 self.device.type == \"cuda\" or generation_config.compile_config._compile_all_devices\n             ):"
        },
        {
            "sha": "46c44b79f26c702569be1fee12292eff43f57a9b",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=9e94801146ceeb3b215bbdb9492be74d7d7b7210",
            "patch": "@@ -271,6 +271,11 @@ def is_qat_trainable(self) -> bool:\n         \"\"\"Flag indicating whether the quantized model can carry out quantization aware training\"\"\"\n         return False\n \n+    @property\n+    def is_compileable(self) -> bool:\n+        \"\"\"Flag indicating whether the quantized model can be compiled\"\"\"\n+        return False\n+\n     @abstractmethod\n     def _process_model_before_weight_loading(self, model, **kwargs): ...\n "
        },
        {
            "sha": "0eb4eec997953763208b9abb5c78661e3fb5064f",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e94801146ceeb3b215bbdb9492be74d7d7b7210/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=9e94801146ceeb3b215bbdb9492be74d7d7b7210",
            "patch": "@@ -243,3 +243,7 @@ def is_trainable(self):\n             \"int8_dynamic_activation_int8_weight\",\n         ]\n         return self.quantization_config.quant_type in supported_quant_types_for_training\n+\n+    @property\n+    def is_compileable(self) -> bool:\n+        return True"
        },
        {
            "sha": "646894653854653d3ebb9cc1b51721ddc08a3d90",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e94801146ceeb3b215bbdb9492be74d7d7b7210/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=9e94801146ceeb3b215bbdb9492be74d7d7b7210",
            "patch": "@@ -771,3 +771,36 @@ def test_set_load_in_8_bit(self):\n         quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n         with self.assertRaisesRegex(ValueError, \"load_in_4bit and load_in_8bit are both True\"):\n             quantization_config.load_in_8bit = True\n+\n+\n+@require_bitsandbytes\n+@require_accelerate\n+@require_torch_gpu_if_bnb_not_multi_backend_enabled\n+@slow\n+@apply_skip_if_not_implemented\n+class Bnb4bitCompile(unittest.TestCase):\n+    model_name = \"hf-internal-testing/tiny-random-LlamaForCausalLM\"\n+    input_text = \"Hello my name is\"\n+\n+    def setUp(self):\n+        # Models and tokenizer\n+        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+        self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)\n+\n+    def test_generate_compile(self):\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n+\n+        # if nothing is set, compile will be disabled for bnb\n+        self.model_4bit.generate(\n+            input_ids=encoded_input[\"input_ids\"].to(self.model_4bit.device),\n+            max_new_tokens=10,\n+            cache_implementation=\"static\",\n+        )\n+        with self.assertRaises(Exception):\n+            # overwrite property\n+            object.__setattr__(self.model_4bit.hf_quantizer, \"is_compileable\", True)\n+            self.model_4bit.generate(\n+                input_ids=encoded_input[\"input_ids\"].to(self.model_4bit.device),\n+                max_new_tokens=10,\n+                cache_implementation=\"static\",\n+            )"
        },
        {
            "sha": "f400fc2427730f912f8a7ebbda49dca6909230c9",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/9e94801146ceeb3b215bbdb9492be74d7d7b7210/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9e94801146ceeb3b215bbdb9492be74d7d7b7210/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=9e94801146ceeb3b215bbdb9492be74d7d7b7210",
            "patch": "@@ -966,3 +966,37 @@ def test_int8_from_pretrained(self):\n         output_sequences = model.generate(input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10)\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n+\n+\n+@require_bitsandbytes\n+@require_accelerate\n+@require_torch\n+@require_torch_gpu_if_bnb_not_multi_backend_enabled\n+@slow\n+@apply_skip_if_not_implemented\n+class Bnb8bitCompile(unittest.TestCase):\n+    model_name = \"hf-internal-testing/tiny-random-LlamaForCausalLM\"\n+    input_text = \"Hello my name is\"\n+\n+    def setUp(self):\n+        # Models and tokenizer\n+        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+        self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)\n+\n+    def test_generate_compile(self):\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n+\n+        # if nothing is set, compile will be disabled for bnb\n+        self.model_8bit.generate(\n+            input_ids=encoded_input[\"input_ids\"].to(self.model_8bit.device),\n+            max_new_tokens=10,\n+            cache_implementation=\"static\",\n+        )\n+\n+        with self.assertRaises(Exception):\n+            object.__setattr__(self.model_8bit.hf_quantizer, \"is_compileable\", True)\n+            self.model_8bit.generate(\n+                input_ids=encoded_input[\"input_ids\"].to(self.model_8bit.device),\n+                max_new_tokens=10,\n+                cache_implementation=\"static\",\n+            )"
        }
    ],
    "stats": {
        "total": 84,
        "additions": 80,
        "deletions": 4
    }
}