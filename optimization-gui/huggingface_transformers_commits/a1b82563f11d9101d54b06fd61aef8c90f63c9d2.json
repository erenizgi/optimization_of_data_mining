{
    "author": "yao-matrix",
    "message": "enable 6 modeling cases on XPU (#37571)\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>",
    "sha": "a1b82563f11d9101d54b06fd61aef8c90f63c9d2",
    "files": [
        {
            "sha": "2159f427bd245614791ee4e64ffbad94cbf8493a",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 34,
            "deletions": 15,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=a1b82563f11d9101d54b06fd61aef8c90f63c9d2",
            "patch": "@@ -19,7 +19,14 @@\n import pytest\n \n from transformers import AutoTokenizer, BambaConfig, is_torch_available\n-from transformers.testing_utils import Expectations, require_torch, require_torch_gpu, slow, torch_device\n+from transformers.testing_utils import (\n+    Expectations,\n+    require_deterministic_for_xpu,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -474,7 +481,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n \n @slow\n @require_torch\n-@require_torch_gpu\n+@require_torch_accelerator\n class BambaModelIntegrationTest(unittest.TestCase):\n     model = None\n     tokenizer = None\n@@ -507,6 +514,10 @@ def test_simple_generate(self):\n                     \"rocm\",\n                     9,\n                 ): \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n+                (\n+                    \"xpu\",\n+                    3,\n+                ): \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all doing well. Today I\",\n             }\n         )\n \n@@ -536,22 +547,30 @@ def test_simple_generate(self):\n \n             torch.testing.assert_close(logits[0, -1, :40].cpu(), EXPECTED_LOGITS_NO_GRAD, rtol=1e-3, atol=1)\n \n+    @require_deterministic_for_xpu\n     def test_simple_batched_generate_with_padding(self):\n         # Key 9 for MI300, Key 8 for A100/A10, and Key 7 for T4.\n         #\n         # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,\n         # considering differences in hardware processing and potential deviations in generated text.\n-        EXPECTED_TEXTS = {\n-            7: [],\n-            8: [\n-                \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n-                \"!!!<|begin_of_text|>I am late! I need to get to work! I have to get to the\",\n-            ],\n-            9: [\n-                \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n-                \"!!!<|begin_of_text|>I am late! I need to be at the airport in 20 minutes! I\",\n-            ],\n-        }\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", 7): [],\n+                (\"cuda\", 8): [\n+                    \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n+                    \"!!!<|begin_of_text|>I am late! I need to get to work! I have to get to the\",\n+                ],\n+                (\"rocm\", 9): [\n+                    \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are doing well. I am here\",\n+                    \"!!!<|begin_of_text|>I am late! I need to be at the airport in 20 minutes! I\",\n+                ],\n+                (\"xpu\", 3): [\n+                    \"<|begin_of_text|>Hey how are you doing on this lovely evening? I hope you are all doing well. Today I\",\n+                    \"!!!<|begin_of_text|>I am late! I need to get to work! I have to get to the\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n \n         self.model.to(torch_device)\n \n@@ -562,8 +581,8 @@ def test_simple_batched_generate_with_padding(self):\n         ).to(torch_device)\n         out = self.model.generate(**inputs, do_sample=False, max_new_tokens=10)\n         output_sentences = self.tokenizer.batch_decode(out)\n-        self.assertEqual(output_sentences[0], EXPECTED_TEXTS[self.cuda_compute_capability_major_version][0])\n-        self.assertEqual(output_sentences[1], EXPECTED_TEXTS[self.cuda_compute_capability_major_version][1])\n+        self.assertEqual(output_sentences[0], EXPECTED_TEXT[0])\n+        self.assertEqual(output_sentences[1], EXPECTED_TEXT[1])\n \n         # TODO: there are significant differences in the logits across major cuda versions, which shouldn't exist\n         if self.cuda_compute_capability_major_version == 8:"
        },
        {
            "sha": "4b7293817ab123e46524df0ea689dee2edfa315e",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=a1b82563f11d9101d54b06fd61aef8c90f63c9d2",
            "patch": "@@ -643,7 +643,7 @@ def test_model_7b_4bit(self):\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_read_token\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2"
        },
        {
            "sha": "844ecfd46e3598dcc17bd49b9789a0addc455e67",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=a1b82563f11d9101d54b06fd61aef8c90f63c9d2",
            "patch": "@@ -520,7 +520,13 @@ def test_model_logits(self):\n \n         outputs = model(dummy_input, output_hidden_states=True)\n \n-        expected_slice = torch.Tensor([-0.2520, -0.2178, -0.1953]).to(torch_device, torch.bfloat16)\n+        expected_slices = Expectations(\n+            {\n+                (\"xpu\", 3): torch.Tensor([-0.2090, -0.2061, -0.1465]),\n+                (\"cuda\", 7): torch.Tensor([-0.2520, -0.2178, -0.1953]),\n+            }\n+        )\n+        expected_slice = expected_slices.get_expectation().to(torch_device, torch.bfloat16)\n         predicted_slice = outputs.hidden_states[-1][0, 0, :3]\n \n         torch.testing.assert_close(expected_slice, predicted_slice, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "d573537b6b5872d09bb5a689ee12bd4dafe408cf",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1b82563f11d9101d54b06fd61aef8c90f63c9d2/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=a1b82563f11d9101d54b06fd61aef8c90f63c9d2",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers import NemotronConfig, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n     is_flaky,\n     require_flash_attn,\n     require_read_token,\n@@ -168,7 +169,7 @@ def test_flash_attn_2_equivalence(self):\n                 assert torch.allclose(logits_fa, logits, atol=1e-2)\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class NemotronIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n@@ -202,9 +203,17 @@ def test_nemotron_8b_generation_sdpa(self):\n     @require_read_token\n     def test_nemotron_8b_generation_eager(self):\n         text = [\"What is the largest planet in solar system?\"]\n-        EXPECTED_TEXT = [\n-            \"What is the largest planet in solar system?\\nAnswer: Jupiter\\n\\nWhat is the answer\",\n-        ]\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"xpu\", 3): [\n+                    \"What is the largest planet in solar system?\\nAnswer: Jupiter\\n\\nWhat is the answer: What is the name of the 19\",\n+                ],\n+                (\"cuda\", 7): [\n+                    \"What is the largest planet in solar system?\\nAnswer: Jupiter\\n\\nWhat is the answer\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n         model_id = \"thhaus/nemotron3-8b\"\n         model = NemotronForCausalLM.from_pretrained(\n             model_id, torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"eager\""
        }
    ],
    "stats": {
        "total": 76,
        "additions": 55,
        "deletions": 21
    }
}