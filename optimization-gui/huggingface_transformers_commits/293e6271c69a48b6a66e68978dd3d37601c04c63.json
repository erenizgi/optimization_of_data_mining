{
    "author": "RUFFY-369",
    "message": " Add sdpa for Vivit (#33757)\n\n* chore:add sdpa to vivit\r\n\r\n* fix:failing slow test_inference_interpolate_pos_encoding(failing on main branch too)\r\n\r\n* chore:fix nits\r\n\r\n* ci:fix repo consistency failure\r\n\r\n* chore:add info and benchmark to model doc\r\n\r\n* [run_slow] vivit\r\n\r\n* chore:revert interpolation test fix for new issue\r\n\r\n* [run_slow] vivit\r\n\r\n* [run_slow] vivit\r\n\r\n* [run_slow] vivit\r\n\r\n* chore:add fallback for output_attentions being True\r\n\r\n* [run_slow] vivit\r\n\r\n* style:make fixup\r\n\r\n* [run_slow] vivit",
    "sha": "293e6271c69a48b6a66e68978dd3d37601c04c63",
    "files": [
        {
            "sha": "c3e3df14ab988b37746a7d95851e6e800b24f24c",
            "filename": "docs/source/en/model_doc/vivit.md",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/293e6271c69a48b6a66e68978dd3d37601c04c63/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/293e6271c69a48b6a66e68978dd3d37601c04c63/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md?ref=293e6271c69a48b6a66e68978dd3d37601c04c63",
            "patch": "@@ -23,6 +23,43 @@ The abstract from the paper is the following:\n \n This model was contributed by [jegormeister](https://huggingface.co/jegormeister). The original code (written in JAX) can be found [here](https://github.com/google-research/scenic/tree/main/scenic/projects/vivit).\n \n+### Using Scaled Dot Product Attention (SDPA)\n+\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+```\n+from transformers import VivitModel\n+model = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+...\n+```\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+\n+On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `google/vivit-b-16x2-kinetics400` model, we saw the following speedups during inference.\n+\n+### Training\n+|   num_training_steps |   batch_size |   is cuda |   Speedup (%) |   Eager peak mem (MB) |   sdpa peak mem (MB) |   Mem saving (%) |\n+|---------------------:|-------------:|----------:|--------------:|----------------------:|---------------------:|-----------------:|\n+|                  100 |            1 |      True |         7.122 |               2575.28 |              5932.54 |           130.364 |\n+\n+\n+\n+### Inference\n+|   num_batches |   batch_size |   is cuda |   is half |   Speedup (%) |   Mem eager (MB) |   Mem BT (MB) |   Mem saved (%) |\n+|---------------|--------------|-----------|-----------|---------------|------------------|---------------|-----------------|\n+|            20 |             1 |   True    |   False   |      15.422   |     715.807      |    317.079    |      125.75     |\n+|            20 |             2 |   True    |   False   |      17.146   |    1234.75       |    447.175    |      176.122    |\n+|            20 |             4 |   True    |   False   |      18.093   |    2275.82       |    709.864    |      220.6      |\n+|            20 |             8 |   True    |   False   |      19.284   |    4358.19       |   1233.24     |      253.393    |\n+           \n+\n ## VivitConfig\n \n [[autodoc]] VivitConfig"
        },
        {
            "sha": "82d7f50f77d902aee9e99ab7a3482fa8d810cdc2",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/293e6271c69a48b6a66e68978dd3d37601c04c63/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/293e6271c69a48b6a66e68978dd3d37601c04c63/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=293e6271c69a48b6a66e68978dd3d37601c04c63",
            "patch": "@@ -278,6 +278,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [ViTMAE](https://huggingface.co/docs/transformers/model_doc/vit_mae#transformers.ViTMAEModel)\n * [ViTMSN](https://huggingface.co/docs/transformers/model_doc/vit_msn#transformers.ViTMSNModel)\n * [VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae#transformers.VideoMAEModell)\n+* [ViViT](https://huggingface.co/docs/transformers/model_doc/vivit#transformers.VivitModel)\n * [wav2vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2#transformers.Wav2Vec2Model)\n * [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperModel)\n * [XLM-RoBERTa](https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaModel)"
        },
        {
            "sha": "9b6516a25af45b63b05328de3967344150ecccd2",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 60,
            "deletions": 1,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/293e6271c69a48b6a66e68978dd3d37601c04c63/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/293e6271c69a48b6a66e68978dd3d37601c04c63/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=293e6271c69a48b6a66e68978dd3d37601c04c63",
            "patch": "@@ -227,6 +227,51 @@ def forward(\n         return outputs\n \n \n+# Adapted from transformers.models.vit.modeling_vit.ViTSdpaSelfAttention with ViT->Vivit\n+class VivitSdpaSelfAttention(VivitSelfAttention):\n+    def __init__(self, config: VivitConfig) -> None:\n+        super().__init__(config)\n+        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n+\n+    def forward(\n+        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        if output_attentions or head_mask is not None:\n+            logger.warning_once(\n+                \"VivitSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support\"\n+                \" `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying\"\n+                \" the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be\"\n+                ' removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                head_mask,\n+                output_attentions,\n+            )\n+\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            head_mask,\n+            self.attention_probs_dropout_prob if self.training else 0.0,\n+            is_causal=False,\n+            scale=None,\n+        )\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        return context_layer, None\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Vivit\n class VivitSelfOutput(nn.Module):\n     \"\"\"\n@@ -286,6 +331,13 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->Vivit\n+class VivitSdpaAttention(VivitAttention):\n+    def __init__(self, config: VivitConfig) -> None:\n+        super().__init__(config)\n+        self.attention = VivitSdpaSelfAttention(config)\n+\n+\n class VivitIntermediate(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -320,14 +372,20 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n+VIVIT_ATTENTION_CLASSES = {\n+    \"eager\": VivitAttention,\n+    \"sdpa\": VivitSdpaAttention,\n+}\n+\n+\n class VivitLayer(nn.Module):\n     \"\"\"This corresponds to the EncoderBlock class in the scenic/vivit implementation.\"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = VivitAttention(config)\n+        self.attention = VIVIT_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.intermediate = VivitIntermediate(config)\n         self.output = VivitOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -436,6 +494,7 @@ class VivitPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "8e6b0825948d402f6698ab0b062a192d2b7d6a1f",
            "filename": "tests/models/vivit/test_modeling_vivit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/293e6271c69a48b6a66e68978dd3d37601c04c63/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/293e6271c69a48b6a66e68978dd3d37601c04c63/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py?ref=293e6271c69a48b6a66e68978dd3d37601c04c63",
            "patch": "@@ -65,6 +65,8 @@ def __init__(\n         layer_norm_eps=1e-06,\n         qkv_bias=True,\n         scope=None,\n+        attn_implementation=\"eager\",\n+        mask_ratio=0.5,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -86,12 +88,15 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.qkv_bias = qkv_bias\n         self.scope = scope\n+        self.attn_implementation = attn_implementation\n \n         self.seq_length = (\n             (self.image_size // self.tubelet_size[2])\n             * (self.image_size // self.tubelet_size[1])\n             * (self.num_frames // self.tubelet_size[0])\n         ) + 1  # CLS token\n+        self.mask_ratio = mask_ratio\n+        self.num_masks = int(mask_ratio * self.seq_length)\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor(\n@@ -122,6 +127,7 @@ def get_config(self):\n             initializer_range=self.initializer_range,\n             layer_norm_eps=self.layer_norm_eps,\n             qkv_bias=self.qkv_bias,\n+            attn_implementation=self.attn_implementation,\n         )\n         config.num_labels = self.num_labels\n         return config"
        }
    ],
    "stats": {
        "total": 105,
        "additions": 104,
        "deletions": 1
    }
}