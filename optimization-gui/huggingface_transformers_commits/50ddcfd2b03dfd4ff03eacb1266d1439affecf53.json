{
    "author": "molbap",
    "message": "Torch distrib smallfix (#43008)\n\n* guard counting of bytes\n\n* add small test\n\n* quality\n\n* simplify a bit",
    "sha": "50ddcfd2b03dfd4ff03eacb1266d1439affecf53",
    "files": [
        {
            "sha": "011d8be1038744bf7f7320783e7791ba455dfd6a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/50ddcfd2b03dfd4ff03eacb1266d1439affecf53/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50ddcfd2b03dfd4ff03eacb1266d1439affecf53/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=50ddcfd2b03dfd4ff03eacb1266d1439affecf53",
            "patch": "@@ -4556,7 +4556,7 @@ def get_total_byte_count(\n \n     total_byte_count = defaultdict(lambda: 0)\n     tied_param_names = model.all_tied_weights_keys.keys()\n-    tp_plan = model._tp_plan\n+    tp_plan = model._tp_plan if torch.distributed.is_available() and torch.distributed.is_initialized() else []\n \n     for param_name, device in accelerator_device_map.items():\n         # Skip if the parameter has already been accounted for (tied weights)"
        },
        {
            "sha": "81e88082980afd9d2b4049cab19023c0986cad03",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/50ddcfd2b03dfd4ff03eacb1266d1439affecf53/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50ddcfd2b03dfd4ff03eacb1266d1439affecf53/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=50ddcfd2b03dfd4ff03eacb1266d1439affecf53",
            "patch": "@@ -133,6 +133,7 @@\n         FLASH_ATTN_KERNEL_FALLBACK,\n         _find_disjoint,\n         _find_identical,\n+        get_total_byte_count,\n     )\n     from transformers.pytorch_utils import isin_mps_friendly\n \n@@ -398,6 +399,23 @@ def tearDown(self):\n         torch.set_default_dtype(self.old_dtype)\n         super().tearDown()\n \n+    @require_torch\n+    def test_get_total_byte_count_does_not_require_process_group(self):\n+        model = BaseModel(PreTrainedConfig())\n+        model._tp_plan = {\"linear.weight\": \"rowwise\"}\n+        accelerator_device_map = {\"linear.weight\": torch.device(\"cpu\")}\n+\n+        with (\n+            patch(\"transformers.modeling_utils.torch.distributed.is_available\", return_value=True),\n+            patch(\"transformers.modeling_utils.torch.distributed.is_initialized\", return_value=False),\n+            patch(\"transformers.modeling_utils.torch.distributed.get_world_size\") as mock_world_size,\n+        ):\n+            total_byte_count = get_total_byte_count(model, accelerator_device_map, None)\n+\n+        mock_world_size.assert_not_called()\n+        self.assertIn(torch.device(\"cpu\"), total_byte_count)\n+        self.assertGreater(total_byte_count[torch.device(\"cpu\")], 0)\n+\n     def test_hub_retry(self):\n         @hub_retry(max_attempts=2)\n         def test_func():"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 19,
        "deletions": 1
    }
}