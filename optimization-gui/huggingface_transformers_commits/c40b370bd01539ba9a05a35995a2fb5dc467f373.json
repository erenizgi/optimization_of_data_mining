{
    "author": "zucchini-nlp",
    "message": "Allow VLMs to have a correct `base_model` (#41589)\n\n* allow VLMs to have a correct `base_model`\n\n* fix copies\n\n* fix copies?\n\n* empty commit\n\n* fix copies\n\n* nits after rebase\n\n* fix copies\n\n* add a test\n\n* skip more tests\n\n* fiix copies, ig have to do it in all PRs after rebase",
    "sha": "c40b370bd01539ba9a05a35995a2fb5dc467f373",
    "files": [
        {
            "sha": "3c0abffe24304a3bcc4a3608f43c8cc0315b83d4",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -596,7 +596,7 @@ def _init_weights(self, module):\n @auto_docstring\n class AriaPreTrainedModel(PreTrainedModel):\n     config: AriaConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"AriaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n@@ -893,6 +893,10 @@ class AriaModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n )\n class AriaModel(AriaPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+\n     def __init__(self, config: AriaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "0afe2ae23e98d8a7031d7685bc1967d04e00ed1b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -1206,7 +1206,7 @@ def _init_weights(self, module):\n \n class AriaPreTrainedModel(LlamaPreTrainedModel):\n     config: AriaConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (dynamic slicing)\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "734dff38241645ba963d98baca693b05c8783ecf",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -90,6 +90,7 @@ def pixel_shuffle(self, image_features):  # B, S, D\n @auto_docstring\n class AyaVisionPreTrainedModel(PreTrainedModel):\n     config: AyaVisionConfig\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -162,6 +163,10 @@ class AyaVisionModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n )\n class AyaVisionModel(AyaVisionPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+\n     def __init__(self, config: AyaVisionConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "d4b19101c861dccfa91cf6501331cbf8bf0a55e3",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -430,6 +430,7 @@ def forward(\n @auto_docstring\n class BltPreTrainedModel(PreTrainedModel):\n     config: BltConfig\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BltTransformerLayer\"]"
        },
        {
            "sha": "835275f4e20cb172eb17a42c270704e0f4db8277",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -778,7 +778,7 @@ def forward(\n @auto_docstring\n class ClvpPreTrainedModel(PreTrainedModel):\n     config: ClvpConfig\n-    base_model_prefix = \"clvp\"\n+    base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "083fde1f91971cd70fbb931bcd22fa52988a7a0f",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -129,6 +129,7 @@ class Cohere2VisionCausalLMOutputWithPast(ModelOutput):\n @auto_docstring\n class Cohere2VisionPreTrainedModel(PreTrainedModel):\n     config: Cohere2VisionConfig\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -142,7 +143,6 @@ class Cohere2VisionPreTrainedModel(PreTrainedModel):\n         \"hidden_states\": \"DecoderLayer\",\n         \"attentions\": \"Attention\",\n     }\n-    base_model_prefix = \"model\"\n \n \n @auto_docstring("
        },
        {
            "sha": "7d900cf4a27a434e079946595cfa0a5404e07792",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -1490,7 +1490,6 @@ def forward(\n \n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n-    base_model_prefix = \"\"\n     output_modalities = [\"image\", \"text\"]\n     _tied_weights_keys = {\"lm_head.weight\": \"model.text_model.embed_tokens.weight\"}\n     _checkpoint_conversion_mapping = {"
        },
        {
            "sha": "1ef5e23a443664b3af3288f35df46314ccc512fc",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -1044,7 +1044,6 @@ def forward(\n \n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n-    base_model_prefix = \"\"\n     output_modalities = [\"image\", \"text\"]\n     _tied_weights_keys = {\"lm_head.weight\": \"model.text_model.embed_tokens.weight\"}\n     _checkpoint_conversion_mapping = {"
        },
        {
            "sha": "d5f263137b3dce5b00a917941d7198cfdbf54c25",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -1298,7 +1298,7 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n )\n class FlavaImageCodebook(FlavaPreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     config: FlavaImageCodebookConfig\n     main_input_name = \"pixel_values\"\n     input_modalities = \"image\""
        },
        {
            "sha": "4d6d38c5db297bfe4ca8154498b10b974bb07c59",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -615,6 +615,7 @@ class Florence2Seq2SeqLMOutput(Seq2SeqLMOutput):\n @auto_docstring\n class Florence2PreTrainedModel(PreTrainedModel):\n     config: Florence2Config\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -627,7 +628,6 @@ class Florence2PreTrainedModel(PreTrainedModel):\n \n     _supports_attention_backend = False\n     config_class = Florence2Config\n-    base_model_prefix = \"model\"\n \n \n @auto_docstring("
        },
        {
            "sha": "c623324226acb35f6cb944f991536764c86e043e",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -446,7 +446,7 @@ def forward(\n @auto_docstring\n class Gemma3PreTrainedModel(PreTrainedModel):\n     config: Gemma3Config\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"Gemma3DecoderLayer\",\n@@ -632,7 +632,6 @@ class Gemma3ForCausalLM(Gemma3PreTrainedModel, GenerationMixin):\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     config: Gemma3TextConfig\n-    base_model_prefix = \"model\"\n \n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "a1b0a57928d1b3cfbd0897fa8141bba47c7f0f98",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -561,7 +561,7 @@ def forward(\n \n \n class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     _no_split_modules = [\n         \"Gemma3DecoderLayer\",\n@@ -717,7 +717,6 @@ def forward(\n \n class Gemma3ForCausalLM(Gemma2ForCausalLM):\n     config: Gemma3TextConfig\n-    base_model_prefix = \"model\"\n \n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "0b3088aadec75e523025144ce732ea1307b1f0a8",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -1939,7 +1939,6 @@ class Gemma3nForCausalLM(Gemma3nPreTrainedModel, GenerationMixin):\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     config: Gemma3nTextConfig\n-    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {\"model.language_model\": \"model\"}\n \n     def __init__(self, config: Gemma3nTextConfig):\n@@ -2349,7 +2348,6 @@ def get_audio_features(\n class Gemma3nForConditionalGeneration(Gemma3nPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {}\n     _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n-    base_model_prefix = \"model\"\n \n     def __init__(self, config: Gemma3nConfig):\n         super().__init__(config)"
        },
        {
            "sha": "02cf9b9f4833eaf65be8db54e15e106f4f951932",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -2116,7 +2116,6 @@ def forward(\n @auto_docstring(custom_intro=\"The base Gemma 3n language model with a language modeling head.\")\n class Gemma3nForCausalLM(Gemma3ForCausalLM):\n     _checkpoint_conversion_mapping = {\"model.language_model\": \"model\"}\n-    base_model_prefix = \"model\"\n \n \n class Gemma3nMultimodalEmbedder(nn.Module):\n@@ -2421,7 +2420,6 @@ def get_audio_features(\n )\n class Gemma3nForConditionalGeneration(PaliGemmaForConditionalGeneration):\n     _checkpoint_conversion_mapping = {}\n-    base_model_prefix = \"model\"\n \n     @property\n     def audio_tower(self):"
        },
        {
            "sha": "8c20786c955e2e42ac4ee98eeea9544c7acc96cc",
            "filename": "src/transformers/models/glm46v/modeling_glm46v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -78,7 +78,7 @@ class Glm46VModelOutputWithPast(ModelOutput):\n \n @auto_docstring\n class Glm46VModel(Glm46VPreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False\n@@ -583,6 +583,8 @@ def forward(\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n             The temporal, height and width of feature shape of each video in LLM.\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n \n         Example:\n "
        },
        {
            "sha": "9942770d70e347048929891f21878f1b775a1576",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -926,7 +926,7 @@ def forward(\n \n @auto_docstring\n class Glm4vModel(Glm4vPreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False\n@@ -1431,6 +1431,8 @@ def forward(\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n             The temporal, height and width of feature shape of each video in LLM.\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n \n         Example:\n "
        },
        {
            "sha": "85cd3ed44085234a77b48649b81893c24e4fe7eb",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -1350,6 +1350,8 @@ def forward(\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n             The temporal, height and width of feature shape of each video in LLM.\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n \n         Example:\n "
        },
        {
            "sha": "a212aed44e81cfa86994f9067a7a87024cc69824",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -537,7 +537,7 @@ def forward(\n @auto_docstring\n class Glm4vMoePreTrainedModel(PreTrainedModel):\n     config: Glm4vMoeConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Glm4vMoeTextDecoderLayer\", \"Glm4vMoeVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -1090,7 +1090,7 @@ def forward(\n \n @auto_docstring\n class Glm4vMoeModel(Glm4vMoePreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False\n@@ -1648,6 +1648,8 @@ def forward(\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n             The temporal, height and width of feature shape of each video in LLM.\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n \n         Example:\n "
        },
        {
            "sha": "67419f310521d61f4d3e939391b8a6787a92d3b6",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -475,7 +475,7 @@ def __init__(self, config: Glm4vMoeTextConfig, layer_idx: int):\n \n class Glm4vMoePreTrainedModel(Glm4MoePreTrainedModel):\n     config: Glm4vMoeConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"text\", \"image\", \"video\"]\n     _no_split_modules = [\"Glm4vMoeTextDecoderLayer\", \"Glm4vMoeVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "3ac6bd1772205294220483a4109cc47e3d9df9b1",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -277,6 +277,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.FloatTensor]:\n @auto_docstring\n class GotOcr2PreTrainedModel(PreTrainedModel):\n     config: GotOcr2Config\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -532,6 +533,10 @@ class GotOcr2ModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n )\n class GotOcr2Model(GotOcr2PreTrainedModel):\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+\n     def __init__(self, config: GotOcr2Config):\n         super().__init__(config)\n         self.vision_tower = GotOcr2VisionEncoder(config.vision_config)"
        },
        {
            "sha": "6e586ce999d5d3fde7486a1944eba716b45fb7dc",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -473,6 +473,7 @@ def forward(\n @auto_docstring\n class InternVLPreTrainedModel(PreTrainedModel):\n     config: InternVLConfig\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\", \"video\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -530,6 +531,10 @@ class InternVLModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n )\n class InternVLModel(InternVLPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+\n     def __init__(self, config: InternVLConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "b102a111e10f137844b5e563d42ab05f08931828",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -568,7 +568,7 @@ def forward(\n @auto_docstring\n class JetMoePreTrainedModel(PreTrainedModel):\n     config: JetMoeConfig\n-    base_model_prefix = \"transformer\"\n+    base_model_prefix = \"model\"\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"JetMoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]"
        },
        {
            "sha": "db8c3e1059c0584e5c403a2232a57af43119bffd",
            "filename": "src/transformers/models/jetmoe/modular_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -429,7 +429,7 @@ class JetMoePreTrainedModel(MixtralPreTrainedModel):\n         \"attentions\": OutputRecorder(JetMoeAttention, index=1),\n     }\n     config: JetMoeConfig\n-    base_model_prefix = \"transformer\"\n+    base_model_prefix = \"model\"\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"JetMoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]"
        },
        {
            "sha": "27ba1ece7af4f8ba1e1fc2d20ed1d7b546d416b5",
            "filename": "src/transformers/models/lfm2_vl/modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -76,6 +76,7 @@ def pixel_unshuffle(self, hidden_states: torch.Tensor):\n @auto_docstring\n class Lfm2VlPreTrainedModel(PreTrainedModel):\n     config: Lfm2VlConfig\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -85,7 +86,6 @@ class Lfm2VlPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n-    base_model_prefix = \"model\"\n \n \n @dataclass"
        },
        {
            "sha": "231e04c8eba21d7546ef39e34fa7a92e876b3d79",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -1166,7 +1166,7 @@ def forward(\n class Llama4ForConditionalGeneration(Llama4PreTrainedModel, GenerationMixin):\n     _no_split_modules = [\"Llama4TextDecoderLayer\", \"Llama4VisionEncoderLayer\"]\n     _tp_plan = {}\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     config: Llama4Config\n \n     def __init__(self, config: Llama4Config):"
        },
        {
            "sha": "1f8a2a9645ea14fa82c68af4b64c05360a7e1f6d",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -110,6 +110,7 @@ def forward(self, image_features):\n @auto_docstring\n class LlavaPreTrainedModel(PreTrainedModel):\n     config: LlavaConfig\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -128,6 +129,10 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     \"\"\"\n )\n class LlavaModel(LlavaPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+\n     def __init__(self, config: LlavaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "a83821d98f96300cd1beaa1c08dbbea3be3a5114",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -223,7 +223,7 @@ def forward(self, image_features):\n @auto_docstring\n class LlavaNextPreTrainedModel(PreTrainedModel):\n     config: LlavaNextConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]"
        },
        {
            "sha": "7f6fbffaec07ead944d62c551778ebb99597f625",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -164,7 +164,7 @@ def forward(self, image_features):\n @auto_docstring\n class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     config: LlavaNextVideoConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]"
        },
        {
            "sha": "efe4f6fb1ba6618ca8a22fa17ccc30818b48ca93",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -105,7 +105,7 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n @auto_docstring\n class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     config: LlavaOnevisionConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]"
        },
        {
            "sha": "6a67f21f216fd6026a05b7194f265082648a48ae",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -176,6 +176,7 @@ class Mistral3ModelOutputWithPast(BaseModelOutputWithPast):\n @auto_docstring\n class Mistral3PreTrainedModel(PreTrainedModel):\n     config: Mistral3Config\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -194,6 +195,10 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     \"\"\"\n )\n class Mistral3Model(Mistral3PreTrainedModel):\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+\n     def __init__(self, config: Mistral3Config):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "9f811fa7f0102802d4241cea75f81871ecc8e152",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -793,6 +793,7 @@ def forward(self, x, position_ids):\n @auto_docstring\n class MllamaPreTrainedModel(PreTrainedModel):\n     config: MllamaConfig\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n@@ -1436,7 +1437,6 @@ def forward(\n     \"\"\"\n )\n class MllamaModel(MllamaPreTrainedModel):\n-    base_model_prefix = \"\"\n     _checkpoint_conversion_mapping = {\n         \"language_model.model\": \"language_model\",\n         \"model.vision_model\": \"vision_model\","
        },
        {
            "sha": "f1464bde5fb2bdf222deba93f88adb11cfec991f",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -214,7 +214,7 @@ def create_causal_mask_mapping(\n @auto_docstring\n class PaliGemmaPreTrainedModel(PreTrainedModel):\n     config: PaliGemmaConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PaliGemmaMultiModalProjector\"]"
        },
        {
            "sha": "0a601deac183ae676b3502d0447850c7b9e95a45",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -89,6 +89,7 @@ def forward(self, features):\n @auto_docstring\n class PerceptionLMPreTrainedModel(PreTrainedModel):\n     config: PerceptionLMConfig\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -99,7 +100,6 @@ class PerceptionLMPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n-    base_model_prefix = \"model\"\n \n \n @dataclass"
        },
        {
            "sha": "4c035c3144eb008a55c4e4b9ee08274ecaccfd88",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -950,7 +950,7 @@ def forward(\n \n @auto_docstring\n class Qwen2_5_VLModel(Qwen2_5_VLPreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {\"^model\": \"language_model\"}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False"
        },
        {
            "sha": "1331c9e70e12b1455ee0a398752f09e8060d6164",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -343,7 +343,7 @@ class Qwen2_5_VLModelOutputWithPast(Qwen2VLModelOutputWithPast):\n \n class Qwen2_5_VLModel(Qwen2VLModel):\n     config: Qwen2_5_VLConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False"
        },
        {
            "sha": "bad9709e32c14790d60aaf6eb299fc771e856301",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -923,7 +923,7 @@ def forward(\n \n @auto_docstring\n class Qwen2VLModel(Qwen2VLPreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {\"^model\": \"language_model\"}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False"
        },
        {
            "sha": "0b6d0277ad4a2e9077c9053b383eee0d2b2c83d3",
            "filename": "src/transformers/models/video_llama_3/modeling_video_llama_3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -517,7 +517,7 @@ class VideoLlama3ModelOutputWithPast(ModelOutput):\n \n @auto_docstring\n class VideoLlama3Model(VideoLlama3PreTrainedModel):\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     _checkpoint_conversion_mapping = {}\n     # Reference: fix gemma3 grad acc #37208\n     accepts_loss_kwargs = False"
        },
        {
            "sha": "559c30ef1f6523743e2ce0c1e172e4aad79d9373",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -125,7 +125,7 @@ def forward(self, image_features):\n @auto_docstring\n class VideoLlavaPreTrainedModel(PreTrainedModel):\n     config: VideoLlavaConfig\n-    base_model_prefix = \"\"\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"video\", \"text\"]\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VideoLlavaVisionAttention\"]"
        },
        {
            "sha": "02fdf4c4638c2071849f4c4adae982c62351e0ee",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -113,6 +113,7 @@ def forward(self, hidden_states):\n @auto_docstring\n class VipLlavaPreTrainedModel(PreTrainedModel):\n     config: VipLlavaConfig\n+    base_model_prefix = \"model\"\n     input_modalities = [\"image\", \"text\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -131,6 +132,10 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     \"\"\"\n )\n class VipLlavaModel(VipLlavaPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\n+        r\"^language_model.model\": \"language_model\",\n+    }\n+\n     def __init__(self, config: VipLlavaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)"
        },
        {
            "sha": "c15c315c69bfb7f7574869d10230ce2a923d7035",
            "filename": "tests/models/audioflamingo3/test_modeling_audioflamingo3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Faudioflamingo3%2Ftest_modeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Faudioflamingo3%2Ftest_modeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudioflamingo3%2Ftest_modeling_audioflamingo3.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -190,6 +190,10 @@ def test_sdpa_can_dispatch_on_flash(self):\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         pass\n \n+    @unittest.skip(reason=\"AudioFlamingo3 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_sdpa_can_dispatch_composite_models(self):\n         # AF3 is audio+text composite; verify SDPA toggles propagate to submodules.\n         if not self.has_attentions:"
        },
        {
            "sha": "295a0baf0f58ab6aa1dd0bee139960f3d58a9eea",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -597,6 +597,10 @@ def test_generate_fp16(self):\n         model.generate(input_ids, attention_mask=attention_mask)\n         model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n \n+    @unittest.skip(\"Bark has no base model due to special archiecture\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n @require_torch\n class BarkCoarseModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n@@ -683,6 +687,10 @@ def test_generate_fp16(self):\n         model.generate(input_ids, attention_mask=attention_mask)\n         model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n \n+    @unittest.skip(\"Bark has no base model due to special archiecture\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n @require_torch\n class BarkFineModelTest(ModelTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "8115c2f89ec2c940e120b4f9212d99771ca9f763",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -500,6 +500,10 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n+    @unittest.skip(reason=\"BLIP2 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n@@ -850,6 +854,10 @@ def test_model_get_set_embeddings(self):\n     def test_cpu_offload(self):\n         pass\n \n+    @unittest.skip(reason=\"BLIP2 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model."
        },
        {
            "sha": "99594f5bf7d49f0c2509c427e84b452d0a02eaa7",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -279,6 +279,10 @@ def test_model_parallel_beam_search(self):\n     def test_tied_weights_keys(self):\n         pass\n \n+    @unittest.skip(reason=\"CSM has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def _get_custom_4d_mask_test_data(self):\n         \"\"\"\n         Overrides [ModelTesterMixin._get_custom_4d_mask_test_data] to handle third input_ids dimension."
        },
        {
            "sha": "732014fb9f62520bcb6255b0a2f0f11727ffa086",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -260,6 +260,10 @@ def test_eager_padding_matches_padding_free_with_position_ids(self):\n     def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n+    @unittest.skip(reason=\"Fuyu has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "66a0545c0ea749ecd470f36bb8b901b922fb1c9c",
            "filename": "tests/models/granite_speech/test_modeling_granite_speech.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -292,6 +292,10 @@ def test_sdpa_can_dispatch_composite_models(self):\n     def test_eager_matches_sdpa_generate(self):\n         pass\n \n+    @unittest.skip(reason=\"GraniteSpeech has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n class GraniteSpeechForConditionalGenerationIntegrationTest(unittest.TestCase):\n     def setUp(self):"
        },
        {
            "sha": "6a6050abb146018a3c1184b3f0b1691fdcb5ee18",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -521,6 +521,10 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n+    @unittest.skip(reason=\"InstructBLIP has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "0046ccb0bf5834a957d9164239e70282c6b736d4",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -534,6 +534,10 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_common_attributes(self):\n         pass\n \n+    @unittest.skip(reason=\"InstructBLIP has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_forward_signature(self):\n         for model_class in self.all_model_classes:\n             config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "508b8c67be1988d562772433a0c15fda3128c206",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -113,6 +113,10 @@ class JetMoeModelTest(CausalLMModelTest, unittest.TestCase):\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(reason=\"JetMoe flash attention does not support right padding\")\n \n+    @unittest.skip(reason=\"JetMoe has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n @require_torch\n class JetMoeIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "0686cd8c6026febd832200289a37b647ce2c7980",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -408,6 +408,10 @@ def test_eager_padding_matches_padding_free_with_position_ids(self):\n     def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n+    @unittest.skip(reason=\"Kosmos2 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n         # Overwrite -- kosmos2 needs to prepare `image_embeds_position_mask`, and it must be padded accordingly"
        },
        {
            "sha": "a2e7afb977998092135a20b279af75cbd75a5e64",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -376,6 +376,10 @@ def test_assisted_decoding_sample(self):\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         pass\n \n+    @unittest.skip(reason=\"Kosmos2-3 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)"
        },
        {
            "sha": "ed0ead4c9bce1d4690655d49a952451d00112ae1",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -734,6 +734,10 @@ def _check_encoder_attention_for_generate(self, attentions, batch_size, config,\n     def test_load_save_without_tied_weights(self):\n         pass\n \n+    @unittest.skip(reason=\"LongT5 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n @require_torch\n class LongT5TGlobalModelTest(LongT5ModelTest):\n@@ -871,6 +875,10 @@ def _check_encoder_attention_for_generate(self, attentions, batch_size, config,\n             [encoder_expected_shape] * len(attentions),\n         )\n \n+    @unittest.skip(reason=\"LongT5 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n class LongT5EncoderOnlyModelTester:\n     def __init__("
        },
        {
            "sha": "090d23b45fdb68fbe333065b68a28e06b8d346da",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -130,6 +130,10 @@ def setUp(self):\n         self.model_tester = MllamaText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=MllamaTextConfig, has_text_modality=True)\n \n+    @unittest.skip(\"Mllama needs a different model prefix to loadd saved checkpoints\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n class MllamaVisionText2TextModelTester:\n     def __init__("
        },
        {
            "sha": "15fdf884e099635163d97c3bf6873291ea4235dc",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -834,6 +834,10 @@ def test_prepare_inputs_for_generation_kwargs_forwards(self):\n             last_hidden_state=torch.randn(2, 3, 32), kwargs_depth_decoder={}\n         )\n \n+    @unittest.skip(reason=\"Moshi has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n def place_dict_on_device(dict_to_place, device):\n     for key in dict_to_place:"
        },
        {
            "sha": "cd0ab3677643a0ff63ea9c94e7acdab9da05eb02",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -735,6 +735,10 @@ def test_model_from_pretrained(self):\n         model = MT5Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @unittest.skip(reason=\"MT5 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n # Copied from tests.models.t5.test_modeling_t5.T5EncoderOnlyModelTester with T5->MT5\n class MT5EncoderOnlyModelTester:"
        },
        {
            "sha": "42353d54ed135090c5dee4e80722666827063d9f",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -648,6 +648,10 @@ def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, c\n             [encoder_expected_shape] * len(hidden_states),\n         )\n \n+    @unittest.skip(\"Pix2Struct has no base model, it was implemented before standardization\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n # We will verify our results on an image of a stop sign\n def prepare_img():"
        },
        {
            "sha": "e5179f179e30ca28104b168a8b126b540abeb75f",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -293,6 +293,10 @@ def test_sdpa_can_dispatch_on_flash(self):\n     def test_model_outputs_equivalence(self):\n         pass\n \n+    @unittest.skip(\"Qwen2Omni has no base model, model architecture is special\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_sdpa_can_dispatch_composite_models(self):\n         # overwrite because Qwen2 is audio+text model (not vision+text)\n         if not self.has_attentions:"
        },
        {
            "sha": "25e1200332dede6af1ea46394350f1429bd53a07",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -156,6 +156,10 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n+    @unittest.skip(reason=\"Qwen2Audio has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_sdpa_can_dispatch_composite_models(self):\n         # overwrite because Qwen2 is audio+text model (not vision+text)\n         if not self.has_attentions:"
        },
        {
            "sha": "df0d53bf404ee1fcc8d3d5d7121594b90946ca63",
            "filename": "tests/models/qwen3_omni_moe/test_modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_modeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_modeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_modeling_qwen3_omni_moe.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -477,6 +477,10 @@ def test_custom_4d_attention_mask(self):\n     def test_model_is_small(self):\n         pass\n \n+    @unittest.skip(\"Qwen3Omni has no base model, model architecture is special\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     @unittest.skip(\"FIXME this is important, but in a rush to merge, cannot investigate now\")\n     def test_get_rope_index_video_with_audio(self):\n         image_grid_thw = torch.empty((0, 3), dtype=torch.long)"
        },
        {
            "sha": "70dc949277866209ddc8929c059a8b68dd6fe4b3",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -708,6 +708,10 @@ def test_load_save_without_tied_weights(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n+    @unittest.skip(reason=\"SwitchTransformers has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n class SwitchTransformersEncoderOnlyModelTester:\n     def __init__("
        },
        {
            "sha": "ae5031e14dc5a4182a4d559146a2228cc6f50a27",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -743,6 +743,10 @@ def test_model_from_pretrained(self):\n         model = T5Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @unittest.skip(reason=\"T5 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n class T5EncoderOnlyModelTester:\n     def __init__("
        },
        {
            "sha": "e3973fd52896ac1a835bfa1fb83706bd703af8da",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -330,6 +330,10 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n+    @unittest.skip(reason=\"Udop has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "a6f90b5edc0620922ba61a7de0371997366e5ed6",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -361,6 +361,10 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n+    @unittest.skip(reason=\"UMT5 has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n \n # Copied from tests.models.t5.test_modeling_t5.T5EncoderOnlyModelTester with T5->UMT5\n class UMT5EncoderOnlyModelTester:"
        },
        {
            "sha": "213c28538c110ced408487c69afb3909369ba908",
            "filename": "tests/models/voxtral/test_modeling_voxtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -189,6 +189,10 @@ def test_flash_attention_3_padding_matches_padding_free_with_position_ids(self):\n     def test_flash_attention_3_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n         pass\n \n+    @unittest.skip(reason=\"Voxtral has no separate base model without a head.\")\n+    def test_model_base_model_prefix(self):\n+        pass\n+\n     def test_sdpa_can_dispatch_composite_models(self):\n         # overwrite because Voxtral is audio+text model (not vision+text)\n         if not self.has_attentions:"
        },
        {
            "sha": "036fea5ee3d130f44611dd9148e2be66045e0569",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c40b370bd01539ba9a05a35995a2fb5dc467f373/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=c40b370bd01539ba9a05a35995a2fb5dc467f373",
            "patch": "@@ -1942,6 +1942,20 @@ def test_model_main_input_name(self):\n             observed_main_input_name = list(model_signature.parameters.keys())[1]\n             self.assertEqual(model_class.main_input_name, observed_main_input_name)\n \n+    def test_model_base_model_prefix(self):\n+        \"\"\"\n+        Normally a generative model is a base model + lm_head on top. If this test\n+        fails for new model, probably the model has incorrect `base_model_prefix` or\n+        the you are re-defining base blocks for a generative model.\n+        There are some models which might not fit this assumption, if the model\n+        has a special architecture. Feel free to skip the test in that case with\n+        a reason in description.\n+        \"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+            self.assertTrue(model.base_model is not model)\n+\n     def test_correct_missing_keys(self):\n         if not self.test_missing_keys:\n             self.skipTest(reason=\"test_missing_keys is set to `False`\")"
        }
    ],
    "stats": {
        "total": 229,
        "additions": 193,
        "deletions": 36
    }
}