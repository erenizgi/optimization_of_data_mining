{
    "author": "avishaiElmakies",
    "message": "Sdpa dino v2 (#33403)\n\n* add sdpa to dinov2\r\n\r\n* fixup\r\n\r\n* add dinov2 to sdpa doc\r\n\r\n* update doc order\r\n\r\n* [run-slow] dinov2\r\n\r\n* common to eager\r\n\r\n* [run-slow] dinov2\r\n\r\n* update attn implementation in common\r\n\r\n* update test_modeling_dinov2 to have mask_ration, num_masks and mask_length similar to vit\r\n\r\n* [run-slow] dinov2\r\n\r\n---------\r\n\r\nCo-authored-by: Avishai Elmakies <avishai.elma@cs.huji.ac.il>",
    "sha": "78b2929c0554b79e0489b451ce4ece14d265ead2",
    "files": [
        {
            "sha": "193af845da659dc4c27a8cc12077fd0cd4b3ee0e",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/78b2929c0554b79e0489b451ce4ece14d265ead2/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/78b2929c0554b79e0489b451ce4ece14d265ead2/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=78b2929c0554b79e0489b451ce4ece14d265ead2",
            "patch": "@@ -217,6 +217,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)\n * [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)\n * [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)\n+* [Dinov2](https://huggingface.co/docs/transformers/en/model_doc/dinov2)\n * [Dpr](https://huggingface.co/docs/transformers/model_doc/dpr#transformers.DprReader)\n * [Falcon](https://huggingface.co/docs/transformers/model_doc/falcon#transformers.FalconModel)\n * [Gemma](https://huggingface.co/docs/transformers/model_doc/gemma#transformers.GemmaModel)\n@@ -275,7 +276,6 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel)\n * [YOLOS](https://huggingface.co/docs/transformers/model_doc/yolos#transformers.YolosModel)\n \n-\n <Tip>\n \n FlashAttention can only be used for models with the `fp16` or `bf16` torch type, so make sure to cast your model to the appropriate type first. The memory-efficient attention backend is able to handle `fp32` models."
        },
        {
            "sha": "ebe322618a2d8fb38b4e861a140b6b3b2a71aaf9",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 47,
            "deletions": 1,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/78b2929c0554b79e0489b451ce4ece14d265ead2/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78b2929c0554b79e0489b451ce4ece14d265ead2/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=78b2929c0554b79e0489b451ce4ece14d265ead2",
            "patch": "@@ -231,6 +231,38 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.vit.modeling_vit.ViTSdpaSelfAttention with ViT->Dinov2\n+class Dinov2SdpaSelfAttention(Dinov2SelfAttention):\n+    def __init__(self, config: Dinov2Config) -> None:\n+        super().__init__(config)\n+        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n+\n+    def forward(\n+        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            head_mask,\n+            self.attention_probs_dropout_prob if self.training else 0.0,\n+            is_causal=False,\n+            scale=None,\n+        )\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        return context_layer, None\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Dinov2\n class Dinov2SelfOutput(nn.Module):\n     \"\"\"\n@@ -290,6 +322,13 @@ def forward(\n         return outputs\n \n \n+# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->Dinov2\n+class Dinov2SdpaAttention(Dinov2Attention):\n+    def __init__(self, config: Dinov2Config) -> None:\n+        super().__init__(config)\n+        self.attention = Dinov2SdpaSelfAttention(config)\n+\n+\n class Dinov2LayerScale(nn.Module):\n     def __init__(self, config) -> None:\n         super().__init__()\n@@ -371,14 +410,20 @@ def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n         return self.weights_out(hidden)\n \n \n+DINOV2_ATTENTION_CLASSES = {\n+    \"eager\": Dinov2Attention,\n+    \"sdpa\": Dinov2SdpaAttention,\n+}\n+\n+\n class Dinov2Layer(nn.Module):\n     \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n \n     def __init__(self, config: Dinov2Config) -> None:\n         super().__init__()\n \n         self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.attention = Dinov2Attention(config)\n+        self.attention = DINOV2_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.layer_scale1 = Dinov2LayerScale(config)\n         self.drop_path = Dinov2DropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n \n@@ -485,6 +530,7 @@ class Dinov2PreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dinov2SwiGLUFFN\"]\n+    _supports_sdpa = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "5caa3baec1a2cae11ee8b0184561aaaa2227c9c6",
            "filename": "tests/models/dinov2/test_modeling_dinov2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/78b2929c0554b79e0489b451ce4ece14d265ead2/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78b2929c0554b79e0489b451ce4ece14d265ead2/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py?ref=78b2929c0554b79e0489b451ce4ece14d265ead2",
            "patch": "@@ -65,6 +65,8 @@ def __init__(\n         type_sequence_label_size=10,\n         initializer_range=0.02,\n         scope=None,\n+        attn_implementation=\"eager\",\n+        mask_ratio=0.5,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -83,10 +85,14 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.initializer_range = initializer_range\n         self.scope = scope\n+        self.attn_implementation = attn_implementation\n+        self.mask_ratio = mask_ratio\n \n         # in Dinov2, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n         num_patches = (image_size // patch_size) ** 2\n         self.seq_length = num_patches + 1\n+        self.num_masks = int(self.mask_ratio * self.seq_length)\n+        self.mask_length = num_patches\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n@@ -113,6 +119,7 @@ def get_config(self):\n             attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n             is_decoder=False,\n             initializer_range=self.initializer_range,\n+            attn_implementation=self.attn_implementation,\n         )\n \n     def create_and_check_model(self, config, pixel_values, labels):"
        }
    ],
    "stats": {
        "total": 57,
        "additions": 55,
        "deletions": 2
    }
}