{
    "author": "baldassarreFe",
    "message": "Add usage example for DINOv2 (#37398)\n\n* Add usage example for DINOv2\n\n* More explicit shape names\n\n* More verbose text\n\n* Moved example to Notes section\n\n* Indentation",
    "sha": "e94a4807df45ec2967bb7f88c3d008f40fc9d550",
    "files": [
        {
            "sha": "749d20d00602451aed65d89ef64382febdd5479e",
            "filename": "docs/source/en/model_doc/dinov2.md",
            "status": "modified",
            "additions": 62,
            "deletions": 27,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/e94a4807df45ec2967bb7f88c3d008f40fc9d550/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e94a4807df45ec2967bb7f88c3d008f40fc9d550/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md?ref=e94a4807df45ec2967bb7f88c3d008f40fc9d550",
            "patch": "@@ -111,33 +111,68 @@ print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n \n ## Notes\n \n-- Use [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) to speedup inference. However, it will produce some mismatched elements. The difference between the original and traced model is 1e-4.\n-\n-    ```py\n-    import torch\n-    from transformers import AutoImageProcessor, AutoModel\n-    from PIL import Image\n-    import requests\n-\n-    url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n-    image = Image.open(requests.get(url, stream=True).raw)\n-\n-    processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n-    model = AutoModel.from_pretrained('facebook/dinov2-base')\n-\n-    inputs = processor(images=image, return_tensors=\"pt\")\n-    outputs = model(**inputs)\n-    last_hidden_states = outputs[0]\n-\n-    # We have to force return_dict=False for tracing\n-    model.config.return_dict = False\n-\n-    with torch.no_grad():\n-        traced_model = torch.jit.trace(model, [inputs.pixel_values])\n-        traced_outputs = traced_model(inputs.pixel_values)\n-\n-    print((last_hidden_states - traced_outputs[0]).abs().max())\n-    ```\n+- The example below shows how to split the output tensor into:\n+  - one embedding for the whole image, commonly referred to as a `CLS` token,\n+    useful for classification and retrieval\n+  - a set of local embeddings, one for each `14x14` patch of the input image,\n+    useful for dense tasks, such as semantic segmentation\n+\n+  ```py\n+  from transformers import AutoImageProcessor, AutoModel\n+  from PIL import Image\n+  import requests\n+  \n+  url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+  image = Image.open(requests.get(url, stream=True).raw)\n+  print(image.height, image.width)  # [480, 640]\n+  \n+  processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n+  model = AutoModel.from_pretrained('facebook/dinov2-base')\n+  patch_size = model.config.patch_size\n+  \n+  inputs = processor(images=image, return_tensors=\"pt\")\n+  print(inputs.pixel_values.shape)  # [1, 3, 224, 224]\n+  batch_size, rgb, img_height, img_width = inputs.pixel_values.shape\n+  num_patches_height, num_patches_width = img_height // patch_size, img_width // patch_size\n+  num_patches_flat = num_patches_height * num_patches_width\n+  \n+  outputs = model(**inputs)\n+  last_hidden_states = outputs[0]\n+  print(last_hidden_states.shape)  # [1, 1 + 256, 768]\n+  assert last_hidden_states.shape == (batch_size, 1 + num_patches_flat, model.config.hidden_size)\n+  \n+  cls_token = last_hidden_states[:, 0, :]\n+  patch_features = last_hidden_states[:, 1:, :].unflatten(1, (num_patches_height, num_patches_width))\n+  ```\n+\n+- Use [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) to speedup inference.\n+  However, it will produce some mismatched elements. The difference between the original and traced model is 1e-4.\n+\n+  ```py\n+  import torch\n+  from transformers import AutoImageProcessor, AutoModel\n+  from PIL import Image\n+  import requests\n+  \n+  url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+  image = Image.open(requests.get(url, stream=True).raw)\n+  \n+  processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n+  model = AutoModel.from_pretrained('facebook/dinov2-base')\n+  \n+  inputs = processor(images=image, return_tensors=\"pt\")\n+  outputs = model(**inputs)\n+  last_hidden_states = outputs[0]\n+  \n+  # We have to force return_dict=False for tracing\n+  model.config.return_dict = False\n+  \n+  with torch.no_grad():\n+      traced_model = torch.jit.trace(model, [inputs.pixel_values])\n+      traced_outputs = traced_model(inputs.pixel_values)\n+  \n+  print((last_hidden_states - traced_outputs[0]).abs().max())\n+  ```\n \n ## Dinov2Config\n "
        }
    ],
    "stats": {
        "total": 89,
        "additions": 62,
        "deletions": 27
    }
}