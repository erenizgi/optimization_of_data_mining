{
    "author": "ArthurZucker",
    "message": "fix tekken pattern matching (#42363)\n\n* fix tekken pattern matching\n\n* add a test\n\n* up\n\n* up\n\n* style",
    "sha": "f221a3b46b4ab5b5efe24f0120446af396f8fc4b",
    "files": [
        {
            "sha": "7647576914c119d5b49517d4b5a5681a81b02a76",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f221a3b46b4ab5b5efe24f0120446af396f8fc4b/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f221a3b46b4ab5b5efe24f0120446af396f8fc4b/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=f221a3b46b4ab5b5efe24f0120446af396f8fc4b",
            "patch": "@@ -2110,7 +2110,7 @@ def from_pretrained(\n         if \"tokenizer_file\" in vocab_files and not re.search(vocab_files[\"tokenizer_file\"], \"\".join(remote_files)):\n             # mistral tokenizer names are different, but we can still convert them if\n             # mistral common is not there\n-            other_pattern = re.escape(\"tekken.json|tokenizer.model.*\")\n+            other_pattern = r\"tekken\\.json|tokenizer\\.model\\.*\"\n             if match := re.search(other_pattern, \"\\n\".join(remote_files)):\n                 vocab_files[\"vocab_file\"] = match.group()\n "
        },
        {
            "sha": "c89ce4d09e4d716889e14704e799f1d655d6434c",
            "filename": "tests/models/auto/test_tokenization_auto.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f221a3b46b4ab5b5efe24f0120446af396f8fc4b/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f221a3b46b4ab5b5efe24f0120446af396f8fc4b/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_tokenization_auto.py?ref=f221a3b46b4ab5b5efe24f0120446af396f8fc4b",
            "patch": "@@ -12,13 +12,15 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import importlib\n import json\n import os\n import shutil\n import sys\n import tempfile\n import unittest\n from pathlib import Path\n+from unittest import mock\n \n import pytest\n \n@@ -181,6 +183,21 @@ def test_from_pretrained_use_fast_toggle(self):\n         )\n         self.assertIsInstance(AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\"), BertTokenizerFast)\n \n+    @require_tokenizers\n+    def test_voxtral_tokenizer_converts_from_tekken(self):\n+        repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+        tokenization_auto = transformers.models.auto.tokenization_auto\n+        with (\n+            mock.patch(\"transformers.utils.import_utils.is_mistral_common_available\", return_value=False),\n+            mock.patch(\"transformers.models.auto.tokenization_auto.is_mistral_common_available\", return_value=False),\n+        ):\n+            tokenization_auto = importlib.reload(tokenization_auto)\n+            tokenizer = tokenization_auto.AutoTokenizer.from_pretrained(repo_id)  # should not raise\n+\n+        self.assertIsInstance(tokenizer, PreTrainedTokenizerFast)\n+        self.assertTrue(tokenizer.is_fast)\n+        self.assertGreater(len(tokenizer(\"Voxtral\")[\"input_ids\"]), 0)\n+\n     @require_tokenizers\n     def test_do_lower_case(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", do_lower_case=False)"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 18,
        "deletions": 1
    }
}