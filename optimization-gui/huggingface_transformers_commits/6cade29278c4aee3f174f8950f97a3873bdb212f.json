{
    "author": "molbap",
    "message": "Add LongCat-Flash (#40730)\n\n* working draft for LongCat\n\n* BC changes to deepseek_v3 for modular\n\n* format\n\n* various modularities\n\n* better tp plan\n\n* better init\n\n* minor changes\n\n* make modular better\n\n* clean up patterns\n\n* Revert a couple of modular commits, because we won't convert in the end\n\n* make things explicit.\n\n* draft test\n\n* toctree, tests and imports\n\n* drop\n\n* woops\n\n* make better things\n\n* update test\n\n* update\n\n* fixes\n\n* style and CI\n\n* convert stuff\n\n* up\n\n* ah, yes, that\n\n* enable gen tests\n\n* fix cache shape in test (sum of 2 things)\n\n* fix tests\n\n* comments\n\n* re-Identitise\n\n* minimize changes\n\n* better defaults\n\n* modular betterment\n\n* fix configuration, add documentation\n\n* fix init\n\n* add integration tests\n\n* add info\n\n* simplify\n\n* update slow tests\n\n* fix\n\n* style\n\n* some additional long tests\n\n* cpu-only long test\n\n* fix last tests?\n\n* urg\n\n* cleaner tests why not\n\n* fix\n\n* improve slow tests, no skip\n\n* style\n\n* don't upcast\n\n* one skip\n\n* finally fix parallelism",
    "sha": "6cade29278c4aee3f174f8950f97a3873bdb212f",
    "files": [
        {
            "sha": "65411024d4a3f7bc5208e3decd53c8d1559f75af",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -559,6 +559,8 @@\n         title: Llama2\n       - local: model_doc/llama3\n         title: Llama3\n+      - local: model_doc/longcat_flash\n+        title: LongCatFlash\n       - local: model_doc/longformer\n         title: Longformer\n       - local: model_doc/longt5"
        },
        {
            "sha": "b2c2d7a0064698781195ef2880bd1019e26b63f1",
            "filename": "docs/source/en/model_doc/longcat_flash.md",
            "status": "added",
            "additions": 128,
            "deletions": 0,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/docs%2Fsource%2Fen%2Fmodel_doc%2Flongcat_flash.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/docs%2Fsource%2Fen%2Fmodel_doc%2Flongcat_flash.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongcat_flash.md?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -0,0 +1,128 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on 2025-09-01 and added to Hugging Face Transformers on 2025-09-15.*\n+\n+\n+# LongCatFlash\n+\n+## Overview\n+\n+The LongCatFlash model was proposed in [LongCat-Flash Technical Report](https://huggingface.co/papers/2509.01322) by the Meituan LongCat Team.\n+LongCat-Flash is a 560B parameter Mixture-of-Experts (MoE) model that activates 18.6B-31.3B parameters dynamically (average ~27B). The model features a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and advanced reasoning capabilities.\n+\n+The abstract from the paper is the following:\n+\n+*We present LongCat-Flash, a 560 billion parameter Mixture-of-Experts (MoE) language model featuring a dynamic computation mechanism that activates 18.6B-31.3B parameters based on context (average ~27B). The model incorporates a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and demonstrates strong performance across multiple benchmarks including 89.71% accuracy on MMLU and exceptional agentic tool use capabilities.*\n+\n+Tips:\n+\n+- LongCat-Flash uses a unique shortcut-connected MoE architecture that enables faster inference compared to traditional MoE models\n+- The model supports up to 128k context length for long-form tasks\n+- Dynamic parameter activation makes it computationally efficient while maintaining high performance\n+- Best suited for applications requiring strong reasoning, coding, and tool-calling capabilities\n+- The MoE architecture includes zero experts (nn.Identity modules) which act as skip connections, allowing tokens to bypass expert computation when appropriate\n+\n+This model was contributed by [Molbap](https://huggingface.co/Molbap).\n+The original code can be found [here](https://huggingface.co/meituan-longcat/LongCat-Flash-Chat).\n+\n+## Usage examples\n+\n+The model is large: you will need 2x8 H100 to run inference.\n+```python\n+# launch_longcat.py\n+from transformers import LongcatFlashForCausalLM, AutoTokenizer\n+import torch\n+\n+model_id = \"meituan-longcat/LongCat-Flash-Chat\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+chat = [\n+      {\"role\": \"user\", \"content\": \"Hello! What is the capital of France? What can you tell me about it?\"},\n+]\n+\n+model = LongcatFlashForCausalLM.from_pretrained(\n+      model_id,\n+      tp_plan=\"auto\",\n+      dtype=torch.bfloat16,\n+      )\n+\n+inputs = tokenizer.apply_chat_template(\n+      chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n+\n+outputs = model.generate(inputs, max_new_tokens=30)\n+print(tokenizer.batch_decode(outputs))\n+```\n+\n+To run with TP, you will need torchrun: \n+\n+```bash\n+torchrun  --nproc_per_node=8 --nnodes=2 --node_rank=0 | 1  --rdzv-id <an_id> --rdzv-backend c10d --rdzv-endpoint $NODE_ID:$NODE_PORT  --log-dir ./logs_longcat launch_longcat.py\n+```\n+\n+And you'll get a nice generation:\n+```json\n+[Round 0] USER:Hello! What is the capital of France? What can you tell me about it? ASSISTANT:Hello! ðŸ˜Š The capital of France is Paris, one of the most famous and beloved cities in the world. Hereâ€™s a quick overview of what makes Paris special:\n+1. Iconic Landmarks\n+\n+    Eiffel Tower â€“ The global symbol of France, built in 1889 for the World's Fair.\n+    Notre-Dame Cathedral â€“ A masterpiece of Gothic architecture (currently under restoration after the 2019 fire).\n+    Louvre Museum â€“ The worldâ€™s largest art museum, home to the Mona Lisa and Venus de Milo.\n+    SacrÃ©-CÅ“ur Basilica â€“ A stunning white church atop Montmartre with panoramic views.\n+    Arc de Triomphe â€“ Honors French military victories, with the Tomb of the Unknown Soldier beneath it.\n+    Champs-Ã‰lysÃ©es â€“ A glamorous avenue leading to the Arc de Triomphe, lined with shops and cafÃ©s.\n+\n+2. Culture & Arts\n+\n+    Paris is the \"City of Light\" (La Ville LumiÃ¨re), a nickname from its early adoption of street lighting and its role as a center of enlightenment.\n+    Itâ€™s a global hub for fashion (haute couture, Paris Fashion Week) and art (Impressionism, Picasso, Dali).\n+    Famous literary figures like Hemingway, Fitzgerald, and Sartre lived and wrote here.\n+\n+3. Food & Cuisine\n+\n+    Croissants, baguettes, macarons, and crÃ¨me brÃ»lÃ©e are just a few of its culinary delights.\n+    Paris has over 100 Michelin-starred restaurants and countless cozy bistros.\n+    The MarchÃ© dâ€™Aligre and Rue Mouffetard are great for fresh produce and local flavors.\n+\n+4. History & Politics\n+\n+    Founded in the 3rd century BC by the Parisii tribe, it became a major European city under the Romans.\n+    The French Revolution (1789â€“1799) began here, leading to the fall of the monarchy.\n+    Today, itâ€™s the political and economic heart of France, housing the French Presidentâ€™s residence (Ã‰lysÃ©e Palace) and the National Assembly.\n+\n+**\n+```\n+\n+## LongcatFlashConfig\n+\n+[[autodoc]] LongcatFlashConfig\n+\n+## LongcatFlashPreTrainedModel\n+\n+[[autodoc]] LongcatFlashPreTrainedModel\n+    - forward\n+\n+## LongcatFlashModel\n+\n+[[autodoc]] LongcatFlashModel\n+    - forward\n+\n+## LongcatFlashForCausalLM\n+\n+[[autodoc]] LongcatFlashForCausalLM"
        },
        {
            "sha": "18d74ade4126af5417337d0ae74f6e0266b8165c",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -190,6 +190,7 @@\n     from .llava_next import *\n     from .llava_next_video import *\n     from .llava_onevision import *\n+    from .longcat_flash import *\n     from .longformer import *\n     from .longt5 import *\n     from .luke import *"
        },
        {
            "sha": "a9303913e86194940387ca7942497281e7d91c44",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -230,6 +230,7 @@\n         (\"llava_next\", \"LlavaNextConfig\"),\n         (\"llava_next_video\", \"LlavaNextVideoConfig\"),\n         (\"llava_onevision\", \"LlavaOnevisionConfig\"),\n+        (\"longcat_flash\", \"LongcatFlashConfig\"),\n         (\"longformer\", \"LongformerConfig\"),\n         (\"longt5\", \"LongT5Config\"),\n         (\"luke\", \"LukeConfig\"),\n@@ -665,6 +666,7 @@\n         (\"llava_next\", \"LLaVA-NeXT\"),\n         (\"llava_next_video\", \"LLaVa-NeXT-Video\"),\n         (\"llava_onevision\", \"LLaVA-Onevision\"),\n+        (\"longcat_flash\", \"LongCatFlash\"),\n         (\"longformer\", \"Longformer\"),\n         (\"longt5\", \"LongT5\"),\n         (\"luke\", \"LUKE\"),"
        },
        {
            "sha": "571f654a9499f2c86831d20f99653bd8a8c10d6a",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -230,6 +230,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"llava_next\", \"LlavaNextModel\"),\n         (\"llava_next_video\", \"LlavaNextVideoModel\"),\n         (\"llava_onevision\", \"LlavaOnevisionModel\"),\n+        (\"longcat_flash\", \"LongcatFlashModel\"),\n         (\"longformer\", \"LongformerModel\"),\n         (\"longt5\", \"LongT5Model\"),\n         (\"luke\", \"LukeModel\"),\n@@ -685,6 +686,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"llama\", \"LlamaForCausalLM\"),\n         (\"llama4\", \"Llama4ForCausalLM\"),\n         (\"llama4_text\", \"Llama4ForCausalLM\"),\n+        (\"longcat_flash\", \"LongcatFlashForCausalLM\"),\n         (\"mamba\", \"MambaForCausalLM\"),\n         (\"mamba2\", \"Mamba2ForCausalLM\"),\n         (\"marian\", \"MarianForCausalLM\"),"
        },
        {
            "sha": "a9a9429d9d05c06395282a24be9c2b60057359ed",
            "filename": "src/transformers/models/longcat_flash/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2F__init__.py?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -0,0 +1,29 @@\n+# coding=utf-8\n+# Copyright 2025 Meituan and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_longcat_flash import *\n+    from .modeling_longcat_flash import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "4c5930db8f3ab1e26236f6db778859f77736764f",
            "filename": "src/transformers/models/longcat_flash/configuration_longcat_flash.py",
            "status": "added",
            "additions": 235,
            "deletions": 0,
            "changes": 235,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -0,0 +1,235 @@\n+# coding=utf-8\n+# Copyright 2025 Meituan and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"LongCat Flash model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class LongcatFlashConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`LongcatFlashModel`]. It is used to instantiate\n+    a LongCat Flash model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the LongCat Flash architecture.\n+    e.g. [meituan-longcat/LongCat-Flash-Chat](https://huggingface.co/meituan-longcat/LongCat-Flash-Chat)\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 131072):\n+            Vocabulary size of the LongCat Flash model. Defines the number of different tokens that can be represented by the\n+            `input_ids` passed when calling [`LongcatFlashModel`]\n+        hidden_size (`int`, *optional*, defaults to 6144):\n+            Dimension of the hidden representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 56):\n+            Number of hidden layers in the Transformer decoder.\n+        num_layers (`int`, *optional*, defaults to 28):\n+            number of layers, each with 2 sublayers.\n+        num_attention_heads (`int`, *optional*, defaults to 64):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting from a multi-head checkpoint to a GQA checkpoint, each group key and value head should be\n+            constructed by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with. Typically set this to something large\n+            just in case (e.g., 512 or 1024 or 2048).\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon value used by the RMS normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie input and output embeddings.\n+        rope_theta (`float`, *optional*, defaults to 10000000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n+            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n+            `{\"type\": strategy name, \"factor\": scaling factor}`.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        ffn_hidden_size (`int`, *optional*, defaults to 12288):\n+            Dimension of the MLP representations.\n+        q_lora_rank (`int`, *optional*, defaults to 1536):\n+            The rank of the query LoRA projection in MLA (Multi-head Latent Attention).\n+        kv_lora_rank (`int`, *optional*, defaults to 512):\n+            The rank of the key-value LoRA projection in MLA.\n+        qk_nope_head_dim (`int`, *optional*, defaults to 128):\n+            The dimension of the non-position encoding part of query/key heads.\n+        qk_rope_head_dim (`int`, *optional*, defaults to 64):\n+            The dimension of the RoPE part of query/key heads.\n+        head_dim (`int`, *optional*, defaults to 64):\n+            Standard dimension of qk heads, unused except for CI.\n+        v_head_dim (`int`, *optional*, defaults to 128):\n+            The dimension of value heads.\n+        qk_head_dim (`int`, *optional*):\n+            The total dimension of query/key heads. If not specified, set to `qk_nope_head_dim + qk_rope_head_dim`.\n+        moe_topk (`int`, *optional*, defaults to 12):\n+            Number of experts to route to for each token in the MoE layer.\n+        n_routed_experts (`int`, *optional*, defaults to 512):\n+            Number of routed experts in the MoE layer.\n+        zero_expert_num (`int`, *optional*, defaults to 256):\n+            Number of zero experts (identity function) to add to the expert pool.\n+        expert_ffn_hidden_size (`int`, *optional*, defaults to 2048):\n+            Hidden size of individual expert FFN layers.\n+        routed_scaling_factor (`float`, *optional*, defaults to 6.0):\n+            Scaling factor applied to the routing weights.\n+\n+    ```python\n+    >>> from transformers import LongcatFlashModel, LongcatFlashConfig\n+\n+    >>> # Initializing a LongCat Flash style configuration\n+    >>> configuration = LongcatFlashConfig()\n+\n+    >>> # Initializing a model from the configuration\n+    >>> model = LongcatFlashModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"longcat_flash\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.*.q_b_proj\": \"colwise\",\n+        \"layers.*.self_attn.*.kv_b_proj\": \"colwise\",\n+        \"layers.*.self_attn.*.o_proj\": \"rowwise\",\n+        \"layers.*.mlps.*.gate_proj\": \"colwise\",\n+        \"layers.*.mlps.*.up_proj\": \"colwise\",\n+        \"layers.*.mlps.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n+        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+    }\n+\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=131072,\n+        hidden_size=6144,\n+        num_hidden_layers=56,\n+        num_layers=28,\n+        num_attention_heads=64,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=131072,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        pad_token_id=None,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        tie_word_embeddings=False,\n+        rope_theta=10000000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        ffn_hidden_size=12288,\n+        q_lora_rank=1536,\n+        kv_lora_rank=512,\n+        qk_nope_head_dim=128,\n+        qk_rope_head_dim=64,\n+        head_dim=64,\n+        v_head_dim=128,\n+        qk_head_dim=None,\n+        moe_topk=12,\n+        n_routed_experts=512,\n+        zero_expert_num=256,\n+        expert_ffn_hidden_size=2048,\n+        routed_scaling_factor=6.0,\n+        **kwargs,\n+    ):\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        if qk_head_dim is None:\n+            qk_head_dim = qk_nope_head_dim + qk_rope_head_dim\n+\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.num_layers = num_layers\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        self.ffn_hidden_size = ffn_hidden_size\n+\n+        self.q_lora_rank = q_lora_rank\n+        self.kv_lora_rank = kv_lora_rank\n+        self.qk_nope_head_dim = qk_nope_head_dim\n+        self.qk_rope_head_dim = qk_rope_head_dim\n+        self.v_head_dim = v_head_dim\n+        self.qk_head_dim = qk_head_dim\n+        self.head_dim = head_dim\n+\n+        self.moe_topk = moe_topk\n+        self.n_routed_experts = n_routed_experts\n+        self.zero_expert_num = zero_expert_num\n+        self.expert_ffn_hidden_size = expert_ffn_hidden_size\n+        self.routed_scaling_factor = routed_scaling_factor\n+\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+\n+        if self.rope_scaling is not None:\n+            for key in [\"beta_fast\", \"beta_slow\", \"factor\"]:\n+                if key in self.rope_scaling:\n+                    self.rope_scaling[key] = float(self.rope_scaling[key])\n+\n+        rope_config_validation(self)\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"LongcatFlashConfig\"]"
        },
        {
            "sha": "87e812852b379593fc6b835fd7265027b27dc432",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "added",
            "additions": 684,
            "deletions": 0,
            "changes": 684,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -0,0 +1,684 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/longcat_flash/modular_longcat_flash.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_longcat_flash.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Meituan and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from .configuration_longcat_flash import LongcatFlashConfig\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class LongcatFlashRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        LongcatFlashRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class LongcatFlashRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: LongcatFlashConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class LongcatFlashMLP(nn.Module):\n+    def __init__(self, config, hidden_size=None, intermediate_size=None):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n+        self.intermediate_size = config.ffn_hidden_size if intermediate_size is None else intermediate_size\n+\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class LongcatFlashTopkRouter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+\n+        self.top_k = config.moe_topk\n+        self.n_routed_experts = config.n_routed_experts + (config.zero_expert_num or 0)\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n+        self.router_bias = getattr(config, \"router_bias\", False)\n+        self.classifier = nn.Linear(config.hidden_size, self.n_routed_experts, bias=self.router_bias)\n+\n+    @torch.no_grad()\n+    def get_topk_indices(self, scores):\n+        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n+        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n+        return topk_indices\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.view(-1, self.config.hidden_size)\n+        router_logits = F.linear(hidden_states.type(torch.float32), self.classifier.weight.type(torch.float32))\n+        scores = router_logits.softmax(dim=-1)\n+        topk_indices = self.get_topk_indices(scores)\n+        topk_weights = scores.gather(1, topk_indices)\n+        topk_weights = topk_weights * self.routed_scaling_factor\n+        return topk_indices, topk_weights\n+\n+\n+class LongcatFlashMoE(nn.Module):\n+    \"\"\"\n+    A mixed expert module containing zero compute (identity) experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.intermediate_size = config.expert_ffn_hidden_size\n+        self.config = config\n+\n+        self.experts = nn.ModuleList(\n+            [LongcatFlashMLP(config, intermediate_size=self.intermediate_size) for _ in range(config.n_routed_experts)]\n+            + [nn.Identity() for _ in range(config.zero_expert_num)]\n+        )\n+\n+        self.router = LongcatFlashTopkRouter(config)\n+\n+    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n+        r\"\"\"\n+        CALL FOR CONTRIBUTION! I don't have time to optimise this right now, but expert weights need to be fused\n+        to not have to do a loop here (deepseek has 256 experts soooo yeah).\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states, dtype=topk_weights.dtype)\n+        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts))\n+        expert_mask = expert_mask.permute(2, 0, 1)\n+\n+        for expert_idx in range(len(self.experts)):\n+            expert = self.experts[expert_idx]\n+            mask = expert_mask[expert_idx]\n+            token_indices, weight_indices = torch.where(mask)\n+\n+            if token_indices.numel() > 0:\n+                expert_weights = topk_weights[token_indices, weight_indices]\n+                expert_input = hidden_states[token_indices]\n+                expert_output = expert(expert_input)\n+                weighted_output = expert_output * expert_weights.unsqueeze(-1)\n+                final_hidden_states.index_add_(0, token_indices, weighted_output)\n+\n+        # in original deepseek, the output of the experts are gathered once we leave this module\n+        # thus the moe module is itelsf an IsolatedParallel module\n+        # and all expert are \"local\" meaning we shard but we don't gather\n+        return final_hidden_states.type(hidden_states.dtype)\n+\n+    def forward(self, hidden_states):\n+        orig_shape = hidden_states.shape\n+        topk_indices, topk_weights = self.router(hidden_states)\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        return hidden_states\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def apply_rotary_pos_emb_interleave(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    r\"\"\"\n+    TODO let's just use the original freqcis computation to not have the view\n+    transpose + reshape! This is not optimized!\n+    Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`):\n+            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n+            used to pass offsetted position ids when working with a KV-cache.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    b, h, s, d = q.shape\n+    q = q.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n+\n+    b, h, s, d = k.shape\n+    k = k.view(b, h, s, d // 2, 2).transpose(4, 3).reshape(b, h, s, d)\n+\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def yarn_get_mscale(scale=1, mscale=1):\n+    if scale <= 1:\n+        return 1.0\n+    return 0.1 * mscale * math.log(scale) + 1.0\n+\n+\n+class LongcatFlashMLA(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.attention_dropout = config.attention_dropout\n+        self.num_heads = config.num_attention_heads\n+        self.rope_theta = config.rope_theta\n+        self.q_lora_rank = config.q_lora_rank\n+        self.qk_rope_head_dim = config.qk_rope_head_dim\n+        self.kv_lora_rank = config.kv_lora_rank\n+        self.v_head_dim = config.v_head_dim\n+        self.qk_nope_head_dim = config.qk_nope_head_dim\n+        self.qk_head_dim = config.qk_head_dim\n+\n+        self.is_causal = True\n+        if self.q_lora_rank is None:\n+            self.q_proj = nn.Linear(config.hidden_size, self.num_heads * self.qk_head_dim, bias=False)\n+        else:\n+            self.q_a_proj = nn.Linear(config.hidden_size, config.q_lora_rank, bias=config.attention_bias)\n+            self.q_a_layernorm = LongcatFlashRMSNorm(config.q_lora_rank)\n+            self.q_b_proj = nn.Linear(config.q_lora_rank, self.num_heads * self.qk_head_dim, bias=False)\n+\n+        self.kv_a_proj_with_mqa = nn.Linear(\n+            config.hidden_size,\n+            self.kv_lora_rank + self.qk_rope_head_dim,\n+            bias=config.attention_bias,\n+        )\n+        self.kv_a_layernorm = LongcatFlashRMSNorm(self.kv_lora_rank)\n+        self.kv_b_proj = nn.Linear(\n+            self.kv_lora_rank,\n+            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),\n+            bias=False,\n+        )\n+\n+        self.o_proj = nn.Linear(\n+            self.num_heads * self.v_head_dim,\n+            config.hidden_size,\n+            bias=config.attention_bias,\n+        )\n+\n+        self.scaling = self.qk_head_dim ** (-0.5)\n+        if self.config.rope_scaling is not None:\n+            mscale_all_dim = self.config.rope_scaling.get(\"mscale_all_dim\", 0)\n+            scaling_factor = self.config.rope_scaling[\"factor\"]\n+            if mscale_all_dim:\n+                mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n+                self.scaling = self.scaling * mscale * mscale\n+\n+        self.mla_scale_q_lora = (config.hidden_size / self.q_lora_rank) ** 0.5\n+        self.mla_scale_kv_lora = (config.hidden_size / self.kv_lora_rank) ** 0.5\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        batch_size, seq_length = hidden_states.shape[:-1]\n+        query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\n+        key_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)\n+        # we always do a lora for queries as well\n+        q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))\n+        q_states = q_states.view(query_shape).transpose(1, 2)\n+        q_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n+\n+        compressed_kv = self.kv_a_proj_with_mqa(hidden_states)\n+        k_pass, k_rot = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n+        k_pass = self.kv_a_layernorm(k_pass)\n+\n+        # apply LoRA scaling\n+        q_pass = q_pass * self.mla_scale_q_lora\n+        q_rot = q_rot * self.mla_scale_q_lora\n+        k_pass = k_pass * self.mla_scale_kv_lora\n+\n+        k_pass = self.kv_b_proj(k_pass).view(key_shape).transpose(1, 2)\n+        k_pass, value_states = torch.split(k_pass, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n+\n+        k_rot = k_rot.view(batch_size, 1, seq_length, self.qk_rope_head_dim)\n+\n+        cos, sin = position_embeddings\n+        q_rot, k_rot = apply_rotary_pos_emb_interleave(q_rot, k_rot, cos, sin)\n+        k_rot = k_rot.expand(*k_pass.shape[:-1], -1)\n+\n+        query_states = torch.cat((q_pass, q_rot), dim=-1)\n+        key_states = torch.cat((k_pass, k_rot), dim=-1)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+            value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+            attn_output = attn_output[:, :, :, : self.v_head_dim]\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class LongcatFlashDecoderLayer(GradientCheckpointingLayer):\n+    \"\"\"\n+    LongCat decoder layer with dual-sublayer + shortcut MoE architecture.\n+\n+    Each logical layer contains:\n+    - 2 attention sublayers (with layer indices: layer_idx*2, layer_idx*2+1)\n+    - 2 MLP sublayers\n+    - 1 shortcut MoE connection\n+    \"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__()\n+        self.layer_idx = layer_idx\n+        self.hidden_size = config.hidden_size\n+\n+        self.mlp = LongcatFlashMoE(config)\n+\n+        self.self_attn = nn.ModuleList([LongcatFlashMLA(config=config, layer_idx=layer_idx * 2 + i) for i in [0, 1]])\n+        self.mlps = nn.ModuleList([LongcatFlashMLP(config) for _ in [0, 1]])\n+        self.input_layernorm = nn.ModuleList(\n+            [LongcatFlashRMSNorm(config.hidden_size, eps=config.rms_norm_eps) for _ in [0, 1]]\n+        )\n+        self.post_attention_layernorm = nn.ModuleList(\n+            [LongcatFlashRMSNorm(config.hidden_size, eps=config.rms_norm_eps) for _ in [0, 1]]\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm[0](hidden_states)\n+\n+        hidden_states, _ = self.self_attn[0](\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm[0](hidden_states)\n+\n+        shortcut_mlp_output = self.mlp(hidden_states)\n+        hidden_states = self.mlps[0](hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # shortcut connection after second sublayer\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm[1](hidden_states)\n+\n+        hidden_states, _ = self.self_attn[1](\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm[1](hidden_states)\n+\n+        hidden_states = self.mlps[1](hidden_states)\n+        hidden_states = residual + hidden_states + shortcut_mlp_output\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class LongcatFlashPreTrainedModel(PreTrainedModel):\n+    config: LongcatFlashConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LongcatFlashDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _can_compile_fullgraph = False\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": LongcatFlashDecoderLayer,\n+        \"attentions\": LongcatFlashMLA,\n+    }\n+\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, LongcatFlashTopkRouter):\n+            module.classifier.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+\n+@auto_docstring\n+class LongcatFlashModel(LongcatFlashPreTrainedModel):\n+    _keys_to_ignore_on_load_unexpected = [r\"model\\.mtp.*\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [LongcatFlashDecoderLayer(config, layer_idx) for layer_idx in range(config.num_layers)]\n+        )\n+        self.norm = LongcatFlashRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = LongcatFlashRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+        # Each layer above has 2 sublayers, config hack to have a correct cache (to avoid a checkpoint change)\n+        self.head_dim = config.head_dim  # For CI happiness (we didn't convert so head_dim is not directly used) # noqa\n+\n+        self.config.num_hidden_layers = 2 * config.num_layers\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+            hidden_states=None,\n+            attentions=None,\n+        )\n+\n+\n+@auto_docstring\n+class LongcatFlashForCausalLM(LongcatFlashPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+    _keys_to_ignore_on_load_unexpected = [r\"model\\.mtp.*\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = LongcatFlashModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, LongcatFlashForCausalLM\n+\n+        >>> model = LongcatFlashForCausalLM.from_pretrained(\"meta-longcat_flash/LongcatFlash-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-longcat_flash/LongcatFlash-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"LongcatFlashPreTrainedModel\", \"LongcatFlashModel\", \"LongcatFlashForCausalLM\"]"
        },
        {
            "sha": "f58ca870aefcbb480393b8cae0a87da8a22aa7a3",
            "filename": "src/transformers/models/longcat_flash/modular_longcat_flash.py",
            "status": "added",
            "additions": 382,
            "deletions": 0,
            "changes": 382,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -0,0 +1,382 @@\n+# coding=utf-8\n+# Copyright 2025 Meituan and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, logging\n+from ..deepseek_v3.modeling_deepseek_v3 import (\n+    DeepseekV3Attention,\n+    DeepseekV3ForCausalLM,\n+    DeepseekV3MLP,\n+    DeepseekV3Model,\n+    DeepseekV3MoE,\n+    DeepseekV3PreTrainedModel,\n+    DeepseekV3RMSNorm,\n+    DeepseekV3RotaryEmbedding,\n+    DeepseekV3TopkRouter,\n+    apply_rotary_pos_emb_interleave,\n+    eager_attention_forward,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class LongcatFlashRMSNorm(DeepseekV3RMSNorm):\n+    pass\n+\n+\n+class LongcatFlashRotaryEmbedding(DeepseekV3RotaryEmbedding):\n+    pass\n+\n+\n+# TODO remap config key ffn_hidden_size -> intermediate_size\n+class LongcatFlashMLP(DeepseekV3MLP):\n+    def __init__(self, config, hidden_size=None, intermediate_size=None):\n+        super().__init__()\n+        self.intermediate_size = config.ffn_hidden_size if intermediate_size is None else intermediate_size\n+\n+\n+# TODO remap config key moe_topk -> num_experts_per_tok\n+class LongcatFlashTopkRouter(DeepseekV3TopkRouter):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.n_group\n+        del self.topk_group\n+        del self.weight\n+        del self.norm_topk_prob\n+\n+        self.top_k = config.moe_topk\n+        self.n_routed_experts = config.n_routed_experts + (config.zero_expert_num or 0)\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n+        self.router_bias = getattr(config, \"router_bias\", False)\n+        self.classifier = nn.Linear(config.hidden_size, self.n_routed_experts, bias=self.router_bias)\n+\n+    @torch.no_grad()\n+    def get_topk_indices(self, scores):\n+        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n+        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n+        return topk_indices\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.view(-1, self.config.hidden_size)\n+        router_logits = F.linear(hidden_states.type(torch.float32), self.classifier.weight.type(torch.float32))\n+        scores = router_logits.softmax(dim=-1)\n+        topk_indices = self.get_topk_indices(scores)\n+        topk_weights = scores.gather(1, topk_indices)\n+        topk_weights = topk_weights * self.routed_scaling_factor\n+        return topk_indices, topk_weights\n+\n+\n+# remap config key expert_ffn_hidden_size -> moe_intermediate_size\n+class LongcatFlashMoE(DeepseekV3MoE):\n+    \"\"\"\n+    A mixed expert module containing zero compute (identity) experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        self.intermediate_size = config.expert_ffn_hidden_size\n+        super().__init__(config)\n+        del self.gate\n+        del self.shared_experts\n+\n+        self.experts = nn.ModuleList(\n+            [LongcatFlashMLP(config, intermediate_size=self.intermediate_size) for _ in range(config.n_routed_experts)]\n+            + [nn.Identity() for _ in range(config.zero_expert_num)]\n+        )\n+\n+        self.router = LongcatFlashTopkRouter(config)\n+\n+    def forward(self, hidden_states):\n+        orig_shape = hidden_states.shape\n+        topk_indices, topk_weights = self.router(hidden_states)\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        return hidden_states\n+\n+\n+class LongcatFlashMLA(DeepseekV3Attention):\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+\n+        self.mla_scale_q_lora = (config.hidden_size / self.q_lora_rank) ** 0.5\n+        self.mla_scale_kv_lora = (config.hidden_size / self.kv_lora_rank) ** 0.5\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        batch_size, seq_length = hidden_states.shape[:-1]\n+        query_shape = (batch_size, seq_length, -1, self.qk_head_dim)\n+        key_shape = (batch_size, seq_length, -1, self.qk_nope_head_dim + self.v_head_dim)\n+        # we always do a lora for queries as well\n+        q_states = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))\n+        q_states = q_states.view(query_shape).transpose(1, 2)\n+        q_pass, q_rot = torch.split(q_states, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n+\n+        compressed_kv = self.kv_a_proj_with_mqa(hidden_states)\n+        k_pass, k_rot = torch.split(compressed_kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n+        k_pass = self.kv_a_layernorm(k_pass)\n+\n+        # apply LoRA scaling\n+        q_pass = q_pass * self.mla_scale_q_lora\n+        q_rot = q_rot * self.mla_scale_q_lora\n+        k_pass = k_pass * self.mla_scale_kv_lora\n+\n+        k_pass = self.kv_b_proj(k_pass).view(key_shape).transpose(1, 2)\n+        k_pass, value_states = torch.split(k_pass, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n+\n+        k_rot = k_rot.view(batch_size, 1, seq_length, self.qk_rope_head_dim)\n+\n+        cos, sin = position_embeddings\n+        q_rot, k_rot = apply_rotary_pos_emb_interleave(q_rot, k_rot, cos, sin)\n+        k_rot = k_rot.expand(*k_pass.shape[:-1], -1)\n+\n+        query_states = torch.cat((q_pass, q_rot), dim=-1)\n+        key_states = torch.cat((k_pass, k_rot), dim=-1)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+            value_states = F.pad(value_states, [0, self.qk_head_dim - self.v_head_dim])\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        if self.config._attn_implementation == \"flash_attention_2\" and self.qk_head_dim != self.v_head_dim:\n+            attn_output = attn_output[:, :, :, : self.v_head_dim]\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class LongcatFlashDecoderLayer(GradientCheckpointingLayer):\n+    \"\"\"\n+    LongCat decoder layer with dual-sublayer + shortcut MoE architecture.\n+\n+    Each logical layer contains:\n+    - 2 attention sublayers (with layer indices: layer_idx*2, layer_idx*2+1)\n+    - 2 MLP sublayers\n+    - 1 shortcut MoE connection\n+    \"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__()\n+        self.layer_idx = layer_idx\n+        self.hidden_size = config.hidden_size\n+\n+        self.mlp = LongcatFlashMoE(config)\n+\n+        self.self_attn = nn.ModuleList([LongcatFlashMLA(config=config, layer_idx=layer_idx * 2 + i) for i in [0, 1]])\n+        self.mlps = nn.ModuleList([LongcatFlashMLP(config) for _ in [0, 1]])\n+        self.input_layernorm = nn.ModuleList(\n+            [LongcatFlashRMSNorm(config.hidden_size, eps=config.rms_norm_eps) for _ in [0, 1]]\n+        )\n+        self.post_attention_layernorm = nn.ModuleList(\n+            [LongcatFlashRMSNorm(config.hidden_size, eps=config.rms_norm_eps) for _ in [0, 1]]\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm[0](hidden_states)\n+\n+        hidden_states, _ = self.self_attn[0](\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm[0](hidden_states)\n+\n+        shortcut_mlp_output = self.mlp(hidden_states)\n+        hidden_states = self.mlps[0](hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # shortcut connection after second sublayer\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm[1](hidden_states)\n+\n+        hidden_states, _ = self.self_attn[1](\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm[1](hidden_states)\n+\n+        hidden_states = self.mlps[1](hidden_states)\n+        hidden_states = residual + hidden_states + shortcut_mlp_output\n+\n+        return hidden_states\n+\n+\n+class LongcatFlashPreTrainedModel(DeepseekV3PreTrainedModel):\n+    _can_record_outputs = {\n+        \"hidden_states\": LongcatFlashDecoderLayer,\n+        \"attentions\": LongcatFlashMLA,\n+    }\n+\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, LongcatFlashTopkRouter):\n+            module.classifier.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+\n+class LongcatFlashModel(DeepseekV3Model):\n+    _keys_to_ignore_on_load_unexpected = [r\"model\\.mtp.*\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [LongcatFlashDecoderLayer(config, layer_idx) for layer_idx in range(config.num_layers)]\n+        )\n+        # Each layer above has 2 sublayers, config hack to have a correct cache (to avoid a checkpoint change)\n+        self.head_dim = config.head_dim  # For CI happiness (we didn't convert so head_dim is not directly used) # noqa\n+\n+        self.config.num_hidden_layers = 2 * config.num_layers\n+        self.norm = LongcatFlashRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = LongcatFlashRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ):\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+            hidden_states=None,\n+            attentions=None,\n+        )\n+\n+\n+class LongcatFlashForCausalLM(DeepseekV3ForCausalLM):\n+    _keys_to_ignore_on_load_unexpected = [r\"model\\.mtp.*\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = LongcatFlashModel(config)\n+\n+\n+__all__ = [\"LongcatFlashPreTrainedModel\", \"LongcatFlashModel\", \"LongcatFlashForCausalLM\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/longcat_flash/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/tests%2Fmodels%2Flongcat_flash%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/tests%2Fmodels%2Flongcat_flash%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongcat_flash%2F__init__.py?ref=6cade29278c4aee3f174f8950f97a3873bdb212f"
        },
        {
            "sha": "bc52e890ce0abb36061d42d28742abd447df3178",
            "filename": "tests/models/longcat_flash/test_modeling_longcat_flash.py",
            "status": "added",
            "additions": 473,
            "deletions": 0,
            "changes": 473,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cade29278c4aee3f174f8950f97a3873bdb212f/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cade29278c4aee3f174f8950f97a3873bdb212f/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py?ref=6cade29278c4aee3f174f8950f97a3873bdb212f",
            "patch": "@@ -0,0 +1,473 @@\n+# Copyright 2025 Meituan and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch LongcatFlash model.\"\"\"\n+\n+import copy\n+import tempfile\n+import unittest\n+\n+from parameterized import parameterized\n+from pytest import mark\n+\n+from transformers import LongcatFlashConfig, is_torch_available, set_seed\n+from transformers.testing_utils import (\n+    require_bitsandbytes,\n+    require_flash_attn,\n+    require_large_cpu_ram,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import AutoTokenizer, LongcatFlashForCausalLM, LongcatFlashModel\n+\n+\n+class LongcatFlashModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = LongcatFlashConfig\n+        base_model_class = LongcatFlashModel\n+        causal_lm_class = LongcatFlashForCausalLM\n+\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        seq_length=7,\n+        is_training=True,\n+        use_input_mask=True,\n+        use_labels=True,\n+        vocab_size=99,\n+        hidden_size=144,\n+        ffn_hidden_size=288,\n+        expert_ffn_hidden_size=48,\n+        num_layers=2,\n+        num_attention_heads=8,\n+        num_key_value_heads=8,\n+        kv_lora_rank=16,\n+        q_lora_rank=48,\n+        qk_rope_head_dim=4,\n+        v_head_dim=8,\n+        qk_nope_head_dim=8,\n+        head_dim=4,\n+        n_routed_experts=4,\n+        zero_expert_num=2,\n+        moe_topk=2,\n+        routed_scaling_factor=1.0,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=128,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        pad_token_id=3,\n+        type_sequence_label_size=2,\n+        num_labels=3,\n+        num_choices=4,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.ffn_hidden_size = ffn_hidden_size\n+        self.expert_ffn_hidden_size = expert_ffn_hidden_size\n+        self.num_layers = num_layers\n+        self.num_hidden_layers = 2 * num_layers  # for compatibility\n+        self.expected_num_hidden_layers = 3  # embedding + 2 layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.kv_lora_rank = kv_lora_rank\n+        self.q_lora_rank = q_lora_rank\n+        self.qk_rope_head_dim = qk_rope_head_dim\n+        self.v_head_dim = v_head_dim\n+        self.qk_nope_head_dim = qk_nope_head_dim\n+        self.head_dim = head_dim\n+        self.n_routed_experts = n_routed_experts\n+        self.zero_expert_num = zero_expert_num\n+        self.moe_topk = moe_topk\n+        self.routed_scaling_factor = routed_scaling_factor\n+        self.hidden_act = hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.num_labels = num_labels\n+        self.num_choices = num_choices\n+\n+    def get_config(self):\n+        return LongcatFlashConfig(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            ffn_hidden_size=self.ffn_hidden_size,\n+            expert_ffn_hidden_size=self.expert_ffn_hidden_size,\n+            num_layers=self.num_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            kv_lora_rank=self.kv_lora_rank,\n+            q_lora_rank=self.q_lora_rank,\n+            qk_rope_head_dim=self.qk_rope_head_dim,\n+            v_head_dim=self.v_head_dim,\n+            qk_nope_head_dim=self.qk_nope_head_dim,\n+            head_dim=self.head_dim,\n+            n_routed_experts=self.n_routed_experts,\n+            zero_expert_num=self.zero_expert_num,\n+            moe_topk=self.moe_topk,\n+            routed_scaling_factor=self.routed_scaling_factor,\n+            hidden_act=self.hidden_act,\n+            max_position_embeddings=self.max_position_embeddings,\n+            initializer_range=self.initializer_range,\n+            rms_norm_eps=self.rms_norm_eps,\n+            pad_token_id=self.pad_token_id,\n+        )\n+\n+    def create_and_check_model(\n+        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n+    ):\n+        model = LongcatFlashModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_for_causal_lm(\n+        self,\n+        config,\n+        input_ids,\n+        token_type_ids,\n+        input_mask,\n+        sequence_labels,\n+        token_labels,\n+        choice_labels,\n+        encoder_hidden_states=None,\n+        encoder_attention_mask=None,\n+    ):\n+        model = LongcatFlashForCausalLM(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        input_mask = None\n+        if self.use_input_mask:\n+            input_mask = torch.tril(torch.ones(self.batch_size, self.seq_length)).to(torch_device)\n+\n+        token_type_ids = None\n+\n+        sequence_labels = None\n+        token_labels = None\n+        choice_labels = None\n+        if self.use_labels:\n+            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n+            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n+\n+        config = self.get_config()\n+\n+        return (\n+            config,\n+            input_ids,\n+            token_type_ids,\n+            input_mask,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels = config_and_inputs\n+\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class LongcatFlashModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (LongcatFlashModel, LongcatFlashForCausalLM) if is_torch_available() else ()\n+    all_generative_model_classes = (LongcatFlashForCausalLM,) if is_torch_available() else ()\n+\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": LongcatFlashModel,\n+            \"text-generation\": LongcatFlashForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    model_split_percents = [0.5, 0.8]\n+\n+    test_headmasking = False\n+    test_pruning = False\n+\n+    model_tester_class = LongcatFlashModelTester\n+\n+    def setUp(self):\n+        self.model_tester = LongcatFlashModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=LongcatFlashConfig, hidden_size=37, num_attention_heads=3)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_for_causal_lm(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_causal_lm(*config_and_inputs)\n+\n+    @unittest.skip(\"LongcatFlash buffers include complex numbers, which breaks this test\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n+    @unittest.skip(\"LongcatFlash buffers include complex numbers, which breaks this test\")\n+    def test_save_load_fast_init_to_base(self):\n+        pass\n+\n+    def test_past_key_values_format(self):\n+        config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+        batch_size, seq_length = inputs[\"input_ids\"].shape\n+\n+        k_embed_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n+        v_embed_dim = config.v_head_dim\n+\n+        self_attention_keys_shape = (batch_size, config.num_key_value_heads, seq_length, k_embed_dim)\n+        self_attention_values_shape = (batch_size, config.num_key_value_heads, seq_length, v_embed_dim)\n+\n+        num_hidden_layers = config.num_hidden_layers\n+        all_cache_shapes = [[self_attention_keys_shape, self_attention_values_shape] for _ in range(num_hidden_layers)]\n+\n+        super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n+\n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        from transformers.cache_utils import Cache\n+\n+        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n+\n+        k_embed_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n+        v_embed_dim = config.v_head_dim\n+\n+        expected_key_shape = (batch_size, config.num_key_value_heads, cache_length, k_embed_dim)\n+        expected_value_shape = (batch_size, config.num_key_value_heads, cache_length, v_embed_dim)\n+\n+        if isinstance(decoder_past_key_values, Cache):\n+            for layer_idx in range(config.num_hidden_layers):\n+                self.assertEqual(decoder_past_key_values.layers[layer_idx].keys.shape, expected_key_shape)\n+                self.assertEqual(decoder_past_key_values.layers[layer_idx].values.shape, expected_value_shape)\n+        else:\n+            for layer_past in decoder_past_key_values:\n+                self.assertEqual(layer_past[0].shape, expected_key_shape)\n+                self.assertEqual(layer_past[1].shape, expected_value_shape)\n+\n+    @unittest.skip(\"MoE experts may not receive gradients with small test data\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\"MoE experts may not receive gradients with small test data\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\"MoE experts may not receive gradients with small test data\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(\"LongcatFlash router uses weight.type() directly in forward which prevents offloading\")\n+    def test_cpu_offload(self):\n+        pass\n+\n+    @unittest.skip(\"LongcatFlash router uses weight.type() directly in forward which prevents offloading\")\n+    def test_disk_offload_bin(self):\n+        pass\n+\n+    @unittest.skip(\"LongcatFlash router uses weight.type() directly in forward which prevents offloading\")\n+    def test_disk_offload_safetensors(self):\n+        pass\n+\n+    @unittest.skip(\"Most probably because of the MOE, the moe and router does not ignore padding tokens\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SDPA can't dispatch on flash due to unsupported head dims\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+    @staticmethod\n+    def _prepare_config_headdim(config, requested_dim):\n+        # there's specific head dims due to lora compressions in longcat\n+        config = copy.deepcopy(config)\n+        config.attention_dropout = 0\n+\n+        if requested_dim > config.qk_rope_head_dim:\n+            config.qk_rope_head_dim = requested_dim\n+            config.qk_nope_head_dim = max(config.qk_nope_head_dim, requested_dim)\n+            config.v_head_dim = max(config.v_head_dim, requested_dim)\n+            config.qk_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n+            config.head_dim = requested_dim\n+            config.q_lora_rank = max(config.q_lora_rank, requested_dim * 4)\n+            config.kv_lora_rank = max(config.kv_lora_rank, requested_dim * 2)\n+            config.hidden_size = max(config.hidden_size, config.num_attention_heads * requested_dim)\n+\n+        return config\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        short_input = ids_tensor([1, 10], config.vocab_size)\n+        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n+\n+        set_seed(42)\n+        original_model = self.model_tester_class.base_model_class(config)\n+        original_model.to(torch_device)\n+        original_model.eval()\n+        original_short_output = original_model(short_input).last_hidden_state\n+        original_long_output = original_model(long_input).last_hidden_state\n+\n+        set_seed(42)\n+        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n+        scaled_model = self.model_tester_class.base_model_class(config)\n+        scaled_model.to(torch_device)\n+        scaled_model.eval()\n+        scaled_short_output = scaled_model(short_input).last_hidden_state\n+        scaled_long_output = scaled_model(long_input).last_hidden_state\n+\n+        if scaling_type == \"dynamic\":\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n+        else:\n+            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+\n+        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @require_bitsandbytes\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_fp32_ln(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        for model_class in self.all_generative_model_classes:  # TODO: this test should run on all classes instead\n+            if not model_class._supports_flash_attn:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                dummy_input = inputs_dict[model.main_input_name]\n+                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n+                batch_size = dummy_attention_mask.shape[0]\n+\n+                is_padding_right = dummy_attention_mask[:, -1].sum().item() != batch_size\n+\n+                # To avoid errors with padding_side==\"right\"\n+                if is_padding_right:\n+                    dummy_attention_mask = torch.ones_like(dummy_input)\n+\n+                model = model_class.from_pretrained(\n+                    tmpdirname,\n+                    dtype=torch.float16,\n+                    attn_implementation=\"flash_attention_2\",\n+                    device_map=\"auto\",  # small change to ensure device placement\n+                )\n+\n+                # no upcasting at all\n+\n+                if model.config.is_encoder_decoder:\n+                    dummy_decoder_input_ids = inputs_dict[\"decoder_input_ids\"]\n+                    dummy_decoder_attention_mask = inputs_dict[\"decoder_attention_mask\"]\n+\n+                    _ = model(dummy_input, decoder_input_ids=dummy_decoder_input_ids)\n+                    # with attention mask\n+                    _ = model(\n+                        dummy_input,\n+                        attention_mask=dummy_attention_mask,\n+                        decoder_input_ids=dummy_decoder_input_ids,\n+                        decoder_attention_mask=dummy_decoder_attention_mask,\n+                    )\n+                else:\n+                    _ = model(dummy_input)\n+                    # with attention mask\n+                    _ = model(dummy_input, attention_mask=dummy_attention_mask)\n+\n+\n+@slow\n+class LongcatFlashIntegrationTest(unittest.TestCase):\n+    short_model_id = \"hf-internal-testing/LongCat-ShortCat\"\n+    # This is a cut-down model that matches part of the early logits of the larger one\n+    # Only a couple experts + layers\n+    # But if it fails, it means the larger model might have issues as well\n+    model_id = \"meituan-longcat/LongCat-Flash-Chat\"\n+\n+    @slow\n+    def test_shortcat_generation(self):\n+        self.model = LongcatFlashForCausalLM.from_pretrained(\n+            self.short_model_id,\n+            device_map=\"auto\",\n+            dtype=torch.bfloat16,\n+        )\n+        self.model.generation_config.bos_token_id = 1\n+        self.model.generation_config.pad_token_id = 3\n+        self.model.generation_config.eos_token_id = 2\n+        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n+\n+        chat = [{\"role\": \"user\", \"content\": \"Paris is...\"}]\n+        inputs = self.tokenizer.apply_chat_template(\n+            chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\"\n+        ).to(self.model.device)\n+\n+        with torch.no_grad():\n+            outputs = self.model.generate(inputs, max_new_tokens=10, do_sample=False)\n+\n+        response = self.tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n+        expected_output = \"[Round 0] USER:Paris is... ASSISTANT: digå¹´è½¦é¾„juanaheastç¨achaotingupebarebones\"\n+\n+        self.assertEqual(response, expected_output)\n+\n+    @slow\n+    @require_large_cpu_ram\n+    def test_longcat_generation_cpu(self):\n+        # takes absolutely forever and a lot RAM, but allows to test the output in the CI\n+        model = LongcatFlashForCausalLM.from_pretrained(self.model_id, device_map=\"cpu\", dtype=torch.bfloat16)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n+\n+        chat = [{\"role\": \"user\", \"content\": \"Paris is...\"}]\n+        inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n+\n+        with torch.no_grad():\n+            outputs = model.generate(inputs, max_new_tokens=10, do_sample=False)\n+\n+        response = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n+        expected_output = \"[Round 0] USER:Paris is... ASSISTANT:Paris is... a city of timeless charm, where\"\n+\n+        self.assertEqual(response, expected_output)"
        }
    ],
    "stats": {
        "total": 1938,
        "additions": 1938,
        "deletions": 0
    }
}