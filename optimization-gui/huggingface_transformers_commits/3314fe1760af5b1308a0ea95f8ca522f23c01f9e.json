{
    "author": "AmirMohammadFakhimi",
    "message": "Add validation for maximum sequence length in modeling_whisper.py (#33196)\n\n* Add validation for maximum sequence length in modeling_whisper.py\r\n\r\nAdded a validation check to ensure that the sequence length of labels does not exceed the maximum allowed length of 448 tokens. If the sequence length exceeds this limit, a ValueError is raised with a descriptive error message.\r\n\r\nThis change prevents the model from encountering errors or unexpected behavior due to excessively long sequences during training or fine-tuning, ensuring consistent input dimensions and improving overall robustness.\r\n\r\n* Change exception message in src/transformers/models/whisper/modeling_whisper.py\r\n\r\nThe exception message is for whisper's label's sequence max length.\r\n\r\nCo-authored-by: Yoach Lacombe <52246514+ylacombe@users.noreply.github.com>\r\n\r\n* Change 448 to config.max_target_positions in src/transformers/models/whisper/modeling_whisper.py\r\n\r\nIt's for whisper's config.max_target_positions.\r\n\r\nCo-authored-by: Yoach Lacombe <52246514+ylacombe@users.noreply.github.com>\r\n\r\n* Change method's documentation in src/transformers/models/whisper/modeling_whisper.py\r\n\r\n* Add test for maximum label's sequence length in test_modeling_whisper.py\r\n\r\n* Add self to modeling_whisper.py\r\n\r\n* Update test_modeling_whisper.py with respect to automatic validations\r\n\r\n* Update modeling_whisper.py with respect to ci/circleci: check_code_quality\r\n\r\n* Update test_modeling_whisper.py with respect to ci/circleci: check_code_quality\r\n\r\n* Update test_modeling_whisper.py with respect to ci/circleci: tests_generate\r\n\r\n* Update test_modeling_whisper.py with respect to ci/circleci: tests_generate\r\n\r\n* Update test_modeling_whisper.py with respect to ci/circleci: check_code_quality\r\n\r\n* Separate test_labels_sequence_max_length tests in test_modeling_whisper.py\r\n\r\n* Update test_modeling_whisper.py with respect to ci/circleci: check_code_quality\r\n\r\n* Remove assert from test_modeling_whisper.py\r\n\r\n* Add max_target_positions to WhisperModelTester in test_modeling_whisper.py\r\n\r\n* Update test_modeling_whisper.py with respect to ci/circleci: check_code_quality\r\n\r\n* Update test_modeling_whisper.py with respect to ci/circleci: tests_generate\r\n\r\n* Update test_modeling_whisper.py\r\n\r\n* Change test_labels_sequence_max_length_error_after_changing_config in test_modeling_whisper.py\r\n\r\n* Change self.config.max_target_positions to self.max_target_positions modeling_whisper.py\r\n\r\n* Add new tests in test_modeling_whisper.py\r\n\r\n* Update test_modeling_whisper.py\r\n\r\n---------\r\n\r\nCo-authored-by: Yoach Lacombe <52246514+ylacombe@users.noreply.github.com>",
    "sha": "3314fe1760af5b1308a0ea95f8ca522f23c01f9e",
    "files": [
        {
            "sha": "b82b978e5e6d956593ae79b376730e8678e6e843",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3314fe1760af5b1308a0ea95f8ca522f23c01f9e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3314fe1760af5b1308a0ea95f8ca522f23c01f9e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=3314fe1760af5b1308a0ea95f8ca522f23c01f9e",
            "patch": "@@ -1671,6 +1671,7 @@ def __init__(self, config: WhisperConfig):\n         super().__init__(config)\n         self.model = WhisperModel(config)\n         self.proj_out = nn.Linear(config.d_model, config.vocab_size, bias=False)\n+        self.max_target_positions = config.max_target_positions\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1723,7 +1724,7 @@ def forward(\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the language modeling loss. Indices should either be in `[0, ..., config.vocab_size]`\n             or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored (masked), the loss is\n-            only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+            only computed for the tokens with labels in `[0, ..., config.vocab_size]`. `sequence_length` should be smaller than or equal to `config.max_target_positions`.\n \n         Returns:\n \n@@ -1751,6 +1752,10 @@ def forward(\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if labels is not None:\n+            if labels.shape[1] > self.max_target_positions:\n+                raise ValueError(\n+                    f\"Labels' sequence length {labels.shape[1]} cannot exceed the maximum allowed length of {self.max_target_positions} tokens.\"\n+                )\n             if decoder_input_ids is None and decoder_inputs_embeds is None:\n                 decoder_input_ids = shift_tokens_right(\n                     labels, self.config.pad_token_id, self.config.decoder_start_token_id"
        },
        {
            "sha": "e503937458ce90363c5f870627829a46bd72f552",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 57,
            "deletions": 0,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/3314fe1760af5b1308a0ea95f8ca522f23c01f9e/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3314fe1760af5b1308a0ea95f8ca522f23c01f9e/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=3314fe1760af5b1308a0ea95f8ca522f23c01f9e",
            "patch": "@@ -1676,6 +1676,63 @@ def test_flash_attn_2_generate_reuse_cache(self):\n                     past_key_values=past_key_values,\n                 )\n \n+    def test_labels_sequence_max_length_correct(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_generative_model_classes:\n+            input_features = input_dict[\"input_features\"]\n+\n+            labels_length = config.max_target_positions\n+            labels = torch.ones(1, labels_length, dtype=torch.int64)\n+\n+            model = model_class(config)\n+            model(input_features=input_features, labels=labels)\n+\n+    def test_labels_sequence_max_length_correct_after_changing_config(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_generative_model_classes:\n+            input_features = input_dict[\"input_features\"]\n+\n+            config.max_target_positions += 100\n+\n+            labels_length = config.max_target_positions\n+            labels = torch.ones(1, labels_length, dtype=torch.int64)\n+\n+            model = model_class(config)\n+            model(input_features=input_features, labels=labels)\n+\n+    def test_labels_sequence_max_length_error(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_generative_model_classes:\n+            input_features = input_dict[\"input_features\"]\n+\n+            labels_length = config.max_target_positions + 1\n+            labels = torch.ones(1, labels_length, dtype=torch.int64)\n+\n+            model = model_class(config)\n+            with self.assertRaises(ValueError):\n+                model(input_features=input_features, labels=labels)\n+\n+    def test_labels_sequence_max_length_error_after_changing_config(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_generative_model_classes:\n+            model = model_class(config)\n+            input_features = input_dict[\"input_features\"]\n+\n+            labels_length = config.max_target_positions + 1\n+            labels = torch.ones(1, labels_length, dtype=torch.int64)\n+\n+            new_max_length = config.max_target_positions + 100\n+            model.config.max_length = new_max_length\n+            model.generation_config.max_length = new_max_length\n+            config.max_target_positions = new_max_length\n+\n+            with self.assertRaises(ValueError):\n+                model(input_features=input_features, labels=labels)\n+\n \n @require_torch\n @require_torchaudio"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 63,
        "deletions": 1
    }
}