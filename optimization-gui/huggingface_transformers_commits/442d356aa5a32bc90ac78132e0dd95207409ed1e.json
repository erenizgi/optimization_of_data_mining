{
    "author": "ArthurZucker",
    "message": "Add moe kernels (#37376)\n\n* the fix that did not get in\n\n* add kernels\n\n* full graph does not work\n\n* simpler is better\n\n* Update src/transformers/integrations/hub_kernels.py\n\nCo-authored-by: Daniël de Kok <me@danieldk.eu>\n\n* Update src/transformers/integrations/fbgemm_fp8.py\n\nCo-authored-by: Daniël de Kok <me@danieldk.eu>\n\n* Update src/transformers/integrations/hub_kernels.py\n\nCo-authored-by: Daniël de Kok <me@danieldk.eu>\n\n* fixup\n\n---------\n\nCo-authored-by: Daniël de Kok <me@danieldk.eu>",
    "sha": "442d356aa5a32bc90ac78132e0dd95207409ed1e",
    "files": [
        {
            "sha": "8dea8adf36071e3b7aaca87a439deb71eed96cf9",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/442d356aa5a32bc90ac78132e0dd95207409ed1e/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/442d356aa5a32bc90ac78132e0dd95207409ed1e/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=442d356aa5a32bc90ac78132e0dd95207409ed1e",
            "patch": "@@ -32,6 +32,13 @@\n                 layer_name=\"MultiScaleDeformableAttention\",\n             )\n         },\n+        \"Llama4TextMoe\": {\n+            \"cuda\": LayerRepository(\n+                # Move to kernels-community/moe once we release.\n+                repo_id=\"kernels-community/moe\",\n+                layer_name=\"Llama4TextMoe\",\n+            )\n+        },\n         \"RMSNorm\": {\n             \"cuda\": LayerRepository(\n                 repo_id=\"kernels-community/triton-layer-norm\","
        },
        {
            "sha": "093a1f77539fd3c46745534bfde7575244a0a788",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/442d356aa5a32bc90ac78132e0dd95207409ed1e/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/442d356aa5a32bc90ac78132e0dd95207409ed1e/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=442d356aa5a32bc90ac78132e0dd95207409ed1e",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridChunkedCache\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -144,6 +145,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n+@use_kernel_forward_from_hub(\"Llama4TextMoe\")\n class Llama4TextMoe(nn.Module):\n     def __init__(self, config):\n         super().__init__()"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 9,
        "deletions": 0
    }
}