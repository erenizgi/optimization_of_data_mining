{
    "author": "lgeiger",
    "message": "Cleanup `BatchFeature` and `BatchEncoding` (#38459)\n\n* Use dict comprehension to create dict\n\n* Fix type annotation\n\nUnion[Any] doesn't really make any sense\n\n* Remove methods that are already implemented in the `UserDict` parent\nclass",
    "sha": "e508965df7edac73caa9fe9935f22a5cad143b1d",
    "files": [
        {
            "sha": "732f044e07719e10aae461b4cc69d65c4ac70190",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 19,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/e508965df7edac73caa9fe9935f22a5cad143b1d/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e508965df7edac73caa9fe9935f22a5cad143b1d/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=e508965df7edac73caa9fe9935f22a5cad143b1d",
            "patch": "@@ -75,7 +75,7 @@ def __init__(self, data: Optional[dict[str, Any]] = None, tensor_type: Union[Non\n         super().__init__(data)\n         self.convert_to_tensors(tensor_type=tensor_type)\n \n-    def __getitem__(self, item: str) -> Union[Any]:\n+    def __getitem__(self, item: str) -> Any:\n         \"\"\"\n         If the key is a string, returns the value of the dict associated to `key` ('input_values', 'attention_mask',\n         etc.).\n@@ -98,18 +98,6 @@ def __setstate__(self, state):\n         if \"data\" in state:\n             self.data = state[\"data\"]\n \n-    # Copied from transformers.tokenization_utils_base.BatchEncoding.keys\n-    def keys(self):\n-        return self.data.keys()\n-\n-    # Copied from transformers.tokenization_utils_base.BatchEncoding.values\n-    def values(self):\n-        return self.data.values()\n-\n-    # Copied from transformers.tokenization_utils_base.BatchEncoding.items\n-    def items(self):\n-        return self.data.items()\n-\n     def _get_is_as_tensor_fns(self, tensor_type: Optional[Union[str, TensorType]] = None):\n         if tensor_type is None:\n             return None, None\n@@ -218,7 +206,6 @@ def to(self, *args, **kwargs) -> \"BatchFeature\":\n         requires_backends(self, [\"torch\"])\n         import torch  # noqa\n \n-        new_data = {}\n         device = kwargs.get(\"device\")\n         non_blocking = kwargs.get(\"non_blocking\", False)\n         # Check if the args are a device or a dtype\n@@ -233,17 +220,19 @@ def to(self, *args, **kwargs) -> \"BatchFeature\":\n             else:\n                 # it's something else\n                 raise ValueError(f\"Attempting to cast a BatchFeature to type {str(arg)}. This is not supported.\")\n+\n         # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n-        for k, v in self.items():\n+        def maybe_to(v):\n             # check if v is a floating point\n             if isinstance(v, torch.Tensor) and torch.is_floating_point(v):\n                 # cast and send to device\n-                new_data[k] = v.to(*args, **kwargs)\n+                return v.to(*args, **kwargs)\n             elif isinstance(v, torch.Tensor) and device is not None:\n-                new_data[k] = v.to(device=device, non_blocking=non_blocking)\n+                return v.to(device=device, non_blocking=non_blocking)\n             else:\n-                new_data[k] = v\n-        self.data = new_data\n+                return v\n+\n+        self.data = {k: maybe_to(v) for k, v in self.items()}\n         return self\n \n "
        },
        {
            "sha": "b6ed3c677b6aa8eee4ddc431021dfb921b213e72",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e508965df7edac73caa9fe9935f22a5cad143b1d/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e508965df7edac73caa9fe9935f22a5cad143b1d/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=e508965df7edac73caa9fe9935f22a5cad143b1d",
            "patch": "@@ -294,15 +294,6 @@ def __setstate__(self, state):\n         if \"encodings\" in state:\n             self._encodings = state[\"encodings\"]\n \n-    def keys(self):\n-        return self.data.keys()\n-\n-    def values(self):\n-        return self.data.values()\n-\n-    def items(self):\n-        return self.data.items()\n-\n     # After this point:\n     # Extended properties and methods only available for fast (Rust-based) tokenizers\n     # provided by HuggingFace tokenizers library."
        }
    ],
    "stats": {
        "total": 36,
        "additions": 8,
        "deletions": 28
    }
}