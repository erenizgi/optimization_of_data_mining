{
    "author": "Wauplin",
    "message": "Adapt and test huggingface_hub v1.0.0 (#40889)\n\n* Adapt and test huggingface_hub v1.0.0.rc0\n\n* forgot to bump hfh\n\n* bump\n\n* code quality\n\n* code quality\n\n* relax dependency table\n\n* fix has_file\n\n* install hfh 1.0.0.rc0 in circle ci jobs\n\n* repostiryo\n\n* push to hub now returns a commit url\n\n* catch HfHubHTTPError\n\n* check commit on branch\n\n* add it back\n\n* fix ?\n\n* remove deprecated test\n\n* uncomment another test\n\n* trigger\n\n* no proxies\n\n* many more small changes\n\n* fix load PIL Image from httpx\n\n* require 1.0.0.rc0\n\n* fix mocked tests\n\n* fix others\n\n* unchange\n\n* unchange\n\n* args\n\n* Update .circleci/config.yml\n\n* Bump to 1.0.0.rc1\n\n* bump kernels version\n\n* fix deps",
    "sha": "44682e71314f004c8cf157f76e6956e58f6f730f",
    "files": [
        {
            "sha": "3945537c49ffc6265cfa33600d060f0746a1fb53",
            "filename": "setup.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -114,12 +114,12 @@\n     \"GitPython<3.1.19\",\n     \"hf-doc-builder>=0.3.0\",\n     \"hf_xet\",\n-    \"huggingface-hub>=0.34.0,<1.0\",\n+    \"huggingface-hub==1.0.0.rc1\",\n     \"importlib_metadata\",\n     \"ipadic>=1.0.0,<2.0\",\n     \"jinja2>=3.1.0\",\n     \"kenlm\",\n-    \"kernels>=0.6.1,<=0.9\",\n+    \"kernels>=0.10.2,<0.11\",\n     \"librosa\",\n     \"natten>=0.14.6,<0.15.0\",\n     \"nltk<=3.8.1\","
        },
        {
            "sha": "189c947fd191e184d4673e543aa214928694f00b",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -25,11 +25,8 @@\n from io import BytesIO\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n-\n-if TYPE_CHECKING:\n-    import torch\n+import httpx\n import numpy as np\n-import requests\n from packaging import version\n \n from .utils import (\n@@ -42,6 +39,9 @@\n )\n \n \n+if TYPE_CHECKING:\n+    import torch\n+\n if is_soundfile_available():\n     import soundfile as sf\n \n@@ -132,7 +132,9 @@ def load_audio_librosa(audio: Union[str, np.ndarray], sampling_rate=16000, timeo\n \n     # Load audio from URL (e.g https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/translate_to_chinese.wav)\n     if audio.startswith(\"http://\") or audio.startswith(\"https://\"):\n-        audio = librosa.load(BytesIO(requests.get(audio, timeout=timeout).content), sr=sampling_rate)[0]\n+        audio = librosa.load(\n+            BytesIO(httpx.get(audio, follow_redirects=True, timeout=timeout).content), sr=sampling_rate\n+        )[0]\n     elif os.path.isfile(audio):\n         audio = librosa.load(audio, sr=sampling_rate)[0]\n     return audio\n@@ -174,7 +176,7 @@ def load_audio_as(\n         # Load audio bytes from URL or file\n         audio_bytes = None\n         if audio.startswith((\"http://\", \"https://\")):\n-            response = requests.get(audio, timeout=timeout)\n+            response = httpx.get(audio, follow_redirects=True, timeout=timeout)\n             response.raise_for_status()\n             audio_bytes = response.content\n         elif os.path.isfile(audio):"
        },
        {
            "sha": "80b107d93c4d36e6879fc2368b6e40cdfde738ca",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -23,12 +23,12 @@\n     \"GitPython\": \"GitPython<3.1.19\",\n     \"hf-doc-builder\": \"hf-doc-builder>=0.3.0\",\n     \"hf_xet\": \"hf_xet\",\n-    \"huggingface-hub\": \"huggingface-hub>=0.34.0,<1.0\",\n+    \"huggingface-hub\": \"huggingface-hub==1.0.0.rc1\",\n     \"importlib_metadata\": \"importlib_metadata\",\n     \"ipadic\": \"ipadic>=1.0.0,<2.0\",\n     \"jinja2\": \"jinja2>=3.1.0\",\n     \"kenlm\": \"kenlm\",\n-    \"kernels\": \"kernels>=0.6.1,<=0.9\",\n+    \"kernels\": \"kernels>=0.10.2,<0.11\",\n     \"librosa\": \"librosa\",\n     \"natten\": \"natten>=0.14.6,<0.15.0\",\n     \"nltk\": \"nltk<=3.8.1\","
        },
        {
            "sha": "d1f456f9a7182218234c192d2cc85fbd81f5b322",
            "filename": "src/transformers/file_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Ffile_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Ffile_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffile_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -17,9 +17,6 @@\n This module should not be update anymore and is only left for backward compatibility.\n \"\"\"\n \n-from huggingface_hub import get_full_repo_name  # for backward compatibility\n-from huggingface_hub.constants import HF_HUB_DISABLE_TELEMETRY as DISABLE_TELEMETRY  # for backward compatibility\n-\n from . import __version__\n \n # Backward compatibility imports, to make sure all those objects can be found in file_utils"
        },
        {
            "sha": "d018174dd83b674cd4d66daa1c474b16cb4ae9da",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -19,8 +19,8 @@\n from io import BytesIO\n from typing import Optional, Union\n \n+import httpx\n import numpy as np\n-import requests\n \n from .utils import (\n     ExplicitEnum,\n@@ -462,7 +462,7 @@ def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: Optional[float] =\n         if image.startswith(\"http://\") or image.startswith(\"https://\"):\n             # We need to actually check for a real protocol, otherwise it's impossible to use a local file\n             # like http_huggingface_co.png\n-            image = PIL.Image.open(BytesIO(requests.get(image, timeout=timeout).content))\n+            image = PIL.Image.open(BytesIO(httpx.get(image, timeout=timeout, follow_redirects=True).content))\n         elif os.path.isfile(image):\n             image = PIL.Image.open(image)\n         else:"
        },
        {
            "sha": "9a9d8145d2ab9c55df0aee3334c88b3eb47b63b2",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -21,7 +21,7 @@\n from pathlib import Path\n from typing import Any, Optional, Union\n \n-import requests\n+import httpx\n import yaml\n from huggingface_hub import model_info\n from huggingface_hub.errors import OfflineModeIsEnabled\n@@ -380,12 +380,7 @@ def __post_init__(self):\n                 for tag in info.tags:\n                     if tag.startswith(\"license:\"):\n                         self.license = tag[8:]\n-            except (\n-                requests.exceptions.HTTPError,\n-                requests.exceptions.ConnectionError,\n-                HFValidationError,\n-                OfflineModeIsEnabled,\n-            ):\n+            except (httpx.HTTPError, HFValidationError, OfflineModeIsEnabled):\n                 pass\n \n     def create_model_index(self, metric_mapping):"
        },
        {
            "sha": "86f0a95a51f96a9cab80decfe5128b8ea6fd862d",
            "filename": "src/transformers/pipelines/audio_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Faudio_classification.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -14,8 +14,8 @@\n import subprocess\n from typing import Any, Union\n \n+import httpx\n import numpy as np\n-import requests\n \n from ..utils import add_end_docstrings, is_torch_available, is_torchaudio_available, is_torchcodec_available, logging\n from .base import Pipeline, build_pipeline_init_args\n@@ -168,7 +168,7 @@ def preprocess(self, inputs):\n             if inputs.startswith(\"http://\") or inputs.startswith(\"https://\"):\n                 # We need to actually check for a real protocol, otherwise it's impossible to use a local file\n                 # like http_huggingface_co.png\n-                inputs = requests.get(inputs).content\n+                inputs = httpx.get(inputs, follow_redirects=True).content\n             else:\n                 with open(inputs, \"rb\") as f:\n                     inputs = f.read()"
        },
        {
            "sha": "1f3c21526169abebfb9ca4f5f5eaca5c3eea518c",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -14,8 +14,8 @@\n from collections import defaultdict\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n+import httpx\n import numpy as np\n-import requests\n \n from ..generation import GenerationConfig\n from ..tokenization_utils import PreTrainedTokenizer\n@@ -355,7 +355,7 @@ def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):\n             if inputs.startswith(\"http://\") or inputs.startswith(\"https://\"):\n                 # We need to actually check for a real protocol, otherwise it's impossible to use a local file\n                 # like http_huggingface_co.png\n-                inputs = requests.get(inputs).content\n+                inputs = httpx.get(inputs, follow_redirects=True).content\n             else:\n                 with open(inputs, \"rb\") as f:\n                     inputs = f.read()"
        },
        {
            "sha": "957284d2ab17e5df7f13e899213a0e0b358d2055",
            "filename": "src/transformers/pipelines/image_to_image.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -46,12 +46,13 @@ class ImageToImagePipeline(Pipeline):\n \n     ```python\n     >>> from PIL import Image\n-    >>> import requests\n+    >>> import httpx\n+    >>> import io\n \n     >>> from transformers import pipeline\n \n     >>> upscaler = pipeline(\"image-to-image\", model=\"caidas/swin2SR-classical-sr-x2-64\")\n-    >>> img = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+    >>> img = Image.open(io.BytesIO(httpx.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\").content))\n     >>> img = img.resize((64, 64))\n     >>> upscaled_img = upscaler(img)\n     >>> img.size"
        },
        {
            "sha": "e073d921e9e2f21e7b8efcdad9605816cd496a2c",
            "filename": "src/transformers/pipelines/video_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -15,7 +15,7 @@\n from io import BytesIO\n from typing import Any, Optional, Union, overload\n \n-import requests\n+import httpx\n \n from ..utils import (\n     add_end_docstrings,\n@@ -142,7 +142,7 @@ def preprocess(self, video, num_frames=None, frame_sampling_rate=1):\n             num_frames = self.model.config.num_frames\n \n         if video.startswith(\"http://\") or video.startswith(\"https://\"):\n-            video = BytesIO(requests.get(video).content)\n+            video = BytesIO(httpx.get(video, follow_redirects=True).content)\n \n         container = av.open(video)\n "
        },
        {
            "sha": "7d5e36e5dd085ddadc7a2c7c9434a797ee03ede4",
            "filename": "src/transformers/pipelines/zero_shot_audio_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_audio_classification.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -15,8 +15,8 @@\n from collections import UserDict\n from typing import Any, Union\n \n+import httpx\n import numpy as np\n-import requests\n \n from ..utils import (\n     add_end_docstrings,\n@@ -107,7 +107,7 @@ def preprocess(self, audio, candidate_labels=None, hypothesis_template=\"This is\n             if audio.startswith(\"http://\") or audio.startswith(\"https://\"):\n                 # We need to actually check for a real protocol, otherwise it's impossible to use a local file\n                 # like http_huggingface_co.png\n-                audio = requests.get(audio).content\n+                audio = httpx.get(audio, follow_redirects=True).content\n             else:\n                 with open(audio, \"rb\") as f:\n                     audio = f.read()"
        },
        {
            "sha": "397240cadc9f17a51c116b554f93b551883a9833",
            "filename": "src/transformers/safetensors_conversion.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fsafetensors_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fsafetensors_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fsafetensors_conversion.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -1,6 +1,6 @@\n from typing import Optional\n \n-import requests\n+import httpx\n from huggingface_hub import Discussion, HfApi, get_repo_discussions\n \n from .utils import cached_file, http_user_agent, logging\n@@ -44,10 +44,10 @@ def start(_sse_connection):\n \n     data = {\"data\": [model_id, private, token]}\n \n-    result = requests.post(sse_url, stream=True, json=data).json()\n+    result = httpx.post(sse_url, follow_redirects=True, json=data).json()\n     event_id = result[\"event_id\"]\n \n-    with requests.get(f\"{sse_url}/{event_id}\", stream=True) as sse_connection:\n+    with httpx.stream(\"GET\", f\"{sse_url}/{event_id}\") as sse_connection:\n         try:\n             logger.debug(\"Spawning safetensors automatic conversion.\")\n             start(sse_connection)"
        },
        {
            "sha": "304683b5f10a628f9194a1ad3d5b4d8e54c33cd0",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -45,10 +45,9 @@\n from unittest import mock\n from unittest.mock import patch\n \n-import huggingface_hub.utils\n-import requests\n+import httpx\n import urllib3\n-from huggingface_hub import delete_repo\n+from huggingface_hub import create_repo, delete_repo\n from packaging import version\n \n from transformers import Trainer\n@@ -1848,7 +1847,7 @@ def __init__(self, namespace: Optional[str] = None, token: Optional[str] = None)\n             repo_id = Path(tmp_dir).name\n             if namespace is not None:\n                 repo_id = f\"{namespace}/{repo_id}\"\n-            self.repo_url = huggingface_hub.create_repo(repo_id, token=self.token)\n+            self.repo_url = create_repo(repo_id, token=self.token)\n \n     def __enter__(self):\n         return self.repo_url\n@@ -2660,13 +2659,14 @@ def wrapper(*args, **kwargs):\n             while retry_count < max_attempts:\n                 try:\n                     return test_func_ref(*args, **kwargs)\n-                # We catch all exceptions related to network issues from requests\n+                # We catch all exceptions related to network issues from httpx\n                 except (\n-                    requests.exceptions.ConnectionError,\n-                    requests.exceptions.Timeout,\n-                    requests.exceptions.ReadTimeout,\n-                    requests.exceptions.HTTPError,\n-                    requests.exceptions.RequestException,\n+                    httpx.HTTPError,\n+                    httpx.RequestError,\n+                    httpx.TimeoutException,\n+                    httpx.ReadTimeout,\n+                    httpx.ConnectError,\n+                    httpx.NetworkError,\n                 ) as err:\n                     logger.error(\n                         f\"Test failed with {err} at try {retry_count}/{max_attempts} as it couldn't connect to the specified Hub repository.\""
        },
        {
            "sha": "76d36327b308f6f524927abe7a9b64b3b95bf139",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -49,7 +49,7 @@\n import numpy as np\n import torch\n import torch.distributed as dist\n-from huggingface_hub import ModelCard, create_repo, upload_folder\n+from huggingface_hub import CommitInfo, ModelCard, create_repo, upload_folder\n from packaging import version\n from torch import nn\n from torch.utils.data import DataLoader, Dataset, IterableDataset, RandomSampler, SequentialSampler\n@@ -5117,7 +5117,7 @@ def push_to_hub(\n         token: Optional[str] = None,\n         revision: Optional[str] = None,\n         **kwargs,\n-    ) -> str:\n+    ) -> CommitInfo:\n         \"\"\"\n         Upload `self.model` and `self.processing_class` to the ðŸ¤— model hub on the repo `self.args.hub_model_id`.\n "
        },
        {
            "sha": "e6fb2104d06e8111f5f7f31457b4a8f6ab5dbd71",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -16,8 +16,6 @@\n \n from functools import lru_cache\n \n-from huggingface_hub import get_full_repo_name  # for backward compatibility\n-from huggingface_hub.constants import HF_HUB_DISABLE_TELEMETRY as DISABLE_TELEMETRY  # for backward compatibility\n from packaging import version\n \n from .. import __version__"
        },
        {
            "sha": "4ddcbd021a62eb6df873cf8685d91fc667897552",
            "filename": "src/transformers/utils/attention_visualizer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fattention_visualizer.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -11,9 +11,9 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import io\n \n-\n-import requests\n+import httpx\n from PIL import Image\n \n from ..masking_utils import create_causal_mask\n@@ -180,7 +180,7 @@ def visualize_attention_mask(self, input_sentence: str, suffix=\"\"):\n         image_seq_length = None\n         if self.config.model_type in PROCESSOR_MAPPING_NAMES:\n             img = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true\"\n-            img = Image.open(requests.get(img, stream=True).raw)\n+            img = Image.open(io.BytesIO(httpx.get(img, follow_redirects=True).content))\n             image_seq_length = 5\n             processor = AutoProcessor.from_pretrained(self.repo_id, image_seq_length=image_seq_length)\n             if hasattr(processor, \"image_token\"):"
        },
        {
            "sha": "3a30878d1e0d3fe5234d2450c6dafd9756d1484d",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 8,
            "deletions": 14,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -27,8 +27,8 @@\n from urllib.parse import urlparse\n from uuid import uuid4\n \n+import httpx\n import huggingface_hub\n-import requests\n from huggingface_hub import (\n     _CACHED_NO_EXIST,\n     CommitOperationAdd,\n@@ -58,7 +58,6 @@\n     hf_raise_for_status,\n     send_telemetry,\n )\n-from requests.exceptions import HTTPError\n \n from . import __version__, logging\n from .generic import working_or_temp_dir\n@@ -176,7 +175,7 @@ def list_repo_templates(\n             ]\n         except (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n             raise  # valid errors => do not catch\n-        except (HTTPError, OfflineModeIsEnabled, requests.exceptions.ConnectionError):\n+        except (HfHubHTTPError, OfflineModeIsEnabled, httpx.NetworkError):\n             pass  # offline mode, internet down, etc. => try local files\n \n     # check local files\n@@ -199,7 +198,7 @@ def is_remote_url(url_or_filename):\n \n def define_sagemaker_information():\n     try:\n-        instance_data = requests.get(os.environ[\"ECS_CONTAINER_METADATA_URI\"]).json()\n+        instance_data = httpx.get(os.environ[\"ECS_CONTAINER_METADATA_URI\"]).json()\n         dlc_container_used = instance_data[\"Image\"]\n         dlc_tag = instance_data[\"Image\"].split(\":\")[1]\n     except Exception:\n@@ -554,7 +553,7 @@ def cached_files(\n                 ) from e\n         # snapshot_download will not raise EntryNotFoundError, but hf_hub_download can. If this is the case, it will be treated\n         # later on anyway and re-raised if needed\n-        elif isinstance(e, HTTPError) and not isinstance(e, EntryNotFoundError):\n+        elif isinstance(e, HfHubHTTPError) and not isinstance(e, EntryNotFoundError):\n             if not _raise_exceptions_for_connection_errors:\n                 return None\n             raise OSError(f\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{e}\") from e\n@@ -677,18 +676,13 @@ def has_file(\n         response = get_session().head(\n             hf_hub_url(path_or_repo, filename=filename, revision=revision, repo_type=repo_type),\n             headers=build_hf_headers(token=token, user_agent=http_user_agent()),\n-            allow_redirects=False,\n-            proxies=proxies,\n+            follow_redirects=False,\n             timeout=10,\n         )\n-    except (requests.exceptions.SSLError, requests.exceptions.ProxyError):\n+    except httpx.ProxyError:\n         # Actually raise for those subclasses of ConnectionError\n         raise\n-    except (\n-        requests.exceptions.ConnectionError,\n-        requests.exceptions.Timeout,\n-        OfflineModeIsEnabled,\n-    ):\n+    except (httpx.ConnectError, httpx.TimeoutException, OfflineModeIsEnabled):\n         return has_file_in_cache\n \n     try:\n@@ -712,7 +706,7 @@ def has_file(\n         ) from e\n     except EntryNotFoundError:\n         return False  # File does not exist\n-    except requests.HTTPError:\n+    except HfHubHTTPError:\n         # Any authentication/authorization error will be caught here => default to cache\n         return has_file_in_cache\n "
        },
        {
            "sha": "30ad1f39f2caf5c68e64c390ce844e2d037fda0e",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -22,8 +22,8 @@\n from typing import Callable, NewType, Optional, Union\n from urllib.parse import urlparse\n \n+import httpx\n import numpy as np\n-import requests\n \n from .image_transforms import PaddingMode, to_channel_dimension_format\n from .image_utils import ChannelDimension, infer_channel_dimension_format, is_valid_image\n@@ -683,7 +683,7 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n         bytes_obj = buffer.getvalue()\n         file_obj = BytesIO(bytes_obj)\n     elif video.startswith(\"http://\") or video.startswith(\"https://\"):\n-        file_obj = BytesIO(requests.get(video).content)\n+        file_obj = BytesIO(httpx.get(video, follow_redirects=True).content)\n     elif os.path.isfile(video):\n         file_obj = video\n     else:"
        },
        {
            "sha": "e745dad3c885074e849c0bedbf4cf62ed493653f",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -19,7 +19,7 @@\n from unittest.mock import patch\n \n import aiohttp.client_exceptions\n-import requests\n+import httpx\n from huggingface_hub import AsyncInferenceClient, ChatCompletionStreamOutput\n from parameterized import parameterized\n \n@@ -509,31 +509,31 @@ def _call_healthcheck(base_url: str):\n     retries = 10\n     while retries > 0:\n         try:\n-            response = requests.get(f\"{base_url}/health\")\n+            response = httpx.get(f\"{base_url}/health\")\n             break\n-        except requests.exceptions.ConnectionError:\n+        except httpx.NetworkError:\n             time.sleep(0.1)\n             retries -= 1\n     return response\n \n \n def _open_stream_and_cancel(base_url: str, request_id: str):\n-    with requests.Session() as s:\n-        with s.post(\n+    with httpx.Client() as s:\n+        with s.stream(\n+            \"POST\",\n             f\"{base_url}/v1/chat/completions\",\n             headers={\"X-Request-ID\": request_id},\n             json={\n                 \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n                 \"stream\": True,\n                 \"messages\": [{\"role\": \"user\", \"content\": \"Count slowly so I can cancel you.\"}],\n             },\n-            stream=True,\n             timeout=30,\n         ) as resp:\n             assert resp.status_code == 200\n \n             wait_for_n_chunks = 3\n-            for i, _ in enumerate(resp.iter_content(chunk_size=None)):\n+            for i, _ in enumerate(resp.iter_bytes(chunk_size=None)):\n                 if i >= wait_for_n_chunks:\n                     resp.close()\n                     break"
        },
        {
            "sha": "adfa1af09d5c80f4ec6ab789fbe120c61835cd76",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -19,7 +19,7 @@\n import unittest\n import warnings\n \n-from huggingface_hub import HfFolder, create_pull_request\n+from huggingface_hub import create_pull_request\n from parameterized import parameterized\n \n from transformers import AutoConfig, GenerationConfig, WatermarkingConfig, is_torch_available\n@@ -688,7 +688,6 @@ class ConfigPushToHubTester(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls._token = TOKEN\n-        HfFolder.save_token(TOKEN)\n \n     def test_push_to_hub(self):\n         with TemporaryHubRepo(token=self._token) as tmp_repo:"
        },
        {
            "sha": "9babbd42e055dc115feaf9966f51221d04cdb4d1",
            "filename": "tests/models/auto/test_modeling_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_auto.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -13,17 +13,13 @@\n # limitations under the License.\n \n import copy\n-import os\n-import os.path\n-import shutil\n import sys\n import tempfile\n import unittest\n from collections import OrderedDict\n from pathlib import Path\n \n import pytest\n-from huggingface_hub import Repository\n \n import transformers\n from transformers import BertConfig, GPT2Model, is_safetensors_available, is_torch_available\n@@ -42,7 +38,6 @@\n sys.path.append(str(Path(__file__).parent.parent.parent.parent / \"utils\"))\n \n from test_module.custom_configuration import CustomConfig  # noqa E402\n-from utils.fetch_hub_objects_for_ci import url_to_local_path\n \n \n if is_torch_available():\n@@ -562,26 +557,6 @@ def test_attr_not_existing(self):\n         _MODEL_MAPPING = _LazyAutoMapping(_CONFIG_MAPPING_NAMES, _MODEL_MAPPING_NAMES)\n         self.assertEqual(_MODEL_MAPPING[BertConfig], GPT2Model)\n \n-    def test_dynamic_saving_from_local_repo(self):\n-        with tempfile.TemporaryDirectory() as tmp_dir, tempfile.TemporaryDirectory() as tmp_dir_out:\n-            # `Repository` is deprecated and will be removed in `huggingface_hub v1.0`.\n-            # TODO: Remove this test when this comes.\n-            # Here is a ugly approach to avoid `too many requests`\n-            repo_id = url_to_local_path(\"hf-internal-testing/tiny-random-custom-architecture\")\n-            if os.path.isdir(repo_id):\n-                shutil.copytree(repo_id, tmp_dir, dirs_exist_ok=True)\n-            else:\n-                _ = Repository(\n-                    local_dir=tmp_dir,\n-                    clone_from=url_to_local_path(\"hf-internal-testing/tiny-random-custom-architecture\"),\n-                )\n-\n-            model = AutoModelForCausalLM.from_pretrained(tmp_dir, trust_remote_code=True)\n-            model.save_pretrained(tmp_dir_out)\n-            _ = AutoModelForCausalLM.from_pretrained(tmp_dir_out, trust_remote_code=True)\n-            self.assertTrue((Path(tmp_dir_out) / \"modeling_fake_custom.py\").is_file())\n-            self.assertTrue((Path(tmp_dir_out) / \"configuration_fake_custom.py\").is_file())\n-\n     def test_custom_model_patched_generation_inheritance(self):\n         \"\"\"\n         Tests that our inheritance patching for generate-compatible models works as expected. Without this feature,"
        },
        {
            "sha": "6eabd690eed9898e3967bbc6d360e4f63eba5dff",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -20,7 +20,7 @@\n from pathlib import Path\n from shutil import copyfile\n \n-from huggingface_hub import HfFolder, Repository\n+from huggingface_hub import snapshot_download, upload_folder\n \n import transformers\n from transformers import (\n@@ -423,7 +423,6 @@ class ProcessorPushToHubTester(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls._token = TOKEN\n-        HfFolder.save_token(TOKEN)\n \n     def test_push_to_hub_via_save_pretrained(self):\n         with TemporaryHubRepo(token=self._token) as tmp_repo:\n@@ -471,7 +470,7 @@ def test_push_to_hub_dynamic_processor(self):\n             processor = CustomProcessor(feature_extractor, tokenizer)\n \n             with tempfile.TemporaryDirectory() as tmp_dir:\n-                repo = Repository(tmp_dir, clone_from=tmp_repo, token=self._token)\n+                snapshot_download(tmp_repo.repo_id, token=self._token)\n                 processor.save_pretrained(tmp_dir)\n \n                 # This has added the proper auto_map field to the feature extractor config\n@@ -499,7 +498,7 @@ def test_push_to_hub_dynamic_processor(self):\n                 self.assertTrue(os.path.isfile(os.path.join(tmp_dir, \"custom_tokenization.py\")))\n                 self.assertTrue(os.path.isfile(os.path.join(tmp_dir, \"custom_processing.py\")))\n \n-                repo.push_to_hub()\n+                upload_folder(repo_id=tmp_repo.repo_id, folder_path=tmp_dir, token=self._token)\n \n                 new_processor = AutoProcessor.from_pretrained(tmp_repo.repo_id, trust_remote_code=True)\n                 # Can't make an isinstance check because the new_processor is from the CustomProcessor class of a dynamic module"
        },
        {
            "sha": "eaf47ad8f52994886187cf9dcf6df00c9fc315e6",
            "filename": "tests/pipelines/test_pipelines_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_common.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -21,8 +21,8 @@\n from pathlib import Path\n \n import datasets\n-from huggingface_hub import HfFolder, Repository, delete_repo\n-from requests.exceptions import HTTPError\n+from huggingface_hub import delete_repo, snapshot_download\n+from huggingface_hub.errors import HfHubHTTPError\n \n from transformers import (\n     AutomaticSpeechRecognitionPipeline,\n@@ -209,7 +209,7 @@ def test_dtype_property(self):\n     @require_torch\n     def test_auto_model_pipeline_registration_from_local_dir(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            _ = Repository(local_dir=tmp_dir, clone_from=\"hf-internal-testing/tiny-random-custom-architecture\")\n+            snapshot_download(\"hf-internal-testing/tiny-random-custom-architecture\", local_dir=tmp_dir)\n             pipe = pipeline(\"text-generation\", tmp_dir, trust_remote_code=True)\n \n             self.assertIsInstance(pipe, TextGenerationPipeline)  # Assert successful load\n@@ -874,13 +874,12 @@ class DynamicPipelineTester(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls._token = TOKEN\n-        HfFolder.save_token(TOKEN)\n \n     @classmethod\n     def tearDownClass(cls):\n         try:\n             delete_repo(token=cls._token, repo_id=\"test-dynamic-pipeline\")\n-        except HTTPError:\n+        except HfHubHTTPError:\n             pass\n \n     @unittest.skip(\"Broken, TODO @Yih-Dar\")"
        },
        {
            "sha": "c926dd004f32dc4ab78da858e5bf93086c37f1c9",
            "filename": "tests/pipelines/test_pipelines_image_segmentation.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -11,13 +11,13 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n+import io\n import tempfile\n import unittest\n \n import datasets\n+import httpx\n import numpy as np\n-import requests\n from datasets import load_dataset\n from huggingface_hub import ImageSegmentationOutputElement\n from huggingface_hub.utils import insecure_hashlib\n@@ -318,7 +318,9 @@ def test_small_model_pt(self):\n         ]\n         # actual links to get files\n         expected_masks = [x.replace(\"/blob/\", \"/resolve/\") for x in expected_masks]\n-        expected_masks = [Image.open(requests.get(image, stream=True).raw) for image in expected_masks]\n+        expected_masks = [\n+            Image.open(io.BytesIO(httpx.get(image, follow_redirects=True).content)) for image in expected_masks\n+        ]\n \n         # Convert masks to numpy array\n         output_masks = [np.array(x) for x in output_masks]"
        },
        {
            "sha": "5ba99695d656dfa6e2b5049be25eb499e7cd1d2d",
            "filename": "tests/pipelines/test_pipelines_image_to_text.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fpipelines%2Ftest_pipelines_image_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Fpipelines%2Ftest_pipelines_image_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_to_text.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -11,10 +11,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n+import io\n import unittest\n \n-import requests\n+import httpx\n \n from transformers import MODEL_FOR_VISION_2_SEQ_MAPPING, is_vision_available\n from transformers.pipelines import ImageToTextPipeline, pipeline\n@@ -172,7 +172,7 @@ def test_large_model_pt(self):\n     def test_generation_pt_blip(self):\n         pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n         url = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png\"\n-        image = Image.open(requests.get(url, stream=True).raw)\n+        image = Image.open(io.BytesIO(httpx.get(url, follow_redirects=True).content))\n \n         outputs = pipe(image)\n         self.assertEqual(outputs, [{\"generated_text\": \"a pink pokemon pokemon with a blue shirt and a blue shirt\"}])\n@@ -182,7 +182,7 @@ def test_generation_pt_blip(self):\n     def test_generation_pt_git(self):\n         pipe = pipeline(\"image-to-text\", model=\"microsoft/git-base-coco\")\n         url = \"https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png\"\n-        image = Image.open(requests.get(url, stream=True).raw)\n+        image = Image.open(io.BytesIO(httpx.get(url, follow_redirects=True).content))\n \n         outputs = pipe(image)\n         self.assertEqual(outputs, [{\"generated_text\": \"a cartoon of a purple character.\"}])\n@@ -192,7 +192,7 @@ def test_generation_pt_git(self):\n     def test_conditional_generation_pt_blip(self):\n         pipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-base\")\n         url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\n-        image = Image.open(requests.get(url, stream=True).raw)\n+        image = Image.open(io.BytesIO(httpx.get(url, follow_redirects=True).content))\n \n         prompt = \"a photography of\"\n \n@@ -207,7 +207,7 @@ def test_conditional_generation_pt_blip(self):\n     def test_conditional_generation_pt_git(self):\n         pipe = pipeline(\"image-to-text\", model=\"microsoft/git-base-coco\")\n         url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\n-        image = Image.open(requests.get(url, stream=True).raw)\n+        image = Image.open(io.BytesIO(httpx.get(url, follow_redirects=True).content))\n \n         prompt = \"a photo of a\"\n \n@@ -222,7 +222,7 @@ def test_conditional_generation_pt_git(self):\n     def test_conditional_generation_pt_pix2struct(self):\n         pipe = pipeline(\"image-to-text\", model=\"google/pix2struct-ai2d-base\")\n         url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\n-        image = Image.open(requests.get(url, stream=True).raw)\n+        image = Image.open(io.BytesIO(httpx.get(url, follow_redirects=True).content))\n \n         prompt = \"What does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud\"\n "
        },
        {
            "sha": "e7c3daa1a380994359bb68c6bb9ebcbc2638637f",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -11,8 +11,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n import inspect\n+import io\n import json\n import os\n import pathlib\n@@ -22,9 +22,9 @@\n import warnings\n from copy import deepcopy\n \n+import httpx\n import numpy as np\n import pytest\n-import requests\n from packaging import version\n \n from transformers import AutoImageProcessor, BatchFeature\n@@ -182,7 +182,9 @@ def test_slow_fast_equivalence(self):\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n         dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+            io.BytesIO(\n+                httpx.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", follow_redirects=True).content\n+            )\n         )\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)"
        },
        {
            "sha": "e340831775870bc791c5f21abca04b46677babb3",
            "filename": "tests/test_tokenization_mistral_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Ftest_tokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Ftest_tokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_mistral_common.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -34,7 +34,7 @@\n     from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n     from mistral_common.tokens.tokenizers.utils import list_local_hf_repo_files\n \n-    # To avoid unnecessary `requests.get` calls which give us `Error: Too Many Requests for url` on CircleCI\n+    # To avoid unnecessary `httpx.get` calls which give us `Error: Too Many Requests for url` on CircleCI\n     mistral_common.tokens.tokenizers.image.download_image = load_image\n \n "
        },
        {
            "sha": "5cce980a6a001f27d5d7994722ed2fe7cba66cd5",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -32,7 +32,7 @@\n \n import numpy as np\n import pytest\n-from huggingface_hub import HfFolder, ModelCard, create_branch, list_repo_commits, list_repo_files\n+from huggingface_hub import ModelCard, create_branch, list_repo_commits, list_repo_files\n from packaging import version\n from parameterized import parameterized\n \n@@ -5284,7 +5284,6 @@ class TrainerIntegrationWithHubTester(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls._token = TOKEN\n-        HfFolder.save_token(TOKEN)\n \n     def test_push_to_hub(self):\n         with TemporaryHubRepo(token=self._token) as tmp_repo:\n@@ -5469,14 +5468,10 @@ def test_push_to_hub_with_revision(self):\n                 )\n                 branch = \"v1.0\"\n                 create_branch(repo_id=trainer.hub_model_id, branch=branch, token=self._token, exist_ok=True)\n-                url = trainer.push_to_hub(revision=branch)\n+                push_commit = trainer.push_to_hub(revision=branch)\n \n-            # Extract branch from the url\n-            re_search = re.search(r\"tree/([^/]+)/\", url)\n-            self.assertIsNotNone(re_search)\n-\n-            branch_name = re_search.groups()[0]\n-            self.assertEqual(branch_name, branch)\n+            commits = list_repo_commits(repo_id=trainer.hub_model_id, revision=branch, token=self._token)\n+            self.assertEqual(commits[0].commit_id, push_commit.oid)\n \n \n @require_torch"
        },
        {
            "sha": "069ca6729bbd20ba7fa427d3c9c45f65ad725b71",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -21,8 +21,7 @@\n import warnings\n from pathlib import Path\n \n-from huggingface_hub import HfFolder\n-from requests.exceptions import HTTPError\n+import httpx\n \n from transformers import AutoConfig, BertConfig, Florence2Config, GPT2Config\n from transformers.configuration_utils import PretrainedConfig\n@@ -93,7 +92,6 @@ class ConfigPushToHubTester(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls._token = TOKEN\n-        HfFolder.save_token(TOKEN)\n \n     def test_push_to_hub(self):\n         with TemporaryHubRepo(token=self._token) as tmp_repo:\n@@ -222,14 +220,16 @@ def test_cached_files_are_used_when_internet_is_down(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = httpx.HTTPStatusError(\n+            \"failed\", request=mock.Mock(), response=mock.Mock()\n+        )\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n         _ = BertConfig.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n         # Under the mock environment we get a 500 error when trying to reach the model.\n-        with mock.patch(\"requests.Session.request\", return_value=response_mock) as mock_head:\n+        with mock.patch(\"httpx.Client.request\", return_value=response_mock) as mock_head:\n             _ = BertConfig.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n             # This check we did call the fake head request\n             mock_head.assert_called()"
        },
        {
            "sha": "b0a6a193d10d980c519ac76e3a17c14853f5921f",
            "filename": "tests/utils/test_feature_extraction_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_feature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_feature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_feature_extraction_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -19,8 +19,7 @@\n import unittest.mock as mock\n from pathlib import Path\n \n-from huggingface_hub import HfFolder\n-from requests.exceptions import HTTPError\n+import httpx\n \n from transformers import AutoFeatureExtractor, Wav2Vec2FeatureExtractor\n from transformers.testing_utils import TOKEN, TemporaryHubRepo, get_tests_dir, is_staging_test\n@@ -40,13 +39,15 @@ def test_cached_files_are_used_when_internet_is_down(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = httpx.HTTPStatusError(\n+            \"failed\", request=mock.Mock(), response=mock.Mock()\n+        )\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n         _ = Wav2Vec2FeatureExtractor.from_pretrained(\"hf-internal-testing/tiny-random-wav2vec2\")\n         # Under the mock environment we get a 500 error when trying to reach the model.\n-        with mock.patch(\"requests.Session.request\", return_value=response_mock) as mock_head:\n+        with mock.patch(\"httpx.Client.request\", return_value=response_mock) as mock_head:\n             _ = Wav2Vec2FeatureExtractor.from_pretrained(\"hf-internal-testing/tiny-random-wav2vec2\")\n             # This check we did call the fake head request\n             mock_head.assert_called()\n@@ -57,7 +58,6 @@ class FeatureExtractorPushToHubTester(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls._token = TOKEN\n-        HfFolder.save_token(TOKEN)\n \n     def test_push_to_hub(self):\n         with TemporaryHubRepo(token=self._token) as tmp_repo:"
        },
        {
            "sha": "a3423245a8946974bbae4b9d5dbc5be6f723d807",
            "filename": "tests/utils/test_hub_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_hub_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_hub_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_hub_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -19,8 +19,7 @@\n from pathlib import Path\n \n from huggingface_hub import hf_hub_download\n-from huggingface_hub.errors import LocalEntryNotFoundError, OfflineModeIsEnabled\n-from requests.exceptions import HTTPError\n+from huggingface_hub.errors import HfHubHTTPError, LocalEntryNotFoundError, OfflineModeIsEnabled\n \n from transformers.utils import (\n     CONFIG_NAME,\n@@ -87,7 +86,10 @@ def test_non_existence_is_cached(self):\n         self.assertIsNone(path)\n \n         # Under the mock environment, hf_hub_download will always raise an HTTPError\n-        with mock.patch(\"transformers.utils.hub.hf_hub_download\", side_effect=HTTPError) as mock_head:\n+        with mock.patch(\n+            \"transformers.utils.hub.hf_hub_download\",\n+            side_effect=HfHubHTTPError(\"failed\", response=mock.Mock(status_code=404)),\n+        ) as mock_head:\n             path = cached_file(RANDOM_BERT, \"conf\", _raise_exceptions_for_connection_errors=False)\n             self.assertIsNone(path)\n             # This check we did call the fake head request"
        },
        {
            "sha": "17e5e305c610344d02793384d6265325003c5408",
            "filename": "tests/utils/test_image_processing_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_image_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_image_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_image_processing_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -18,8 +18,7 @@\n import unittest.mock as mock\n from pathlib import Path\n \n-from huggingface_hub import HfFolder\n-from requests.exceptions import HTTPError\n+import httpx\n \n from transformers import AutoImageProcessor, ViTImageProcessor, ViTImageProcessorFast\n from transformers.image_processing_utils import get_size_dict\n@@ -40,15 +39,17 @@ def test_cached_files_are_used_when_internet_is_down(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = httpx.HTTPStatusError(\n+            \"failed\", request=mock.Mock(), response=mock.Mock()\n+        )\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n         _ = ViTImageProcessor.from_pretrained(\"hf-internal-testing/tiny-random-vit\")\n         _ = ViTImageProcessorFast.from_pretrained(\"hf-internal-testing/tiny-random-vit\")\n \n         # Under the mock environment we get a 500 error when trying to reach the model.\n-        with mock.patch(\"requests.Session.request\", return_value=response_mock) as mock_head:\n+        with mock.patch(\"httpx.Client.request\", return_value=response_mock) as mock_head:\n             _ = ViTImageProcessor.from_pretrained(\"hf-internal-testing/tiny-random-vit\")\n             _ = ViTImageProcessorFast.from_pretrained(\"hf-internal-testing/tiny-random-vit\")\n             # This check we did call the fake head request\n@@ -71,7 +72,6 @@ class ImageProcessorPushToHubTester(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls._token = TOKEN\n-        HfFolder.save_token(TOKEN)\n \n     def test_push_to_hub(self):\n         with TemporaryHubRepo(token=self._token) as tmp_repo:"
        },
        {
            "sha": "6c2db861ffe27c8ba1e4d6c3ae4dc9c9e93621bc",
            "filename": "tests/utils/test_image_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_image_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_image_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_image_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -19,11 +19,10 @@\n from io import BytesIO\n from typing import Optional\n \n+import httpx\n import numpy as np\n import pytest\n-import requests\n from huggingface_hub.file_download import hf_hub_url, http_get\n-from requests import ConnectTimeout, ReadTimeout\n \n from tests.pipelines.test_pipelines_document_question_answering import INVOICE_URL\n from transformers import is_torch_available, is_vision_available\n@@ -49,7 +48,7 @@\n \n def get_image_from_hub_dataset(dataset_id: str, filename: str, revision: Optional[str] = None) -> \"PIL.Image.Image\":\n     url = hf_hub_url(dataset_id, filename, repo_type=\"dataset\", revision=revision)\n-    return PIL.Image.open(BytesIO(requests.get(url).content))\n+    return PIL.Image.open(BytesIO(httpx.get(url, follow_redirects=True).content))\n \n \n def get_random_image(height, width):\n@@ -727,7 +726,7 @@ def test_load_img_url(self):\n \n     @is_flaky()\n     def test_load_img_url_timeout(self):\n-        with self.assertRaises((ReadTimeout, ConnectTimeout)):\n+        with self.assertRaises(httpx.ConnectTimeout):\n             load_image(INVOICE_URL, timeout=0.001)\n \n     def test_load_img_local(self):"
        },
        {
            "sha": "045f110407845d31a68160c5c6c06fcf78acaa5a",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 13,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -27,12 +27,11 @@\n import warnings\n from pathlib import Path\n \n+import httpx\n import pytest\n-import requests\n-from huggingface_hub import HfApi, HfFolder, split_torch_state_dict_into_shards\n+from huggingface_hub import HfApi, split_torch_state_dict_into_shards\n from parameterized import parameterized\n from pytest import mark\n-from requests.exceptions import HTTPError\n \n from transformers import (\n     AutoConfig,\n@@ -419,7 +418,7 @@ def test_func():\n             # First attempt will fail with a connection error\n             if not hasattr(test_func, \"attempt\"):\n                 test_func.attempt = 1\n-                raise requests.exceptions.ConnectionError(\"Connection failed\")\n+                raise httpx.ConnectError(\"Connection failed\")\n             # Second attempt will succeed\n             return True\n \n@@ -1172,14 +1171,16 @@ def test_cached_files_are_used_when_internet_is_down(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = httpx.HTTPStatusError(\n+            \"failed\", request=mock.Mock(), response=mock.Mock()\n+        )\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n         _ = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n         # Under the mock environment we get a 500 error when trying to reach the model.\n-        with mock.patch(\"requests.Session.request\", return_value=response_mock) as mock_head:\n+        with mock.patch(\"httpx.Client.request\", return_value=response_mock) as mock_head:\n             _ = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n             # This check we did call the fake head request\n             mock_head.assert_called()\n@@ -2205,10 +2206,7 @@ def test_safetensors_on_the_fly_conversion_gated(self):\n         initial_model = BertModel(config)\n \n         initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False)\n-        headers = {\"Authorization\": f\"Bearer {self.token}\"}\n-        requests.put(\n-            f\"https://huggingface.co/api/models/{self.repo_name}/settings\", json={\"gated\": \"auto\"}, headers=headers\n-        )\n+        self.api.update_repo_settings(self.repo_name, gated=\"auto\")\n         converted_model = BertModel.from_pretrained(self.repo_name, use_safetensors=True, token=self.token)\n \n         with self.subTest(\"Initial and converted models are equal\"):\n@@ -2269,7 +2267,7 @@ def test_safetensors_on_the_fly_sharded_conversion_gated(self):\n \n         initial_model.push_to_hub(self.repo_name, token=self.token, max_shard_size=\"200kb\", safe_serialization=False)\n         headers = {\"Authorization\": f\"Bearer {self.token}\"}\n-        requests.put(\n+        httpx.put(\n             f\"https://huggingface.co/api/models/{self.repo_name}/settings\", json={\"gated\": \"auto\"}, headers=headers\n         )\n         converted_model = BertModel.from_pretrained(self.repo_name, use_safetensors=True, token=self.token)\n@@ -2368,7 +2366,7 @@ def test_absence_of_safetensors_triggers_conversion(self):\n \n     @mock.patch(\"transformers.safetensors_conversion.spawn_conversion\")\n     def test_absence_of_safetensors_triggers_conversion_failed(self, spawn_conversion_mock):\n-        spawn_conversion_mock.side_effect = HTTPError()\n+        spawn_conversion_mock.side_effect = httpx.HTTPError(\"failed\")\n \n         config = BertConfig(\n             vocab_size=99, hidden_size=32, num_hidden_layers=5, num_attention_heads=4, intermediate_size=37\n@@ -2388,7 +2386,6 @@ class ModelPushToHubTester(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls._token = TOKEN\n-        HfFolder.save_token(TOKEN)\n \n     @unittest.skip(reason=\"This test is flaky\")\n     def test_push_to_hub(self):"
        },
        {
            "sha": "ecda49fd9bd8bc181f0088742b47fed720cef13d",
            "filename": "tests/utils/test_tokenization_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/tests%2Futils%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_tokenization_utils.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -19,9 +19,8 @@\n import unittest.mock as mock\n from pathlib import Path\n \n-from huggingface_hub import HfFolder\n+import httpx\n from huggingface_hub.file_download import http_get\n-from requests.exceptions import HTTPError\n \n from transformers import (\n     AlbertTokenizer,\n@@ -50,14 +49,16 @@ def test_cached_files_are_used_when_internet_is_down(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = httpx.HTTPStatusError(\n+            \"failed\", request=mock.Mock(), response=mock.Mock()\n+        )\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n         _ = BertTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n         # Under the mock environment we get a 500 error when trying to reach the tokenizer.\n-        with mock.patch(\"requests.Session.request\", return_value=response_mock) as mock_head:\n+        with mock.patch(\"httpx.Client.request\", return_value=response_mock) as mock_head:\n             _ = BertTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n             # This check we did call the fake head request\n             mock_head.assert_called()\n@@ -68,14 +69,16 @@ def test_cached_files_are_used_when_internet_is_down_missing_files(self):\n         response_mock = mock.Mock()\n         response_mock.status_code = 500\n         response_mock.headers = {}\n-        response_mock.raise_for_status.side_effect = HTTPError\n+        response_mock.raise_for_status.side_effect = httpx.HTTPStatusError(\n+            \"failed\", request=mock.Mock(), response=mock.Mock()\n+        )\n         response_mock.json.return_value = {}\n \n         # Download this model to make sure it's in the cache.\n         _ = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\")\n \n         # Under the mock environment we get a 500 error when trying to reach the tokenizer.\n-        with mock.patch(\"requests.Session.request\", return_value=response_mock) as mock_head:\n+        with mock.patch(\"httpx.Client.request\", return_value=response_mock) as mock_head:\n             _ = GPT2TokenizerFast.from_pretrained(\"openai-community/gpt2\")\n             # This check we did call the fake head request\n             mock_head.assert_called()\n@@ -115,7 +118,6 @@ class TokenizerPushToHubTester(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls._token = TOKEN\n-        HfFolder.save_token(TOKEN)\n \n     def test_push_to_hub(self):\n         with TemporaryHubRepo(token=self._token) as tmp_repo:"
        },
        {
            "sha": "1e5c67bb909fef643cf8d8b513f0333be46d5bd7",
            "filename": "utils/create_dummy_models.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/utils%2Fcreate_dummy_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/utils%2Fcreate_dummy_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dummy_models.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -28,7 +28,7 @@\n from check_config_docstrings import get_checkpoint_from_config_class\n from datasets import load_dataset\n from get_test_info import get_model_to_tester_mapping, get_tester_classes_for_model\n-from huggingface_hub import Repository, create_repo, hf_api, upload_folder\n+from huggingface_hub import Repository, create_repo, hf_api, upload_folder  # TODO: remove Repository\n \n from transformers import (\n     CONFIG_MAPPING,"
        },
        {
            "sha": "0f41ec344f5db0cb33e25a6569531048193cd785",
            "filename": "utils/fetch_hub_objects_for_ci.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/utils%2Ffetch_hub_objects_for_ci.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/utils%2Ffetch_hub_objects_for_ci.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ffetch_hub_objects_for_ci.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -1,7 +1,7 @@\n import os\n \n import requests\n-from huggingface_hub import Repository, hf_hub_download\n+from huggingface_hub import hf_hub_download, snapshot_download\n \n from transformers.testing_utils import _run_pipeline_tests, _run_staging\n from transformers.utils.import_utils import is_mistral_common_available\n@@ -173,9 +173,9 @@ def url_to_local_path(url, return_url_if_not_found=True):\n     # But this repo. is never used in a test decorated by `is_staging_test`.\n     if not _run_staging:\n         if not os.path.isdir(\"tiny-random-custom-architecture\"):\n-            _ = Repository(\n+            snapshot_download(\n+                \"hf-internal-testing/tiny-random-custom-architecture\",\n                 local_dir=\"tiny-random-custom-architecture\",\n-                clone_from=\"hf-internal-testing/tiny-random-custom-architecture\",\n             )\n \n         # For `tests/test_tokenization_mistral_common.py:TestMistralCommonTokenizer`, which eventually calls"
        },
        {
            "sha": "336770c540ec4880bd6d357f2556142aa22e2dec",
            "filename": "utils/update_tiny_models.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/44682e71314f004c8cf157f76e6956e58f6f730f/utils%2Fupdate_tiny_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44682e71314f004c8cf157f76e6956e58f6f730f/utils%2Fupdate_tiny_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fupdate_tiny_models.py?ref=44682e71314f004c8cf157f76e6956e58f6f730f",
            "patch": "@@ -27,7 +27,7 @@\n import time\n \n from create_dummy_models import COMPOSITE_MODELS, create_tiny_models\n-from huggingface_hub import ModelFilter, hf_api\n+from huggingface_hub import HfApi\n \n import transformers\n from transformers import AutoFeatureExtractor, AutoImageProcessor, AutoTokenizer\n@@ -65,15 +65,12 @@ def get_tiny_model_names_from_repo():\n \n \n def get_tiny_model_summary_from_hub(output_path):\n+    api = HfApi()\n     special_models = COMPOSITE_MODELS.values()\n \n     # All tiny model base names on Hub\n     model_names = get_all_model_names()\n-    models = hf_api.list_models(\n-        filter=ModelFilter(\n-            author=\"hf-internal-testing\",\n-        )\n-    )\n+    models = api.list_models(author=\"hf-internal-testing\")\n     _models = set()\n     for x in models:\n         model = x.id\n@@ -94,7 +91,7 @@ def get_tiny_model_summary_from_hub(output_path):\n         repo_id = f\"hf-internal-testing/tiny-random-{model}\"\n         model = model.split(\"-\")[0]\n         try:\n-            repo_info = hf_api.repo_info(repo_id)\n+            repo_info = api.repo_info(repo_id)\n             content = {\n                 \"tokenizer_classes\": set(),\n                 \"processor_classes\": set(),"
        }
    ],
    "stats": {
        "total": 329,
        "additions": 142,
        "deletions": 187
    }
}