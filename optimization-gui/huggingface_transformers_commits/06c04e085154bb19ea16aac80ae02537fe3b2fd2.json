{
    "author": "SunMarc",
    "message": "Deprecate `half_precision_backend` (#41134)\n\n* deprecate\n\n* remove\n\n* rm apex\n\n* fix\n\n* fix\n\n* fix doc",
    "sha": "06c04e085154bb19ea16aac80ae02537fe3b2fd2",
    "files": [
        {
            "sha": "55b8408a43bba1e88cab2664bb8f306b6db2a124",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -452,7 +452,7 @@ For Ampere GPUs and PyTorch 1.7+, the more efficient [tf32](https://pytorch.org/\n </hfoption>\n <hfoption id=\"fp16\">\n \n-To configure AMP-like fp16 mixed precision, set up the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically enables or disables fp16 based on the value of `fp16_backend`, and the rest of the config can be set by you. fp16 is enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend amp` or `--fp16_full_eval`.\n+To configure fp16 mixed precision, set up the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically enables or disables fp16 based on the value of `fp16` or `fp16_full_eval`, and the rest of the config can be set by you. fp16 is enabled from the command line when the following arguments are passed: `--fp16` or `--fp16_full_eval` also.\n \n ```json\n {\n@@ -469,17 +469,6 @@ To configure AMP-like fp16 mixed precision, set up the config as shown below wit\n \n For additional DeepSpeed fp16 training options, take a look at the [FP16 Training Options](https://www.deepspeed.ai/docs/config-json/#fp16-training-options) reference.\n \n-To configure Apex-like fp16 mixed precision, set up the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically configures `amp` based on the values of `fp16_backend` and `fp16_opt_level`. It can also be enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend apex` or `--fp16_opt_level 01`.\n-\n-```json\n-{\n-    \"amp\": {\n-        \"enabled\": \"auto\",\n-        \"opt_level\": \"auto\"\n-    }\n-}\n-```\n-\n </hfoption>\n <hfoption id=\"bf16\">\n "
        },
        {
            "sha": "affb6c0a724ca322c1fd73a720bde15cfcd0b631",
            "filename": "docs/source/ja/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -1390,8 +1390,6 @@ Ampere アーキテクチャ ベースの GPU を使用している場合、pyto\n \n ### Automatic Mixed Precision\n \n-pytorch のような AMP の方法または apex のような方法で自動混合精度を使用できます。\n-\n ### fp16\n \n fp16 (float16) を設定して pytorch AMP のようなモードを設定するには:\n@@ -1491,40 +1489,6 @@ bf16 が有効な状態で [勾配累積](#gradient-accumulation) を使用す\n \n 注: ステージ ゼロ 3 には、bf16 通信タイプに関するバグがあり、`deepspeed==0.8.1`で修正されました。\n \n-### apex\n-\n-apex AMP のようなモード セットを設定するには:\n-\n-```json\n-\"amp\": {\n-    \"enabled\": \"auto\",\n-    \"opt_level\": \"auto\"\n-}\n-```\n-\n-[`Trainer`] は `args.fp16_backend` の値に基づいて自動的に設定します。\n-`args.fp16_opt_level`。\n-\n-このモードは、`--fp16 --fp16_backend apex --fp16_opt_level 01`コマンド ライン引数が渡されると有効になります。\n-\n-このモードを明示的に構成することもできます。\n-\n-```json\n-{\n-    \"amp\": {\n-        \"enabled\": true,\n-        \"opt_level\": \"O1\"\n-    }\n-}\n-```\n-\n-ただし、[`Trainer`] コマンドライン引数と DeepSpeed を自分で同期することになります。\n-構成。\n-\n-これは[ドキュメント](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options)です。\n-\n-<a id='deepspeed-bs'></a>\n-\n ### Batch Size\n \n バッチサイズを設定するには、次を使用します。"
        },
        {
            "sha": "e6e6e28d308b17b453f80bb5d384995fc6a793ed",
            "filename": "docs/source/ja/main_classes/trainer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n \n [`Trainer`] をインスタンス化する前に、トレーニング中にカスタマイズのすべてのポイントにアクセスするために [`TrainingArguments`] を作成します。\n \n-この API は、複数の GPU/TPU での分散トレーニング、[NVIDIA Apex](https://github.com/NVIDIA/apex) および PyTorch のネイティブ AMP による混合精度をサポートします。\n+この API は、複数の GPU/TPU での分散トレーニング、PyTorch のネイティブ AMP による混合精度をサポートします。\n \n [`Trainer`] には、上記の機能をサポートする基本的なトレーニング ループが含まれています。カスタム動作を挿入するには、それらをサブクラス化し、次のメソッドをオーバーライドします。\n "
        },
        {
            "sha": "d0955ee3db808f782a60f7c967cc2ea4df727885",
            "filename": "docs/source/ko/deepspeed.md",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fdeepspeed.md?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -509,17 +509,6 @@ PyTorch AMP와 같은 fp16 혼합 정밀도를 구성하면 메모리 사용량\n \n 추가 딥스피드 fp16 훈련 옵션은 [fp16 훈련 옵션](https://www.deepspeed.ai/docs/config-json/#fp16-training-options) 참조를 참조하세요.\n \n-Apex와 같은 fp16 혼합 정밀도를 구성하려면 아래 그림과 같이 `\"auto\"` 또는 직접 값을 설정합니다.[`Trainer`]는 `args.fp16_backend` 및 `args.fp16_opt_level`의 값에 따라 `amp`를 자동으로 구성합니다. 다음 인수를 전달하면 명령줄에서 활성화할 수도 있습니다: `fp16`, `--fp16_backend apex` 또는 `--fp16_opt_level 01`.\n-\n-```yaml\n-{\n-    \"amp\": {\n-        \"enabled\": \"auto\",\n-        \"opt_level\": \"auto\"\n-    }\n-}\n-```\n-\n </hfoption>\n <hfoption id=\"bf16\">\n "
        },
        {
            "sha": "8319f5cad4a3393eb00d9a9f6564db8082043cb5",
            "filename": "docs/source/zh/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -1293,8 +1293,6 @@ DeepSpeed支持完整的fp32和fp16混合精度。\n \n ### 自动混合精度\n \n-您可以使用自动混合精度，可以选择使用类似 PyTorch AMP 的方式，也可以选择使用类似 Apex 的方式：\n-\n ### fp16\n \n 要配置PyTorch AMP-like 的 fp16（float16） 模式，请设置：\n@@ -1393,38 +1391,6 @@ bf16具有与fp32相同的动态范围，因此不需要损失缩放。\n \n 注意：在stage zero 3中，bf16通信数据类型存在一个bug，该问题已在`deepspeed==0.8.1`版本中得到修复。\n \n-\n-### apex\n-\n-配置apex AMP-like模式：\n-\n-```json\n-\"amp\": {\n-    \"enabled\": \"auto\",\n-    \"opt_level\": \"auto\"\n-}\n-```\n-\n-并且，[`Trainer`]将根据`args.fp16_backend`和`args.fp16_opt_level`的值自动配置它。\n-\n-当传递`--fp16 --fp16_backend apex --fp16_opt_level 01`命令行参数时，此模式将被启用。\n-\n-您还可以显式配置此模式：\n-\n-```json\n-{\n-    \"amp\": {\n-        \"enabled\": true,\n-        \"opt_level\": \"O1\"\n-    }\n-}\n-```\n-\n-但是，您需要自己同步[`Trainer`]命令行参数和DeepSpeed配置。\n-\n-这里是[文档](https://www.deepspeed.ai/docs/config-json/#automatic-mixed-precision-amp-training-options)\n-\n-\n <a id='deepspeed-bs'></a>\n \n ### Batch Size"
        },
        {
            "sha": "159477fe64a0534d1026ef47dfafcdc232fc9b48",
            "filename": "docs/source/zh/main_classes/trainer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fzh%2Fmain_classes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/docs%2Fsource%2Fzh%2Fmain_classes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Ftrainer.md?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n \n 在实例化你的 [`Trainer`] 之前，创建一个 [`TrainingArguments`]，以便在训练期间访问所有定制点。\n \n-这个 API 支持在多个 GPU/TPU 上进行分布式训练，支持 [NVIDIA Apex](https://github.com/NVIDIA/apex) 的混合精度和 PyTorch 的原生 AMP。\n+这个 API 支持在多个 GPU/TPU 上进行分布式训练，支持 PyTorch 的原生 AMP。\n \n [`Trainer`] 包含基本的训练循环，支持上述功能。如果需要自定义训练，你可以继承 `Trainer` 并覆盖以下方法：\n "
        },
        {
            "sha": "831ae1f4cd0335e4b849603d7121be5413aa0bed",
            "filename": "examples/legacy/seq2seq/README.md",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/examples%2Flegacy%2Fseq2seq%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/examples%2Flegacy%2Fseq2seq%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2FREADME.md?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -94,23 +94,18 @@ test.target\n ```\n The `.source` files are the input, the `.target` files are the desired output.\n \n-### Potential issues\n-\n-- native AMP (`--fp16` and no apex) may lead to a huge memory leak and require 10x gpu memory. This has been fixed in pytorch-nightly and the minimal official version to have this fix will be pytorch-1.7.1. Until then if you have to use mixed precision please use AMP only with pytorch-nightly or NVIDIA's apex. Reference: https://github.com/huggingface/transformers/issues/8403\n-\n-\n ### Tips and Tricks\n \n General Tips:\n - since you need to run from `examples/legacy/seq2seq`, and likely need to modify code, the easiest workflow is fork transformers, clone your fork, and run `pip install -e .` before you get started.\n - try `--freeze_encoder` or `--freeze_embeds` for faster training/larger batch size.  (3hr per epoch with bs=8, see the \"xsum_shared_task\" command below)\n-- `fp16_opt_level=O1` (the default works best).\n+\n - In addition to the pytorch-lightning .ckpt checkpoint, a transformers checkpoint will be saved.\n Load it with `BartForConditionalGeneration.from_pretrained(f'{output_dir}/best_tfmr)`.\n - At the moment, `--do_predict` does not work in a multi-gpu setting. You need to use `evaluate_checkpoint` or the `run_eval.py` code.\n - This warning can be safely ignored:\n     > \"Some weights of BartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large-xsum and are newly initialized: ['final_logits_bias']\"\n-- Both finetuning and eval are 30% faster with `--fp16`. For that you need to [install apex](https://github.com/NVIDIA/apex#quick-start).\n+- Both finetuning and eval are 30% faster with `--fp16`.\n - Read scripts before you run them!\n \n Summarization Tips:\n@@ -130,8 +125,6 @@ Future work/help wanted: A new dataset to support multilingual tasks.\n ### Fine-tuning using Seq2SeqTrainer\n To use `Seq2SeqTrainer` for fine-tuning you should use the `finetune_trainer.py` script. It subclasses `Trainer` to extend it for seq2seq training. Except the `Trainer`-related `TrainingArguments`, it shares the same argument names as that of `finetune.py` file. One notable difference is that calculating generative metrics (BLEU, ROUGE) is optional and is controlled using the `--predict_with_generate` argument.\n \n-With PyTorch 1.6+ it'll automatically use `native AMP` when `--fp16` is set.\n-\n To see all the possible command line options, run:\n \n ```bash"
        },
        {
            "sha": "18ed7ae8e38b83a497cf3972cf13d5ed2fb5662f",
            "filename": "examples/pytorch/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/examples%2Fpytorch%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/examples%2Fpytorch%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2FREADME.md?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -123,8 +123,7 @@ torchrun \\\n ```\n \n If you have a GPU with mixed precision capabilities (architecture Pascal or more recent), you can use mixed precision\n-training with PyTorch 1.6.0 or latest, or by installing the [Apex](https://github.com/NVIDIA/apex) library for previous\n-versions. Just add the flag `--fp16` to your command launching one of the scripts mentioned above!\n+training with PyTorch 1.6.0 or latest. Just add the flag `--fp16` to your command launching one of the scripts mentioned above!\n \n Using mixed precision training usually results in 2x-speedup for training with the same final results (as shown in\n [this table](https://github.com/huggingface/transformers/tree/main/examples/text-classification#mixed-precision-training)"
        },
        {
            "sha": "f426824b5104c092a81e64c25f340f3e33fb0242",
            "filename": "examples/pytorch/text-classification/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/examples%2Fpytorch%2Ftext-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/examples%2Fpytorch%2Ftext-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2FREADME.md?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -134,8 +134,7 @@ python run_classification.py \\\n ### Mixed precision training\n \n If you have a GPU with mixed precision capabilities (architecture Pascal or more recent), you can use mixed precision\n-training with PyTorch 1.6.0 or latest, or by installing the [Apex](https://github.com/NVIDIA/apex) library for previous\n-versions. Just add the flag `--fp16` to your command launching one of the scripts mentioned above!\n+training with PyTorch 1.6.0 or latest. Just add the flag `--fp16` to your command launching one of the scripts mentioned above!\n \n Using mixed precision training usually results in 2x-speedup for training with the same final results:\n "
        },
        {
            "sha": "1552897ecd47cd6be1c8f2ca042f1711b9831741",
            "filename": "examples/pytorch/text-generation/run_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -333,7 +333,7 @@ def main():\n     parser.add_argument(\n         \"--fp16\",\n         action=\"store_true\",\n-        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n+        help=\"Whether to use 16-bit (mixed) precision instead of 32-bit\",\n     )\n     parser.add_argument(\"--jit\", action=\"store_true\", help=\"Whether or not to use jit trace to accelerate inference\")\n     args = parser.parse_args()"
        },
        {
            "sha": "0b7efad561da383d8c491647cf4cfe42a7bfd1df",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 4,
            "deletions": 20,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -217,39 +217,23 @@ def trainer_config_process(self, args, auto_find_batch_size=False):\n         self.fill_match(\"scheduler.params.warmup_max_lr\", args.learning_rate, \"learning_rate\")\n         # total_num_steps - will get set in trainer_config_finalize\n \n-        # fp16\n-        if args.fp16 or args.fp16_full_eval:\n-            fp16_backend = \"apex\" if args.fp16_backend == \"apex\" else \"amp\"\n-        else:\n-            fp16_backend = None\n-\n         if args.save_on_each_node:\n             # deepspeed uses shared storage by default. Let's override this setting if save_on_each_node == True\n             self.config[\"checkpoint\"] = self.config.get(\"checkpoint\", {})\n             self.config[\"checkpoint\"][\"use_node_local_storage\"] = args.save_on_each_node\n \n         # amp: similar to the pytorch native amp - it has a bunch of optional params but we won't set\n         # any here unless the user did the work\n-        self.fill_match(\n-            \"fp16.enabled\",\n-            ((args.fp16 or args.fp16_full_eval) and fp16_backend == \"amp\"),\n-            \"fp16|fp16_full_eval+fp16_backend(amp)\",\n-        )\n-\n-        # apex: delegates amp work to apex (which needs to be available), but it cannot be used with any\n-        # ZeRO features\n-        self.fill_match(\"amp.enabled\", fp16_backend == \"apex\", \"fp16+fp16_backend(apex)\")\n-        self.fill_match(\"amp.opt_level\", args.fp16_opt_level, \"fp16_opt_level\")\n-\n+        self.fill_match(\"fp16.enabled\", (args.fp16 or args.fp16_full_eval), \"fp16|fp16_full_eval\")\n         self.fill_match(\"bf16.enabled\", (args.bf16 or args.bf16_full_eval), \"bf16|bf16_full_eval\")\n \n         # deepspeed's default mode is fp16 unless there is a config that says differently\n         if self.is_true(\"bf16.enabled\"):\n             self._dtype = torch.bfloat16\n-        elif self.is_false(\"fp16.enabled\"):\n-            self._dtype = torch.float32\n-        else:\n+        elif self.is_true(\"fp16.enabled\"):\n             self._dtype = torch.float16\n+        else:\n+            self._dtype = torch.float32\n \n     def trainer_config_finalize(self, args, model, num_training_steps):\n         \"\"\""
        },
        {
            "sha": "2a1c77b128202b5f160418bf1fbea73c547c74d9",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -764,10 +764,7 @@ def extract_hyperparameters_from_trainer(trainer):\n         hyperparameters[\"num_epochs\"] = trainer.args.num_train_epochs\n \n     if trainer.args.fp16:\n-        if trainer.use_apex:\n-            hyperparameters[\"mixed_precision_training\"] = f\"Apex, opt level {trainer.args.fp16_opt_level}\"\n-        else:\n-            hyperparameters[\"mixed_precision_training\"] = \"Native AMP\"\n+        hyperparameters[\"mixed_precision_training\"] = \"Native AMP\"\n \n     if trainer.args.label_smoothing_factor != 0.0:\n         hyperparameters[\"label_smoothing_factor\"] = trainer.args.label_smoothing_factor"
        },
        {
            "sha": "dbeb11fca059a8618588d2bc07f1c43c9e748a6a",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 19,
            "deletions": 69,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -705,10 +705,6 @@ def __init__(\n \n         self._signature_columns = None\n \n-        # Mixed precision setup\n-        self.use_apex = False\n-        self.use_cpu_amp = False\n-\n         # Mixed precision setup for SageMaker Model Parallel\n         if is_sagemaker_mp_enabled():\n             # BF16 + model parallelism in SageMaker: currently not supported, raise an error\n@@ -731,22 +727,8 @@ def __init__(\n                         f\"FP16 provided in SM_HP_MP_PARAMETERS is {smp.state.cfg.fp16}, \"\n                         \"but SageMaker Model Parallelism < 1.10 does not support FP16 in trainer.\"\n                     )\n-        if (args.fp16 or args.bf16) and args.half_precision_backend == \"auto\":\n-            if args.device == torch.device(\"cpu\"):\n-                if args.fp16:\n-                    if not is_torch_greater_or_equal_than_2_3:\n-                        raise ValueError(\"Tried to use `fp16` but it is not supported on cpu\")\n-                else:\n-                    args.half_precision_backend = \"cpu_amp\"\n-            logger.info(f\"Using {args.half_precision_backend} half precision backend\")\n-\n-        if (args.fp16 or args.bf16) and not (self.is_deepspeed_enabled or is_sagemaker_mp_enabled()):\n-            # deepspeed and SageMaker Model Parallel manage their own half precision\n-            if args.half_precision_backend == \"cpu_amp\":\n-                self.use_cpu_amp = True\n-                self.amp_dtype = torch.bfloat16\n-            elif args.half_precision_backend == \"apex\":\n-                self.use_apex = True\n+        if args.fp16 and args.device == torch.device(\"cpu\") and not is_torch_greater_or_equal_than_2_3:\n+            raise ValueError(\"Tried to use `fp16` but it is not supported on cpu. You need to have torch>=2.3\")\n \n         # Label smoothing\n         if self.args.label_smoothing_factor != 0:\n@@ -2014,7 +1996,6 @@ def torch_jit_model_eval(self, model, dataloader, training=False):\n                     jit_model(**example_batch)\n                     jit_model(**example_batch)\n                 model = jit_model\n-                self.use_cpu_amp = False\n             except (RuntimeError, TypeError, ValueError, NameError, IndexError) as e:\n                 logger.warning(f\"failed to use PyTorch jit mode due to: {e}.\")\n \n@@ -2059,13 +2040,7 @@ def _wrap_model(self, model, training=True, dataloader=None):\n         if self.accelerator.unwrap_model(model, keep_torch_compile=False) is not model:\n             return model\n \n-        # Mixed precision training with apex\n-        if self.use_apex and training:\n-            from apex import amp\n-\n-            model, self.optimizer = amp.initialize(model, self.optimizer, opt_level=self.args.fp16_opt_level)\n-\n-        # Multi-gpu training (should be after apex fp16 initialization) / 8bit models does not support DDP\n+        # Multi-gpu training, 8bit models does not support DP\n         if self.args.n_gpu > 1 and not getattr(model, \"is_loaded_in_8bit\", False):\n             model = nn.DataParallel(model)\n \n@@ -2079,7 +2054,6 @@ def _wrap_model(self, model, training=True, dataloader=None):\n         if not training:\n             return model\n \n-        # Distributed training (should be after apex fp16 initialization)\n         # Distributed training using PyTorch FSDP\n         if self.is_fsdp_xla_enabled:\n             try:\n@@ -2462,14 +2436,11 @@ def _inner_training_loop(\n         if use_accelerator_prepare:\n             self.model.train()\n             if hasattr(self.lr_scheduler, \"step\"):\n-                if self.use_apex:\n-                    model = self.accelerator.prepare(self.model)\n+                # We should avoid accelerate preparing the model in TP case since we dont need it as it is handled by transformers from_pretrained and also it goes into DDP based preparation.\n+                if self.is_tp_enabled:\n+                    self.optimizer = self.accelerator.prepare(self.optimizer)\n                 else:\n-                    # We should avoid accelerate preparing the model in TP case since we dont need it as it is handled by transformers from_pretrained and also it goes into DDP based preparation.\n-                    if self.is_tp_enabled:\n-                        self.optimizer = self.accelerator.prepare(self.optimizer)\n-                    else:\n-                        model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n+                    model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)\n             else:\n                 # to handle cases wherein we pass \"DummyScheduler\" such as when it is specified in DeepSpeed config.\n                 model, self.optimizer, self.lr_scheduler = self.accelerator.prepare(\n@@ -2690,14 +2661,6 @@ def _inner_training_loop(\n                         if args.max_grad_norm is not None and args.max_grad_norm > 0:\n                             if is_sagemaker_mp_enabled() and args.fp16:\n                                 _grad_norm = self.optimizer.clip_master_grads(args.max_grad_norm)\n-                            elif self.use_apex:\n-                                from apex import amp\n-\n-                                # Revert to normal clipping otherwise, handling Apex or full precision\n-                                _grad_norm = nn.utils.clip_grad_norm_(\n-                                    amp.master_params(self.optimizer),\n-                                    args.max_grad_norm,\n-                                )\n                             else:\n                                 grad_norm_context = contextlib.nullcontext\n                                 if self.is_tp_enabled:\n@@ -3964,14 +3927,9 @@ def compute_loss_context_manager(self):\n     def autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True):\n         \"\"\"\n         A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\n-        arguments, depending on the situation.\n+        arguments, depending on the situation. We rely on accelerate for autocast, hence we do nothing here.\n         \"\"\"\n-        if self.use_cpu_amp:\n-            ctx_manager = torch.autocast(device_type=\"cpu\", cache_enabled=cache_enabled, dtype=self.amp_dtype)\n-        else:\n-            ctx_manager = contextlib.nullcontext()\n-\n-        return ctx_manager\n+        return contextlib.nullcontext()\n \n     def training_step(\n         self,\n@@ -4045,25 +4003,17 @@ def training_step(\n             if self.args.n_gpu > 1:\n                 loss = loss.mean()  # mean() to average on multi-gpu parallel training\n \n-            if self.use_apex:\n-                from apex import amp\n+            # Finally we need to normalize the loss for reporting if GA loss bug is not fixed during compute loss\n+            if (not self.model_accepts_loss_kwargs or num_items_in_batch is None) and self.compute_loss_func is None:\n+                # If the model does not accept loss kwargs, we need to normalize the loss by the number of gradient accumulation steps\n+                loss = loss / self.current_gradient_accumulation_steps\n \n-                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n-                    scaled_loss.backward()\n-            else:\n-                # Finally we need to normalize the loss for reporting if GA loss bug is not fixed during compute loss\n-                if (\n-                    not self.model_accepts_loss_kwargs or num_items_in_batch is None\n-                ) and self.compute_loss_func is None:\n-                    # If the model does not accept loss kwargs, we need to normalize the loss by the number of gradient accumulation steps\n-                    loss = loss / self.current_gradient_accumulation_steps\n-\n-                # Turning off loss scaling w.r.t. gradient accumulation when DeepSpeed is enabled\n-                # https://github.com/huggingface/transformers/pull/35808\n-                if self.accelerator.distributed_type == DistributedType.DEEPSPEED:\n-                    kwargs[\"scale_wrt_gas\"] = False\n-\n-                self.accelerator.backward(loss, **kwargs)\n+            # Turning off loss scaling w.r.t. gradient accumulation when DeepSpeed is enabled\n+            # https://github.com/huggingface/transformers/pull/35808\n+            if self.accelerator.distributed_type == DistributedType.DEEPSPEED:\n+                kwargs[\"scale_wrt_gas\"] = False\n+\n+            self.accelerator.backward(loss, **kwargs)\n \n             return loss.detach()\n "
        },
        {
            "sha": "3520ea5702aa72ba686258961c2dec2c4028b1e4",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 15,
            "deletions": 46,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -36,7 +36,6 @@\n     ACCELERATE_MIN_VERSION,\n     ExplicitEnum,\n     is_accelerate_available,\n-    is_apex_available,\n     is_ipex_available,\n     is_safetensors_available,\n     is_sagemaker_dp_enabled,\n@@ -395,13 +394,6 @@ class TrainingArguments:\n             NVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n         fp16 (`bool`, *optional*, defaults to `False`):\n             Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n-        fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n-            For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n-            the [Apex documentation](https://nvidia.github.io/apex/amp).\n-        half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n-            The backend to use for mixed precision training. Must be one of `\"auto\", \"apex\", \"cpu_amp\"`. `\"auto\"` will\n-            use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\n-            requested backend.\n         bf16_full_eval (`bool`, *optional*, defaults to `False`):\n             Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n             metric values. This is an experimental API and it may change.\n@@ -609,7 +601,7 @@ class TrainingArguments:\n \n             The options should be separated by whitespaces.\n         optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"` (for torch>=2.8 `\"adamw_torch_fused\"`)):\n-            The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n+            The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_anyprecision\",\n             \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n             for a full list of optimizers.\n         optim_args (`str`, *optional*):\n@@ -1027,20 +1019,10 @@ class TrainingArguments:\n         default=False,\n         metadata={\"help\": \"Whether to use fp16 (mixed) precision instead of 32-bit\"},\n     )\n-    fp16_opt_level: str = field(\n-        default=\"O1\",\n-        metadata={\n-            \"help\": (\n-                \"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. \"\n-                \"See details at https://nvidia.github.io/apex/amp.html\"\n-            )\n-        },\n-    )\n-    half_precision_backend: str = field(\n-        default=\"auto\",\n+    half_precision_backend: Optional[str] = field(\n+        default=None,\n         metadata={\n-            \"help\": \"The backend to be used for half precision.\",\n-            \"choices\": [\"auto\", \"apex\", \"cpu_amp\"],\n+            \"help\": \"The backend to be used for half precision. This argument is deprecated. We will always use CPU/CUDA AMP from torch\",\n         },\n     )\n     bf16_full_eval: bool = field(\n@@ -1610,29 +1592,17 @@ def __post_init__(self):\n                         # gpu\n                         raise ValueError(error_message)\n \n+        if self.half_precision_backend is not None:\n+            raise ValueError(\n+                \"half_precision_backend is deprecated. For mixed precision, we will always use CPU/CUDA AMP from torch\"\n+            )\n+\n         if self.fp16 and self.bf16:\n             raise ValueError(\"At most one of fp16 and bf16 can be True, but not both\")\n \n         if self.fp16_full_eval and self.bf16_full_eval:\n             raise ValueError(\"At most one of fp16 and bf16 can be True for full eval, but not both\")\n \n-        if self.bf16:\n-            if self.half_precision_backend == \"apex\":\n-                raise ValueError(\" `--half_precision_backend apex`: GPU bf16 is not supported by apex.\")\n-\n-        if self.half_precision_backend == \"apex\":\n-            if not is_apex_available():\n-                raise ImportError(\n-                    \"Using FP16 with APEX but APEX is not installed, please refer to\"\n-                    \" https://www.github.com/nvidia/apex.\"\n-                )\n-            try:\n-                from apex import amp  # noqa: F401\n-            except ImportError as e:\n-                raise ImportError(\n-                    f\"apex.amp is deprecated in the latest version of apex, causing this error {e}. Either revert to an older version or use pytorch amp by setting half_precision_backend='auto' instead. See https://github.com/NVIDIA/apex/pull/1896 \"\n-                )\n-\n         if self.lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU:\n             if self.eval_strategy == IntervalStrategy.NO:\n                 raise ValueError(\"lr_scheduler_type reduce_lr_on_plateau requires an eval strategy\")\n@@ -1909,13 +1879,12 @@ def __post_init__(self):\n \n         # Set mixed precision environment variable after DeepSpeed processing\n         # This ensures DeepSpeed config overrides have been applied to fp16/bf16 settings\n-        if self.half_precision_backend != \"apex\":\n-            mixed_precision_dtype = os.environ.get(\"ACCELERATE_MIXED_PRECISION\", \"no\")\n-            if self.fp16:\n-                mixed_precision_dtype = \"fp16\"\n-            elif self.bf16:\n-                mixed_precision_dtype = \"bf16\"\n-            os.environ[\"ACCELERATE_MIXED_PRECISION\"] = mixed_precision_dtype\n+        mixed_precision_dtype = os.environ.get(\"ACCELERATE_MIXED_PRECISION\", \"no\")\n+        if self.fp16:\n+            mixed_precision_dtype = \"fp16\"\n+        elif self.bf16:\n+            mixed_precision_dtype = \"bf16\"\n+        os.environ[\"ACCELERATE_MIXED_PRECISION\"] = mixed_precision_dtype\n \n         if self.use_cpu:\n             self.dataloader_pin_memory = False"
        },
        {
            "sha": "1789f9f6c98b84d21f46008857f641a12d2bad89",
            "filename": "tests/extended/test_trainer_ext.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/tests%2Fextended%2Ftest_trainer_ext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/tests%2Fextended%2Ftest_trainer_ext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fextended%2Ftest_trainer_ext.py?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -29,11 +29,8 @@\n     backend_device_count,\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n-    require_apex,\n     require_bitsandbytes,\n-    require_non_xpu,\n     require_torch,\n-    require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torch_non_multi_accelerator,\n     slow,\n@@ -107,23 +104,6 @@ def test_run_seq2seq_dp(self):\n     def test_run_seq2seq_ddp(self):\n         self.run_seq2seq_quick(distributed=True)\n \n-    @require_non_xpu\n-    @require_apex\n-    @require_torch_gpu\n-    def test_run_seq2seq_apex(self):\n-        # XXX: apex breaks the trainer if it's run twice e.g. run_seq2seq.main() from the same\n-        # program and it breaks other tests that run from the same pytest worker, therefore until this is\n-        # sorted out it must be run only in an external program, that is distributed=True in this\n-        # test and only under one or more gpus - if we want cpu will need to make a special test\n-        #\n-        # specifically to the problem traced it to self.optimizer.step() - if it's run 2nd time via\n-        # 2nd main() call it botches the future eval.\n-        #\n-        self.run_seq2seq_quick(distributed=True, extra_args_str=\"--fp16 --fp16_backend=apex\")\n-        # test 2nd time - was getting eval_loss': nan'\n-        # to reproduce the problem set distributed=False\n-        self.run_seq2seq_quick(distributed=True, extra_args_str=\"--fp16 --fp16_backend=apex\")\n-\n     @parameterized.expand([\"base\", \"low\", \"high\", \"mixed\"])\n     @require_torch_multi_accelerator\n     def test_trainer_log_level_replica(self, experiment_id):"
        },
        {
            "sha": "1765666342cc646d1b532e1f7ce2ba441239749f",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/06c04e085154bb19ea16aac80ae02537fe3b2fd2/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06c04e085154bb19ea16aac80ae02537fe3b2fd2/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=06c04e085154bb19ea16aac80ae02537fe3b2fd2",
            "patch": "@@ -1291,13 +1291,6 @@ def test_mixed_bf16(self):\n             trainer.train()\n             self.check_trained_model(trainer.model, atol=ATOL, rtol=RTOL)\n \n-        # --bf16 --half_precision_backend apex can't be used together\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            with self.assertRaises(ValueError):\n-                trainer = get_regression_trainer(\n-                    learning_rate=0.1, bf16=True, half_precision_backend=\"apex\", output_dir=tmp_dir\n-                )\n-\n     @require_torch_gpu\n     @require_torch_tf32\n     def test_tf32(self):"
        }
    ],
    "stats": {
        "total": 322,
        "additions": 47,
        "deletions": 275
    }
}