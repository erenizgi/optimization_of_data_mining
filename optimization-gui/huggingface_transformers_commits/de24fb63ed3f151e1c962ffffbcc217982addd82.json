{
    "author": "qgallouedec",
    "message": "Use HF papers (#38184)\n\n* Use hf papers\n\n* Hugging Face papers\n\n* doi to hf papers\n\n* style",
    "sha": "de24fb63ed3f151e1c962ffffbcc217982addd82",
    "files": [
        {
            "sha": "78f85b5c1060f6827d05fabfe1330760bb3c2f3e",
            "filename": "docs/source/ar/bertology.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fbertology.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -3,16 +3,16 @@\n ููุดูุฏ ูู ุงูุขููุฉ ุงูุฃุฎูุฑุฉ ููู ูุฌุงู ุฏุฑุงุณู ููุนูู ุจุงุณุชูุดุงู ุขููุฉ ุนูู ููุงุฐุฌ ุงููุญููุงุช ุงูุถุฎูุฉ ูุซู BERT (ูุงูุฐู ููุทูู ุนูููุง ุงูุจุนุถ ุงุณู \"BERTology\"). ููู ุงูุฃูุซูุฉ ุงูุจุงุฑุฒุฉ ุนูู ูุฐุง ุงููุฌุงู ูุง ููู:\n \n - BERT Rediscovers the Classical NLP Pipeline ุจูุงุณุทุฉ Ian Tenney ู Dipanjan Das ู Ellie Pavlick:\n-  https://arxiv.org/abs/1905.05950\n-- Are Sixteen Heads Really Better than One? ุจูุงุณุทุฉ Paul Michel ู Omer Levy ู Graham Neubig: https://arxiv.org/abs/1905.10650\n+  https://huggingface.co/papers/1905.05950\n+- Are Sixteen Heads Really Better than One? ุจูุงุณุทุฉ Paul Michel ู Omer Levy ู Graham Neubig: https://huggingface.co/papers/1905.10650\n - What Does BERT Look At? An Analysis of BERT's Attention ุจูุงุณุทุฉ Kevin Clark ู Urvashi Khandelwal ู Omer Levy ู Christopher D.\n-  Manning: https://arxiv.org/abs/1906.04341\n-- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: https://arxiv.org/abs/2210.04633\n+  Manning: https://huggingface.co/papers/1906.04341\n+- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: https://huggingface.co/papers/2210.04633\n \n-ูุฅุซุฑุงุก ูุฐุง ุงููุฌุงู ุงููุงุดุฆุ ูููุง ุจุชุถููู ุจุนุถ ุงูููุฒุงุช ุงูุฅุถุงููุฉ ูู ููุงุฐุฌ BERT/GPT/GPT-2 ููุณูุงุญ ูููุงุณ ุจุงููุตูู ุฅูู ุงูุชูุซููุงุช ุงูุฏุงุฎููุฉุ ูุงูุชู ุชู ุชูููููุง ุจุดูู ุฃุณุงุณู ูู ุงูุนูู ุงูุฑุงุฆุฏ ูู Paul Michel (https://arxiv.org/abs/1905.10650):\n+ูุฅุซุฑุงุก ูุฐุง ุงููุฌุงู ุงููุงุดุฆุ ูููุง ุจุชุถููู ุจุนุถ ุงูููุฒุงุช ุงูุฅุถุงููุฉ ูู ููุงุฐุฌ BERT/GPT/GPT-2 ููุณูุงุญ ูููุงุณ ุจุงููุตูู ุฅูู ุงูุชูุซููุงุช ุงูุฏุงุฎููุฉุ ูุงูุชู ุชู ุชูููููุง ุจุดูู ุฃุณุงุณู ูู ุงูุนูู ุงูุฑุงุฆุฏ ูู Paul Michel (https://huggingface.co/papers/1905.10650):\n \n - ุงููุตูู ุฅูู ุฌููุน ุงูุญุงูุงุช ุงููุฎููุฉ ูู BERT/GPT/GPT-2ุ\n - ุงููุตูู ุฅูู ุฌููุน ุฃูุฒุงู ุงูุงูุชุจุงู ููู ุฑุฃุณ ูู BERT/GPT/GPT-2ุ\n-- ุงุณุชุฑุฌุงุน ููู ููุดุชูุงุช  ูุฎุฑุฌุงุช ุงูุฑุฃุณ ูุญุณุงุจ ุฏุฑุฌุฉ ุฃูููุฉ ุงูุฑุฃุณ ูุญุฐูู ููุง ูู ููุถุญ ูู https://arxiv.org/abs/1905.10650.\n+- ุงุณุชุฑุฌุงุน ููู ููุดุชูุงุช  ูุฎุฑุฌุงุช ุงูุฑุฃุณ ูุญุณุงุจ ุฏุฑุฌุฉ ุฃูููุฉ ุงูุฑุฃุณ ูุญุฐูู ููุง ูู ููุถุญ ูู https://huggingface.co/papers/1905.10650.\n \n ูููุณุงุนุฏุชู ุนูู ููู ูุงุณุชุฎุฏุงู ูุฐู ุงูููุฒุงุช ุจุณูููุฉุ ุฃุถููุง ูุซุงููุง ุจุฑูุฌููุง ูุญุฏุฏูุง: [bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py) ุฃุซูุงุก ุงุณุชุฎุฑุงุฌ ุงููุนูููุงุช  ูุชูููุต ูู ูููุฐุฌ ุชู ุชุฏุฑูุจู ูุณุจููุง ุนูู GLUE.\n\\ No newline at end of file"
        },
        {
            "sha": "b1c59a68c399e96b75eee58070185da4e2277ac7",
            "filename": "docs/source/ar/glossary.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fglossary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fglossary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fglossary.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -135,7 +135,7 @@\n ูู ูู ูุญุฏุฉ ุงูุงูุชุจุงู ุงูุจุงููุฉ ูู ุงููุญููุงุชุ ุชูู ุทุจูุฉ ุงูุงูุชูุงู ุงูุงูุชุจุงู ุนุงุฏุฉ ุทุจูุชุงู ููุชุบุฐูุฉ ุงูุฃูุงููุฉ.\n ุญุฌู ุชุถููู ุงูุทุจูุฉ ุงูุฃูุงููุฉ ุงููุณูุทุฉ ุฃูุจุฑ ุนุงุฏุฉ ูู ุญุฌู ุงููุฎูู ูููููุฐุฌ (ุนูู ุณุจูู ุงููุซุงูุ ูู\n `google-bert/bert-base-uncased`).\n-ุจุงููุณุจุฉ ูุฅุฏุฎุงู ุจุญุฌู `[batch_size, sequence_length]`ุ ูููู ุฃู ุชูุซู ุงูุฐุงูุฑุฉ ุงููุทููุจุฉ ูุชุฎุฒูู ุงูุชุถูููุงุช ุงูุฃูุงููุฉ ุงููุณูุทุฉ `[batch_sizeุ sequence_length, config.intermediate_size]` ุฌุฒุกูุง ูุจูุฑูุง ูู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ. ูุงุญุธ ูุคููู (https://arxiv.org/abs/2001.04451)[Reformer: The Efficient Transformer] ุฃูู ูุธุฑูุง ูุฃู ุงูุญุณุงุจ ูุณุชูู ุนู ุจุนุฏ `sequence_length`ุ ูุฅูู ูู ุงูููุงูุฆ ุฑูุงุถููุง ุญุณุงุจ ุชุถูููุงุช ุงูุฅุฎุฑุงุฌ ุงูุฃูุงููุฉ `[batch_sizeุ config.hidden_size]_0, ..., [batch_sizeุ `config_size]_n\n+ุจุงููุณุจุฉ ูุฅุฏุฎุงู ุจุญุฌู `[batch_size, sequence_length]`ุ ูููู ุฃู ุชูุซู ุงูุฐุงูุฑุฉ ุงููุทููุจุฉ ูุชุฎุฒูู ุงูุชุถูููุงุช ุงูุฃูุงููุฉ ุงููุณูุทุฉ `[batch_sizeุ sequence_length, config.intermediate_size]` ุฌุฒุกูุง ูุจูุฑูุง ูู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ. ูุงุญุธ ูุคููู (https://huggingface.co/papers/2001.04451)[Reformer: The Efficient Transformer] ุฃูู ูุธุฑูุง ูุฃู ุงูุญุณุงุจ ูุณุชูู ุนู ุจุนุฏ `sequence_length`ุ ูุฅูู ูู ุงูููุงูุฆ ุฑูุงุถููุง ุญุณุงุจ ุชุถูููุงุช ุงูุฅุฎุฑุงุฌ ุงูุฃูุงููุฉ `[batch_sizeุ config.hidden_size]_0, ..., [batch_sizeุ `config_size]_n\n ูุฑุฏูุงู ูุงูุชูุตูู ุจูุง ูุงุญููุง ุฅูู `[batch_size, sequence_length, config.hidden_size]` ูุน `n = sequence_length`ุ ูุงูุฐู ูุชุฏุงูู ุฒูุงุฏุฉ ููุช ุงูุญุณุงุจ ููุงุจู ุชูููู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉุ ููููู ููุชุฌ ุนูู ูุชูุฌุฉ ููุงูุฆุฉ ุฑูุงุถูุง.\n \n ุจุงููุณุจุฉ ููููุงุฐุฌ ุงูุชู ุชุณุชุฎุฏู ุงูุฏุงูุฉ `[apply_chunking_to_forward]`ุ ูุญุฏุฏ `chunk_size` ุนุฏุฏ ุงูุชุถูููุงุช ูุชู ุญุณุงุจ ุงูุฅุฎุฑุงุฌ ุจุงูุชูุงุฒู ูุจุงูุชุงูู ูุญุฏุฏ ุงูููุงูุถุฉ ุจูู ุญุฌู ุงูุฐุงูุฑุฉ ูุงูุชุนููุฏ ุงูููุช. ุฅุฐุง ุชู ุชุนููู `chunk_size` ุฅูู `0`ุ ููู ูุชู ุฅุฌุฑุงุก ุชุฌุฒุฆุฉ ุงูุชุบุฐูุฉ ุงูุฃูุงููุฉ.\n@@ -173,7 +173,7 @@\n \n <Youtube id=\"VFp38yj8h3A\"/>\n \n-ูุนูู ูู ูุญูู ูุบูู ุจุดูู ูุฎุชูู ูููู ุงูุขููุฉ ุงูุฃุณุงุณูุฉ ุชุจูู ููุง ูู. ุฅููู ูุซุงู ุจุงุณุชุฎุฏุงู ูุญูู BERT ุงููุบููุ ูุงูุฐู ูุนุฏ ูุญูู ูุบูู [WordPiece](https://arxiv.org/pdf/1609.08144.pdf):\n+ูุนูู ูู ูุญูู ูุบูู ุจุดูู ูุฎุชูู ูููู ุงูุขููุฉ ุงูุฃุณุงุณูุฉ ุชุจูู ููุง ูู. ุฅููู ูุซุงู ุจุงุณุชุฎุฏุงู ูุญูู BERT ุงููุบููุ ูุงูุฐู ูุนุฏ ูุญูู ูุบูู [WordPiece](https://huggingface.co/papers/1609.08144):\n \n ```python\n >>> from transformers import BertTokenizer"
        },
        {
            "sha": "c4a45a609a1bd8ae9f4e2bef31a3273eefd842a1",
            "filename": "docs/source/ar/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -6,7 +6,7 @@\n ุชุญูู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ (LLMs) ูุซู GPT3/4ุ [Falcon](https://huggingface.co/tiiuae/falcon-40b)ุ ู [Llama](https://huggingface.co/meta-llama/Llama-2-70b-hf) ุชูุฏููุง ุณุฑูุนูุง ูู ูุฏุฑุชูุง ุนูู ูุนุงูุฌุฉ ุงูููุงู ุงูุชู ุชุฑูุฒ ุนูู ุงูุฅูุณุงูุ ููุง ูุฌุนููุง ุฃุฏูุงุช ุฃุณุงุณูุฉ ูู ุงูุตูุงุนุงุช ุงููุงุฆูุฉ ุนูู ุงููุนุฑูุฉ ุงูุญุฏูุซุฉ.\n ูุง ูุฒุงู ูุดุฑ ูุฐู ุงูููุงุฐุฌ ูู ุงูููุงู ุงููุงูุนูุฉ ููุซู ุชุญุฏููุงุ ููุน ุฐูู:\n \n--   ููู ุชุธูุฑ ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ูุฏุฑุงุช ููู ูุชูููุฏ ุงููุตูุต ูุฑูุจุฉ ูู ูุฏุฑุงุช ุงูุฅูุณุงูุ ูุฅููุง ุชุชุทูุจ ุญุงูููุง  ุฅูู ุชูููููุง ูู ูููุงุฑุงุช ุงููุนููุงุช (ุงูุธุฑ [ูุงุจูุงู ูุขุฎุฑูู](https://arxiv.org/abs/2001.08361)ุ [ูู ูุขุฎุฑูู](https://arxiv.org/abs/2206.07682)). ููุฐุง ุจุฏูุฑู ูุฒูุฏ ูู ูุชุทูุจุงุช ุงูุฐุงูุฑุฉ ููุงุณุชุฏูุงู.\n+-   ููู ุชุธูุฑ ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ูุฏุฑุงุช ููู ูุชูููุฏ ุงููุตูุต ูุฑูุจุฉ ูู ูุฏุฑุงุช ุงูุฅูุณุงูุ ูุฅููุง ุชุชุทูุจ ุญุงูููุง  ุฅูู ุชูููููุง ูู ูููุงุฑุงุช ุงููุนููุงุช (ุงูุธุฑ [ูุงุจูุงู ูุขุฎุฑูู](https://huggingface.co/papers/2001.08361)ุ [ูู ูุขุฎุฑูู](https://huggingface.co/papers/2206.07682)). ููุฐุง ุจุฏูุฑู ูุฒูุฏ ูู ูุชุทูุจุงุช ุงูุฐุงูุฑุฉ ููุงุณุชุฏูุงู.\n -   ูู ุงูุนุฏูุฏ ูู ุงูููุงู ุงููุงูุนูุฉุ ุชุญุชุงุฌ ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ุฅูู ูุนูููุงุช ุณูุงููุฉ ุดุงููุฉ. ูุชุทูุจ ุฐูู ูุฏุฑุฉ ุงููููุฐุฌ ุนูู ุฅุฏุงุฑุฉ ุชุณูุณูุงุช ุฅุฏุฎุงู ุทูููุฉ ููุบุงูุฉ ุฃุซูุงุก ุงูุงุณุชุฏูุงู.\n \n ูููู ุฌููุฑ ุตุนูุจุฉ ูุฐู ุงูุชุญุฏูุงุช ูู ุชุนุฒูุฒ ุงููุฏุฑุงุช ุงูุญุณุงุจูุฉ ูุงูุฐุงูุฑุฉ ูููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉุ ุฎุงุตุฉ ุนูุฏ ุงูุชุนุงูู ูุน ุชุณูุณูุงุช ุงูุฅุฏุฎุงู ุงูุถุฎูุฉ.\n@@ -17,7 +17,7 @@\n \n 2.  **ุงFlash Attention:** ุฅู Flash Attention ููู ูุณุฎุฉ ููุนุฏูููุฉ ูู ุฎูุงุฑุฒููุฉ ุงูุงูุชุจุงู ุงูุชู ูุง ุชููุฑ ููุท ููุฌูุง ุฃูุซุฑ ููุงุกุฉ ูู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉุ ูููููุง ุชุญูู ุฃูุถูุง ููุงุกุฉ ูุชุฒุงูุฏุฉ ุจุณุจุจ ุงูุงุณุชุฎุฏุงู ุงูุฃูุซู ูุฐุงูุฑุฉ GPU.\n \n-3.  **ุงูุงุจุชูุงุฑุงุช ุงููุนูุงุฑูุฉ:** ุญูุซ ุชู ุงูุชุฑุงุญ ููุงูู ูุชุฎุตุตุฉ ุชุณูุญ ุจุงุณุชุฏูุงู ุฃูุซุฑ ูุนุงููุฉ ูุธุฑูุง ูุฃู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ูุชู ูุดุฑูุง ุฏุงุฆููุง ุจููุณ ุงูุทุฑููุฉ ุฃุซูุงุก ุนูููุฉ ุงูุงุณุชุฏูุงูุ ุฃู ุชูููุฏ ุงููุต ุงูุชูุจุคู ุงูุชููุงุฆู ูุน ุณูุงู ุงูุฅุฏุฎุงู ุงูุทูููุ ููุฏ ุชู ุงูุชุฑุงุญ ุจููุงุช ูููุฐุฌ ูุชุฎุตุตุฉ ุชุณูุญ ุจุงูุงุณุชุฏูุงู ุงูุฃูุซุฑ ููุงุกุฉ. ุฃูู ุชูุฏู ูู ุจููุงุช ุงูููุงุฐุฌ ููุง ูู [ุนุฐุฑ](https://arxiv.org/abs/2108.12409)ุ [ุงูุชุฑููุฒ ุงูุฏูุงุฑ](https://arxiv.org/abs/2104.09864)ุ [ุงูุงูุชูุงู ูุชุนุฏุฏ ุงูุงุณุชุนูุงูุงุช (MQA)](https://arxiv.org/abs/1911.02150) ู [ูุฌููุนุฉ ุงูุงูุชุจุงู ุจุงูุงุณุชุนูุงู (GQA)]((https://arxiv.org/abs/2305.13245)).\n+3.  **ุงูุงุจุชูุงุฑุงุช ุงููุนูุงุฑูุฉ:** ุญูุซ ุชู ุงูุชุฑุงุญ ููุงูู ูุชุฎุตุตุฉ ุชุณูุญ ุจุงุณุชุฏูุงู ุฃูุซุฑ ูุนุงููุฉ ูุธุฑูุง ูุฃู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ูุชู ูุดุฑูุง ุฏุงุฆููุง ุจููุณ ุงูุทุฑููุฉ ุฃุซูุงุก ุนูููุฉ ุงูุงุณุชุฏูุงูุ ุฃู ุชูููุฏ ุงููุต ุงูุชูุจุคู ุงูุชููุงุฆู ูุน ุณูุงู ุงูุฅุฏุฎุงู ุงูุทูููุ ููุฏ ุชู ุงูุชุฑุงุญ ุจููุงุช ูููุฐุฌ ูุชุฎุตุตุฉ ุชุณูุญ ุจุงูุงุณุชุฏูุงู ุงูุฃูุซุฑ ููุงุกุฉ. ุฃูู ุชูุฏู ูู ุจููุงุช ุงูููุงุฐุฌ ููุง ูู [ุนุฐุฑ](https://huggingface.co/papers/2108.12409)ุ [ุงูุชุฑููุฒ ุงูุฏูุงุฑ](https://huggingface.co/papers/2104.09864)ุ [ุงูุงูุชูุงู ูุชุนุฏุฏ ุงูุงุณุชุนูุงูุงุช (MQA)](https://huggingface.co/papers/1911.02150) ู [ูุฌููุนุฉ ุงูุงูุชุจุงู ุจุงูุงุณุชุนูุงู (GQA)]((https://huggingface.co/papers/2305.13245)).\n \n ุนูู ูุฏุงุฑ ูุฐุง ุงูุฏูููุ ุณููุฏู ุชุญููููุง ููุชูููุฏ ุงูุชูุจุคู ุงูุชููุงุฆู ูู ููุธูุฑ ุงููููุชููุฑุงุช. ูุชุนูู ูู ูุฒุงูุง ูุนููุจ ุงุณุชุฎุฏุงู ุฏูุฉ ุฃููุ ูููุฏู ุงุณุชูุดุงููุง ุดุงููุงู ูุฎูุงุฑุฒููุงุช ุงูุงูุชุจุงู ุงูุฃุญุฏุซุ ูููุงูุด ุจููุงุช ููุงุฐุฌ ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ุงููุญุณูุฉ. ุณูุฏุนู ุงูุดุฑุญ ุจุฃูุซูุฉ ุนูููุฉ ุชูุจุฑูุฒ ูู ุชุญุณูู ุนูู ุญุฏุฉ.\n \n@@ -152,8 +152,8 @@ from accelerate.utils import release_memory\n release_memory(model)\n ```\n \n-ูุงูุขู ูุงุฐุง ูู ูู ููู ูุฏู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ูุฏูู 32 ุฌูุฌุง ุจุงูุช ูู ุฐุงูุฑุฉ ุงูููุฏูู ุงูุนุดูุงุฆูุฉ (VRAM)ุ ููุฏ ูุฌุฏ ุฃู ุฃูุฒุงู ุงูููุงุฐุฌ ูููู ุชุญููููุง ุฅูู 8 ุจุชุงุช ุฃู 4 ุจุชุงุช ุฏูู ุฎุณุงุฑุฉ ูุจูุฑุฉ ูู ุงูุฃุฏุงุก (ุงูุธุฑ [Dettmers et al.](https://arxiv.org/abs/2208.07339)).\n-ูููู ุชุญููู ุงููููุฐุฌ ุฅูู 3 ุจุชุงุช ุฃู 2 ุจุชุงุช ูุน ููุฏุงู ููุจูู ูู ุงูุฃุฏุงุก ููุง ูู ููุถุญ ูู ูุฑูุฉ [GPTQ](https://arxiv.org/abs/2210.17323) ๐คฏ.\n+ูุงูุขู ูุงุฐุง ูู ูู ููู ูุฏู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ูุฏูู 32 ุฌูุฌุง ุจุงูุช ูู ุฐุงูุฑุฉ ุงูููุฏูู ุงูุนุดูุงุฆูุฉ (VRAM)ุ ููุฏ ูุฌุฏ ุฃู ุฃูุฒุงู ุงูููุงุฐุฌ ูููู ุชุญููููุง ุฅูู 8 ุจุชุงุช ุฃู 4 ุจุชุงุช ุฏูู ุฎุณุงุฑุฉ ูุจูุฑุฉ ูู ุงูุฃุฏุงุก (ุงูุธุฑ [Dettmers et al.](https://huggingface.co/papers/2208.07339)).\n+ูููู ุชุญููู ุงููููุฐุฌ ุฅูู 3 ุจุชุงุช ุฃู 2 ุจุชุงุช ูุน ููุฏุงู ููุจูู ูู ุงูุฃุฏุงุก ููุง ูู ููุถุญ ูู ูุฑูุฉ [GPTQ](https://huggingface.co/papers/2210.17323) ๐คฏ.\n \n ุฏูู ุงูุฏุฎูู ูู ุงููุซูุฑ ูู ุงูุชูุงุตููุ ุชูุฏู ูุฎุทุทุงุช ุงูุชูููู ุฅูู ุชุฎููุถ ุฏูุฉ ุงูุฃูุฒุงู ูุน ูุญุงููุฉ ุงูุญูุงุธ ุนูู ุฏูุฉ ูุชุงุฆุฌ ุงููููุฐุฌ ููุง ูู (*ุฃู* ุฃูุฑุจ ูุง ูููู ุฅูู bfloat16).\n ูุงุญุธ ุฃู ุงูุชูููู ูุนูู ุจุดูู ุฎุงุต ุฌูุฏูุง ูุชูููุฏ ุงููุต ุญูุซ ูู ูุง ููุชู ุจู ูู ุงุฎุชูุงุฑ *ูุฌููุนุฉ ุงูุฑููุฒ ุงูุฃูุซุฑ ุงุญุชูุงููุง ุงูุชุงููุฉ* ููุง ููุชู ุญููุง ุจุงูููู ุงูุฏูููุฉ ูุชูุฒูุน ุงูุฑูุฒ ุงูุชุงูู *logit*.\n@@ -304,7 +304,7 @@ $$ \\textbf{O} = \\text{Attn}(\\mathbf{X}) = \\mathbf{V} \\times \\text{Softmax}(\\math\n \n ูุน ุชุญุณู LLMs ูู ููู ุงููุต ูุชูููุฏ ุงููุตุ ูุชู ุชุทุจูููุง ุนูู ููุงู ูุชุฒุงูุฏุฉ ุงูุชุนููุฏ. ูู ุญูู ุฃู ุงูููุงุฐุฌ ูุงูุช ุชุชุนุงูู ุณุงุจููุง ูุน ุชุฑุฌูุฉ ุฃู ุชูุฎูุต ุจุถุน ุฌููุ ูุฅููุง ุงูุขู ุชุฏูุฑ ุตูุญุงุช ูุงููุฉุ ููุง ูุชุทูุจ ุงููุฏุฑุฉ ุนูู ูุนุงูุฌุฉ ุฃุทูุงู ุฅุฏุฎุงู ูุงุณุนุฉ.\n \n-ููู ูููููุง ุงูุชุฎูุต ูู ูุชุทูุจุงุช ุงูุฐุงูุฑุฉ ุงูุจุงูุธุฉ ููุชุทูููุงุช ุงููุฏุฎูุฉ ุงููุจูุฑุฉุ ูุญู ุจุญุงุฌุฉ ุฅูู ุทุฑููุฉ ุฌุฏูุฏุฉ ูุญุณุงุจ ุขููุฉ ุงูุงูุชูุงู ุงูุฐุงุชู ุงูุชู ุชุชุฎูุต ูู ูุตูููุฉ \\\\( QK^T \\\\). [ุทุฑููู ุฏุงู ูุขุฎุฑูู.](Https://arxiv.org/abs/2205.14135) ุทูุฑูุง ุจุงูุถุจุท ูุซู ูุฐุง ุงูุฎูุงุฑุฒููุฉ ุงูุฌุฏูุฏุฉ ูุฃุทูููุง ุนูููุง ุงุณู **Flash Attention**.\n+ููู ูููููุง ุงูุชุฎูุต ูู ูุชุทูุจุงุช ุงูุฐุงูุฑุฉ ุงูุจุงูุธุฉ ููุชุทูููุงุช ุงููุฏุฎูุฉ ุงููุจูุฑุฉุ ูุญู ุจุญุงุฌุฉ ุฅูู ุทุฑููุฉ ุฌุฏูุฏุฉ ูุญุณุงุจ ุขููุฉ ุงูุงูุชูุงู ุงูุฐุงุชู ุงูุชู ุชุชุฎูุต ูู ูุตูููุฉ \\\\( QK^T \\\\). [ุทุฑููู ุฏุงู ูุขุฎุฑูู.](https://huggingface.co/papers/2205.14135) ุทูุฑูุง ุจุงูุถุจุท ูุซู ูุฐุง ุงูุฎูุงุฑุฒููุฉ ุงูุฌุฏูุฏุฉ ูุฃุทูููุง ุนูููุง ุงุณู **Flash Attention**.\n \n ุจุงุฎุชุตุงุฑุ ููุณุฑ ุงูุงูุชูุงู ุงูููุงุดู ุญุณุงุจ \\\\( \\mathbf{V} \\times \\operatorname{Softmax}(\\mathbf{QK}^T\\\\)) ููุญุณุจ ุจุฏูุงู ูู ุฐูู ูุทุนูุง ุฃุตุบุฑ ูู ุงูุฅุฎุฑุงุฌ ุนู ุทุฑูู ุงูุชูุฑุงุฑ ุนุจุฑ ุงูุนุฏูุฏ ูู ุฎุทูุงุช ุญุณุงุจ Softmax:\n \n@@ -318,7 +318,7 @@ $$ \\textbf{O}_i \\leftarrow s^a_{ij} * \\textbf{O}_i + s^b_{ij} * \\mathbf{V}_{j} \\\n \n > ูู ุฎูุงู ุชุชุจุน ุฅุญุตุงุฆูุงุช ุงูุชุทุจูุน softmax ูุงุณุชุฎุฏุงู ุจุนุถ ุงูุฑูุงุถูุงุช ุงูุฐููุฉุ ูุนุทู Flash Attention **ูุฎุฑุฌุงุช ูุชุทุงุจูุฉ ุฑููููุง** ููุงุฑูุฉ ุจุทุจูุฉ ุงูุงูุชูุงู ุงูุฐุงุชู ุงูุงูุชุฑุงุถูุฉ ุจุชูููุฉ ุฐุงูุฑุฉ ูุง ุชุฒูุฏ ุฎุทููุง ูุน \\\\( N \\\\).\n \n-ุนูุฏ ุงููุธุฑ ุฅูู ุงูุตูุบุฉุ ูุฏ ูููู ุงููุฑุก ุจุฏููููุง ุฃู ุงูุงูุชูุงู ุงูููุงุดู ูุฌุจ ุฃู ูููู ุฃุจุทุฃ ุจูุซูุฑ ููุงุฑูุฉ ุจุตูุบุฉ ุงูุงูุชูุงู ุงูุงูุชุฑุงุถูุฉ ุญูุซ ููุฒู ุฅุฌุฑุงุก ุงููุฒูุฏ ูู ุงูุญุณุงุจุงุช. ูู ุงููุงูุนุ ูุชุทูุจ Flash Attention ุงููุฒูุฏ ูู ุนูููุงุช ุงููุงุตูุฉ ุงูุนุงุฆูุฉ ููุงุฑูุฉ ุจุงูุงูุชูุงู ุงูุนุงุฏู ุญูุซ ูุฌุจ ุฅุนุงุฏุฉ ุญุณุงุจ ุฅุญุตุงุฆูุงุช ุงูุชุทุจูุน softmax ุจุงุณุชูุฑุงุฑ (ุฑุงุฌุน [ุงููุฑูุฉ](https://arxiv.org/abs/2205.14135) ููุฒูุฏ ูู ุงูุชูุงุตูู ุฅุฐุง ููุช ููุชููุง)\n+ุนูุฏ ุงููุธุฑ ุฅูู ุงูุตูุบุฉุ ูุฏ ูููู ุงููุฑุก ุจุฏููููุง ุฃู ุงูุงูุชูุงู ุงูููุงุดู ูุฌุจ ุฃู ูููู ุฃุจุทุฃ ุจูุซูุฑ ููุงุฑูุฉ ุจุตูุบุฉ ุงูุงูุชูุงู ุงูุงูุชุฑุงุถูุฉ ุญูุซ ููุฒู ุฅุฌุฑุงุก ุงููุฒูุฏ ูู ุงูุญุณุงุจุงุช. ูู ุงููุงูุนุ ูุชุทูุจ Flash Attention ุงููุฒูุฏ ูู ุนูููุงุช ุงููุงุตูุฉ ุงูุนุงุฆูุฉ ููุงุฑูุฉ ุจุงูุงูุชูุงู ุงูุนุงุฏู ุญูุซ ูุฌุจ ุฅุนุงุฏุฉ ุญุณุงุจ ุฅุญุตุงุฆูุงุช ุงูุชุทุจูุน softmax ุจุงุณุชูุฑุงุฑ (ุฑุงุฌุน [ุงููุฑูุฉ](https://huggingface.co/papers/2205.14135) ููุฒูุฏ ูู ุงูุชูุงุตูู ุฅุฐุง ููุช ููุชููุง)\n \n > ููุน ุฐููุ ูุฅู ุงูุงูุชูุงู ุงูููุงุดู ุฃุณุฑุน ุจูุซูุฑ ูู ุงูุงุณุชุฏูุงู ููุงุฑูุฉ ุจุงูุงูุชูุงู ุงูุงูุชุฑุงุถู ุงูุฐู ูุฃุชู ูู ูุฏุฑุชู ุนูู ุชูููู ุงูุทูุจุงุช ุนูู ุฐุงูุฑุฉ GPU ุงูุฃุจุทุฃ ุฐุงุช ุงููุทุงู ุงูุชุฑุฏุฏู ุงูุนุงูู (VRAM)ุ ูุงูุชุฑููุฒ ุจุฏูุงู ูู ุฐูู ุนูู ุฐุงูุฑุฉ SRAM ุงูุฃุณุฑุน ุงูููุฌูุฏุฉ ุนูู ุงูุดุฑูุญุฉ.\n \n@@ -535,20 +535,20 @@ flush()\n ููู ูููู LLM ุชุฑุชูุจ ุงูุฌููุฉุ ููุฒู ูุฌูุฏ *ุฅุดุงุฑุฉ* ุฅุถุงููุฉ ููุชู ุชุทุจูููุง ุนุงุฏุฉู ูู ุดูู *ุงูุชุฑููุฒุงุช ุงูููุถุนูุฉ* (ุฃู ูุง ููุทูู ุนููู ุฃูุถูุง *ุงูุชุฑููุฒุงุช ุงูููุถุนูุฉ*).\n ูู ูุชู ุชุฑุฌูุฉ ุงููุต ุงูุฎุงุต ูุงูุฑูุงุจุท ูุฃููุงุฏ HTML ูCSS ุจูุงุกู ุนูู ุทูุจู.\n \n-ูุฏู ูุคููู ุงููุฑูุฉ ุงูุจุญุซูุฉ [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) ุชุถูููุงุช ููุถุนูุฉ ุฌูุจูุฉ ูุซูุซูุฉ \\\\( \\mathbf{P} = \\mathbf{p}_1, \\ldots, \\mathbf{p}_N \\\\) ุญูุซ ูุชู ุญุณุงุจ ูู ูุชุฌู \\\\( \\mathbf{p}_i \\\\) ูุฏุงูุฉ ุฌูุจูุฉ ูููุถุนู \\\\( i \\\\) .\n+ูุฏู ูุคููู ุงููุฑูุฉ ุงูุจุญุซูุฉ [*Attention Is All You Need*](https://huggingface.co/papers/1706.03762) ุชุถูููุงุช ููุถุนูุฉ ุฌูุจูุฉ ูุซูุซูุฉ \\\\( \\mathbf{P} = \\mathbf{p}_1, \\ldots, \\mathbf{p}_N \\\\) ุญูุซ ูุชู ุญุณุงุจ ูู ูุชุฌู \\\\( \\mathbf{p}_i \\\\) ูุฏุงูุฉ ุฌูุจูุฉ ูููุถุนู \\\\( i \\\\) .\n ุจุนุฏ ุฐูู ูุชู ุจุจุณุงุทุฉ ุฅุถุงูุฉ ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุฅูู ูุชุฌูุงุช ุชุณูุณู ุงูุฅุฏุฎุงู \\\\( \\mathbf{\\hat{X}} = \\mathbf{\\hat{x}}_1, \\ldots, \\mathbf{\\hat{x}}_N \\\\) = \\\\( \\mathbf{x}_1 + \\mathbf{p}_1, \\ldots, \\mathbf{x}_N + \\mathbf{p}_N \\\\) ูุจุงูุชุงูู ุชูุฌูู ุงููููุฐุฌ ูุชุนูู ุชุฑุชูุจ ุงูุฌููุฉ ุจุดูู ุฃูุถู.\n \n-ุจุฏูุงู ูู ุงุณุชุฎุฏุงู ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงูุซุงุจุชุฉุ ุงุณุชุฎุฏู ุขุฎุฑูู (ูุซู [Devlin et al.](https://arxiv.org/abs/1810.04805)) ุชุถูููุงุช ููุถุนูุฉ ููุชุณุจุฉ ูุชู ูู ุฎูุงููุง ุชุนูู ุงูุชุถูููุงุช ุงูููุถุนูุฉ \\\\( \\mathbf{P} \\\\) ุฃุซูุงุก ุงูุชุฏุฑูุจ.\n+ุจุฏูุงู ูู ุงุณุชุฎุฏุงู ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงูุซุงุจุชุฉุ ุงุณุชุฎุฏู ุขุฎุฑูู (ูุซู [Devlin et al.](https://huggingface.co/papers/1810.04805)) ุชุถูููุงุช ููุถุนูุฉ ููุชุณุจุฉ ูุชู ูู ุฎูุงููุง ุชุนูู ุงูุชุถูููุงุช ุงูููุถุนูุฉ \\\\( \\mathbf{P} \\\\) ุฃุซูุงุก ุงูุชุฏุฑูุจ.\n \n ูุงูุช ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงูุฌูุจูุฉ ูุงูููุชุณุจุฉ ูู ุงูุทุฑู ุงูุณุงุฆุฏุฉ ูุชุฑููุฒ ุชุฑุชูุจ ุงูุฌููุฉ ูู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉุ ูููู ุชู ุงูุนุซูุฑ ุนูู ุจุนุถ ุงููุดููุงุช ุงููุชุนููุฉ ุจูุฐู ุงูุชุถูููุงุช ุงูููุถุนูุฉ:\n \n-1. ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงูุฌูุจูุฉ ูุงูููุชุณุจุฉ ูู ุชุถูููุงุช ููุถุนูุฉ ูุทููุฉุ ุฃู ุชุฑููุฒ ุชุถููู ูุฑูุฏ ููู ูุนุฑู ููุถุนู: \\\\( 0, \\ldots, N \\\\) . ููุง ุฃุธูุฑ [Huang et al.](https://arxiv.org/abs/2009.13658) ู [Su et al.](https://arxiv.org/abs/2104.09864)ุ ุชุคุฏู ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงููุทููุฉ ุฅูู ุฃุฏุงุก ุถุนูู ูููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ูููุฏุฎูุงุช ุงููุตูุฉ ุงูุทูููุฉ. ุจุงููุณุจุฉ ูููุฏุฎูุงุช ุงููุตูุฉ ุงูุทูููุฉุ ูููู ูู ุงููููุฏ ุฅุฐุง ุชุนูู ุงููููุฐุฌ ุงููุณุงูุฉ ุงูููุถุนูุฉ ุงููุณุจูุฉ ุงูุชู ุชูุชูููุง ุฑููุฒ ุงููุฏุฎูุงุช ุฅูู ุจุนุถูุง ุงูุจุนุถ ุจุฏูุงู ูู ููุถุนูุง ุงููุทูู.\n+1. ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงูุฌูุจูุฉ ูุงูููุชุณุจุฉ ูู ุชุถูููุงุช ููุถุนูุฉ ูุทููุฉุ ุฃู ุชุฑููุฒ ุชุถููู ูุฑูุฏ ููู ูุนุฑู ููุถุนู: \\\\( 0, \\ldots, N \\\\) . ููุง ุฃุธูุฑ [Huang et al.](https://huggingface.co/papers/2009.13658) ู [Su et al.](https://huggingface.co/papers/2104.09864)ุ ุชุคุฏู ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงููุทููุฉ ุฅูู ุฃุฏุงุก ุถุนูู ูููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ูููุฏุฎูุงุช ุงููุตูุฉ ุงูุทูููุฉ. ุจุงููุณุจุฉ ูููุฏุฎูุงุช ุงููุตูุฉ ุงูุทูููุฉุ ูููู ูู ุงููููุฏ ุฅุฐุง ุชุนูู ุงููููุฐุฌ ุงููุณุงูุฉ ุงูููุถุนูุฉ ุงููุณุจูุฉ ุงูุชู ุชูุชูููุง ุฑููุฒ ุงููุฏุฎูุงุช ุฅูู ุจุนุถูุง ุงูุจุนุถ ุจุฏูุงู ูู ููุถุนูุง ุงููุทูู.\n 2. ุนูุฏ ุงุณุชุฎุฏุงู ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงูููุชุณุจุฉุ ูุฌุจ ุชุฏุฑูุจ ูููุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ุนูู ุทูู ุฅุฏุฎุงู ุซุงุจุช \\\\( N \\\\)ุ ููุง ูุฌุนู ูู ุงูุตุนุจ ุงูุงุณุชูุฑุงุก ุฅูู ุทูู ุฅุฏุฎุงู ุฃุทูู ููุง ุชู ุชุฏุฑูุจู ุนููู.\n \n ูู ุงูุขููุฉ ุงูุฃุฎูุฑุฉุ ุฃุตุจุญุช ุงูุชุถูููุงุช ุงูููุถุนูุฉ ุงููุณุจูุฉ ุงูุชู ูููููุง ูุนุงูุฌุฉ ุงููุดููุงุช ุงููุฐููุฑุฉ ุฃุนูุงู ุฃูุซุฑ ุดุนุจูุฉุ ูุฃุจุฑุฒูุง:\n \n--   [ุชุถููู ุงูููุถุน ุงูุฏูุฑุงูู (RoPE)](https://arxiv.org/abs/2104.09864)\n--   [ALiBi](https://arxiv.org/abs/2108.12409)\n+-   [ุชุถููู ุงูููุถุน ุงูุฏูุฑุงูู (RoPE)](https://huggingface.co/papers/2104.09864)\n+-   [ALiBi](https://huggingface.co/papers/2108.12409)\n \n ูุคูุฏ ูู ูู *RoPE* ู *ALiBi* ุฃูู ูู ุงูุฃูุถู ุชูุฌูู ูููุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ุญูู ุชุฑุชูุจ ุงูุฌููุฉ ูุจุงุดุฑุฉ ูู ุฎูุงุฑุฒููุฉ ุงูุงูุชุจุงู ุงูุฐุงุชู ุญูุซ ูุชู ูุถุน ุฑููุฒ ุงููููุงุช ูู ุนูุงูุฉ ูุน ุจุนุถูุง ุงูุจุนุถ. ุนูู ูุฌู ุงูุชุญุฏูุฏุ ูุฌุจ ุชูุฌูู ุชุฑุชูุจ ุงูุฌููุฉ ุนู ุทุฑูู ุชุนุฏูู ุนูููุฉ \\\\( \\mathbf{QK}^T \\\\) .\n \n@@ -563,14 +563,14 @@ $$ \\mathbf{\\hat{q}}_i^T \\mathbf{\\hat{x}}_j = \\mathbf{{q}}_i^T \\mathbf{R}_{\\theta\n ูุณุชุฎุฏู *RoPE* ูู ุงูุนุฏูุฏ ูู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ุงูุฃูุซุฑ ุฃูููุฉ ุงููููุ ูุซู:\n \n -   [**Falcon**](https://huggingface.co/tiiuae/falcon-40b)\n--   [**Llama**](https://arxiv.org/abs/2302.13971)\n--   [**PaLM**](https://arxiv.org/abs/2204.02311)\n+-   [**Llama**](https://huggingface.co/papers/2302.13971)\n+-   [**PaLM**](https://huggingface.co/papers/2204.02311)\n \n ูุจุฏููุ ููุชุฑุญ *ALiBi* ูุฎุทุท ุชุฑููุฒ ููุถุนู ูุณุจู ุฃุจุณุท ุจูุซูุฑ. ูุชู ุฅุถุงูุฉ ุงููุณุงูุฉ ุงููุณุจูุฉ ุงูุชู ุชูุชูููุง ุฑููุฒ ุงููุฏุฎูุงุช ุฅูู ุจุนุถูุง ุงูุจุนุถ ูุนุฏุฏ ุตุญูุญ ุณูุจู ูููุงุณ ุจูููุฉ ูุญุฏุฏุฉ ูุณุจููุง `m` ุฅูู ูู ุฅุฏุฎุงู ุงุณุชุนูุงู-ููุชุงุญ ููุตูููุฉ \\\\( \\mathbf{QK}^T \\\\) ูุจุงุดุฑุฉ ูุจู ุญุณุงุจ softmax.\n \n ![](/blog/assets/163_optimize_llm/alibi.png)\n \n-ููุง ูู ููุถุญ ูู ูุฑูุฉ [ALiBi](https://arxiv.org/abs/2108.12409)ุ ูุณูุญ ูุฐุง ุงูุชุฑููุฒ ุงูููุถุนู ุงููุณุจู ุงูุจุณูุท ูููููุฐุฌ ุจุงูุญูุงุธ ุนูู ุฃุฏุงุก ุนุงูู ุญุชู ูู ุชุณูุณูุงุช ุงููุฏุฎูุงุช ุงููุตูุฉ ุงูุทูููุฉ ุฌุฏูุง.\n+ููุง ูู ููุถุญ ูู ูุฑูุฉ [ALiBi](https://huggingface.co/papers/2108.12409)ุ ูุณูุญ ูุฐุง ุงูุชุฑููุฒ ุงูููุถุนู ุงููุณุจู ุงูุจุณูุท ูููููุฐุฌ ุจุงูุญูุงุธ ุนูู ุฃุฏุงุก ุนุงูู ุญุชู ูู ุชุณูุณูุงุช ุงููุฏุฎูุงุช ุงููุตูุฉ ุงูุทูููุฉ ุฌุฏูุง.\n \n ููุณุชุฎุฏู *ALiBi* ูู ุงูุนุฏูุฏ ูู ุฃูู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉ ุงููุณุชุฎุฏูุฉ ุงููููุ ูุซู:\n \n@@ -579,7 +579,7 @@ $$ \\mathbf{\\hat{q}}_i^T \\mathbf{\\hat{x}}_j = \\mathbf{{q}}_i^T \\mathbf{R}_{\\theta\n \n ูููู ููู ูู ุชุฑููุฒุงุช ุงูููุถุน *RoPE* ู *ALiBi* ุงูุงุณุชูุฑุงุก ุฅูู ุฃุทูุงู ุฅุฏุฎุงู ูู ูุชู ููุงุญุธุชูุง ุฃุซูุงุก ุงูุชุฏุฑูุจุ ูู ุญูู ุซุจุช ุฃู ุงูุงุณุชูุฑุงุก ูุนูู ุจุดูู ุฃูุถู ุจูุซูุฑ ุฎุงุฑุฌ ุงูุตูุฏูู ูู *ALiBi* ููุงุฑูุฉ ุจู *RoPE*.\n ุจุงููุณุจุฉ ูู ALiBiุ ูุง ุนููู ุณูู ุฒูุงุฏุฉ ููู ูุตูููุฉ ุงูููุถุน ุงููุซูุซ ุงูุณููู ููุทุงุจูุฉ ุทูู ุชุณูุณู ุงูุฅุฏุฎุงู.\n-ุจุงููุณุจุฉ ูู *RoPE*ุ ูุคุฏู ุงูุญูุงุธ ุนูู ููุณ \\\\( \\theta \\\\) ุงูุฐู ุชู ุงุณุชุฎุฏุงูู ุฃุซูุงุก ุงูุชุฏุฑูุจ ุฅูู ูุชุงุฆุฌ ุณูุฆุฉ ุนูุฏ ุชูุฑูุฑ ุฅุฏุฎุงูุงุช ูุตูุฉ ุฃุทูู ุจูุซูุฑ ูู ุชูู ุงูุชู ุดููุฏุช ุฃุซูุงุก ุงูุชุฏุฑูุจุ ุฑุงุฌุน [Press et al.](https://arxiv.org/abs/2108.12409). ููุน ุฐููุ ูุฌุฏ ุงููุฌุชูุน ุจุนุถ ุงูุญูู ุงููุนุงูุฉ ุงูุชู ุชููู ุจุชุนุฏูู \\\\( \\theta \\\\)ุ ููุง ูุณูุญ ูุชุฑููุฒุงุช ุงูููุถุน *RoPE* ุจุงูุนูู ุจุดูู ุฌูุฏ ูุชุณูุณูุงุช ุฅุฏุฎุงู ุงููุต ุงููุณุชูุฑุฆุฉ (ุฑุงุฌุน [ููุง](https://github.com/huggingface/transformers/pull/24653)).\n+ุจุงููุณุจุฉ ูู *RoPE*ุ ูุคุฏู ุงูุญูุงุธ ุนูู ููุณ \\\\( \\theta \\\\) ุงูุฐู ุชู ุงุณุชุฎุฏุงูู ุฃุซูุงุก ุงูุชุฏุฑูุจ ุฅูู ูุชุงุฆุฌ ุณูุฆุฉ ุนูุฏ ุชูุฑูุฑ ุฅุฏุฎุงูุงุช ูุตูุฉ ุฃุทูู ุจูุซูุฑ ูู ุชูู ุงูุชู ุดููุฏุช ุฃุซูุงุก ุงูุชุฏุฑูุจุ ุฑุงุฌุน [Press et al.](https://huggingface.co/papers/2108.12409). ููุน ุฐููุ ูุฌุฏ ุงููุฌุชูุน ุจุนุถ ุงูุญูู ุงููุนุงูุฉ ุงูุชู ุชููู ุจุชุนุฏูู \\\\( \\theta \\\\)ุ ููุง ูุณูุญ ูุชุฑููุฒุงุช ุงูููุถุน *RoPE* ุจุงูุนูู ุจุดูู ุฌูุฏ ูุชุณูุณูุงุช ุฅุฏุฎุงู ุงููุต ุงููุณุชูุฑุฆุฉ (ุฑุงุฌุน [ููุง](https://github.com/huggingface/transformers/pull/24653)).\n \n > ูู ูู RoPE ู ALiBi ุนุจุงุฑุฉ ุนู ุชุฑููุฒุงุช ููุถุน ูุณุจู *ูุง* ูุชู ุชุนูููุง ุฃุซูุงุก ุงูุชุฏุฑูุจุ ูููู ุจุฏูุงู ูู ุฐูู ุชุณุชูุฏ ุฅูู ุงูุญุฏุณ ุงูุชุงูู:\n  -   ูุฌุจ ุฅุนุทุงุก ุงูุฅุดุงุฑุงุช ุงูููุถุนูุฉ ุญูู ุฅุฏุฎุงูุงุช ุงููุต ูุจุงุดุฑุฉ ุฅูู ูุตูููุฉ \\\\( QK^T \\\\) ูุทุจูุฉ ุงูุงูุชูุงู ุงูุฐุงุชู\n@@ -755,29 +755,29 @@ Roughly 8 ูููุงุฑ ูููุฉ ุนุงุฆูุฉ! ูุชุทูุจ ุชุฎุฒูู 8 ูููุงุฑุงุช\n \n #### 3.2.2 Multi-Query-Attention (MQA)\n \n-[Multi-Query-Attention](https://arxiv.org/abs/1911.02150) ุงูุชุฑุญูุง Noam Shazeer ูู ูุฑูุชู *Fast Transformer Decoding: One Write-Head is All You Need*. ููุง ูููู ุงูุนููุงูุ ุงูุชุดู Noam ุฃูู ุจุฏูุงู ูู ุงุณุชุฎุฏุงู `n_head` ูู ุฃูุฒุงู ุฅุณูุงุท ุงููููุฉ ุงูุฑุฆูุณูุฉุ ูููู ุงุณุชุฎุฏุงู ุฒูุฌ ูุงุญุฏ ูู ุฃูุฒุงู ุฅุณูุงุท ุฑุฃุณ ุงููููุฉ ุงูุชู ูุชู ูุดุงุฑูุชูุง ุนุจุฑ ุฌููุน ุฑุคูุณ ุงูุงูุชูุงู ุฏูู ุฃู ูุชุฏููุฑ ุฃุฏุงุก ุงููููุฐุฌ ุจุดูู ูุจูุฑ.\n+[Multi-Query-Attention](https://huggingface.co/papers/1911.02150) ุงูุชุฑุญูุง Noam Shazeer ูู ูุฑูุชู *Fast Transformer Decoding: One Write-Head is All You Need*. ููุง ูููู ุงูุนููุงูุ ุงูุชุดู Noam ุฃูู ุจุฏูุงู ูู ุงุณุชุฎุฏุงู `n_head` ูู ุฃูุฒุงู ุฅุณูุงุท ุงููููุฉ ุงูุฑุฆูุณูุฉุ ูููู ุงุณุชุฎุฏุงู ุฒูุฌ ูุงุญุฏ ูู ุฃูุฒุงู ุฅุณูุงุท ุฑุฃุณ ุงููููุฉ ุงูุชู ูุชู ูุดุงุฑูุชูุง ุนุจุฑ ุฌููุน ุฑุคูุณ ุงูุงูุชูุงู ุฏูู ุฃู ูุชุฏููุฑ ุฃุฏุงุก ุงููููุฐุฌ ุจุดูู ูุจูุฑ.\n \n > ุจุงุณุชุฎุฏุงู ุฒูุฌ ูุงุญุฏ ูู ุฃูุฒุงู ุฅุณูุงุท ุฑุฃุณ ุงููููุฉุ ูุฌุจ ุฃู ุชููู ูุชุฌูุงุช ุงููููุฉ ุงูุฑุฆูุณูุฉ \\\\( \\mathbf{k}_iุ \\mathbf{v}_i \\\\) ูุชุทุงุจูุฉ ุนุจุฑ ุฌููุน ุฑุคูุณ ุงูุงูุชูุงู ูุงูุชู ุจุฏูุฑูุง ุชุนูู ุฃููุง ุจุญุงุฌุฉ ููุท ุฅูู ุชุฎุฒูู ุฒูุฌ ุฅุณูุงุท ูููุฉ ุฑุฆูุณู ูุงุญุฏ ูู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ุจุฏูุงู ูู `n_head` ูููุง.\n \n ูุธุฑูุง ูุฃู ูุนุธู LLMs ุชุณุชุฎุฏู ูุง ุจูู 20 ู100 ุฑุฃุณ ุงูุชูุงูุ ูุฅู MQA ูููู ุจุดูู ูุจูุฑ ูู ุงุณุชููุงู ุงูุฐุงูุฑุฉ ูุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช key-value. ุจุงููุณุจุฉ ุฅูู LLM ุงููุณุชุฎุฏู ูู ูุฐุง ุงูุฏูุชุฑุ ูููููุง ุชูููู ุงุณุชููุงู ุงูุฐุงูุฑุฉ ุงููุทููุจุฉ ูู 15 ุฌูุฌุงุจุงูุช ุฅูู ุฃูู ูู 400 ููุฌุงุจุงูุช ุนูุฏ ุทูู ุชุณูุณู ุงูุฅุฏุฎุงู 16000.\n \n ุจุงูุฅุถุงูุฉ ุฅูู ุชูููุฑ ุงูุฐุงูุฑุฉุ ูุคุฏู MQA ุฃูุถูุง ุฅูู ุชุญุณูู ุงูููุงุกุฉ ุงูุญุณุงุจูุฉ ููุง ูู ููุถุญ ูู ูุง ููู.\n-ูู ูู ุงูุชุดููุฑ ุงูุชููุงุฆูุ ูุฌุจ ุฅุนุงุฏุฉ ุชุญููู ูุชุฌูุงุช ุงููููุฉ ุงูุฑุฆูุณูุฉ ุงููุจูุฑุฉุ ูุฏูุฌูุง ูุน ุฒูุฌ ูุชุฌู ุงููููุฉ ุงูุญุงููุ ุซู ุฅุฏุฎุงููุง ูู \\\\( \\mathbf{q}_c\\mathbf{K}^T \\\\) ุงูุญุณุงุจ ูู ูู ุฎุทูุฉ. ุจุงููุณุจุฉ ููู ุงูุชุดููุฑ ุงูุชููุงุฆูุ ูููู ุฃู ุชุตุจุญ ุนุฑุถ ุงููุทุงู ุงูุชุฑุฏุฏู ููุฐุงูุฑุฉ ุงููุทููุจุฉ ูุฅุนุงุฏุฉ ุงูุชุญููู ุงููุณุชูุฑ ุนูู ุฒุฌุงุฌุฉ ุฒููููุง ุฎุทูุฑูุง. ูู ุฎูุงู ุชูููู ุญุฌู ูุชุฌูุงุช ุงููููุฉ ุงูุฑุฆูุณูุฉุ ูุฌุจ ุงููุตูู ุฅูู ุฐุงูุฑุฉ ุฃููุ ูุจุงูุชุงูู ุชูููู ุนูู ุงูุฒุฌุงุฌุฉ ูู ุนุฑุถ ุงููุทุงู ุงูุชุฑุฏุฏู ููุฐุงูุฑุฉ. ููุฒูุฏ ูู ุงูุชูุงุตููุ ูุฑุฌู ุฅููุงุก ูุธุฑุฉ ุนูู [ูุฑูุฉ Noam](https://arxiv.org/abs/1911.02150).\n+ูู ูู ุงูุชุดููุฑ ุงูุชููุงุฆูุ ูุฌุจ ุฅุนุงุฏุฉ ุชุญููู ูุชุฌูุงุช ุงููููุฉ ุงูุฑุฆูุณูุฉ ุงููุจูุฑุฉุ ูุฏูุฌูุง ูุน ุฒูุฌ ูุชุฌู ุงููููุฉ ุงูุญุงููุ ุซู ุฅุฏุฎุงููุง ูู \\\\( \\mathbf{q}_c\\mathbf{K}^T \\\\) ุงูุญุณุงุจ ูู ูู ุฎุทูุฉ. ุจุงููุณุจุฉ ููู ุงูุชุดููุฑ ุงูุชููุงุฆูุ ูููู ุฃู ุชุตุจุญ ุนุฑุถ ุงููุทุงู ุงูุชุฑุฏุฏู ููุฐุงูุฑุฉ ุงููุทููุจุฉ ูุฅุนุงุฏุฉ ุงูุชุญููู ุงููุณุชูุฑ ุนูู ุฒุฌุงุฌุฉ ุฒููููุง ุฎุทูุฑูุง. ูู ุฎูุงู ุชูููู ุญุฌู ูุชุฌูุงุช ุงููููุฉ ุงูุฑุฆูุณูุฉุ ูุฌุจ ุงููุตูู ุฅูู ุฐุงูุฑุฉ ุฃููุ ูุจุงูุชุงูู ุชูููู ุนูู ุงูุฒุฌุงุฌุฉ ูู ุนุฑุถ ุงููุทุงู ุงูุชุฑุฏุฏู ููุฐุงูุฑุฉ. ููุฒูุฏ ูู ุงูุชูุงุตููุ ูุฑุฌู ุฅููุงุก ูุธุฑุฉ ุนูู [ูุฑูุฉ Noam](https://huggingface.co/papers/1911.02150).\n \n ุงูุฌุฒุก ุงูููู ุงูุฐู ูุฌุจ ูููู ููุง ูู ุฃู ุชูููู ุนุฏุฏ ุฑุคูุณ ุงูุงูุชูุงู ุจุงููููุฉ ุงูุฑุฆูุณูุฉ ุฅูู 1 ูุง ูุนูู ูู ุฅูุง ุฅุฐุง ุชู ุงุณุชุฎุฏุงู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ูููููุฉ ุงูุฑุฆูุณูุฉ. ูุธู ุงูุงุณุชููุงู ุงูุฐุฑูู ูุฐุงูุฑุฉ ุงููููุฐุฌ ููุฑูุฑ ูุงุญุฏ ููุฃูุงู ุจุฏูู ุฐุงูุฑุฉ ุงูุชุฎุฒูู ุงููุคูุช ูููููุฉ ุงูุฑุฆูุณูุฉ ุฏูู ุชุบููุฑ ูุฃู ูู ุฑุฃุณ ุงูุชูุงู ูุง ูุฒุงู ูุฏูู ูุชุฌู ุงุณุชุนูุงู ูุฑูุฏ ุจุญูุซ ูููู ููู ุฑุฃุณ ุงูุชูุงู ูุตูููุฉ \\\\( \\mathbf{QK}^T \\\\) ูุฎุชููุฉ.\n \n ุดูุฏุช MQA ุงุนุชูุงุฏูุง ูุงุณุน ุงููุทุงู ูู ูุจู ุงููุฌุชูุน ููุชู ุงุณุชุฎุฏุงููุง ุงูุขู ุจูุงุณุทุฉ ุงูุนุฏูุฏ ูู LLMs ุงูุฃูุซุฑ ุดูุฑุฉ:\n \n -   [**Falcon**](https://huggingface.co/tiiuae/falcon-40b)\n--   [**PaLM**](https://arxiv.org/abs/2204.02311)\n+-   [**PaLM**](https://huggingface.co/papers/2204.02311)\n -   [**MPT**](https://huggingface.co/mosaicml/mpt-30b)\n -   [**BLOOM**](https://huggingface.co/bigscience/bloom)\n \n ููุง ูุณุชุฎุฏู ููุทุฉ ุงูุชุญูู ุงููุณุชุฎุฏูุฉ ูู ูุฐุง ุงูุฏูุชุฑ - `bigcode/octocoder` - MQA.\n \n #### 3.2.3 ูุฌููุนุฉ ุงูุงุณุชุนูุงู ุงูุงูุชูุงู (GQA)\n \n-[ูุฌููุนุฉ ุงูุงุณุชุนูุงู ุงูุงูุชูุงู](https://arxiv.org/abs/2305.13245)ุ ููุง ุงูุชุฑุญ Ainslie et al. ูู Googleุ ูุฌุฏ ุฃู ุงุณุชุฎุฏุงู MQA ูููู ุฃู ูุคุฏู ุบุงูุจูุง ุฅูู ุชุฏููุฑ ุงูุฌูุฏุฉ ููุงุฑูุฉ ุจุงุณุชุฎุฏุงู ุฅุณูุงุทุงุช ุฑุฃุณ ุงููููุฉ ุงูุฑุฆูุณูุฉ ุงููุชุนุฏุฏุฉ. ุชุฌุงุฏู ุงููุฑูุฉ ุจุฃูู ูููู ุงูุญูุงุธ ุนูู ุฃุฏุงุก ุงููููุฐุฌ ุจุดูู ุฃูุจุฑ ุนู ุทุฑูู ุชูููู ุนุฏุฏ ุฃูุฒุงู ุฅุณูุงุท ุฑุฃุณ ุงูุงุณุชุนูุงู ุจุดูู ุฃูู ุญุฏุฉ. ุจุฏูุงู ูู ุงุณุชุฎุฏุงู ูุฒู ุฅุณูุงุท ูููุฉ ุฑุฆูุณูุฉ ูุงุญุฏุฉ ููุทุ ูุฌุจ ุงุณุชุฎุฏุงู `n <n_head` ุฃูุฒุงู ุฅุณูุงุท ูููุฉ ุฑุฆูุณูุฉ. ูู ุฎูุงู ุงุฎุชูุงุฑ `n` ุฅูู ูููุฉ ุฃูู ุจูุซูุฑ ูู `n_head`ุ ูุซู 2 ุฃู 4 ุฃู 8ุ ูููู ุงูุงุญุชูุงุธ ุจูุนุธู ููุงุณุจ ุงูุฐุงูุฑุฉ ูุงูุณุฑุนุฉ ูู MQA ูุน ุงูุชุถุญูุฉ ุจูุฏุฑ ุฃูู ูู ุณุนุฉ ุงููููุฐุฌ ูุจุงูุชุงููุ ูู ุงูููุชุฑุถุ ุฃูู ุฃุฏุงุก.\n+[ูุฌููุนุฉ ุงูุงุณุชุนูุงู ุงูุงูุชูุงู](https://huggingface.co/papers/2305.13245)ุ ููุง ุงูุชุฑุญ Ainslie et al. ูู Googleุ ูุฌุฏ ุฃู ุงุณุชุฎุฏุงู MQA ูููู ุฃู ูุคุฏู ุบุงูุจูุง ุฅูู ุชุฏููุฑ ุงูุฌูุฏุฉ ููุงุฑูุฉ ุจุงุณุชุฎุฏุงู ุฅุณูุงุทุงุช ุฑุฃุณ ุงููููุฉ ุงูุฑุฆูุณูุฉ ุงููุชุนุฏุฏุฉ. ุชุฌุงุฏู ุงููุฑูุฉ ุจุฃูู ูููู ุงูุญูุงุธ ุนูู ุฃุฏุงุก ุงููููุฐุฌ ุจุดูู ุฃูุจุฑ ุนู ุทุฑูู ุชูููู ุนุฏุฏ ุฃูุฒุงู ุฅุณูุงุท ุฑุฃุณ ุงูุงุณุชุนูุงู ุจุดูู ุฃูู ุญุฏุฉ. ุจุฏูุงู ูู ุงุณุชุฎุฏุงู ูุฒู ุฅุณูุงุท ูููุฉ ุฑุฆูุณูุฉ ูุงุญุฏุฉ ููุทุ ูุฌุจ ุงุณุชุฎุฏุงู `n <n_head` ุฃูุฒุงู ุฅุณูุงุท ูููุฉ ุฑุฆูุณูุฉ. ูู ุฎูุงู ุงุฎุชูุงุฑ `n` ุฅูู ูููุฉ ุฃูู ุจูุซูุฑ ูู `n_head`ุ ูุซู 2 ุฃู 4 ุฃู 8ุ ูููู ุงูุงุญุชูุงุธ ุจูุนุธู ููุงุณุจ ุงูุฐุงูุฑุฉ ูุงูุณุฑุนุฉ ูู MQA ูุน ุงูุชุถุญูุฉ ุจูุฏุฑ ุฃูู ูู ุณุนุฉ ุงููููุฐุฌ ูุจุงูุชุงููุ ูู ุงูููุชุฑุถุ ุฃูู ุฃุฏุงุก.\n \n ุนูุงูุฉ ุนูู ุฐููุ ุงูุชุดู ูุคููู GQA ุฃูู ูููู *ุชุฏุฑูุจ* ููุงุท ุชูุชูุด ุงููููุฐุฌ ุงูููุฌูุฏุฉ ููููู ููุง ุจููุฉ GQA ุจุงุณุชุฎุฏุงู 5% ููุท ูู ุงูุญูุณุจุฉ ุงูุฃุตููุฉ ููุชุนููู ุงููุณุจู. ูู ุญูู ุฃู 5% ูู ุงูุญูุณุจุฉ ุงูุฃุตููุฉ ููุชุนููู ุงููุณุจู ูููู ุฃู ุชููู ูููุฉ ูุงุฆูุฉุ ูุณูุญ GQA *uptraining* ุจููุงุท ุชูุชูุด ููุฌูุฏุฉ ููุงุณุชูุงุฏุฉ ูู ุชุณูุณูุงุช ุงูุฅุฏุฎุงู ุงูุฃุทูู.\n \n@@ -789,7 +789,7 @@ Roughly 8 ูููุงุฑ ูููุฉ ุนุงุฆูุฉ! ูุชุทูุจ ุชุฎุฒูู 8 ูููุงุฑุงุช\n \n ## ุงูุฎุงุชูุฉ\n \n-ูุฌุชูุน ุงูุจุญุซ ูุฃุชู ุจุงุณุชูุฑุงุฑ ุจุทุฑู ุฌุฏูุฏุฉ ููุจุชูุฑุฉ ูุชุณุฑูุน ููุช ุงูุงุณุชุฏูุงู ููููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ ุนูู ุงูุฅุทูุงู. ููุซุงูุ ุฃุญุฏ ุงุชุฌุงูุงุช ุงูุจุญุซ ุงููุงุนุฏุฉ ูู [ูู ุงูุชุดููุฑ ุงูุชุฎูููู](https://arxiv.org/abs/2211.17192) ุญูุซ ุชููู \"ุงูุฑููุฒ ุงูุณููุฉ\" ุจุฅูุดุงุฆูุง ููุงุฐุฌ ุงููุบุฉ ุงูุฃุตุบุฑ ูุงูุฃุณุฑุน ููุชู ุฅูุดุงุก \"ุงูุฑููุฒ ุงูุตุนุจุฉ\" ููุท ุจูุงุณุทุฉ LLM ููุณู. ุฅู ุงูุชุนูู ูู ุงูุชูุงุตูู ูุชุฌุงูุฒ ูุทุงู ูุฐุง ุงูุฏูุชุฑุ ูููู ูููู ูุฑุงุกุชู ูู ูุฐู [ุชุฏูููุฉ ุงููุฏููุฉ ุงููุทููุฉ](https://huggingface.co/blog/assisted-generation).\n+ูุฌุชูุน ุงูุจุญุซ ูุฃุชู ุจุงุณุชูุฑุงุฑ ุจุทุฑู ุฌุฏูุฏุฉ ููุจุชูุฑุฉ ูุชุณุฑูุน ููุช ุงูุงุณุชุฏูุงู ููููุงุฐุฌ ุงููุบููุฉ ุงููุจูุฑุฉ ุนูู ุงูุฅุทูุงู. ููุซุงูุ ุฃุญุฏ ุงุชุฌุงูุงุช ุงูุจุญุซ ุงููุงุนุฏุฉ ูู [ูู ุงูุชุดููุฑ ุงูุชุฎูููู](https://huggingface.co/papers/2211.17192) ุญูุซ ุชููู \"ุงูุฑููุฒ ุงูุณููุฉ\" ุจุฅูุดุงุฆูุง ููุงุฐุฌ ุงููุบุฉ ุงูุฃุตุบุฑ ูุงูุฃุณุฑุน ููุชู ุฅูุดุงุก \"ุงูุฑููุฒ ุงูุตุนุจุฉ\" ููุท ุจูุงุณุทุฉ LLM ููุณู. ุฅู ุงูุชุนูู ูู ุงูุชูุงุตูู ูุชุฌุงูุฒ ูุทุงู ูุฐุง ุงูุฏูุชุฑุ ูููู ูููู ูุฑุงุกุชู ูู ูุฐู [ุชุฏูููุฉ ุงููุฏููุฉ ุงููุทููุฉ](https://huggingface.co/blog/assisted-generation).\n \n ุงูุณุจุจ ูู ุฃู LLMs ุงูุถุฎูุฉ ูุซู GPT3/4ุ ูLlama-2-70bุ ูClaudeุ ูPaLM ูููู ุฃู ุชุนูู ุจุณุฑุนุฉ ูุจูุฑุฉ ูู ูุงุฌูุงุช ุงูุฏุฑุฏุดุฉ ูุซู [Hugging Face Chat](https://huggingface.co/chat/) ุฃู ChatGPT ูุฑุฌุน ุฅูู ุญุฏ ูุจูุฑ ุฅูู ุงูุชุญุณููุงุช ุงููุฐููุฑุฉ ุฃุนูุงู ูู ุงูุฏูุฉ ูุงูุฎูุงุฑุฒููุงุช ูุงูููุฏุณุฉ ุงููุนูุงุฑูุฉ.\n ูู ุงููุณุชูุจูุ ุณุชููู ุฃุฌูุฒุฉ ุงูุชุณุฑูุน ูุซู ูุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPUs) ููุญุฏุงุช ูุนุงูุฌุฉ ุงูุฑุณููุงุช (TPUs)ุ ููุง ุฅูู ุฐูู... ุณุชููู ุฃุณุฑุน ููุท ูุณุชุณูุญ ุจูุฒูุฏ ูู ุงูุฐุงูุฑุฉุ ูููู ูุฌุจ ุฏุงุฆููุง ุงูุชุฃูุฏ ูู ุงุณุชุฎุฏุงู ุฃูุถู ุงูุฎูุงุฑุฒููุงุช ูุงูููุฏุณุฉ ุงููุนูุงุฑูุฉ ุงููุชุงุญุฉ ููุญุตูู ุนูู ุฃูุจุฑ ูุฏุฑ ูู ุงููุงู"
        },
        {
            "sha": "db3473e5e02ac8bea6abf19a5db07cc54d390969",
            "filename": "docs/source/ar/model_memory_anatomy.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fmodel_memory_anatomy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fmodel_memory_anatomy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fmodel_memory_anatomy.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -165,7 +165,7 @@ default_args = {\n \n ูููู ุฃู ุชููู ูุฐู ุงููุนุฑูุฉ ูููุฏุฉ ููุนุฑูุฉ ุนูุฏ ุชุญููู ุงุฎุชูุงูุงุช ุงูุฃุฏุงุก.\n \n-ูุฐุง ุงูููุฎุต ููุดุชู ูู [ููู ุงูุจูุงูุงุช ูู ูู ูุง ุชุญุชุงุฌู: ุฏุฑุงุณุฉ ุญุงูุฉ ุญูู ุชุญุณูู ุงููุญููุงุช 2020](https://arxiv.org/abs/2007.00072)\n+ูุฐุง ุงูููุฎุต ููุดุชู ูู [ููู ุงูุจูุงูุงุช ูู ูู ูุง ุชุญุชุงุฌู: ุฏุฑุงุณุฉ ุญุงูุฉ ุญูู ุชุญุณูู ุงููุญููุงุช 2020](https://huggingface.co/papers/2007.00072)\n \n \n ## ุชุดุฑูุญ ุฐุงูุฑุฉ ุงููููุฐุฌ"
        },
        {
            "sha": "97a75dd4b608bafabdc6631635d6dd008aa6225d",
            "filename": "docs/source/ar/model_summary.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fmodel_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fmodel_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fmodel_summary.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -1,6 +1,6 @@\n # ุนุงุฆูุฉ ููุงุฐุฌ ุงููุญูู\n \n-ููุฐ ุฅุทูุงูู ูู ุนุงู 2017ุ ุฃููู ูููุฐุฌ [ุงููุญูู ุงูุฃุตูู](https://arxiv.org/abs/1706.03762) (ุฑุงุฌุน ูุฏููุฉ [ุงููุญูู ุงููุดุฑูุญ](http://nlp.seas.harvard.edu/2018/04/03/attention.html) ูููุฏูุฉ ุชูููุฉ ูุจุณุทุฉ)ุ ุฃููู ุงูุนุฏูุฏ ูู ุงูููุงุฐุฌ ุงูุฌุฏูุฏุฉ ูุงููุจุชูุฑุฉ ุงูุชู ุชุชุฌุงูุฒ ููุงู ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ (NLP). ููุงู ููุงุฐุฌ ููุชูุจุค [ุจุงูุจููุฉ ุงูุจุฑูุชููุงุช ุงููุทููุฉ](https://huggingface.co/blog/deep-learning-with-proteins)ุ ู[ุชุฏุฑูุจ ุนูู ุงุชุฎุงุฐ ุงููุฑุงุฑ](https://huggingface.co/blog/train-decision-transformers)ุ ู[ุงูุชูุจุค ุจุงูุณูุงุณู ุงูุฒูููุฉ](https://huggingface.co/blog/time-series-transformers). ูุน ูุฌูุฏ ุงูุนุฏูุฏ ูู ูุชุบูุฑุงุช ุงููุญูู ุงููุชุงุญุฉุ ูุฏ ูููู ูู ุงูุณูู ุฃู ุชููุชู ุงูุตูุฑุฉ ุงูุฃูุจุฑ. ูุง ุชุดุชุฑู ููู ุฌููุน ูุฐู ุงูููุงุฐุฌ ูู ุฃููุง ุชุณุชูุฏ ุฅูู ุจููุฉ ุงููุญูู ุงูุฃุตููุฉ. ุชุณุชุฎุฏู ุจุนุถ ุงูููุงุฐุฌ ููุท ุงูุชุฑููุฒ ุฃู ูู ุงูุชุฑููุฒุ ุจูููุง ุชุณุชุฎุฏู ููุงุฐุฌ ุฃุฎุฑู ูููููุง. ูููุฑ ูุฐุง ุชุตููููุง ูููุฏูุง ูุชุตููู ูุงุณุชุนุฑุงุถ ุงููุฑููุงุช ุงูุฑุฆูุณูุฉ ุจูู ููุงุฐุฌ ุนุงุฆูุฉ ุงููุญููุงุชุ ูุณูุณุงุนุฏู ุนูู ููู ุงูููุงุฐุฌ ุงูุชู ูู ุชุตุงุฏููุง ูู ูุจู.\n+ููุฐ ุฅุทูุงูู ูู ุนุงู 2017ุ ุฃููู ูููุฐุฌ [ุงููุญูู ุงูุฃุตูู](https://huggingface.co/papers/1706.03762) (ุฑุงุฌุน ูุฏููุฉ [ุงููุญูู ุงููุดุฑูุญ](http://nlp.seas.harvard.edu/2018/04/03/attention.html) ูููุฏูุฉ ุชูููุฉ ูุจุณุทุฉ)ุ ุฃููู ุงูุนุฏูุฏ ูู ุงูููุงุฐุฌ ุงูุฌุฏูุฏุฉ ูุงููุจุชูุฑุฉ ุงูุชู ุชุชุฌุงูุฒ ููุงู ูุนุงูุฌุฉ ุงููุบุงุช ุงูุทุจูุนูุฉ (NLP). ููุงู ููุงุฐุฌ ููุชูุจุค [ุจุงูุจููุฉ ุงูุจุฑูุชููุงุช ุงููุทููุฉ](https://huggingface.co/blog/deep-learning-with-proteins)ุ ู[ุชุฏุฑูุจ ุนูู ุงุชุฎุงุฐ ุงููุฑุงุฑ](https://huggingface.co/blog/train-decision-transformers)ุ ู[ุงูุชูุจุค ุจุงูุณูุงุณู ุงูุฒูููุฉ](https://huggingface.co/blog/time-series-transformers). ูุน ูุฌูุฏ ุงูุนุฏูุฏ ูู ูุชุบูุฑุงุช ุงููุญูู ุงููุชุงุญุฉุ ูุฏ ูููู ูู ุงูุณูู ุฃู ุชููุชู ุงูุตูุฑุฉ ุงูุฃูุจุฑ. ูุง ุชุดุชุฑู ููู ุฌููุน ูุฐู ุงูููุงุฐุฌ ูู ุฃููุง ุชุณุชูุฏ ุฅูู ุจููุฉ ุงููุญูู ุงูุฃุตููุฉ. ุชุณุชุฎุฏู ุจุนุถ ุงูููุงุฐุฌ ููุท ุงูุชุฑููุฒ ุฃู ูู ุงูุชุฑููุฒุ ุจูููุง ุชุณุชุฎุฏู ููุงุฐุฌ ุฃุฎุฑู ูููููุง. ูููุฑ ูุฐุง ุชุตููููุง ูููุฏูุง ูุชุตููู ูุงุณุชุนุฑุงุถ ุงููุฑููุงุช ุงูุฑุฆูุณูุฉ ุจูู ููุงุฐุฌ ุนุงุฆูุฉ ุงููุญููุงุชุ ูุณูุณุงุนุฏู ุนูู ููู ุงูููุงุฐุฌ ุงูุชู ูู ุชุตุงุฏููุง ูู ูุจู.\n \n ุฅุฐุง ูู ุชูู ุนูู ุฏุฑุงูุฉ ุจูููุฐุฌ ุงููุญูู ุงูุฃุตูู ุฃู ุชุญุชุงุฌ ุฅูู ุชุฐููุฑุ ูุฑุงุฌุน ุงููุตู ุงูุฎุงุต ุจู [ููู ุชุนูู ุงููุญููุงุช](https://huggingface.co/course/chapter1/4ุfw=pt) ูู ุฏูุฑุฉ Hugging Face.\n \n@@ -14,7 +14,7 @@\n \n ### ุงูุดุจูุฉ ุงูุชูุงููููุฉ (Convolutional network)\n \n-ูุทุงููุง ูุงูุช ุงูุดุจูุงุช ุงูุชูุงููููุฉ (CNNs) ุงูุทุฑููุฉ ุงูุณุงุฆุฏุฉ ูููุงู ุฑุคูุฉ ุงูุญุงุณุจ ุญุชู ุจุฑุฒ [ูุญูู ุงูุฑุคูุฉ](https://arxiv.org/abs/2010.11929) ูุงุจููุชู ููุชุทููุฑ ูููุงุกุชู ุงูุนุงููุฉ. ูุญุชู ุจุนุฏ ุฐููุ ูุง ุชุฒุงู ุจุนุถ ุฃูุถู ุตูุงุช CNNุ ูุซู ุซุจุงุช ุงูุฅุฒุงุญุฉุ ูููุฉ ุฌุฏูุง (ุฎุงุตุฉ ุจุงููุณุจุฉ ูููุงู ูุนููุฉ) ูุฏุฑุฌุฉ ุฃู ุจุนุถ ุงููุญููุงุช ุชุฏูุฌ ุงูุชูุงููู ูู ุจููุชูุง. ููุจ [ConvNeXt](model_doc/convnext) ูุฐุง ุงูุชุจุงุฏู ุฑุฃุณูุง ุนูู ุนูุจ ูุฃุฏุฑุฌ ุฎูุงุฑุงุช ุงูุชุตููู ูู ุงููุญููุงุช ูุชุญุฏูุซ CNN. ุนูู ุณุจูู ุงููุซุงูุ ูุณุชุฎุฏู ConvNeXt ููุงูุฐ ููุฒููุฉ ุบูุฑ ูุชุฏุงุฎูุฉ ูุชูุณูู ุงูุตูุฑุฉ ุฅูู ุฑูุน ูุฒูุงุฏุฉ ุญูู ูุฌุงู ุงูุนุงู ุงูุฎุงุต ุจูุง. ููุง ูููู ConvNeXt ุจุนุฏุฉ ุฎูุงุฑุงุช ูุซู ุชุตููู ุงูุทุจูุฉ ูุชููู ุฃูุซุฑ ููุงุกุฉ ูู ุงูุฐุงูุฑุฉ ูุชุญุณูู ุงูุฃุฏุงุกุ ููุง ูุฌุนูู ููุงูุณูุง ููููุง ูููุญููุงุช!\n+ูุทุงููุง ูุงูุช ุงูุดุจูุงุช ุงูุชูุงููููุฉ (CNNs) ุงูุทุฑููุฉ ุงูุณุงุฆุฏุฉ ูููุงู ุฑุคูุฉ ุงูุญุงุณุจ ุญุชู ุจุฑุฒ [ูุญูู ุงูุฑุคูุฉ](https://huggingface.co/papers/2010.11929) ูุงุจููุชู ููุชุทููุฑ ูููุงุกุชู ุงูุนุงููุฉ. ูุญุชู ุจุนุฏ ุฐููุ ูุง ุชุฒุงู ุจุนุถ ุฃูุถู ุตูุงุช CNNุ ูุซู ุซุจุงุช ุงูุฅุฒุงุญุฉุ ูููุฉ ุฌุฏูุง (ุฎุงุตุฉ ุจุงููุณุจุฉ ูููุงู ูุนููุฉ) ูุฏุฑุฌุฉ ุฃู ุจุนุถ ุงููุญููุงุช ุชุฏูุฌ ุงูุชูุงููู ูู ุจููุชูุง. ููุจ [ConvNeXt](model_doc/convnext) ูุฐุง ุงูุชุจุงุฏู ุฑุฃุณูุง ุนูู ุนูุจ ูุฃุฏุฑุฌ ุฎูุงุฑุงุช ุงูุชุตููู ูู ุงููุญููุงุช ูุชุญุฏูุซ CNN. ุนูู ุณุจูู ุงููุซุงูุ ูุณุชุฎุฏู ConvNeXt ููุงูุฐ ููุฒููุฉ ุบูุฑ ูุชุฏุงุฎูุฉ ูุชูุณูู ุงูุตูุฑุฉ ุฅูู ุฑูุน ูุฒูุงุฏุฉ ุญูู ูุฌุงู ุงูุนุงู ุงูุฎุงุต ุจูุง. ููุง ูููู ConvNeXt ุจุนุฏุฉ ุฎูุงุฑุงุช ูุซู ุชุตููู ุงูุทุจูุฉ ูุชููู ุฃูุซุฑ ููุงุกุฉ ูู ุงูุฐุงูุฑุฉ ูุชุญุณูู ุงูุฃุฏุงุกุ ููุง ูุฌุนูู ููุงูุณูุง ููููุง ูููุญููุงุช!\n \n ### ุงูุชุฑููุฒ[[cv-encoder]] (Encoder)\n \n@@ -40,7 +40,7 @@\n \n ูููุฐุฌ [BERT](model_doc/bert) ูู ูุญููู (Transformer)  ูุนุชูุฏ ุนูู ุงูุชุฑููุฒ ููุท ูููู ุจุดูู ุนุดูุงุฆู ุจุฅุฎูุงุก ุฑููุฒ ูุนููุฉ ูู ุงููุฏุฎูุงุช ูุชุฌูุจ ุฑุคูุฉ ุจุงูู ุงูุฑููุฒ ุงูุฃุฎุฑูุ ููุง ูุณูุญ ูู \"ุจุงูุบุด\". ูุชูุซู ูุฏู ุงูุชุฏุฑูุจ ุงููุณุจู ูู ุงูุชูุจุค ุจุงูุฑูุฒ ุงููุฎูู ุจูุงุกู ุนูู ุงูุณูุงู. ูุณูุญ ูุฐุง ูู BERT ุจุงุณุชุฎุฏุงู ุงูุณูุงูุงุช ุงููููู ูุงููุณุฑู ุจุงููุงูู ููุณุงุนุฏุชู ูู ุชุนูู ุชูุซูู ุฃุนูู ูุฃุบูู ููุจูุงูุงุช ุงููุฏุฎูุฉ. ููุน ุฐููุ ูุงู ููุงู ูุฌุงู ููุชุญุณูู ูู ุงุณุชุฑุงุชูุฌูุฉ ุงูุชุฏุฑูุจ ุงููุณุจู ูู BERT. ูููุฐุฌ [RoBERTa](model_doc/roberta) ุงุถุงู ุชุญุณูู ูู ุฎูุงู ุชูุฏูู ูุตูุฉ ุชุฏุฑูุจ ูุณุจู ุฌุฏูุฏุฉ ุชุดูู ุงูุชุฏุฑูุจ ููุชุฑุฉ ุฃุทูู ูุนูู ุฏูุนุงุช ุฃูุจุฑุ ูุฅุฎูุงุก ุงูุฑููุฒ ุนุดูุงุฆููุง ูู ูู ุญูุจุฉ ุจุฏูุงู ูู ูุฑุฉ ูุงุญุฏุฉ ููุท ุฃุซูุงุก ุงููุนุงูุฌุฉ ุงููุณุจูุฉุ ูุฅุฒุงูุฉ ูุฏู ุงูุชูุจุค ุจุงูุฌููุฉ ุงูุชุงููุฉ.\n \n-ุชุชูุซู ุงูุงุณุชุฑุงุชูุฌูุฉ ุงูุณุงุฆุฏุฉ ูุชุญุณูู ุงูุฃุฏุงุก ูู ุฒูุงุฏุฉ ุญุฌู ุงููููุฐุฌ. ูููู ุชุฏุฑูุจ ุงูููุงุฐุฌ ุงููุจูุฑุฉ ูููู ูู ุงููุงุญูุฉ ุงูุญุณุงุจูุฉ. ุฅุญุฏู ุทุฑู ุชูููู ุงูุชูุงููู ุงูุญุณุงุจูุฉ ูู ุงุณุชุฎุฏุงู ูููุฐุฌ ุฃุตุบุฑ ูุซู [DistilBERT](model_doc/distilbert). ูุณุชุฎุฏู DistilBERT [ ุชูููุฉ ุชูุทูุฑ ุงููุนุฑูุฉ](https://arxiv.org/abs/1503.02531) - ููู ุชูููุฉ ุถุบุท - ูุฅูุดุงุก ูููุฐุฌ ุฃุตุบุฑ ูู BERT ูุน ุงูุญูุงุธ ุนูู ูุนุธู ูุฏุฑุงุชู ุนูู ููู ุงููุบุฉุง.\n+ุชุชูุซู ุงูุงุณุชุฑุงุชูุฌูุฉ ุงูุณุงุฆุฏุฉ ูุชุญุณูู ุงูุฃุฏุงุก ูู ุฒูุงุฏุฉ ุญุฌู ุงููููุฐุฌ. ูููู ุชุฏุฑูุจ ุงูููุงุฐุฌ ุงููุจูุฑุฉ ูููู ูู ุงููุงุญูุฉ ุงูุญุณุงุจูุฉ. ุฅุญุฏู ุทุฑู ุชูููู ุงูุชูุงููู ุงูุญุณุงุจูุฉ ูู ุงุณุชุฎุฏุงู ูููุฐุฌ ุฃุตุบุฑ ูุซู [DistilBERT](model_doc/distilbert). ูุณุชุฎุฏู DistilBERT [ ุชูููุฉ ุชูุทูุฑ ุงููุนุฑูุฉ](https://huggingface.co/papers/1503.02531) - ููู ุชูููุฉ ุถุบุท - ูุฅูุดุงุก ูููุฐุฌ ุฃุตุบุฑ ูู BERT ูุน ุงูุญูุงุธ ุนูู ูุนุธู ูุฏุฑุงุชู ุนูู ููู ุงููุบุฉุง.\n \n ูุฑุช ูุนุธู ููุงุฐุฌ ุงููุญูู ูู ุงูุงุชุฌุงู ูุญู ุงููุฒูุฏ ูู ุงููุนููุงุชุ ููุง ุฃุฏู ุฅูู ุธููุฑ ููุงุฐุฌ ุฌุฏูุฏุฉ ุชุฑูุฒ ุนูู ุชุญุณูู ููุงุกุฉ ุงูุชุฏุฑูุจ. ููููู [ALBERT](model_doc/albert) ูู ุงุณุชููุงู ุงูุฐุงูุฑุฉ ุนู ุทุฑูู ุชูููู ุนุฏุฏ ุงููุนููุงุช ุจุทุฑููุชูู: ูุตู ุชุถููู ุงูููุฑุฏุงุช ุงูุฃูุจุฑ ุฅูู ูุตูููุชูู ุฃุตุบุฑ ูุงูุณูุงุญ ูููุณุชููุงุช ุจูุดุงุฑูุฉ ุงููุนููุงุช. ุฃุถุงู [DeBERTa](model_doc/deberta) ุขููุฉ ุงูุชุจุงู ูููุตูุฉ ุญูุซ ูุชู ุชุฑููุฒ ุงููููุฉ ูููุถุนูุง ุจุดูู ูููุตู ูู ูุชุฌููู. ูุชู ุญุณุงุจ ุงูุงูุชุจุงู ูู ูุฐู ุงููุชุฌูุงุช ุงููููุตูุฉ ุจุฏูุงู ูู ูุชุฌู ูุงุญุฏ ูุญุชูู ุนูู ุชุถููู ุงููููุฉ ูุงููููุน. ุฑูุฒ [Longformer](model_doc/longformer) ุฃูุถูุง ุนูู ุฌุนู ุงูุงูุชุจุงู ุฃูุซุฑ ููุงุกุฉุ ุฎุงุตุฉ ููุนุงูุฌุฉ ุงููุณุชูุฏุงุช ุฐุงุช ุชุณูุณูุงุช ุฃุทููู. ููู ูุณุชุฎุฏู ูุฒูุฌูุง ูู  ุงูุชุจุงู ุงูููุงูุฐ ุงููุญููุฉ (ูุชู ุญุณุงุจ ุงูุงูุชุจุงู ููุท ู ูุงูุฐุฉ ุฐุงุช ุญุฌู ุซุงุจุช ุญูู ูู ุฑูุฒ) ูุงูุงูุชุจุงู ุงูุนุงู (ููุท ูุฑููุฒ ูููุฉ ูุญุฏุฏุฉ ูุซู `[CLS]` ููุชุตููู) ูุฅูุดุงุก ูุตูููุฉ ุงูุชุจุงู ูุชูุฑูุฉ ุจุฏูุงู ูู ูุตูููุฉ ุงูุชุจุงู ูุงููุฉ.\n "
        },
        {
            "sha": "892d70eb5f6d1657a107ff5c5d7e065aa897c2b1",
            "filename": "docs/source/ar/peft.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fpeft.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Fpeft.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fpeft.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -33,7 +33,7 @@ pip install git+https://github.com/huggingface/peft.git\n \n - [ูุญููุงุช ุงูุฑุชุจุฉ ุงูููุฎูุถุฉ](https://huggingface.co/docs/peft/conceptual_guides/lora)\n - [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)\n-- [AdaLoRA](https://arxiv.org/abs/2303.10512)\n+- [AdaLoRA](https://huggingface.co/papers/2303.10512)\n \n ุฅุฐุง ููุช ุชุฑูุฏ ุงุณุชุฎุฏุงู ุทุฑู PEFT ุงูุฃุฎุฑูุ ูุซู ุชุนูู ุงููุญุซ ุฃู ุถุจุท ุงููุญุซุ ุฃู ุญูู ููุชุจุฉ ๐ค PEFT ุจุดูู ุนุงูุ ูุฑุฌู ุงูุฑุฌูุน ุฅูู [ุงููุซุงุฆู](https://huggingface.co/docs/peft/index).\n "
        },
        {
            "sha": "dbc50192b4e3d4bfdd5cce4cec96961aab6b58d6",
            "filename": "docs/source/ar/tasks_explained.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Ftasks_explained.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Ftasks_explained.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks_explained.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -103,7 +103,7 @@\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\n </div>\n \n-<small>ุนูููุฉ ุงูุชูุงู ุฃุณุงุณูุฉ ุจุฏูู ุญุดู ุฃู ุฎุทู ุฎุทูุฉ ูุงุณุนุฉุ ูุฃุฎูุฐุฉ ูู  <a href=\"https://arxiv.org/abs/1603.07285\">ุฏููู ูุญุณุงุจ ุงูุงูุชูุงู ููุชุนูู ุงูุนููู.</a></small>\n+<small>ุนูููุฉ ุงูุชูุงู ุฃุณุงุณูุฉ ุจุฏูู ุญุดู ุฃู ุฎุทู ุฎุทูุฉ ูุงุณุนุฉุ ูุฃุฎูุฐุฉ ูู  <a href=\"https://huggingface.co/papers/1603.07285\">ุฏููู ูุญุณุงุจ ุงูุงูุชูุงู ููุชุนูู ุงูุนููู.</a></small>\n \n ููููู ุชุบุฐูุฉ ูุฐุง ุงููุงุชุฌ ุฅูู ุทุจูุฉ ุงูุชูุงู ุฃุฎุฑูุ  ููุน ูู ุทุจูุฉ ูุชุชุงููุฉุ ุชุชุนูู ุงูุดุจูุฉ ุฃุดูุงุก ุฃูุซุฑ ุชุนููุฏูุง ูุชุฌุฑูุฏูุฉ ูุซู ุงูููุงูู ุฃู ุงูุตูุงุฑูุฎ. ุจูู ุทุจูุงุช ุงูุงูุชูุงูุ ูู ุงูุดุงุฆุน ุฅุถุงูุฉ ุทุจูุฉ ุชุฌููุน ูุชูููู ุงูุฃุจุนุงุฏ ูุฌุนู ุงููููุฐุฌ ุฃูุซุฑ ููุฉ ููุชุบูุฑุงุช ูู ููุถุน ุงูููุฒุฉ.\n "
        },
        {
            "sha": "5b0f24aba1143ae17478b3ebf6d27a4f4c3a29a1",
            "filename": "docs/source/ar/tokenizer_summary.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Ftokenizer_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Ftokenizer_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftokenizer_summary.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -94,7 +94,7 @@\n \n ### ุชุฑููุฒ ุงูุฃุฒูุงุฌ ุงูุจุงูุชูุฉ (BPE)\n \n-ุชู ุชูุฏูู ุฑููุฒ ุฃุฒูุงุฌ ุงูุจุงูุช (BPE) ูู ูุฑูุฉ ุจุญุซูุฉ ุจุนููุงู [ุงูุชุฑุฌูุฉ ุงูุขููุฉ ุงูุนุตุจูุฉ ูููููุงุช ุงููุงุฏุฑุฉ ุจุงุณุชุฎุฏุงู ูุญุฏุงุช subword (Sennrich et al.ุ 2015)](https://arxiv.org/abs/1508.07909). ูุนุชูุฏ BPE ุนูู ููุฌุฒูุฆ ุฃููู ููุณู ุจูุงูุงุช ุงูุชุฏุฑูุจ ุฅูู\n+ุชู ุชูุฏูู ุฑููุฒ ุฃุฒูุงุฌ ุงูุจุงูุช (BPE) ูู ูุฑูุฉ ุจุญุซูุฉ ุจุนููุงู [ุงูุชุฑุฌูุฉ ุงูุขููุฉ ุงูุนุตุจูุฉ ูููููุงุช ุงููุงุฏุฑุฉ ุจุงุณุชุฎุฏุงู ูุญุฏุงุช subword (Sennrich et al.ุ 2015)](https://huggingface.co/papers/1508.07909). ูุนุชูุฏ BPE ุนูู ููุฌุฒูุฆ ุฃููู ููุณู ุจูุงูุงุช ุงูุชุฏุฑูุจ ุฅูู\n ูููุงุช. ูููู ุฃู ูููู ุงูุชุญููู ุงููุณุจู ุจุณูุทูุง ูุซู ุงูุชูุณูู ุงูููุงููุ ุนูู ุณุจูู ุงููุซุงู [GPT-2](model_doc/gpt2)ุ [RoBERTa](model_doc/roberta). ุชุดูู ุงูุชูุณูู ุงูุฃูุซุฑ ุชูุฏููุง ูุนุชูุฏ ุนูู ุงูุชุญููู ุงููุงุฆู ุนูู ุงูููุงุนุฏุ ุนูู ุณุจูู ุงููุซุงู [XLM](model_doc/xlm)ุ [FlauBERT](model_doc/flaubert) ุงูุฐู ูุณุชุฎุฏู Moses ููุนุธู ุงููุบุงุชุ ุฃู [GPT](model_doc/openai-gpt) ุงูุฐู ูุณุชุฎุฏู spaCy ู ftfyุ ูุญุณุงุจ ุชูุฑุงุฑ ูู ูููุฉ ูู ูุฌููุนุฉ ุจูุงูุงุช ุงูุชุฏุฑูุจ.\n \n ุจุนุฏ ุงูุชุญููู ุงููุณุจูุ ูุชู ุฅูุดุงุก ูุฌููุนุฉ ูู ุงููููุงุช ุงููุฑูุฏุฉ ููุฏ ุชู ุชุญุฏูุฏ ุชูุฑุงุฑ ูู ูููุฉ ูู ุชู ุชุญุฏูุฏ ุจูุงูุงุช ุงูุชุฏุฑูุจ. ุจุนุฏ ุฐููุ ูููู BPE ุจุฅูุดุงุก ููุฑุฏุงุช ุฃุณุงุณูุฉ ุชุชููู ูู ุฌููุน ุงูุฑููุฒ ุงูุชู ุชุญุฏุซ ูู ูุฌููุนุฉ ุงููููุงุช ุงููุฑูุฏุฉ ููุชุนูู ููุงุนุฏ ุงูุฏูุฌ ูุชุดููู ุฑูุฒ ุฌุฏูุฏ ูู ุฑูุฒูู ูู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ. ุฅูู ููุนู ุฐูู ุญุชู ุชุตู ุงูููุฑุฏุงุช ุฅูู ุญุฌู ุงูููุฑุฏุงุช ุงููุทููุจ. ูุงุญุธ ุฃู ุญุฌู ุงูููุฑุฏุงุช ูู ูุฑุท ูุนููุฉ ูุชุญุฏูุฏ ูุจู ุชุฏุฑูุจ ููุฌุฒูุฆ  ุงููุตูุต.\n@@ -158,7 +158,7 @@ BPE. ุฃููุงูุ ูููู WordPiece ุจุชูููู ุงูููุฑุฏุงุช ูุชุถููู\n ### Unigram\n \n Unigram ูู ุฎูุงุฑุฒููุฉ ุชููููุฒ subword ุงูุชู ุชู ุชูุฏูููุง ูู [ุชูุธูู subword: ุชุญุณูู ููุงุฐุฌ ุงูุชุฑุฌูุฉ ุงูุดุจูุฉ ุงูุนุตุจูุฉ\n-ููุงุฐุฌ ูุน ูุฑุดุญูู subword ูุชุนุฏุฏุฉ (Kudoุ 2018)](https://arxiv.org/pdf/1804.10959.pdf). ุนูู ุนูุณ BPE ุฃู\n+ููุงุฐุฌ ูุน ูุฑุดุญูู subword ูุชุนุฏุฏุฉ (Kudoุ 2018)](https://huggingface.co/papers/1804.10959). ุนูู ุนูุณ BPE ุฃู\n WordPieceุ ูููู Unigram ุจุชูููู ููุฑุฏุงุชู ุงูุฃุณุงุณูุฉ ุฅูู ุนุฏุฏ ูุจูุฑ ูู ุงูุฑููุฒ ููููููุง ุชุฏุฑูุฌูุงู ููุญุตูู ุนูู ููุฑุฏุงุช ุฃุตุบุฑ. ูููู ุฃู ุชุชูุงูู ุงูููุฑุฏุงุช ุงูุฃุณุงุณูุฉ ุนูู ุณุจูู ุงููุซุงู ูุน ุฌููุน ุงููููุงุช ุงููุณุจูุฉ ุงูุชูููุฒ ูุงูุณูุงุณู ุงููุฑุนูุฉ ุงูุฃูุซุฑ ุดููุนูุง. ูุง ูุชู ุงุณุชุฎุฏุงู Unigram ูุจุงุดุฑุฉ ูุฃู ูู ุงูููุงุฐุฌ ูู ุงููุญููุงุชุ ููููู ูุณุชุฎุฏู ุจุงูุงูุชุฑุงู ูุน [SentencePiece](#sentencepiece).\n \n ูู ูู ุฎุทูุฉ ุชุฏุฑูุจุ ูุญุฏุฏ ุฎูุงุฑุฒููุฉ Unigram ุฎุณุงุฑุฉ (ุบุงูุจูุง ูุง ูุชู ุชุนุฑูููุง ุนูู ุฃููุง ุงูููุบุงุฑูุชู) ุนุจุฑ ุจูุงูุงุช ุงูุชุฏุฑูุจ ุจุงููุธุฑ ุฅูู ุงูููุฑุฏุงุช ุงูุญุงููุฉ ููููุฐุฌ ุงููุบุฉ unigram. ุจุนุฏ ุฐููุ ุจุงููุณุจุฉ ููู ุฑูุฒ ูู ุงูููุฑุฏุงุชุ ูุญุณุจ ุงูุฎูุงุฑุฒููุฉ ููุฏุงุฑ ุฒูุงุฏุฉ ุงูุฎุณุงุฑุฉ ุงูุฅุฌูุงููุฉ ุฅุฐุง ุชู ุฅุฒุงูุฉ ุงูุฑูุฒ ูู ุงูููุฑุฏุงุช. ุซู ูููู Unigram ุจุฅุฒุงูุฉ p (ูุน p ุนุงุฏุฉ ูุง ุชููู 10% ุฃู 20%) ูู ุงููุงุฆุฉ ูู ุงูุฑููุฒ ุงูุชู ุชููู ุฒูุงุฏุฉ ุงูุฎุณุงุฑุฉ ูููุง ูู ุงูุฃุฏููุ *ุฃู* ุชูู\n@@ -188,7 +188,7 @@ $$\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )\n \n ุชุญุชูู ุฌููุน ุฎูุงุฑุฒููุงุช ุชูููุฒ ุงูููุตููุฉ ุญุชู ุงูุขู ุนูู ููุณ ุงููุดููุฉ: ูู ุงูููุชุฑุถ ุฃู ุงููุต ุงููุฏุฎู ูุณุชุฎุฏู ุงููุณุงูุงุช ููุตู ุงููููุงุช. ููุน ุฐููุ ูุง ุชุณุชุฎุฏู ุฌููุน ุงููุบุงุช ุงููุณุงูุงุช ููุตู ุงููููุงุช. ุฃุญุฏ ุงูุญููู ุงูููููุฉ ูู ุงุณุชุฎุฏุงููุนุงูุฌ ูุณุจู ููุบุฉ ูุญุฏุฏุ *ูุซุงู* [XLM](model_doc/xlm) ููุฐู ูุณุชุฎุฏู ูุนุงูุฌุงุช ูุณุจูุฉ ูุญุฏุฏุฉ ููุตูููุฉ ูุงููุงุจุงููุฉ ูุงูุชุงููุงูุฏูุฉ.\n ูุญู ูุฐู ุงููุดููุฉ ุจุดูู ุฃุนูุ [SentencePiece: A simple and language independent subword tokenizer and\n-detokenizer for Neural Text Processing (Kudo et al.ุ 2018)](https://arxiv.org/pdf/1808.06226.pdf) ูุชุนุงูู ูุน ุงููุฏุฎูุงุช\n+detokenizer for Neural Text Processing (Kudo et al.ุ 2018)](https://huggingface.co/papers/1808.06226) ูุชุนุงูู ูุน ุงููุฏุฎูุงุช\n ูุชุฏูู ุจูุงูุงุช ุฎุงูุ ูุจุงูุชุงูู ูุดูู ุงููุณุงูุฉ ูู ูุฌููุนุฉ ุงูุฃุญุฑู ุงูุชู ุณูุชู ุงุณุชุฎุฏุงููุง. ุซู ูุณุชุฎุฏู ุฎูุงุฑุฒููุฉ BPE ุฃู unigram\n ูุจูุงุก ุงูููุฑุฏุงุช ุงูููุงุณุจุฉ.\n "
        },
        {
            "sha": "7bdebdca2a2842f4c5437a987843544730792de6",
            "filename": "docs/source/ar/trainer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Far%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftrainer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -377,7 +377,7 @@ trainer = trl.SFTTrainer(\n \n trainer.train()\n ```\n-ููููู ูุฑุงุกุฉ ุงููุฒูุฏ ุญูู ุงูุทุฑููุฉ ูู [ุงููุณุชูุฏุน ุงูุฃุตูู](https://github.com/jiaweizzhao/GaLore) ุฃู [ุงููุฑูุฉ ุงูุจุญุซูุฉ](https://arxiv.org/abs/2403.03507).\n+ููููู ูุฑุงุกุฉ ุงููุฒูุฏ ุญูู ุงูุทุฑููุฉ ูู [ุงููุณุชูุฏุน ุงูุฃุตูู](https://github.com/jiaweizzhao/GaLore) ุฃู [ุงููุฑูุฉ ุงูุจุญุซูุฉ](https://huggingface.co/papers/2403.03507).\n \n ุญุงูููุงุ ููููู ููุท ุชุฏุฑูุจ ุงูุทุจูุงุช ุงูุฎุทูุฉ ุงูุชู ุชุนุชุจุฑ ุทุจูุงุช GaLore ูุณุชุณุชุฎุฏู ุงูุชุญูู  ุฐู ุงูุฑุชุจุฉ ุงูููุฎูุถุฉ ููุชุฏุฑูุจ ุจูููุง ุณูุชู ุชุญุณูู ุงูุทุจูุงุช ุงููุชุจููุฉ ุจุงูุทุฑููุฉ ุงูุชูููุฏูุฉ.\n "
        },
        {
            "sha": "1c4d4e767520431e7241a6d489f044f28a6f1cbe",
            "filename": "docs/source/de/index.md",
            "status": "modified",
            "additions": 130,
            "deletions": 130,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fde%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fde%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Findex.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -55,148 +55,148 @@ Die Bibliothek enthรคlt derzeit JAX-, PyTorch- und TensorFlow-Implementierungen,\n \n <!--This list is updated automatically from the README with _make fix-copies_. Do not update manually! -->\n \n-1. **[ALBERT](model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n-1. **[ALIGN](model_doc/align)** (from Google Research) released with the paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.\n-1. **[BART](model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n-1. **[BARThez](model_doc/barthez)** (from รcole polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n-1. **[BARTpho](model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n-1. **[BEiT](model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\n-1. **[BERT](model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n-1. **[BERT For Sequence Generation](model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+1. **[ALBERT](model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://huggingface.co/papers/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n+1. **[ALIGN](model_doc/align)** (from Google Research) released with the paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://huggingface.co/papers/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.\n+1. **[BART](model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://huggingface.co/papers/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n+1. **[BARThez](model_doc/barthez)** (from รcole polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://huggingface.co/papers/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n+1. **[BARTpho](model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://huggingface.co/papers/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n+1. **[BEiT](model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://huggingface.co/papers/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\n+1. **[BERT](model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://huggingface.co/papers/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n+1. **[BERT For Sequence Generation](model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n 1. **[BERTweet](model_doc/bertweet)** (from VinAI Research) released with the paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.\n-1. **[BigBird-Pegasus](model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n-1. **[BigBird-RoBERTa](model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n-1. **[Blenderbot](model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n-1. **[BlenderbotSmall](model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n+1. **[BigBird-Pegasus](model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n+1. **[BigBird-RoBERTa](model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n+1. **[Blenderbot](model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n+1. **[BlenderbotSmall](model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n 1. **[BLOOM](model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).\n-1. **[BORT](model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) by Adrian de Wynter and Daniel J. Perry.\n-1. **[ByT5](model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n-1. **[CamemBERT](model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suรกrez*, Yoann Dupont, Laurent Romary, รric Villemonte de la Clergerie, Djamรฉ Seddah and Benoรฎt Sagot.\n-1. **[CANINE](model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n-1. **[CLIP](model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n-1. **[CodeGen](model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.\n-1. **[ConvBERT](model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n-1. **[ConvNeXT](model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n-1. **[ConvNeXTV2](model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n-1. **[CPM](model_doc/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n-1. **[CTRL](model_doc/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\n-1. **[CvT](model_doc/cvt)** (from Microsoft) released with the paper [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\n-1. **[Data2Vec](model_doc/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n-1. **[DeBERTa](model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n-1. **[DeBERTa-v2](model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n-1. **[Decision Transformer](model_doc/decision_transformer)** (from Berkeley/Facebook/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n-1. **[DeiT](model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervรฉ Jรฉgou.\n-1. **[DETR](model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n-1. **[DialoGPT](model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n-1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n-1. **[DiT](model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n-1. **[DPR](model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas Oฤuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n-1. **[DPT](master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n-1. **[EfficientNet](model_doc/efficientnet)** (from Google Research) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)  by Mingxing Tan and Quoc V. Le.\n-1. **[ELECTRA](model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n-1. **[EncoderDecoder](model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n-1. **[FlauBERT](model_doc/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Loรฏc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoรฎt Crabbรฉ, Laurent Besacier, Didier Schwab.\n-1. **[FLAVA](model_doc/flava)** (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.\n-1. **[FNet](model_doc/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n-1. **[Funnel Transformer](model_doc/funnel)** (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n-1. **[GLPN](model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n+1. **[BORT](model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://huggingface.co/papers/2010.10499) by Adrian de Wynter and Daniel J. Perry.\n+1. **[ByT5](model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://huggingface.co/papers/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n+1. **[CamemBERT](model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://huggingface.co/papers/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suรกrez*, Yoann Dupont, Laurent Romary, รric Villemonte de la Clergerie, Djamรฉ Seddah and Benoรฎt Sagot.\n+1. **[CANINE](model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://huggingface.co/papers/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n+1. **[CLIP](model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://huggingface.co/papers/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n+1. **[CodeGen](model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://huggingface.co/papers/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.\n+1. **[ConvBERT](model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://huggingface.co/papers/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n+1. **[ConvNeXT](model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://huggingface.co/papers/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n+1. **[ConvNeXTV2](model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://huggingface.co/papers/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n+1. **[CPM](model_doc/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://huggingface.co/papers/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n+1. **[CTRL](model_doc/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://huggingface.co/papers/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\n+1. **[CvT](model_doc/cvt)** (from Microsoft) released with the paper [CvT: Introducing Convolutions to Vision Transformers](https://huggingface.co/papers/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\n+1. **[Data2Vec](model_doc/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://huggingface.co/papers/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n+1. **[DeBERTa](model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n+1. **[DeBERTa-v2](model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n+1. **[Decision Transformer](model_doc/decision_transformer)** (from Berkeley/Facebook/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://huggingface.co/papers/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n+1. **[DeiT](model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://huggingface.co/papers/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervรฉ Jรฉgou.\n+1. **[DETR](model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://huggingface.co/papers/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n+1. **[DialoGPT](model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://huggingface.co/papers/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n+1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://huggingface.co/papers/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n+1. **[DiT](model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://huggingface.co/papers/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n+1. **[DPR](model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://huggingface.co/papers/2004.04906) by Vladimir Karpukhin, Barlas Oฤuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n+1. **[DPT](master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://huggingface.co/papers/2103.13413) by Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n+1. **[EfficientNet](model_doc/efficientnet)** (from Google Research) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://huggingface.co/papers/1905.11946)  by Mingxing Tan and Quoc V. Le.\n+1. **[ELECTRA](model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://huggingface.co/papers/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n+1. **[EncoderDecoder](model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+1. **[FlauBERT](model_doc/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://huggingface.co/papers/1912.05372) by Hang Le, Loรฏc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoรฎt Crabbรฉ, Laurent Besacier, Didier Schwab.\n+1. **[FLAVA](model_doc/flava)** (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://huggingface.co/papers/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.\n+1. **[FNet](model_doc/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://huggingface.co/papers/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n+1. **[Funnel Transformer](model_doc/funnel)** (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://huggingface.co/papers/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n+1. **[GLPN](model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://huggingface.co/papers/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n 1. **[GPT](model_doc/openai-gpt)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://openai.com/research/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n 1. **[GPT Neo](model_doc/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.\n-1. **[GPT NeoX](model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\n+1. **[GPT NeoX](model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://huggingface.co/papers/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\n 1. **[GPT-2](model_doc/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://openai.com/research/better-language-models/) by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever.\n 1. **[GPT-J](model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.\n 1. **[GPTSAN-japanese](model_doc/gptsan-japanese)** released in the repository [tanreinama/GPTSAN](https://github.com/tanreinama/GPTSAN/blob/main/report/model.md) by Toshiyuki Sakamoto(tanreinama).\n-1. **[GroupViT](model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\n-1. **[Hubert](model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n-1. **[I-BERT](model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n+1. **[GroupViT](model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://huggingface.co/papers/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\n+1. **[Hubert](model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://huggingface.co/papers/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n+1. **[I-BERT](model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://huggingface.co/papers/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n 1. **[ImageGPT](model_doc/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.\n-1. **[LayoutLM](model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n-1. **[LayoutLMv2](model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n-1. **[LayoutLMv3](model_doc/layoutlmv3)** (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.\n-1. **[LayoutXLM](model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n-1. **[LED](model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n-1. **[LeViT](model_doc/levit)** (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervรฉ Jรฉgou, Matthijs Douze.\n-1. **[Longformer](model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n-1. **[LongT5](model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.\n-1. **[LUKE](model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n-1. **[LXMERT](model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) by Hao Tan and Mohit Bansal.\n-1. **[M-CTC-T](model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.\n-1. **[M2M100](model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n+1. **[LayoutLM](model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://huggingface.co/papers/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n+1. **[LayoutLMv2](model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://huggingface.co/papers/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n+1. **[LayoutLMv3](model_doc/layoutlmv3)** (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://huggingface.co/papers/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.\n+1. **[LayoutXLM](model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://huggingface.co/papers/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n+1. **[LED](model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n+1. **[LeViT](model_doc/levit)** (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://huggingface.co/papers/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervรฉ Jรฉgou, Matthijs Douze.\n+1. **[Longformer](model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n+1. **[LongT5](model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://huggingface.co/papers/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.\n+1. **[LUKE](model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n+1. **[LXMERT](model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) by Hao Tan and Mohit Bansal.\n+1. **[M-CTC-T](model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://huggingface.co/papers/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.\n+1. **[M2M100](model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://huggingface.co/papers/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n 1. **[MarianMT](model_doc/marian)** Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by Jรถrg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.\n-1. **[Mask2Former](model_doc/mask2former)** (from FAIR and UIUC) released with the paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.\n-1. **[MaskFormer](model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n-1. **[mBART](model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n-1. **[mBART-50](model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n-1. **[Megatron-BERT](model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n-1. **[Megatron-GPT2](model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n-1. **[mLUKE](model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\n-1. **[MobileBERT](model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\n-1. **[MobileViT](model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari.\n-1. **[MPNet](model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n-1. **[MT5](model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n-1. **[MVP](model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n-1. **[Nezha](model_doc/nezha)** (from Huawei Noahโs Ark Lab) released with the paper [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.\n-1. **[NLLB](model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.\n-1. **[Nystrรถmformer](model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n-1. **[OneFormer](model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n-1. **[OPT](master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\n-1. **[OWL-ViT](model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.\n-1. **[Pegasus](model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.\n-1. **[Perceiver IO](model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hรฉnaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Joรฃo Carreira.\n+1. **[Mask2Former](model_doc/mask2former)** (from FAIR and UIUC) released with the paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://huggingface.co/papers/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.\n+1. **[MaskFormer](model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://huggingface.co/papers/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n+1. **[mBART](model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://huggingface.co/papers/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n+1. **[mBART-50](model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://huggingface.co/papers/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n+1. **[Megatron-BERT](model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://huggingface.co/papers/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n+1. **[Megatron-GPT2](model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://huggingface.co/papers/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n+1. **[mLUKE](model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://huggingface.co/papers/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\n+1. **[MobileBERT](model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://huggingface.co/papers/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\n+1. **[MobileViT](model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://huggingface.co/papers/2110.02178) by Sachin Mehta and Mohammad Rastegari.\n+1. **[MPNet](model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://huggingface.co/papers/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n+1. **[MT5](model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://huggingface.co/papers/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n+1. **[MVP](model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://huggingface.co/papers/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n+1. **[Nezha](model_doc/nezha)** (from Huawei Noahโs Ark Lab) released with the paper [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://huggingface.co/papers/1909.00204) by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.\n+1. **[NLLB](model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://huggingface.co/papers/2207.04672) by the NLLB team.\n+1. **[Nystrรถmformer](model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention](https://huggingface.co/papers/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n+1. **[OneFormer](model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://huggingface.co/papers/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n+1. **[OPT](master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://huggingface.co/papers/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\n+1. **[OWL-ViT](model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://huggingface.co/papers/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.\n+1. **[Pegasus](model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://huggingface.co/papers/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.\n+1. **[Perceiver IO](model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://huggingface.co/papers/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hรฉnaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Joรฃo Carreira.\n 1. **[PhoBERT](model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.\n-1. **[PLBart](model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n-1. **[PoolFormer](model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.\n-1. **[ProphetNet](model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n-1. **[QDQBert](model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.\n-1. **[RAG](model_doc/rag)** (from Facebook) released with the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kรผttler, Mike Lewis, Wen-tau Yih, Tim Rocktรคschel, Sebastian Riedel, Douwe Kiela.\n-1. **[REALM](model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.\n-1. **[Reformer](model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n-1. **[RegNet](model_doc/regnet)** (from META Platforms) released with the paper [Designing Network Design Space](https://arxiv.org/abs/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n-1. **[RemBERT](model_doc/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, M. Johnson, Sebastian Ruder.\n-1. **[ResNet](model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n-1. **[RoBERTa](model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n-1. **[RoFormer](model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n-1. **[SegFormer](model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n-1. **[SEW](model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n-1. **[SEW-D](model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n-1. **[SpeechToTextTransformer](model_doc/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n-1. **[SpeechToTextTransformer2](model_doc/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n-1. **[Splinter](model_doc/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n-1. **[SqueezeBERT](model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n-1. **[Swin Transformer](model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n-1. **[Swin Transformer V2](model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\n-1. **[T5](model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n+1. **[PLBart](model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://huggingface.co/papers/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n+1. **[PoolFormer](model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://huggingface.co/papers/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.\n+1. **[ProphetNet](model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n+1. **[QDQBert](model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://huggingface.co/papers/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.\n+1. **[RAG](model_doc/rag)** (from Facebook) released with the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://huggingface.co/papers/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kรผttler, Mike Lewis, Wen-tau Yih, Tim Rocktรคschel, Sebastian Riedel, Douwe Kiela.\n+1. **[REALM](model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://huggingface.co/papers/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.\n+1. **[Reformer](model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) by Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n+1. **[RegNet](model_doc/regnet)** (from META Platforms) released with the paper [Designing Network Design Space](https://huggingface.co/papers/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n+1. **[RemBERT](model_doc/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https://huggingface.co/papers/2010.12821) by Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, M. Johnson, Sebastian Ruder.\n+1. **[ResNet](model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://huggingface.co/papers/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n+1. **[RoBERTa](model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://huggingface.co/papers/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n+1. **[RoFormer](model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://huggingface.co/papers/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n+1. **[SegFormer](model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://huggingface.co/papers/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n+1. **[SEW](model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n+1. **[SEW-D](model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n+1. **[SpeechToTextTransformer](model_doc/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://huggingface.co/papers/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n+1. **[SpeechToTextTransformer2](model_doc/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://huggingface.co/papers/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n+1. **[Splinter](model_doc/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https://huggingface.co/papers/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n+1. **[SqueezeBERT](model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://huggingface.co/papers/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n+1. **[Swin Transformer](model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://huggingface.co/papers/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n+1. **[Swin Transformer V2](model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://huggingface.co/papers/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\n+1. **[T5](model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://huggingface.co/papers/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n 1. **[T5v1.1](model_doc/t5v1.1)** (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n-1. **[TAPAS](model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Paweล Krzysztof Nowak, Thomas Mรผller, Francesco Piccinno and Julian Martin Eisenschlos.\n-1. **[TAPEX](model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\n-1. **[Trajectory Transformer](model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine\n-1. **[Transformer-XL](model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n-1. **[TrOCR](model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n-1. **[UL2](model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler\n+1. **[TAPAS](model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://huggingface.co/papers/2004.02349) by Jonathan Herzig, Paweล Krzysztof Nowak, Thomas Mรผller, Francesco Piccinno and Julian Martin Eisenschlos.\n+1. **[TAPEX](model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://huggingface.co/papers/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\n+1. **[Trajectory Transformer](model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://huggingface.co/papers/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine\n+1. **[Transformer-XL](model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://huggingface.co/papers/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n+1. **[TrOCR](model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://huggingface.co/papers/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n+1. **[UL2](model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://huggingface.co/papers/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler\n 1. **[UMT5](model_doc/umt5)** (from Google Research) released with the paper [UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining](https://openreview.net/forum?id=kXwdL1cWOAi) by Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant.\n-1. **[UniSpeech](model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n-1. **[UniSpeechSat](model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n-1. **[VAN](model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n-1. **[VideoMAE](model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\n-1. **[ViLT](model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.\n-1. **[Vision Transformer (ViT)](model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n-1. **[VisualBERT](model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n-1. **[ViTMAE](model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollรกr, Ross Girshick.\n-1. **[Wav2Vec2](model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n-1. **[Wav2Vec2-Conformer](model_doc/wav2vec2-conformer)** (from Facebook AI) released with the paper [FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.\n-1. **[Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n-1. **[WavLM](model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n-1. **[XGLM](model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n-1. **[XLM](model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\n-1. **[XLM-ProphetNet](model_doc/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n-1. **[XLM-RoBERTa](model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmรกn, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.\n-1. **[XLM-RoBERTa-XL](model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n-1. **[XLM-V](model_doc/xlm-v)** (from Meta AI) released with the paper [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472) by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.\n-1. **[XLNet](model_doc/xlnet)** (from Google/CMU) released with the paper [โXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n-1. **[XLS-R](model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n-1. **[XLSR-Wav2Vec2](model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n-1. **[YOLOS](model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n-1. **[YOSO](model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n+1. **[UniSpeech](model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://huggingface.co/papers/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n+1. **[UniSpeechSat](model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://huggingface.co/papers/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n+1. **[VAN](model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://huggingface.co/papers/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n+1. **[VideoMAE](model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://huggingface.co/papers/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\n+1. **[ViLT](model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://huggingface.co/papers/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.\n+1. **[Vision Transformer (ViT)](model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://huggingface.co/papers/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n+1. **[VisualBERT](model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://huggingface.co/papers/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n+1. **[ViTMAE](model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://huggingface.co/papers/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollรกr, Ross Girshick.\n+1. **[Wav2Vec2](model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://huggingface.co/papers/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n+1. **[Wav2Vec2-Conformer](model_doc/wav2vec2-conformer)** (from Facebook AI) released with the paper [FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ](https://huggingface.co/papers/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.\n+1. **[Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://huggingface.co/papers/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n+1. **[WavLM](model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://huggingface.co/papers/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n+1. **[XGLM](model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://huggingface.co/papers/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n+1. **[XLM](model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://huggingface.co/papers/1901.07291) by Guillaume Lample and Alexis Conneau.\n+1. **[XLM-ProphetNet](model_doc/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n+1. **[XLM-RoBERTa](model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://huggingface.co/papers/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmรกn, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.\n+1. **[XLM-RoBERTa-XL](model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://huggingface.co/papers/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n+1. **[XLM-V](model_doc/xlm-v)** (from Meta AI) released with the paper [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://huggingface.co/papers/2301.10472) by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa.\n+1. **[XLNet](model_doc/xlnet)** (from Google/CMU) released with the paper [โXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://huggingface.co/papers/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n+1. **[XLS-R](model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://huggingface.co/papers/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n+1. **[XLSR-Wav2Vec2](model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://huggingface.co/papers/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n+1. **[YOLOS](model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://huggingface.co/papers/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n+1. **[YOSO](model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://huggingface.co/papers/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n \n \n ### Unterstรผtzte Frameworks"
        },
        {
            "sha": "f43d227a9a6e54d08d15e6dd5ec7cb156978d88a",
            "filename": "docs/source/de/peft.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fde%2Fpeft.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fde%2Fpeft.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fpeft.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -44,7 +44,7 @@ Transformers unterstรผtzt nativ einige PEFT-Methoden, d.h. Sie kรถnnen lokal ode\n \n - [Low Rank Adapters](https://huggingface.co/docs/peft/conceptual_guides/lora)\n - [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)\n-- [AdaLoRA](https://arxiv.org/abs/2303.10512)\n+- [AdaLoRA](https://huggingface.co/papers/2303.10512)\n \n Wenn Sie andere PEFT-Methoden, wie z.B. Prompt Learning oder Prompt Tuning, verwenden mรถchten, oder รผber die ๐ค PEFT-Bibliothek im Allgemeinen, lesen Sie bitte die [Dokumentation](https://huggingface.co/docs/peft/index).\n "
        },
        {
            "sha": "b65f45341e3934dd182c481f91e8c3fc4cb559ad",
            "filename": "docs/source/en/glossary.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fglossary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fglossary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fglossary.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -163,7 +163,7 @@ The intermediate embedding size of the feed forward layers is often bigger than\n \n For an input of size `[batch_size, sequence_length]`, the memory required to store the intermediate feed forward\n embeddings `[batch_size, sequence_length, config.intermediate_size]` can account for a large fraction of the memory\n-use. The authors of [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) noticed that since the\n+use. The authors of [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) noticed that since the\n computation is independent of the `sequence_length` dimension, it is mathematically equivalent to compute the output\n embeddings of both feed forward layers `[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n`\n individually and concat them afterward to `[batch_size, sequence_length, config.hidden_size]` with `n = sequence_length`, which trades increased computation time against reduced memory use, but yields a mathematically\n@@ -207,7 +207,7 @@ numerical representations of tokens building the sequences that will be used as\n <Youtube id=\"VFp38yj8h3A\"/>\n \n Each tokenizer works differently but the underlying mechanism remains the same. Here's an example using the BERT\n-tokenizer, which is a [WordPiece](https://arxiv.org/pdf/1609.08144.pdf) tokenizer:\n+tokenizer, which is a [WordPiece](https://huggingface.co/papers/1609.08144) tokenizer:\n \n ```python\n >>> from transformers import BertTokenizer"
        },
        {
            "sha": "f52bccdda8c037bf5ad7b4eebf041e8cbe9d962a",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n Large Language Models (LLMs) such as GPT3/4, [Falcon](https://huggingface.co/tiiuae/falcon-40b), and [Llama](https://huggingface.co/meta-llama/Llama-2-70b-hf) are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries.\n Deploying these models in real-world tasks remains challenging, however:\n \n--   To exhibit near-human text understanding and generation capabilities, LLMs currently require to be composed of billions of parameters (see [Kaplan et al](https://arxiv.org/abs/2001.08361), [Wei et. al](https://arxiv.org/abs/2206.07682)). This consequently amplifies the memory demands for inference.\n+-   To exhibit near-human text understanding and generation capabilities, LLMs currently require to be composed of billions of parameters (see [Kaplan et al](https://huggingface.co/papers/2001.08361), [Wei et. al](https://huggingface.co/papers/2206.07682)). This consequently amplifies the memory demands for inference.\n -   In many real-world tasks, LLMs need to be given extensive contextual information. This necessitates the model's capability to manage very long input sequences during inference.\n \n The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.\n@@ -27,7 +27,7 @@ In this guide, we will go over the effective techniques for efficient LLM deploy\n \n 2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.\n \n-3.  **Architectural Innovations:** Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are [Alibi](https://arxiv.org/abs/2108.12409), [Rotary embeddings](https://arxiv.org/abs/2104.09864), [Multi-Query Attention (MQA)](https://arxiv.org/abs/1911.02150) and [Grouped-Query-Attention (GQA)]((https://arxiv.org/abs/2305.13245)).\n+3.  **Architectural Innovations:** Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are [Alibi](https://huggingface.co/papers/2108.12409), [Rotary embeddings](https://huggingface.co/papers/2104.09864), [Multi-Query Attention (MQA)](https://huggingface.co/papers/1911.02150) and [Grouped-Query-Attention (GQA)]((https://huggingface.co/papers/2305.13245)).\n \n Throughout this guide, we will offer an analysis of auto-regressive generation from a tensor's perspective. We delve into the pros and cons of adopting lower precision, provide a comprehensive exploration of the latest attention algorithms, and discuss improved LLM architectures. While doing so, we run practical examples showcasing each of the feature improvements.\n \n@@ -157,8 +157,8 @@ from accelerate.utils import release_memory\n release_memory(model)\n ```\n \n-Now what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be quantized to 8-bit or 4-bits without a significant loss in performance (see [Dettmers et al.](https://arxiv.org/abs/2208.07339)).\n-Model can be quantized to even 3 or 2 bits with an acceptable loss in performance as shown in the recent [GPTQ paper](https://arxiv.org/abs/2210.17323) ๐คฏ.\n+Now what if your GPU does not have 32 GB of VRAM? It has been found that model weights can be quantized to 8-bit or 4-bits without a significant loss in performance (see [Dettmers et al.](https://huggingface.co/papers/2208.07339)).\n+Model can be quantized to even 3 or 2 bits with an acceptable loss in performance as shown in the recent [GPTQ paper](https://huggingface.co/papers/2210.17323) ๐คฏ.\n \n Without going into too many details, quantization schemes aim at reducing the precision of weights while trying to keep the model's inference results as accurate as possible (*a.k.a* as close as possible to bfloat16).\n Note that quantization works especially well for text generation since all we care about is choosing the *set of most likely next tokens* and don't really care about the exact values of the next token *logit* distribution.\n@@ -308,21 +308,21 @@ Long story short, the default self-attention algorithm quickly becomes prohibiti\n \n As LLMs improve in text comprehension and generation, they are applied to increasingly complex tasks. While models once handled the translation or summarization of a few sentences, they now manage entire pages, demanding the capability to process extensive input lengths.\n \n-How can we get rid of the exorbitant memory requirements for large input lengths? We need a new way to compute the self-attention mechanism that gets rid of the \\\\( QK^T \\\\) matrix. [Tri Dao et al.](https://arxiv.org/abs/2205.14135) developed exactly such a new algorithm and called it **Flash Attention**.\n+How can we get rid of the exorbitant memory requirements for large input lengths? We need a new way to compute the self-attention mechanism that gets rid of the \\\\( QK^T \\\\) matrix. [Tri Dao et al.](https://huggingface.co/papers/2205.14135) developed exactly such a new algorithm and called it **Flash Attention**.\n \n In a nutshell, Flash Attention breaks the  \\\\(\\mathbf{V} \\times \\text{Softmax}(\\mathbf{QK}^T\\\\)) computation apart and instead computes smaller chunks of the output by iterating over multiple softmax computation steps:\n \n $$ \\textbf{O}_i \\leftarrow s^a_{ij} * \\textbf{O}_i + s^b_{ij} * \\mathbf{V}_{j} \\times \\text{Softmax}(\\mathbf{QK}^T_{i,j}) \\text{ for multiple } i, j \\text{ iterations} $$\n \n with \\\\( s^a_{ij} \\\\) and \\\\( s^b_{ij} \\\\) being some softmax normalization statistics that need to be recomputed for every \\\\( i \\\\) and \\\\( j \\\\) .\n \n-Please note that the whole Flash Attention is a bit more complex and is greatly simplified here as going in too much depth is out of scope for this guide. The reader is invited to take a look at the well-written [Flash Attention paper](https://arxiv.org/abs/2205.14135) for more details.\n+Please note that the whole Flash Attention is a bit more complex and is greatly simplified here as going in too much depth is out of scope for this guide. The reader is invited to take a look at the well-written [Flash Attention paper](https://huggingface.co/papers/2205.14135) for more details.\n \n The main takeaway here is:\n \n > By keeping track of softmax normalization statistics and by using some smart mathematics, Flash Attention gives **numerical identical** outputs compared to the default self-attention layer at a memory cost that only increases linearly with \\\\( N \\\\) .\n \n-Looking at the formula, one would intuitively say that Flash Attention must be much slower compared to the default self-attention formula as more computation needs to be done. Indeed Flash Attention requires more FLOPs compared to normal attention as the softmax normalization statistics have to constantly be recomputed (see [paper](https://arxiv.org/abs/2205.14135) for more details if interested)\n+Looking at the formula, one would intuitively say that Flash Attention must be much slower compared to the default self-attention formula as more computation needs to be done. Indeed Flash Attention requires more FLOPs compared to normal attention as the softmax normalization statistics have to constantly be recomputed (see [paper](https://huggingface.co/papers/2205.14135) for more details if interested)\n \n > However, Flash Attention is much faster in inference compared to default attention which comes from its ability to significantly reduce the demands on the slower, high-bandwidth memory of the GPU (VRAM), focusing instead on the faster on-chip memory (SRAM).\n \n@@ -526,22 +526,22 @@ Therefore, for the LLM without position embeddings each token appears to have th\n For the LLM to understand sentence order, an additional *cue* is needed and is usually applied in the form of *positional encodings* (or also called *positional embeddings*).\n Positional encodings, encode the position of each token into a numerical presentation that the LLM can leverage to better understand sentence order.\n \n-The authors of the [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) paper introduced sinusoidal positional embeddings \\\\( \\mathbf{P} = \\mathbf{p}_1, \\ldots, \\mathbf{p}_N \\\\) .\n+The authors of the [*Attention Is All You Need*](https://huggingface.co/papers/1706.03762) paper introduced sinusoidal positional embeddings \\\\( \\mathbf{P} = \\mathbf{p}_1, \\ldots, \\mathbf{p}_N \\\\) .\n where each vector \\\\( \\mathbf{p}_i \\\\) is computed as a sinusoidal function of its position \\\\( i \\\\) .\n The positional encodings are then simply added to the input sequence vectors \\\\( \\mathbf{\\hat{X}} = \\mathbf{\\hat{x}}_1, \\ldots, \\mathbf{\\hat{x}}_N \\\\) = \\\\( \\mathbf{x}_1 + \\mathbf{p}_1, \\ldots, \\mathbf{x}_N + \\mathbf{p}_N \\\\) thereby cueing the model to better learn sentence order.\n \n-Instead of using fixed position embeddings, others (such as [Devlin et al.](https://arxiv.org/abs/1810.04805)) used learned positional encodings for which the positional embeddings\n+Instead of using fixed position embeddings, others (such as [Devlin et al.](https://huggingface.co/papers/1810.04805)) used learned positional encodings for which the positional embeddings\n \\\\( \\mathbf{P} \\\\) are learned during training.\n \n Sinusoidal and learned position embeddings used to be the predominant methods to encode sentence order into LLMs, but a couple of problems related to these positional encodings were found:\n \n-  1. Sinusoidal and learned position embeddings are both absolute positional embeddings, *i.e.* encoding a unique embedding for each position id: \\\\( 0, \\ldots, N \\\\) . As shown by [Huang et al.](https://arxiv.org/abs/2009.13658) and [Su et al.](https://arxiv.org/abs/2104.09864), absolute positional embeddings lead to poor LLM performance for long text inputs. For long text inputs, it is advantageous if the model learns the relative positional distance input tokens have to each other instead of their absolute position.\n+  1. Sinusoidal and learned position embeddings are both absolute positional embeddings, *i.e.* encoding a unique embedding for each position id: \\\\( 0, \\ldots, N \\\\) . As shown by [Huang et al.](https://huggingface.co/papers/2009.13658) and [Su et al.](https://huggingface.co/papers/2104.09864), absolute positional embeddings lead to poor LLM performance for long text inputs. For long text inputs, it is advantageous if the model learns the relative positional distance input tokens have to each other instead of their absolute position.\n   2. When using learned position embeddings, the LLM has to be trained on a fixed input length \\\\( N \\\\), which makes it difficult to extrapolate to an input length longer than what it was trained on.\n \n Recently, relative positional embeddings that can tackle the above mentioned problems have become more popular, most notably:\n \n--   [Rotary Position Embedding (RoPE)](https://arxiv.org/abs/2104.09864)\n--   [ALiBi](https://arxiv.org/abs/2108.12409)\n+-   [Rotary Position Embedding (RoPE)](https://huggingface.co/papers/2104.09864)\n+-   [ALiBi](https://huggingface.co/papers/2108.12409)\n \n Both *RoPE* and *ALiBi* argue that it's best to cue the LLM about sentence order directly in the self-attention algorithm as it's there that word tokens are put into relation with each other. More specifically, sentence order should be cued by modifying the \\\\( \\mathbf{QK}^T \\\\) computation.\n \n@@ -556,14 +556,14 @@ $$ \\mathbf{\\hat{q}}_i^T \\mathbf{\\hat{x}}_j = \\mathbf{{q}}_i^T \\mathbf{R}_{\\theta\n *RoPE* is used in multiple of today's most important LLMs, such as:\n \n -   [**Falcon**](https://huggingface.co/tiiuae/falcon-40b)\n--   [**Llama**](https://arxiv.org/abs/2302.13971)\n--   [**PaLM**](https://arxiv.org/abs/2204.02311)\n+-   [**Llama**](https://huggingface.co/papers/2302.13971)\n+-   [**PaLM**](https://huggingface.co/papers/2204.02311)\n \n As an alternative, *ALiBi* proposes a much simpler relative position encoding scheme. The relative distance that input tokens have to each other is added as a negative integer scaled by a pre-defined value `m` to each query-key entry of the \\\\( \\mathbf{QK}^T \\\\) matrix right before the softmax computation.\n \n ![](/blog/assets/163_optimize_llm/alibi.png)\n \n-As shown in the [ALiBi](https://arxiv.org/abs/2108.12409) paper, this simple relative positional encoding allows the model to retain a high performance even at very long text input sequences.\n+As shown in the [ALiBi](https://huggingface.co/papers/2108.12409) paper, this simple relative positional encoding allows the model to retain a high performance even at very long text input sequences.\n \n *ALiBi* is used in multiple of today's most important LLMs, such as:\n \n@@ -572,7 +572,7 @@ As shown in the [ALiBi](https://arxiv.org/abs/2108.12409) paper, this simple rel\n \n Both *RoPE* and *ALiBi* position encodings can extrapolate to input lengths not seen during training whereas it has been shown that extrapolation works much better out-of-the-box for *ALiBi* as compared to *RoPE*.\n For ALiBi, one simply increases the values of the lower triangular position matrix to match the length of the input sequence.\n-For *RoPE*, keeping the same \\\\( \\theta \\\\) that was used during training leads to poor results when passing text inputs much longer than those seen during training, *c.f* [Press et al.](https://arxiv.org/abs/2108.12409). However, the community has found a couple of effective tricks that adapt \\\\( \\theta \\\\), thereby allowing *RoPE* position embeddings to work well for extrapolated text input sequences (see [here](https://github.com/huggingface/transformers/pull/24653)).\n+For *RoPE*, keeping the same \\\\( \\theta \\\\) that was used during training leads to poor results when passing text inputs much longer than those seen during training, *c.f* [Press et al.](https://huggingface.co/papers/2108.12409). However, the community has found a couple of effective tricks that adapt \\\\( \\theta \\\\), thereby allowing *RoPE* position embeddings to work well for extrapolated text input sequences (see [here](https://github.com/huggingface/transformers/pull/24653)).\n \n > Both RoPE and ALiBi are relative positional embeddings that are *not* learned during training, but instead are based on the following intuitions:\n  -   Positional cues about the text inputs should be given directly to the \\\\( QK^T \\\\) matrix of the self-attention layer\n@@ -742,29 +742,29 @@ Researchers have proposed two methods that allow to significantly reduce the mem\n \n #### 3.2.2 Multi-Query-Attention (MQA)\n \n-[Multi-Query-Attention](https://arxiv.org/abs/1911.02150) was proposed in Noam Shazeer's *Fast Transformer Decoding: One Write-Head is All You Need* paper. As the title says, Noam found out that instead of using `n_head` key-value projections weights, one can use a single head-value projection weight pair that is shared across all attention heads without that the model's performance significantly degrades.\n+[Multi-Query-Attention](https://huggingface.co/papers/1911.02150) was proposed in Noam Shazeer's *Fast Transformer Decoding: One Write-Head is All You Need* paper. As the title says, Noam found out that instead of using `n_head` key-value projections weights, one can use a single head-value projection weight pair that is shared across all attention heads without that the model's performance significantly degrades.\n \n > By using a single head-value projection weight pair, the key value vectors \\\\( \\mathbf{k}_i, \\mathbf{v}_i \\\\) have to be identical across all attention heads which in turn means that we only need to store 1 key-value projection pair in the cache instead of `n_head` ones.\n \n As most LLMs use between 20 and 100 attention heads, MQA significantly reduces the memory consumption of the key-value cache. For the LLM used in this notebook we could therefore reduce the required memory consumption from 15 GB to less than 400 MB at an input sequence length of 16000.\n \n In addition to memory savings, MQA also leads to improved computational efficiency as explained in the following.\n-In auto-regressive decoding, large key-value vectors need to be reloaded, concatenated with the current key-value vector pair to be then fed into the \\\\( \\mathbf{q}_c\\mathbf{K}^T \\\\) computation at every step. For auto-regressive decoding, the required memory bandwidth for the constant reloading can become a serious time bottleneck. By reducing the size of the key-value vectors less memory needs to be accessed, thus reducing the memory bandwidth bottleneck. For more detail, please have a look at [Noam's paper](https://arxiv.org/abs/1911.02150).\n+In auto-regressive decoding, large key-value vectors need to be reloaded, concatenated with the current key-value vector pair to be then fed into the \\\\( \\mathbf{q}_c\\mathbf{K}^T \\\\) computation at every step. For auto-regressive decoding, the required memory bandwidth for the constant reloading can become a serious time bottleneck. By reducing the size of the key-value vectors less memory needs to be accessed, thus reducing the memory bandwidth bottleneck. For more detail, please have a look at [Noam's paper](https://huggingface.co/papers/1911.02150).\n \n The important part to understand here is that reducing the number of key-value attention heads to 1 only makes sense if a key-value cache is used. The peak memory consumption of the model for a single forward pass without key-value cache stays unchanged as every attention head still has a unique query vector so that each attention head still has a different \\\\( \\mathbf{QK}^T \\\\) matrix.\n \n MQA has seen wide adoption by the community and is now used by many of the most popular LLMs:\n \n -   [**Falcon**](https://huggingface.co/tiiuae/falcon-40b)\n--   [**PaLM**](https://arxiv.org/abs/2204.02311)\n+-   [**PaLM**](https://huggingface.co/papers/2204.02311)\n -   [**MPT**](https://huggingface.co/mosaicml/mpt-30b)\n -   [**BLOOM**](https://huggingface.co/bigscience/bloom)\n \n Also, the checkpoint used in this notebook - `bigcode/octocoder` - makes use of MQA.\n \n #### 3.2.3 Grouped-Query-Attention (GQA)\n \n-[Grouped-Query-Attention](https://arxiv.org/abs/2305.13245), as proposed by Ainslie et al. from Google, found that using MQA can often lead to quality degradation compared to using vanilla multi-key-value head projections. The paper argues that more model performance can be kept by less drastically reducing the number of query head projection weights. Instead of using just a single key-value projection weight, `n < n_head` key-value projection weights should be used. By choosing `n` to a significantly smaller value than `n_head`, such as 2,4 or 8 almost all of the memory and speed gains from MQA can be kept while sacrificing less model capacity and thus arguably less performance.\n+[Grouped-Query-Attention](https://huggingface.co/papers/2305.13245), as proposed by Ainslie et al. from Google, found that using MQA can often lead to quality degradation compared to using vanilla multi-key-value head projections. The paper argues that more model performance can be kept by less drastically reducing the number of query head projection weights. Instead of using just a single key-value projection weight, `n < n_head` key-value projection weights should be used. By choosing `n` to a significantly smaller value than `n_head`, such as 2,4 or 8 almost all of the memory and speed gains from MQA can be kept while sacrificing less model capacity and thus arguably less performance.\n \n Moreover, the authors of GQA found out that existing model checkpoints can be *uptrained* to have a GQA architecture with as little as 5% of the original pre-training compute. While 5% of the original pre-training compute can still be a massive amount, GQA *uptraining* allows existing checkpoints to be useful for longer input sequences.\n \n@@ -776,7 +776,7 @@ The most notable application of GQA is [Llama-v2](https://huggingface.co/meta-ll\n \n ## Conclusion\n \n-The research community is constantly coming up with new, nifty ways to speed up inference time for ever-larger LLMs. As an example, one such promising research direction is [speculative decoding](https://arxiv.org/abs/2211.17192) where \"easy tokens\" are generated by smaller, faster language models and only \"hard tokens\" are generated by the LLM itself. Going into more detail is out of the scope of this notebook, but can be read upon in this [nice blog post](https://huggingface.co/blog/assisted-generation).\n+The research community is constantly coming up with new, nifty ways to speed up inference time for ever-larger LLMs. As an example, one such promising research direction is [speculative decoding](https://huggingface.co/papers/2211.17192) where \"easy tokens\" are generated by smaller, faster language models and only \"hard tokens\" are generated by the LLM itself. Going into more detail is out of the scope of this notebook, but can be read upon in this [nice blog post](https://huggingface.co/blog/assisted-generation).\n \n The reason massive LLMs such as GPT3/4, Llama-2-70b, Claude, PaLM can run so quickly in chat-interfaces such as [Hugging Face Chat](https://huggingface.co/chat/) or ChatGPT is to a big part thanks to the above-mentioned improvements in precision, algorithms, and architecture.\n Going forward, accelerators such as GPUs, TPUs, etc... will only get faster and allow for more memory, but one should nevertheless always make sure to use the best available algorithms and architectures to get the most bang for your buck ๐ค"
        },
        {
            "sha": "2c2e0cd31b72eeb2c17d6695474fedcbac2346a1",
            "filename": "docs/source/en/main_classes/processors.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmain_classes%2Fprocessors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmain_classes%2Fprocessors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fprocessors.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -78,7 +78,7 @@ Additionally, the following method can be used to load values from a data file a\n quality of cross-lingual text representations. XNLI is crowd-sourced dataset based on [*MultiNLI*](http://www.nyu.edu/projects/bowman/multinli/): pairs of text are labeled with textual entailment annotations for 15\n different languages (including both high-resource language such as English and low-resource languages such as Swahili).\n \n-It was released together with the paper [XNLI: Evaluating Cross-lingual Sentence Representations](https://arxiv.org/abs/1809.05053)\n+It was released together with the paper [XNLI: Evaluating Cross-lingual Sentence Representations](https://huggingface.co/papers/1809.05053)\n \n This library hosts the processor to load the XNLI data:\n \n@@ -93,8 +93,8 @@ An example using these processors is given in the [run_xnli.py](https://github.c\n \n [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer//) is a benchmark that\n evaluates the performance of models on question answering. Two versions are available, v1.1 and v2.0. The first version\n-(v1.1) was released together with the paper [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250). The second version (v2.0) was released alongside the paper [Know What You Don't\n-Know: Unanswerable Questions for SQuAD](https://arxiv.org/abs/1806.03822).\n+(v1.1) was released together with the paper [SQuAD: 100,000+ Questions for Machine Comprehension of Text](https://huggingface.co/papers/1606.05250). The second version (v2.0) was released alongside the paper [Know What You Don't\n+Know: Unanswerable Questions for SQuAD](https://huggingface.co/papers/1806.03822).\n \n This library hosts a processor for each of the two versions:\n "
        },
        {
            "sha": "d121e370da5e96b9358c00541c778582959b6a07",
            "filename": "docs/source/en/model_doc/albert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\n+The ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://huggingface.co/papers/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\n Radu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\n speed of BERT:\n "
        },
        {
            "sha": "4d04173df712a41a777a6f45f3808035375a90b2",
            "filename": "docs/source/en/model_doc/altclip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -21,7 +21,7 @@ rendered properly in your Markdown viewer.\n \n # AltCLIP\n \n-[AltCLIP](https://huggingface.co/papers/2211.06679v2) replaces the [CLIP](./clip) text encoder with a multilingual XLM-R encoder and aligns image and text representations with teacher learning and contrastive learning.\n+[AltCLIP](https://huggingface.co/papers/2211.06679) replaces the [CLIP](./clip) text encoder with a multilingual XLM-R encoder and aligns image and text representations with teacher learning and contrastive learning.\n \n You can find all the original AltCLIP checkpoints under the [AltClip](https://huggingface.co/collections/BAAI/alt-clip-diffusion-66987a97de8525205f1221bf) collection.\n "
        },
        {
            "sha": "46544de1f61b83eab50d793009ac8a8674e43846",
            "filename": "docs/source/en/model_doc/audio-spectrogram-transformer.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\n+The Audio Spectrogram Transformer model was proposed in [AST: Audio Spectrogram Transformer](https://huggingface.co/papers/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\n The Audio Spectrogram Transformer applies a [Vision Transformer](vit) to audio, by turning audio into an image (spectrogram). The model obtains state-of-the-art results\n for audio classification.\n \n@@ -35,7 +35,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/audio_spectogram_transformer_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Audio Spectrogram Transformer architecture. Taken from the <a href=\"https://arxiv.org/abs/2104.01778\">original paper</a>.</small>\n+<small> Audio Spectrogram Transformer architecture. Taken from the <a href=\"https://huggingface.co/papers/2104.01778\">original paper</a>.</small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/YuanGongND/ast).\n@@ -47,7 +47,7 @@ sure the input has mean of 0 and std of 0.5). [`ASTFeatureExtractor`] takes care\n mean and std by default. You can check [`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py) to see how\n the authors compute the stats for a downstream dataset.\n - Note that the AST needs a low learning rate (the authors use a 10 times smaller learning rate compared to their CNN model proposed in the\n-[PSLA paper](https://arxiv.org/abs/2102.01243)) and converges quickly, so please search for a suitable learning rate and learning rate scheduler for your task.\n+[PSLA paper](https://huggingface.co/papers/2102.01243)) and converges quickly, so please search for a suitable learning rate and learning rate scheduler for your task.\n \n ### Using Scaled Dot Product Attention (SDPA)\n "
        },
        {
            "sha": "0fd38901324faaf03210db50bfa5dd7bcdafa38c",
            "filename": "docs/source/en/model_doc/autoformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fautoformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fautoformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fautoformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Autoformer model was proposed in [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.\n+The Autoformer model was proposed in [Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://huggingface.co/papers/2106.13008) by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long.\n \n This model augments the Transformer as a deep decomposition architecture, which can progressively decompose the trend and seasonal components during the forecasting process.\n "
        },
        {
            "sha": "0f8568cc05ecfb347626a7541148a46c651baddd",
            "filename": "docs/source/en/model_doc/barthez.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbarthez.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The BARThez model was proposed in [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis on 23 Oct,\n+The BARThez model was proposed in [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://huggingface.co/papers/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis on 23 Oct,\n 2020.\n \n The abstract of the paper:"
        },
        {
            "sha": "78c26c06a5a1b91eb59f226dfae56b5e5adc9268",
            "filename": "docs/source/en/model_doc/bartpho.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbartpho.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The BARTpho model was proposed in [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n+The BARTpho model was proposed in [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://huggingface.co/papers/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "32a0c160a1c7e26e7c1bc7c904d6370db8a9573a",
            "filename": "docs/source/en/model_doc/beit.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,11 +25,11 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The BEiT model was proposed in [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by\n+The BEiT model was proposed in [BEiT: BERT Pre-Training of Image Transformers](https://huggingface.co/papers/2106.08254) by\n Hangbo Bao, Li Dong and Furu Wei. Inspired by BERT, BEiT is the first paper that makes self-supervised pre-training of\n Vision Transformers (ViTs) outperform supervised pre-training. Rather than pre-training the model to predict the class\n-of an image (as done in the [original ViT paper](https://arxiv.org/abs/2010.11929)), BEiT models are pre-trained to\n-predict visual tokens from the codebook of OpenAI's [DALL-E model](https://arxiv.org/abs/2102.12092) given masked\n+of an image (as done in the [original ViT paper](https://huggingface.co/papers/2010.11929)), BEiT models are pre-trained to\n+predict visual tokens from the codebook of OpenAI's [DALL-E model](https://huggingface.co/papers/2102.12092) given masked\n patches.\n \n The abstract from the paper is the following:\n@@ -76,7 +76,7 @@ contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/beit_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> BEiT pre-training. Taken from the <a href=\"https://arxiv.org/abs/2106.08254\">original paper.</a> </small>\n+<small> BEiT pre-training. Taken from the <a href=\"https://huggingface.co/papers/2106.08254\">original paper.</a> </small>\n \n ### Using Scaled Dot Product Attention (SDPA)\n "
        },
        {
            "sha": "a14966ce3ab992d7cb19bfde3a32d4dcf1400823",
            "filename": "docs/source/en/model_doc/bert-generation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n The BertGeneration model is a BERT model that can be leveraged for sequence-to-sequence tasks using\n [`EncoderDecoderModel`] as proposed in [Leveraging Pre-trained Checkpoints for Sequence Generation\n-Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+Tasks](https://huggingface.co/papers/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "16f99043c6b002d7cf8053d6edbf0bd75db9d8d1",
            "filename": "docs/source/en/model_doc/big_bird.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbig_bird.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -26,7 +26,6 @@ rendered properly in your Markdown viewer.\n \n [BigBird](https://huggingface.co/papers/2007.14062) is a transformer model built to handle sequence lengths up to 4096 compared to 512 for [BERT](./bert). Traditional transformers struggle with long inputs because attention gets really expensive as the sequence length grows. BigBird fixes this by using a sparse attention mechanism, which means it doesnโt try to look at everything at once. Instead, it mixes in local attention, random attention, and a few global tokens to process the whole input. This combination gives it the best of both worlds. It keeps the computation efficient while still capturing enough of the sequence to understand it well. Because of this, BigBird is great at tasks involving long documents, like question answering, summarization, and genomic applications.\n \n-\n You can find all the original BigBird checkpoints under the [Google](https://huggingface.co/google?search_models=bigbird) organization.\n \n > [!TIP]"
        },
        {
            "sha": "5f7d37eef4a75f61d84761f32fb773ed4d2b8363",
            "filename": "docs/source/en/model_doc/bigbird_pegasus.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbigbird_pegasus.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by\n+The BigBird model was proposed in [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) by\n Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon,\n Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others. BigBird, is a sparse-attention\n based transformer which extends Transformer based models, such as BERT to much longer sequences. In addition to sparse"
        },
        {
            "sha": "ea0c09b86237cf06da35afd22ada071fe7cb9247",
            "filename": "docs/source/en/model_doc/bit.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The BiT model was proposed in [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.\n+The BiT model was proposed in [Big Transfer (BiT): General Visual Representation Learning](https://huggingface.co/papers/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.\n BiT is a simple recipe for scaling up pre-training of [ResNet](resnet)-like architectures (specifically, ResNetv2). The method results in significant improvements for transfer learning.\n \n The abstract from the paper is the following:\n@@ -34,8 +34,8 @@ The original code can be found [here](https://github.com/google-research/big_tra\n \n ## Usage tips\n \n-- BiT models are equivalent to ResNetv2 in terms of architecture, except that: 1) all batch normalization layers are replaced by [group normalization](https://arxiv.org/abs/1803.08494),\n-2) [weight standardization](https://arxiv.org/abs/1903.10520) is used for convolutional layers. The authors show that the combination of both is useful for training with large batch sizes, and has a significant\n+- BiT models are equivalent to ResNetv2 in terms of architecture, except that: 1) all batch normalization layers are replaced by [group normalization](https://huggingface.co/papers/1803.08494),\n+2) [weight standardization](https://huggingface.co/papers/1903.10520) is used for convolutional layers. The authors show that the combination of both is useful for training with large batch sizes, and has a significant\n impact on transfer learning.\n \n ## Resources"
        },
        {
            "sha": "2bf2b8e7b26abfa5a50485968ed509efd774ce2f",
            "filename": "docs/source/en/model_doc/bitnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbitnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n \n Trained on a corpus of 4 trillion tokens, this model demonstrates that native 1-bit LLMs can achieve performance comparable to leading open-weight, full-precision models of similar size, while offering substantial advantages in computational efficiency (memory, energy, latency).\n \n-โก๏ธ **Technical Report:** [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)\n+โก๏ธ **Technical Report:** [BitNet b1.58 2B4T Technical Report](https://huggingface.co/papers/2504.12285)\n \n โก๏ธ **Official Inference Code:** [microsoft/BitNet (bitnet.cpp)](https://github.com/microsoft/BitNet)\n "
        },
        {
            "sha": "181fd013429556badc6effbc7b3b4e98244cbbba",
            "filename": "docs/source/en/model_doc/blenderbot-small.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot-small.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -33,7 +33,7 @@ instead be used with [`BlenderbotModel`] and\n \n ## Overview\n \n-The Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\n+The Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\n Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston on 30 Apr 2020.\n \n The abstract of the paper is the following:"
        },
        {
            "sha": "cea6c49c368926d82b2d2bed0deff87cc655d3af",
            "filename": "docs/source/en/model_doc/blenderbot.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblenderbot.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\n+The Blender chatbot model was proposed in [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu,\n Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston on 30 Apr 2020.\n \n The abstract of the paper is the following:\n@@ -67,7 +67,7 @@ An example:\n \n ## Implementation Notes\n \n-- Blenderbot uses a standard [seq2seq model transformer](https://arxiv.org/pdf/1706.03762.pdf) based architecture.\n+- Blenderbot uses a standard [seq2seq model transformer](https://huggingface.co/papers/1706.03762) based architecture.\n - Available checkpoints can be found in the [model hub](https://huggingface.co/models?search=blenderbot).\n - This is the *default* Blenderbot model class. However, some smaller checkpoints, such as\n   `facebook/blenderbot_small_90M`, have a different architecture and consequently should be used with"
        },
        {
            "sha": "fbfcda46134508354852bf04712e877523973c0d",
            "filename": "docs/source/en/model_doc/blip-2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,9 +22,9 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by\n+The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://huggingface.co/papers/2301.12597) by\n Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer\n-encoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://arxiv.org/abs/2204.14198), an 80 billion parameter model, by 8.7%\n+encoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://huggingface.co/papers/2204.14198), an 80 billion parameter model, by 8.7%\n on zero-shot VQAv2 with 54x fewer trainable parameters. \n \n The abstract from the paper is the following:\n@@ -34,7 +34,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/blip2_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/> \n \n-<small> BLIP-2 architecture. Taken from the <a href=\"https://arxiv.org/abs/2301.12597\">original paper.</a> </small>\n+<small> BLIP-2 architecture. Taken from the <a href=\"https://huggingface.co/papers/2301.12597\">original paper.</a> </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207)."
        },
        {
            "sha": "e97f7374fd1bf5eec5981578a81c59bac3c189da",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\n+The BLIP model was proposed in [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://huggingface.co/papers/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\n \n BLIP is a model that is able to perform various multi-modal tasks including:\n - Visual Question Answering "
        },
        {
            "sha": "5d5b923906492e4b7cb352289cce1d60c95033fa",
            "filename": "docs/source/en/model_doc/bort.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbort.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbort.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbort.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -34,7 +34,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The BORT model was proposed in [Optimal Subarchitecture Extraction for BERT](https://arxiv.org/abs/2010.10499) by\n+The BORT model was proposed in [Optimal Subarchitecture Extraction for BERT](https://huggingface.co/papers/2010.10499) by\n Adrian de Wynter and Daniel J. Perry. It is an optimal subset of architectural parameters for the BERT, which the\n authors refer to as \"Bort\".\n "
        },
        {
            "sha": "fe634535234504e8ea0364d9acf9d3324e0f6e6c",
            "filename": "docs/source/en/model_doc/bridgetower.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbridgetower.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The BridgeTower model was proposed in [BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan. The goal of this model is to build a\n+The BridgeTower model was proposed in [BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning](https://huggingface.co/papers/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan. The goal of this model is to build a\n bridge between each uni-modal encoder and the cross-modal encoder to enable comprehensive and detailed interaction at each layer of the cross-modal encoder thus achieving remarkable performance on various downstream tasks with almost negligible additional performance and computational costs.\n \n This paper has been accepted to the [AAAI'23](https://aaai.org/Conferences/AAAI-23/) conference. \n@@ -39,7 +39,7 @@ Notably, when further scaling the model, BRIDGETOWER achieves an accuracy of 81.\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/bridgetower_architecture%20.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> BridgeTower architecture. Taken from the <a href=\"https://arxiv.org/abs/2206.08657\">original paper.</a> </small>\n+<small> BridgeTower architecture. Taken from the <a href=\"https://huggingface.co/papers/2206.08657\">original paper.</a> </small>\n \n This model was contributed by [Anahita Bhiwandiwalla](https://huggingface.co/anahita-b), [Tiep Le](https://huggingface.co/Tile) and [Shaoyen Tseng](https://huggingface.co/shaoyent). The original code can be found [here](https://github.com/microsoft/BridgeTower).\n \n@@ -126,7 +126,7 @@ Tips:\n \n - This implementation of BridgeTower uses [`RobertaTokenizer`] to generate text embeddings and OpenAI's CLIP/ViT model to compute visual embeddings.\n - Checkpoints for pre-trained [bridgeTower-base](https://huggingface.co/BridgeTower/bridgetower-base) and [bridgetower masked language modeling and image text matching](https://huggingface.co/BridgeTower/bridgetower-base-itm-mlm) are released.\n-- Please refer to [Table 5](https://arxiv.org/pdf/2206.08657.pdf) for BridgeTower's performance on Image Retrieval and other down stream tasks.\n+- Please refer to [Table 5](https://huggingface.co/papers/2206.08657) for BridgeTower's performance on Image Retrieval and other down stream tasks.\n - The PyTorch version of this model is only available in torch 1.10 and higher.\n \n "
        },
        {
            "sha": "67b4bffd258b429cbd14db17f74ca65a0c1f8060",
            "filename": "docs/source/en/model_doc/bros.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbros.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The BROS model was proposed in [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.\n+The BROS model was proposed in [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://huggingface.co/papers/2108.04539) by Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park.\n \n BROS stands for *BERT Relying On Spatiality*. It is an encoder-only Transformer model that takes a sequence of tokens and their bounding boxes as inputs and outputs a sequence of hidden states. BROS encode relative spatial information instead of using absolute spatial information.\n "
        },
        {
            "sha": "aad9662de9d5782c03d960d49978663601ebf1a8",
            "filename": "docs/source/en/model_doc/camembert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcamembert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The CamemBERT model was proposed in [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by\n+The CamemBERT model was proposed in [CamemBERT: a Tasty French Language Model](https://huggingface.co/papers/1911.03894) by\n [Louis Martin](https://huggingface.co/louismartin), [Benjamin Muller](https://huggingface.co/benjamin-mlr), [Pedro Javier Ortiz Suรกrez](https://huggingface.co/pjox), Yoann Dupont, Laurent Romary, รric Villemonte de la\n Clergerie, [Djamรฉ Seddah](https://huggingface.co/Djame), and [Benoรฎt Sagot](https://huggingface.co/sagot). It is based on Facebook's RoBERTa model released in 2019. It is a model\n trained on 138GB of French text."
        },
        {
            "sha": "e7c04811de602b24dd59c7749ccab62d32520904",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The Chameleon model was proposed in [Chameleon: Mixed-Modal Early-Fusion Foundation Models\n-](https://arxiv.org/abs/2405.09818v1) by META AI Chameleon Team. Chameleon is a Vision-Language Model that use vector quantization to tokenize images which enables the model to generate multimodal output. The model takes images and texts as input, including an interleaved format, and generates textual response. Image generation module is not released yet.\n+](https://huggingface.co/papers/2405.09818) by META AI Chameleon Team. Chameleon is a Vision-Language Model that use vector quantization to tokenize images which enables the model to generate multimodal output. The model takes images and texts as input, including an interleaved format, and generates textual response. Image generation module is not released yet.\n \n \n The abstract from the paper is the following:\n@@ -46,7 +46,7 @@ text. Chameleon marks a significant step forward in unified modeling of full mul\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/chameleon_arch.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Chameleon incorporates a vector quantizer module to transform images into discrete tokens. That also enables image generation using an auto-regressive transformer. Taken from the <a href=\"https://arxiv.org/abs/2405.09818v1\">original paper.</a> </small>\n+<small> Chameleon incorporates a vector quantizer module to transform images into discrete tokens. That also enables image generation using an auto-regressive transformer. Taken from the <a href=\"https://huggingface.co/papers/2405.09818\">original paper.</a> </small>\n \n This model was contributed by [joaogante](https://huggingface.co/joaogante) and [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n The original code can be found [here](https://github.com/facebookresearch/chameleon)."
        },
        {
            "sha": "2607c56e5ec89d8a2d3e83c8f13ef134779f36f9",
            "filename": "docs/source/en/model_doc/chinese_clip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Chinese-CLIP model was proposed in [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.\n+The Chinese-CLIP model was proposed in [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://huggingface.co/papers/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.\n Chinese-CLIP is an implementation of CLIP (Radford et al., 2021) on a large-scale dataset of Chinese image-text pairs. It is capable of performing cross-modal retrieval and also playing as a vision backbone for vision tasks like zero-shot image classification, open-domain object detection, etc. The original Chinese-CLIP code is released [at this link](https://github.com/OFA-Sys/Chinese-CLIP).\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "c6684579d7d8c0d043c34bc6daad05123f43d7b1",
            "filename": "docs/source/en/model_doc/clap.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The CLAP model was proposed in [Large Scale Contrastive Language-Audio pretraining with\n-feature fusion and keyword-to-caption augmentation](https://arxiv.org/pdf/2211.06687.pdf) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.\n+feature fusion and keyword-to-caption augmentation](https://huggingface.co/papers/2211.06687) by Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, Shlomo Dubnov.\n \n CLAP (Contrastive Language-Audio Pretraining) is a neural network trained on a variety of (audio, text) pairs. It can be instructed in to predict the most relevant text snippet, given an audio, without directly optimizing for the task. The CLAP model uses a SWINTransformer to get audio features from a log-Mel spectrogram input, and a RoBERTa model to get text features. Both the text and audio features are then projected to a latent space with identical dimension. The dot product between the projected audio and text features is then used as a similar score.\n "
        },
        {
            "sha": "afc357b2ca20c34b75ce388beae1b1c226f2467d",
            "filename": "docs/source/en/model_doc/clipseg.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lรผddecke\n+The CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://huggingface.co/papers/2112.10003) by Timo Lรผddecke\n and Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero-shot and one-shot image segmentation.\n \n The abstract from the paper is the following:\n@@ -48,7 +48,7 @@ to generalized queries involving affordances or properties*\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\n alt=\"drawing\" width=\"600\"/> \n \n-<small> CLIPSeg overview. Taken from the <a href=\"https://arxiv.org/abs/2112.10003\">original paper.</a> </small>\n+<small> CLIPSeg overview. Taken from the <a href=\"https://huggingface.co/papers/2112.10003\">original paper.</a> </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/timojl/clipseg)."
        },
        {
            "sha": "7d3f18b34d528b91e091bc7f1645bdfdf9bae65e",
            "filename": "docs/source/en/model_doc/clvp.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclvp.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The CLVP (Contrastive Language-Voice Pretrained Transformer) model was proposed in [Better speech synthesis through scaling](https://arxiv.org/abs/2305.07243) by James Betker.\n+The CLVP (Contrastive Language-Voice Pretrained Transformer) model was proposed in [Better speech synthesis through scaling](https://huggingface.co/papers/2305.07243) by James Betker.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "73890f13d6cd2ea19b56d583259daff1cc5d65a8",
            "filename": "docs/source/en/model_doc/codegen.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcodegen.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The CodeGen model was proposed in [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\n+The CodeGen model was proposed in [A Conversational Paradigm for Program Synthesis](https://huggingface.co/papers/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\n \n CodeGen is an autoregressive language model for program synthesis trained sequentially on [The Pile](https://pile.eleuther.ai/), BigQuery, and BigPython.\n "
        },
        {
            "sha": "8a1a4de6ce770c0d3ec94757caad5112171b2e88",
            "filename": "docs/source/en/model_doc/colqwen2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n # ColQwen2\n \n-[ColQwen2](https://doi.org/10.48550/arXiv.2407.01449) is a variant of the [ColPali](./colpali) model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColQwen2 treats each page as an image. It uses the [Qwen2-VL](./qwen2_vl) backbone to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\n+[ColQwen2](https://huggingface.co/papers/2407.01449) is a variant of the [ColPali](./colpali) model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColQwen2 treats each page as an image. It uses the [Qwen2-VL](./qwen2_vl) backbone to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed multi-vector embeddings that can be used for retrieval by computing pairwise late interaction similarity scores. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\n \n This model was contributed by [@tonywu71](https://huggingface.co/tonywu71) (ILLUIN Technology) and [@yonigozlan](https://huggingface.co/yonigozlan) (HuggingFace).\n "
        },
        {
            "sha": "68eda90e70024b0d997eb97ac12f41c319eb56c2",
            "filename": "docs/source/en/model_doc/conditional_detr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fconditional_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fconditional_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconditional_detr.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Conditional DETR model was proposed in [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang. Conditional DETR presents a conditional cross-attention mechanism for fast DETR training. Conditional DETR converges 6.7ร to 10ร faster than DETR.\n+The Conditional DETR model was proposed in [Conditional DETR for Fast Training Convergence](https://huggingface.co/papers/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang. Conditional DETR presents a conditional cross-attention mechanism for fast DETR training. Conditional DETR converges 6.7ร to 10ร faster than DETR.\n \n The abstract from the paper is the following:\n \n@@ -31,7 +31,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/conditional_detr_curve.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Conditional DETR shows much faster convergence compared to the original DETR. Taken from the <a href=\"https://arxiv.org/abs/2108.06152\">original paper</a>.</small>\n+<small> Conditional DETR shows much faster convergence compared to the original DETR. Taken from the <a href=\"https://huggingface.co/papers/2108.06152\">original paper</a>.</small>\n \n This model was contributed by [DepuMeng](https://huggingface.co/DepuMeng). The original code can be found [here](https://github.com/Atten4Vis/ConditionalDETR).\n "
        },
        {
            "sha": "62d9d1168806d9e8b5208eba125305baa8af7c8e",
            "filename": "docs/source/en/model_doc/convbert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvbert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The ConvBERT model was proposed in [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng\n+The ConvBERT model was proposed in [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://huggingface.co/papers/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng\n Yan.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "5a65c9f6cc9c6c1434ee77f487bbcf44c6f9561c",
            "filename": "docs/source/en/model_doc/convnext.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnext.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnext.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnext.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The ConvNeXT model was proposed in [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n+The ConvNeXT model was proposed in [A ConvNet for the 2020s](https://huggingface.co/papers/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them.\n \n The abstract from the paper is the following:\n@@ -40,7 +40,7 @@ and outperforming Swin Transformers on COCO detection and ADE20K segmentation, w\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> ConvNeXT architecture. Taken from the <a href=\"https://arxiv.org/abs/2201.03545\">original paper</a>.</small>\n+<small> ConvNeXT architecture. Taken from the <a href=\"https://huggingface.co/papers/2201.03545\">original paper</a>.</small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). TensorFlow version of the model was contributed by [ariG23498](https://github.com/ariG23498),\n [gante](https://github.com/gante), and [sayakpaul](https://github.com/sayakpaul) (equal contribution). The original code can be found [here](https://github.com/facebookresearch/ConvNeXt)."
        },
        {
            "sha": "4779c511fe018594249fdb1a1a72c80903254341",
            "filename": "docs/source/en/model_doc/convnextv2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnextv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnextv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconvnextv2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The ConvNeXt V2 model was proposed in [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n+The ConvNeXt V2 model was proposed in [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://huggingface.co/papers/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n ConvNeXt V2 is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, and a successor of [ConvNeXT](convnext).\n \n The abstract from the paper is the following:\n@@ -33,7 +33,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnextv2_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> ConvNeXt V2 architecture. Taken from the <a href=\"https://arxiv.org/abs/2301.00808\">original paper</a>.</small>\n+<small> ConvNeXt V2 architecture. Taken from the <a href=\"https://huggingface.co/papers/2301.00808\">original paper</a>.</small>\n \n This model was contributed by [adirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/facebookresearch/ConvNeXt-V2).\n "
        },
        {
            "sha": "e639622087599785bf6b30373e8a65fbe9feda0f",
            "filename": "docs/source/en/model_doc/cpm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcpm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The CPM model was proposed in [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin,\n+The CPM model was proposed in [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://huggingface.co/papers/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin,\n Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen,\n Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n "
        },
        {
            "sha": "4b5fee2b0a7183349462f2e044153373c7ca6672",
            "filename": "docs/source/en/model_doc/ctrl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fctrl.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-CTRL model was proposed in [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and\n+CTRL model was proposed in [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://huggingface.co/papers/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and\n Richard Socher. It's a causal (unidirectional) transformer pre-trained using language modeling on a very large corpus\n of ~140 GB of text data with the first token reserved as a control code (such as Links, Books, Wikipedia etc.).\n "
        },
        {
            "sha": "d92dea065e08b79575dcb085249326e5e73a39b3",
            "filename": "docs/source/en/model_doc/cvt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcvt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The CvT model was proposed in [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan and Lei Zhang. The Convolutional vision Transformer (CvT) improves the [Vision Transformer (ViT)](vit) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs.\n+The CvT model was proposed in [CvT: Introducing Convolutions to Vision Transformers](https://huggingface.co/papers/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan and Lei Zhang. The Convolutional vision Transformer (CvT) improves the [Vision Transformer (ViT)](vit) in performance and efficiency by introducing convolutions into ViT to yield the best of both designs.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "b0ed57650841621ee09bfaf04809f28191558bab",
            "filename": "docs/source/en/model_doc/d_fine.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fd_fine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fd_fine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fd_fine.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The D-FINE model was proposed in [D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement](https://arxiv.org/abs/2410.13842) by\n+The D-FINE model was proposed in [D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement](https://huggingface.co/papers/2410.13842) by\n Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, Feng Wu\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "0f9e8dc3f30471316c41dc50ab3cef8bd04baf8d",
            "filename": "docs/source/en/model_doc/dab-detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The DAB-DETR model was proposed in [DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR](https://arxiv.org/abs/2201.12329) by Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang.\n+The DAB-DETR model was proposed in [DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR](https://huggingface.co/papers/2201.12329) by Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, Lei Zhang.\n DAB-DETR is an enhanced variant of Conditional DETR. It utilizes dynamically updated anchor boxes to provide both a reference query point (x, y) and a reference anchor size (w, h), improving cross-attention computation. This new approach achieves 45.7% AP when trained for 50 epochs with a single ResNet-50 model as the backbone.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dab_detr_convergence_plot.png\""
        },
        {
            "sha": "e8408db5024404204b57cd3e4295e0d7dc1ae5e4",
            "filename": "docs/source/en/model_doc/dac.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdac.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdac.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdac.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n \n-The DAC model was proposed in [Descript Audio Codec: High-Fidelity Audio Compression with Improved RVQGAN](https://arxiv.org/abs/2306.06546) by Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, Kundan Kumar.\n+The DAC model was proposed in [Descript Audio Codec: High-Fidelity Audio Compression with Improved RVQGAN](https://huggingface.co/papers/2306.06546) by Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, Kundan Kumar.\n \n The Descript Audio Codec (DAC) model is a powerful tool for compressing audio data, making it highly efficient for storage and transmission. By compressing 44.1 KHz audio into tokens at just 8kbps bandwidth, the DAC model enables high-quality audio processing while significantly reducing the data footprint. This is particularly useful in scenarios where bandwidth is limited or storage space is at a premium, such as in streaming applications, remote conferencing, and archiving large audio datasets.\n "
        },
        {
            "sha": "f2df85e24430c597291201ac674765ad226b288d",
            "filename": "docs/source/en/model_doc/data2vec.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Data2Vec model was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.\n+The Data2Vec model was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language](https://huggingface.co/papers/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and Michael Auli.\n Data2Vec proposes a unified framework for self-supervised learning across different data modalities - text, audio and images.\n Importantly, predicted targets for pre-training are contextualized latent representations of the inputs, rather than modality-specific, context-independent targets.\n "
        },
        {
            "sha": "3c5dd4d5ae3d85c59efe8313a3c405742880e80e",
            "filename": "docs/source/en/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta-v2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\n+The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\n BERT model released in 2018 and Facebook's RoBERTa model released in 2019.\n \n It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in"
        },
        {
            "sha": "c5aa1d6a2ef756d0b0007f244db38cc63deebe56",
            "filename": "docs/source/en/model_doc/deberta.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeberta.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\n+The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen It is based on Google's\n BERT model released in 2018 and Facebook's RoBERTa model released in 2019.\n \n It builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in"
        },
        {
            "sha": "6f820336b5de486a30080cf18b037fac6255e871",
            "filename": "docs/source/en/model_doc/decision_transformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdecision_transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdecision_transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdecision_transformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Decision Transformer model was proposed in [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345)  \n+The Decision Transformer model was proposed in [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://huggingface.co/papers/2106.01345)  \n by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "9da98b5785f784e12757625b99f445712e01ce00",
            "filename": "docs/source/en/model_doc/deepseek_v3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The DeepSeek-V3 model was proposed in [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) by DeepSeek-AI Team.\n+The DeepSeek-V3 model was proposed in [DeepSeek-V3 Technical Report](https://huggingface.co/papers/2412.19437) by DeepSeek-AI Team.\n \n The abstract from the paper is the following:\n We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3."
        },
        {
            "sha": "a260bbdb8e527d3256540865864b9359cff44ecb",
            "filename": "docs/source/en/model_doc/deformable_detr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeformable_detr.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Deformable DETR model was proposed in [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.\n+The Deformable DETR model was proposed in [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://huggingface.co/papers/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.\n Deformable DETR mitigates the slow convergence issues and limited feature spatial resolution of the original [DETR](detr) by leveraging a new deformable attention module which only attends to a small set of key sampling points around a reference.\n \n The abstract from the paper is the following:\n@@ -32,7 +32,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/deformable_detr_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Deformable DETR architecture. Taken from the <a href=\"https://arxiv.org/abs/2010.04159\">original paper</a>.</small>\n+<small> Deformable DETR architecture. Taken from the <a href=\"https://huggingface.co/papers/2010.04159\">original paper</a>.</small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/fundamentalvision/Deformable-DETR).\n "
        },
        {
            "sha": "c2f0f17c06675377213e0ab7293c357d09ae530d",
            "filename": "docs/source/en/model_doc/deit.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,8 +25,8 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The DeiT model was proposed in [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\n-Sablayrolles, Hervรฉ Jรฉgou. The [Vision Transformer (ViT)](vit) introduced in [Dosovitskiy et al., 2020](https://arxiv.org/abs/2010.11929) has shown that one can match or even outperform existing convolutional neural\n+The DeiT model was proposed in [Training data-efficient image transformers & distillation through attention](https://huggingface.co/papers/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\n+Sablayrolles, Hervรฉ Jรฉgou. The [Vision Transformer (ViT)](vit) introduced in [Dosovitskiy et al., 2020](https://huggingface.co/papers/2010.11929) has shown that one can match or even outperform existing convolutional neural\n networks using a Transformer encoder (BERT-like). However, the ViT models introduced in that paper required training on\n expensive infrastructure for multiple weeks, using external data. DeiT (data-efficient image transformers) are more\n efficiently trained transformers for image classification, requiring far less data and far less computing resources"
        },
        {
            "sha": "28a5c709409a4f21b6842b7f86aa87418fa3da14",
            "filename": "docs/source/en/model_doc/deplot.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeplot.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview \n \n-DePlot was proposed in the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://arxiv.org/abs/2212.10505) from Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.\n+DePlot was proposed in the paper [DePlot: One-shot visual language reasoning by plot-to-table translation](https://huggingface.co/papers/2212.10505) from Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun.\n \n The abstract of the paper states the following:\n "
        },
        {
            "sha": "413273b05d14a25e742cdc1a439fb11ad3ac21dd",
            "filename": "docs/source/en/model_doc/depth_anything_v2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_anything_v2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-Depth Anything V2 was introduced in [the paper of the same name](https://arxiv.org/abs/2406.09414) by Lihe Yang et al. It uses the same architecture as the original [Depth Anything model](depth_anything), but uses synthetic data and a larger capacity teacher model to achieve much finer and robust depth predictions.\n+Depth Anything V2 was introduced in [the paper of the same name](https://huggingface.co/papers/2406.09414) by Lihe Yang et al. It uses the same architecture as the original [Depth Anything model](depth_anything), but uses synthetic data and a larger capacity teacher model to achieve much finer and robust depth predictions.\n \n The abstract from the paper is the following:\n \n@@ -27,7 +27,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_anything_overview.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Depth Anything overview. Taken from the <a href=\"https://arxiv.org/abs/2401.10891\">original paper</a>.</small>\n+<small> Depth Anything overview. Taken from the <a href=\"https://huggingface.co/papers/2401.10891\">original paper</a>.</small>\n \n The Depth Anything models were contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/DepthAnything/Depth-Anything-V2)."
        },
        {
            "sha": "84f350a2a0883933ec760ae41f97251c9a8e819b",
            "filename": "docs/source/en/model_doc/depth_pro.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The DepthPro model was proposed in [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/abs/2410.02073) by Aleksei Bochkovskii, Amaรซl Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun.\n+The DepthPro model was proposed in [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://huggingface.co/papers/2410.02073) by Aleksei Bochkovskii, Amaรซl Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun.\n \n DepthPro is a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. It employs a multi-scale Vision Transformer (ViT)-based architecture, where images are downsampled, divided into patches, and processed using a shared Dinov2 encoder. The extracted patch-level features are merged, upsampled, and refined using a DPT-like fusion stage, enabling precise depth estimation.\n \n@@ -78,7 +78,7 @@ The DepthPro model processes an input image by first downsampling it at multiple\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_pro_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> DepthPro architecture. Taken from the <a href=\"https://arxiv.org/abs/2410.02073\" target=\"_blank\">original paper</a>. </small>\n+<small> DepthPro architecture. Taken from the <a href=\"https://huggingface.co/papers/2410.02073\" target=\"_blank\">original paper</a>. </small>\n \n The `DepthProForDepthEstimation` model uses a `DepthProEncoder`, for encoding the input image and a `FeatureFusionStage` for fusing the output features from encoder.\n \n@@ -151,7 +151,7 @@ On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32`\n \n A list of official Hugging Face and community (indicated by ๐) resources to help you get started with DepthPro:\n \n-- Research Paper: [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/pdf/2410.02073)\n+- Research Paper: [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://huggingface.co/papers/2410.02073)\n - Official Implementation: [apple/ml-depth-pro](https://github.com/apple/ml-depth-pro)\n - DepthPro Inference Notebook: [DepthPro Inference](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/DepthPro_inference.ipynb)\n - DepthPro for Super Resolution and Image Segmentation"
        },
        {
            "sha": "c151734f92ac33a56177e3f8ba3812f07df1465e",
            "filename": "docs/source/en/model_doc/deta.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeta.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -30,7 +30,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The DETA model was proposed in [NMS Strikes Back](https://arxiv.org/abs/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krรคhenbรผhl.\n+The DETA model was proposed in [NMS Strikes Back](https://huggingface.co/papers/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krรคhenbรผhl.\n DETA (short for Detection Transformers with Assignment) improves [Deformable DETR](deformable_detr) by replacing the one-to-one bipartite Hungarian matching loss\n with one-to-many label assignments used in traditional detectors with non-maximum suppression (NMS). This leads to significant gains of up to 2.5 mAP.\n \n@@ -41,7 +41,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/deta_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> DETA overview. Taken from the <a href=\"https://arxiv.org/abs/2212.06137\">original paper</a>. </small>\n+<small> DETA overview. Taken from the <a href=\"https://huggingface.co/papers/2212.06137\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/jozhang97/DETA)."
        },
        {
            "sha": "54094f94dfa69df35a043680a5b141d381e7d6b6",
            "filename": "docs/source/en/model_doc/detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The DETR model was proposed in [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by\n+The DETR model was proposed in [End-to-End Object Detection with Transformers](https://huggingface.co/papers/2005.12872) by\n Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov and Sergey Zagoruyko. DETR\n consists of a convolutional backbone followed by an encoder-decoder Transformer which can be trained end-to-end for\n object detection. It greatly simplifies a lot of the complexity of models like Faster-R-CNN and Mask-R-CNN, which use"
        },
        {
            "sha": "946c61b3053e0b01a4fe02ef94c18b839fcbdff6",
            "filename": "docs/source/en/model_doc/dialogpt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdialogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdialogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdialogpt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-DialoGPT was proposed in [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,\n+DialoGPT was proposed in [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://huggingface.co/papers/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,\n Jianfeng Gao, Jingjing Liu, Bill Dolan. It's a GPT2 Model trained on 147M conversation-like exchanges extracted from\n Reddit.\n "
        },
        {
            "sha": "83ea51ac12377deb3acee850eea6b21e9fd83bf7",
            "filename": "docs/source/en/model_doc/diffllama.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdiffllama.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The DiffLlama model was proposed in [Differential Transformer](https://arxiv.org/abs/2410.05258) by Kazuma Matsumoto and .\n+The DiffLlama model was proposed in [Differential Transformer](https://huggingface.co/papers/2410.05258) by Kazuma Matsumoto and .\n This model is combine Llama model and Differential Transformer's Attention.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "aab1c6388f43c2d92842f5fe2c8770a7f1db3fb0",
            "filename": "docs/source/en/model_doc/dinat.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinat.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-DiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001)\n+DiNAT was proposed in [Dilated Neighborhood Attention Transformer](https://huggingface.co/papers/2209.15001)\n by Ali Hassani and Humphrey Shi.\n \n It extends [NAT](nat) by adding a Dilated Neighborhood Attention pattern to capture global context,\n@@ -53,7 +53,7 @@ src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/ma\n alt=\"drawing\" width=\"600\"/>\n \n <small> Neighborhood Attention with different dilation values.\n-Taken from the <a href=\"https://arxiv.org/abs/2209.15001\">original paper</a>.</small>\n+Taken from the <a href=\"https://huggingface.co/papers/2209.15001\">original paper</a>.</small>\n \n This model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr).\n The original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)."
        },
        {
            "sha": "8bca569bc90e32643716b09de7d5bc29ec7456d5",
            "filename": "docs/source/en/model_doc/dinov2_with_registers.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -17,7 +17,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The DINOv2 with Registers model was proposed in [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588) by Timothรฉe Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski.\n+The DINOv2 with Registers model was proposed in [Vision Transformers Need Registers](https://huggingface.co/papers/2309.16588) by Timothรฉe Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski.\n \n The [Vision Transformer](vit) (ViT) is a transformer encoder model (BERT-like) originally introduced to do supervised image classification on ImageNet.\n \n@@ -35,7 +35,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dinov2_with_registers_visualization.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Visualization of attention maps of various models trained with vs. without registers. Taken from the <a href=\"https://arxiv.org/abs/2309.16588\">original paper</a>. </small>\n+<small> Visualization of attention maps of various models trained with vs. without registers. Taken from the <a href=\"https://huggingface.co/papers/2309.16588\">original paper</a>. </small>\n \n Tips:\n "
        },
        {
            "sha": "4b3d3f4a26a4daefa55af607853b4cb931987429",
            "filename": "docs/source/en/model_doc/dpr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpr.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was\n-introduced in [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by\n+introduced in [Dense Passage Retrieval for Open-Domain Question Answering](https://huggingface.co/papers/2004.04906) by\n Vladimir Karpukhin, Barlas Oฤuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "16992079738d9071aa7f088f7c45699607c6cd88",
            "filename": "docs/source/en/model_doc/dpt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The DPT model was proposed in [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n+The DPT model was proposed in [Vision Transformers for Dense Prediction](https://huggingface.co/papers/2103.13413) by Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n DPT is a model that leverages the [Vision Transformer (ViT)](vit) as backbone for dense prediction tasks like semantic segmentation and depth estimation.\n \n The abstract from the paper is the following:\n@@ -34,7 +34,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dpt_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> DPT architecture. Taken from the <a href=\"https://arxiv.org/abs/2103.13413\" target=\"_blank\">original paper</a>. </small>\n+<small> DPT architecture. Taken from the <a href=\"https://huggingface.co/papers/2103.13413\" target=\"_blank\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/isl-org/DPT).\n "
        },
        {
            "sha": "31b1d37f0f9bf47e26b17bfa2f29356db8b6de14",
            "filename": "docs/source/en/model_doc/efficientformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -31,7 +31,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The EfficientFormer model was proposed in [EfficientFormer: Vision Transformers at MobileNet Speed](https://arxiv.org/abs/2206.01191)\n+The EfficientFormer model was proposed in [EfficientFormer: Vision Transformers at MobileNet Speed](https://huggingface.co/papers/2206.01191)\n by Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.  EfficientFormer proposes a\n dimension-consistent pure transformer that can be run on mobile devices for dense prediction tasks like image classification, object\n detection and semantic segmentation."
        },
        {
            "sha": "e11eab612cc64120f6123e8d05e593f7bc1ee257",
            "filename": "docs/source/en/model_doc/efficientnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The EfficientNet model was proposed in [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946) \n+The EfficientNet model was proposed in [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://huggingface.co/papers/1905.11946) \n by Mingxing Tan and Quoc V. Le. EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet being an order-of-magnitude smaller and faster than previous models.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "5f515660843945309fa2af8129d1d994dd0295df",
            "filename": "docs/source/en/model_doc/emu3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Emu3 model was proposed in [Emu3: Next-Token Prediction is All You Need](https://arxiv.org/abs/2409.18869) by Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang.\n+The Emu3 model was proposed in [Emu3: Next-Token Prediction is All You Need](https://huggingface.co/papers/2409.18869) by Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, Zhongyuan Wang.\n \n Emu3 is a multimodal LLM that uses vector quantization to tokenize images into discrete tokens. Discretized image tokens are later fused with text token ids for image and text generation. The model can additionally generate images by predicting image token ids. \n "
        },
        {
            "sha": "06ce1e2fafc8548b96aeb8753c84cbcc95fdc313",
            "filename": "docs/source/en/model_doc/encodec.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fencodec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fencodec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fencodec.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The EnCodec neural codec model was proposed in [High Fidelity Neural Audio Compression](https://arxiv.org/abs/2210.13438) by Alexandre Dรฉfossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\n+The EnCodec neural codec model was proposed in [High Fidelity Neural Audio Compression](https://huggingface.co/papers/2210.13438) by Alexandre Dรฉfossez, Jade Copet, Gabriel Synnaeve, Yossi Adi.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "f697c213b78bb9aca5320e6d87e17738e5df5251",
            "filename": "docs/source/en/model_doc/encoder-decoder.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fencoder-decoder.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -30,14 +30,14 @@ The [`EncoderDecoderModel`] can be used to initialize a sequence-to-sequence mod\n pretrained autoencoding model as the encoder and any pretrained autoregressive model as the decoder.\n \n The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation tasks\n-was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by\n+was shown in [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) by\n Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n \n After such an [`EncoderDecoderModel`] has been trained/fine-tuned, it can be saved/loaded just like\n any other models (see the examples for more information).\n \n An application of this architecture could be to leverage two pretrained [`BertModel`] as the encoder\n-and decoder for a summarization model as was shown in: [Text Summarization with Pretrained Encoders](https://arxiv.org/abs/1908.08345) by Yang Liu and Mirella Lapata.\n+and decoder for a summarization model as was shown in: [Text Summarization with Pretrained Encoders](https://huggingface.co/papers/1908.08345) by Yang Liu and Mirella Lapata.\n \n ## Randomly initializing `EncoderDecoderModel` from model configurations.\n "
        },
        {
            "sha": "596a7b1f4b38adcdfc4b560ed75a764b550eea91",
            "filename": "docs/source/en/model_doc/ernie.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,8 +22,8 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n ERNIE is a series of powerful models proposed by baidu, especially in Chinese tasks,\n-including [ERNIE1.0](https://arxiv.org/abs/1904.09223), [ERNIE2.0](https://ojs.aaai.org/index.php/AAAI/article/view/6428),\n-[ERNIE3.0](https://arxiv.org/abs/2107.02137), [ERNIE-Gram](https://arxiv.org/abs/2010.12148), [ERNIE-health](https://arxiv.org/abs/2110.07244), etc.\n+including [ERNIE1.0](https://huggingface.co/papers/1904.09223), [ERNIE2.0](https://ojs.aaai.org/index.php/AAAI/article/view/6428),\n+[ERNIE3.0](https://huggingface.co/papers/2107.02137), [ERNIE-Gram](https://huggingface.co/papers/2010.12148), [ERNIE-health](https://huggingface.co/papers/2110.07244), etc.\n \n These models are contributed by [nghuyong](https://huggingface.co/nghuyong) and the official code can be found in [PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP) (in PaddlePaddle).\n "
        },
        {
            "sha": "292fce2ac335af48ff176f205b394ec571897288",
            "filename": "docs/source/en/model_doc/ernie_m.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie_m.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -31,7 +31,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n ## Overview\n \n The ErnieM model was proposed in [ERNIE-M: Enhanced Multilingual Representation by Aligning\n-Cross-lingual Semantics with Monolingual Corpora](https://arxiv.org/abs/2012.15674)  by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\n+Cross-lingual Semantics with Monolingual Corpora](https://huggingface.co/papers/2012.15674)  by Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun,\n Hao Tian, Hua Wu, Haifeng Wang.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "f6abf6125f37e6c445946eaf0ee9924eff6334fe",
            "filename": "docs/source/en/model_doc/fastspeech2_conformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffastspeech2_conformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The FastSpeech2Conformer model was proposed with the paper [Recent Developments On Espnet Toolkit Boosted By Conformer](https://arxiv.org/abs/2010.13956) by Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun Wei, Wangyou Zhang, and Yuekai Zhang.\n+The FastSpeech2Conformer model was proposed with the paper [Recent Developments On Espnet Toolkit Boosted By Conformer](https://huggingface.co/papers/2010.13956) by Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun Wei, Wangyou Zhang, and Yuekai Zhang.\n \n The abstract from the original FastSpeech2 paper is the following:\n "
        },
        {
            "sha": "8f6f41389442325f9bfb833011f2c8cbb0e646d8",
            "filename": "docs/source/en/model_doc/flan-t5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-t5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-t5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflan-t5.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-FLAN-T5 was released in the paper [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf) - it is an enhanced version of T5 that has been finetuned in a mixture of tasks.\n+FLAN-T5 was released in the paper [Scaling Instruction-Finetuned Language Models](https://huggingface.co/papers/2210.11416) - it is an enhanced version of T5 that has been finetuned in a mixture of tasks.\n \n One can directly use FLAN-T5 weights without finetuning the model:\n "
        },
        {
            "sha": "f921cfdce15addba7a1e436e95d63fd074143b91",
            "filename": "docs/source/en/model_doc/flaubert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflaubert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The FlauBERT model was proposed in the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le et al. It's a transformer model pretrained using a masked language\n+The FlauBERT model was proposed in the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://huggingface.co/papers/1912.05372) by Hang Le et al. It's a transformer model pretrained using a masked language\n modeling (MLM) objective (like BERT).\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "9360bb7a972d9e9141234d664602e435e7977022",
            "filename": "docs/source/en/model_doc/flava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022.\n+The FLAVA model was proposed in [FLAVA: A Foundational Language And Vision Alignment Model](https://huggingface.co/papers/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022.\n \n The paper aims at creating a single unified foundation model which can work across vision, language\n as well as vision-and-language multimodal tasks."
        },
        {
            "sha": "5d1a7d498c6d249d4205cc55708ca07e1326aa9e",
            "filename": "docs/source/en/model_doc/fnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\n+The FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://huggingface.co/papers/2105.03824) by\n James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\n model with a fourier transform which returns only the real parts of the transform. The model is significantly faster\n than the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%"
        },
        {
            "sha": "02cd9e173d2c6fd2058bda0dfaa340c0e5af0932",
            "filename": "docs/source/en/model_doc/focalnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffocalnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffocalnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffocalnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The FocalNet model was proposed in [Focal Modulation Networks](https://arxiv.org/abs/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.\n+The FocalNet model was proposed in [Focal Modulation Networks](https://huggingface.co/papers/2203.11926) by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.\n FocalNets completely replace self-attention (used in models like [ViT](vit) and [Swin](swin)) by a focal modulation mechanism for modeling token interactions in vision.\n The authors claim that FocalNets outperform self-attention based models with similar computational costs on the tasks of image classification, object detection, and segmentation.\n "
        },
        {
            "sha": "acce6979ba3438bdfe305cd8a4c1d9f2e9475b0a",
            "filename": "docs/source/en/model_doc/fsmt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffsmt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffsmt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffsmt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-FSMT (FairSeq MachineTranslation) models were introduced in [Facebook FAIR's WMT19 News Translation Task Submission](https://arxiv.org/abs/1907.06616) by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov.\n+FSMT (FairSeq MachineTranslation) models were introduced in [Facebook FAIR's WMT19 News Translation Task Submission](https://huggingface.co/papers/1907.06616) by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov.\n \n The abstract of the paper is the following:\n "
        },
        {
            "sha": "8eb35ea1d362a27f1912acd56537a1acea2ffe3a",
            "filename": "docs/source/en/model_doc/funnel.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffunnel.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The Funnel Transformer model was proposed in the paper [Funnel-Transformer: Filtering out Sequential Redundancy for\n-Efficient Language Processing](https://arxiv.org/abs/2006.03236). It is a bidirectional transformer model, like\n+Efficient Language Processing](https://huggingface.co/papers/2006.03236). It is a bidirectional transformer model, like\n BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks\n (CNN) in computer vision.\n "
        },
        {
            "sha": "c1b7dba82096cd7d12544d8da226471b1b3ae2d4",
            "filename": "docs/source/en/model_doc/git.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The GIT model was proposed in [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by\n+The GIT model was proposed in [GIT: A Generative Image-to-text Transformer for Vision and Language](https://huggingface.co/papers/2205.14100) by\n Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang. GIT is a decoder-only Transformer\n that leverages [CLIP](clip)'s vision encoder to condition the model on vision inputs besides text. The model obtains state-of-the-art results on\n image captioning and visual question answering benchmarks.\n@@ -34,7 +34,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/git_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> GIT architecture. Taken from the <a href=\"https://arxiv.org/abs/2205.14100\" target=\"_blank\">original paper</a>. </small>\n+<small> GIT architecture. Taken from the <a href=\"https://huggingface.co/papers/2205.14100\" target=\"_blank\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/microsoft/GenerativeImage2Text)."
        },
        {
            "sha": "bf5b95ac14fd5accca2884d16c8f0608b815ab6e",
            "filename": "docs/source/en/model_doc/glm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The GLM Model was proposed\n-in [ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://arxiv.org/html/2406.12793v1)\n+in [ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://huggingface.co/papers/2406.12793)\n by GLM Team, THUDM & ZhipuAI.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "4a4433626f1f4d4d51801e832681d782357429fb",
            "filename": "docs/source/en/model_doc/glpn.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fglpn.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fglpn.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglpn.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -29,7 +29,7 @@ breaking changes to fix it in the future. If you see something strange, file a [\n \n ## Overview\n \n-The GLPN model was proposed in [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436)  by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n+The GLPN model was proposed in [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://huggingface.co/papers/2201.07436)  by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n GLPN combines [SegFormer](segformer)'s hierarchical mix-Transformer with a lightweight decoder for monocular depth estimation. The proposed decoder shows better performance than the previously proposed decoders, with considerably\n less computational complexity.\n \n@@ -40,7 +40,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Summary of the approach. Taken from the <a href=\"https://arxiv.org/abs/2201.07436\" target=\"_blank\">original paper</a>. </small>\n+<small> Summary of the approach. Taken from the <a href=\"https://huggingface.co/papers/2201.07436\" target=\"_blank\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/vinvino02/GLPDepth).\n "
        },
        {
            "sha": "6f15f2526f158b1590724c93670fa36be533eae1",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The GOT-OCR2 model was proposed in [General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model](https://arxiv.org/abs/2409.01704) by Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang.\n+The GOT-OCR2 model was proposed in [General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model](https://huggingface.co/papers/2409.01704) by Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, Chunrui Han, Xiangyu Zhang.\n \n The abstract from the paper is the following:\n \n@@ -31,7 +31,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/got_ocr_overview.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> GOT-OCR2 training stages. Taken from the <a href=\"https://arxiv.org/abs/2409.01704\">original paper.</a> </small>\n+<small> GOT-OCR2 training stages. Taken from the <a href=\"https://huggingface.co/papers/2409.01704\">original paper.</a> </small>\n \n \n Tips:"
        },
        {
            "sha": "9e25f3c19ea33106d12e05ce50d6eba7f68964bb",
            "filename": "docs/source/en/model_doc/gpt_bigcode.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The GPTBigCode model was proposed in [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) by BigCode. The listed authors are: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garcรญa del Rรญo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.\n+The GPTBigCode model was proposed in [SantaCoder: don't reach for the stars!](https://huggingface.co/papers/2301.03988) by BigCode. The listed authors are: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garcรญa del Rรญo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "be5714a3ab36e1cade47e12d10afc78213e0cc44",
            "filename": "docs/source/en/model_doc/granite_speech.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,11 +23,11 @@ rendered properly in your Markdown viewer.\n ## Overview\n The Granite Speech model is a multimodal language model, consisting of a speech encoder, speech projector, large language model, and LoRA adapter(s). More details regarding each component for the current (Granite 3.2 Speech) model architecture may be found below.\n \n-1. Speech Encoder: A [Conformer](https://arxiv.org/abs/2005.08100) encoder trained with Connectionist Temporal Classification (CTC) on character-level targets on ASR corpora. The encoder uses block-attention and self-conditioned CTC from the middle layer.\n+1. Speech Encoder: A [Conformer](https://huggingface.co/papers/2005.08100) encoder trained with Connectionist Temporal Classification (CTC) on character-level targets on ASR corpora. The encoder uses block-attention and self-conditioned CTC from the middle layer.\n \n 2. Speech Projector: A query transformer (q-former) operating on the outputs of the last encoder block. The encoder and projector temporally downsample the audio features to be merged into the multimodal embeddings to be processed by the llm.\n \n-3. Large Language Model: The Granite Speech model leverages Granite LLMs, which were originally proposed in [this paper](https://arxiv.org/abs/2408.13359).\n+3. Large Language Model: The Granite Speech model leverages Granite LLMs, which were originally proposed in [this paper](https://huggingface.co/papers/2408.13359).\n \n 4. LoRA adapter(s): The Granite Speech model contains a modality specific LoRA, which will be enabled when audio features are provided, and disabled otherwise.\n "
        },
        {
            "sha": "3334008f0cad2a9edf670538d207f0a505920543",
            "filename": "docs/source/en/model_doc/granitemoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoe.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n+The GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://huggingface.co/papers/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n \n PowerMoE-3B is a 3B sparse Mixture-of-Experts (sMoE) language model trained with the Power learning rate scheduler. It sparsely activates 800M parameters for each token. It is trained on a mix of open-source and proprietary datasets. PowerMoE-3B has shown promising results compared to other dense models with 2x activate parameters across various benchmarks, including natural language multi-choices, code generation, and math reasoning.\n "
        },
        {
            "sha": "54a956c0f332d4db08c3d9b7481e16396ef1a445",
            "filename": "docs/source/en/model_doc/granitemoeshared.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoeshared.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoeshared.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitemoeshared.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -19,7 +19,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n \n-The GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://arxiv.org/abs/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n+The GraniteMoe model was proposed in [Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler](https://huggingface.co/papers/2408.13359) by Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox and Rameswar Panda.\n \n Additionally this class GraniteMoeSharedModel adds shared experts for Moe.\n "
        },
        {
            "sha": "b602bc9b0df21f2ab00e928090aedda56195562e",
            "filename": "docs/source/en/model_doc/graphormer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgraphormer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgraphormer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgraphormer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -28,7 +28,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The Graphormer model was proposed in [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234)  by\n+The Graphormer model was proposed in [Do Transformers Really Perform Bad for Graph Representation?](https://huggingface.co/papers/2106.05234)  by\n Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen and Tie-Yan Liu. It is a Graph Transformer model, modified to allow computations on graphs instead of text sequences by generating embeddings and features of interest during preprocessing and collation, then using a modified attention.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "145913da63ed73cc5fd0236cd3db5433b5ab7e84",
            "filename": "docs/source/en/model_doc/grounding-dino.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Grounding DINO model was proposed in [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://arxiv.org/abs/2303.05499) by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang. Grounding DINO extends a closed-set object detection model with a text encoder, enabling open-set object detection. The model achieves remarkable results, such as 52.5 AP on COCO zero-shot.\n+The Grounding DINO model was proposed in [Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection](https://huggingface.co/papers/2303.05499) by Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang. Grounding DINO extends a closed-set object detection model with a text encoder, enabling open-set object detection. The model achieves remarkable results, such as 52.5 AP on COCO zero-shot.\n \n The abstract from the paper is the following:\n \n@@ -31,7 +31,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/grouding_dino_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Grounding DINO overview. Taken from the <a href=\"https://arxiv.org/abs/2303.05499\">original paper</a>. </small>\n+<small> Grounding DINO overview. Taken from the <a href=\"https://huggingface.co/papers/2303.05499\">original paper</a>. </small>\n \n This model was contributed by [EduardoPacheco](https://huggingface.co/EduardoPacheco) and [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/IDEA-Research/GroundingDINO).\n@@ -85,7 +85,7 @@ Detected a cat with confidence 0.426 at location [11.74, 51.55, 316.51, 473.22]\n \n ## Grounded SAM\n \n-One can combine Grounding DINO with the [Segment Anything](sam) model for text-based mask generation as introduced in [Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks](https://arxiv.org/abs/2401.14159). You can refer to this [demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Grounding%20DINO/GroundingDINO_with_Segment_Anything.ipynb) ๐ for details.\n+One can combine Grounding DINO with the [Segment Anything](sam) model for text-based mask generation as introduced in [Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks](https://huggingface.co/papers/2401.14159). You can refer to this [demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Grounding%20DINO/GroundingDINO_with_Segment_Anything.ipynb) ๐ for details.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/grounded_sam.png\"\n alt=\"drawing\" width=\"900\"/>"
        },
        {
            "sha": "dbe83b64c89b75538bf4dd85bc948bfec4eadb0e",
            "filename": "docs/source/en/model_doc/groupvit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgroupvit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The GroupViT model was proposed in [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\n+The GroupViT model was proposed in [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://huggingface.co/papers/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\n Inspired by [CLIP](clip), GroupViT is a vision-language model that can perform zero-shot semantic segmentation on any given vocabulary categories.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "a2e594b5f9d1fe5f8c365eed62da6fbdba188a06",
            "filename": "docs/source/en/model_doc/hgnet_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhgnet_v2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -19,7 +19,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n A HGNet-V2 (High Performance GPU Net) image classification model.\n-HGNet arhtictecture was proposed in [HGNET: A Hierarchical Feature Guided Network for Occupancy Flow Field Prediction](https://arxiv.org/abs/2407.01097) by\n+HGNet arhtictecture was proposed in [HGNET: A Hierarchical Feature Guided Network for Occupancy Flow Field Prediction](https://huggingface.co/papers/2407.01097) by\n Zhan Chen, Chen Tang, Lu Xiong\n \n The abstract from the HGNET paper is the following:"
        },
        {
            "sha": "9d20f346700b422b48c49dd483481eb67e23df60",
            "filename": "docs/source/en/model_doc/hiera.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fhiera.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fhiera.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhiera.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-Hiera was proposed in [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://arxiv.org/abs/2306.00989) by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer\n+Hiera was proposed in [Hiera: A Hierarchical Vision Transformer without the Bells-and-Whistles](https://huggingface.co/papers/2306.00989) by Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, Christoph Feichtenhofer\n \n The paper introduces \"Hiera,\" a hierarchical Vision Transformer that simplifies the architecture of modern hierarchical vision transformers by removing unnecessary components without compromising on accuracy or efficiency. Unlike traditional transformers that add complex vision-specific components to improve supervised classification performance, Hiera demonstrates that such additions, often termed \"bells-and-whistles,\" are not essential for high accuracy. By leveraging a strong visual pretext task (MAE) for pretraining, Hiera retains simplicity and achieves superior accuracy and speed both in inference and training across various image and video recognition tasks. The approach suggests that spatial biases required for vision tasks can be effectively learned through proper pretraining, eliminating the need for added architectural complexity. \n \n@@ -33,7 +33,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/hiera_overview.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Hiera architecture. Taken from the <a href=\"https://arxiv.org/abs/2306.00989\">original paper.</a> </small>\n+<small> Hiera architecture. Taken from the <a href=\"https://huggingface.co/papers/2306.00989\">original paper.</a> </small>\n \n This model was a joint contribution by [EduardoPacheco](https://huggingface.co/EduardoPacheco) and [namangarg110](https://huggingface.co/namangarg110). The original code can be found [here] (https://github.com/facebookresearch/hiera).\n "
        },
        {
            "sha": "17255fa8d4b7d90516a57ca3760a86531be0c877",
            "filename": "docs/source/en/model_doc/hubert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-Hubert was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan\n+Hubert was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://huggingface.co/papers/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan\n Salakhutdinov, Abdelrahman Mohamed.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "34893c6c1dd1b807841a44b52de4cf5f9b0f7b15",
            "filename": "docs/source/en/model_doc/ibert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fibert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fibert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fibert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The I-BERT model was proposed in [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by\n+The I-BERT model was proposed in [I-BERT: Integer-only BERT Quantization](https://huggingface.co/papers/2101.01321) by\n Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney and Kurt Keutzer. It's a quantized version of RoBERTa running\n inference up to four times faster.\n "
        },
        {
            "sha": "24d3fd23c7ba0d7272d4b82e64561d526d01e93b",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Idefics2 model was proposed in [What matters when building vision-language models?](https://arxiv.org/abs/2405.02246) by Lรฉo Tronchon, Hugo Laurencon, Victor Sanh. The accompanying blog post can be found [here](https://huggingface.co/blog/idefics2).\n+The Idefics2 model was proposed in [What matters when building vision-language models?](https://huggingface.co/papers/2405.02246) by Lรฉo Tronchon, Hugo Laurencon, Victor Sanh. The accompanying blog post can be found [here](https://huggingface.co/blog/idefics2).\n \n Idefics2 is an open multimodal model that accepts arbitrary sequences of image and text inputs and produces text\n outputs. The model can answer questions about images, describe visual content, create stories grounded on multiple\n@@ -39,7 +39,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/idefics2_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Idefics2 architecture. Taken from the <a href=\"https://arxiv.org/abs/2405.02246\">original paper.</a> </small>\n+<small> Idefics2 architecture. Taken from the <a href=\"https://huggingface.co/papers/2405.02246\">original paper.</a> </small>\n \n This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts).\n The original code can be found [here](https://huggingface.co/HuggingFaceM4/idefics2)."
        },
        {
            "sha": "02c05b0bddb6b0fb3229e24f2c52f23ebc6d42e6",
            "filename": "docs/source/en/model_doc/ijepa.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The I-JEPA model was proposed in [Image-based Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2301.08243) by Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas.\n+The I-JEPA model was proposed in [Image-based Joint-Embedding Predictive Architecture](https://huggingface.co/papers/2301.08243) by Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas.\n I-JEPA is a self-supervised learning method that predicts the representations of one part of an image based on other parts of the same image. This approach focuses on learning semantic features without relying on pre-defined invariances from hand-crafted data transformations, which can bias specific tasks, or on filling in pixel-level details, which often leads to less meaningful representations.\n \n The abstract from the paper is the following:\n@@ -34,7 +34,7 @@ This paper demonstrates an approach for learning highly semantic image represent\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/ijepa_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> I-JEPA architecture. Taken from the <a href=\"https://arxiv.org/abs/2301.08243\">original paper.</a> </small>\n+<small> I-JEPA architecture. Taken from the <a href=\"https://huggingface.co/papers/2301.08243\">original paper.</a> </small>\n \n This model was contributed by [jmtzt](https://huggingface.co/jmtzt).\n The original code can be found [here](https://github.com/facebookresearch/ijepa)."
        },
        {
            "sha": "d511d0f498d120d054a30352965acfab18cd056a",
            "filename": "docs/source/en/model_doc/informer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Finformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Finformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Informer model was proposed in [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\n+The Informer model was proposed in [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://huggingface.co/papers/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.\n \n This method introduces a Probabilistic Attention mechanism to select the \"active\" queries rather than the \"lazy\" queries and provides a sparse Transformer thus mitigating the quadratic compute and memory requirements of vanilla attention.\n "
        },
        {
            "sha": "c297ca0ac4a52c2217054f4a0a88081fe9936cb6",
            "filename": "docs/source/en/model_doc/instructblip.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The InstructBLIP model was proposed in [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.\n+The InstructBLIP model was proposed in [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://huggingface.co/papers/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.\n InstructBLIP leverages the [BLIP-2](blip2) architecture for visual instruction tuning.\n \n The abstract from the paper is the following:\n@@ -28,7 +28,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/instructblip_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> InstructBLIP architecture. Taken from the <a href=\"https://arxiv.org/abs/2305.06500\">original paper.</a> </small>\n+<small> InstructBLIP architecture. Taken from the <a href=\"https://huggingface.co/papers/2305.06500\">original paper.</a> </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)."
        },
        {
            "sha": "d0b4dc3cc0826b3127d94d4aae9b289cb5c042d2",
            "filename": "docs/source/en/model_doc/instructblipvideo.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The InstructBLIPVideo is an extension of the models proposed in [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.\n+The InstructBLIPVideo is an extension of the models proposed in [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://huggingface.co/papers/2305.06500) by Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi.\n InstructBLIPVideo uses the same architecture as [InstructBLIP](instructblip) and works with the same checkpoints as [InstructBLIP](instructblip). The only difference is the ability to process videos.\n \n The abstract from the paper is the following:\n@@ -28,7 +28,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/instructblip_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> InstructBLIPVideo architecture. Taken from the <a href=\"https://arxiv.org/abs/2305.06500\">original paper.</a> </small>\n+<small> InstructBLIPVideo architecture. Taken from the <a href=\"https://huggingface.co/papers/2305.06500\">original paper.</a> </small>\n \n This model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n The original code can be found [here](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)."
        },
        {
            "sha": "d3973c45c11a7bca174c215ded4f36fe2339e655",
            "filename": "docs/source/en/model_doc/janus.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Janus Model was originally proposed in [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) by DeepSeek AI team and later refined in [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/abs/2501.17811). Janus is a vision-language model that can generate both image and text output, it can also take both images and text as input.\n+The Janus Model was originally proposed in [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://huggingface.co/papers/2410.13848) by DeepSeek AI team and later refined in [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://huggingface.co/papers/2501.17811). Janus is a vision-language model that can generate both image and text output, it can also take both images and text as input.\n \n > [!NOTE]\n > The model doesn't generate both images and text in an interleaved format. The user has to pass a parameter indicating whether to generate text or image."
        },
        {
            "sha": "897270a3837b48d6b82c0691827059dd607c370d",
            "filename": "docs/source/en/model_doc/jetmoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fjetmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fjetmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjetmoe.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n \n **JetMoe-8B** is an 8B Mixture-of-Experts (MoE) language model developed by [Yikang Shen](https://scholar.google.com.hk/citations?user=qff5rRYAAAAJ) and [MyShell](https://myshell.ai/).\n JetMoe project aims to provide a LLaMA2-level performance and efficient language model with a limited budget.\n-To achieve this goal, JetMoe uses a sparsely activated architecture inspired by the [ModuleFormer](https://arxiv.org/abs/2306.04640). \n+To achieve this goal, JetMoe uses a sparsely activated architecture inspired by the [ModuleFormer](https://huggingface.co/papers/2306.04640). \n Each JetMoe block consists of two MoE layers: Mixture of Attention Heads and Mixture of MLP Experts.\n Given the input tokens, it activates a subset of its experts to process them.\n This sparse activation schema enables JetMoe to achieve much better training throughput than similar size dense models. "
        },
        {
            "sha": "75351801b8542a946097fd2becb1d7ff59e2145b",
            "filename": "docs/source/en/model_doc/jukebox.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjukebox.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -29,7 +29,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The Jukebox model was proposed in [Jukebox: A generative model for music](https://arxiv.org/pdf/2005.00341.pdf)\n+The Jukebox model was proposed in [Jukebox: A generative model for music](https://huggingface.co/papers/2005.00341)\n by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,\n Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditioned on\n an artist, genres and lyrics.\n@@ -38,7 +38,7 @@ The abstract from the paper is the following:\n \n *We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.*\n \n-As shown on the following figure, Jukebox is made of 3 `priors` which are decoder only models. They follow the architecture described in [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509), modified to support longer context length.\n+As shown on the following figure, Jukebox is made of 3 `priors` which are decoder only models. They follow the architecture described in [Generating Long Sequences with Sparse Transformers](https://huggingface.co/papers/1904.10509), modified to support longer context length.\n First, a autoencoder is used to encode the text lyrics. Next, the first (also called `top_prior`) prior attends to the last hidden states extracted from the lyrics encoder. The priors are linked to the previous priors respectively via an `AudioConditioner` module. The`AudioConditioner` upsamples the outputs of the previous prior to raw tokens at a certain audio frame per second resolution.\n The metadata such as *artist, genre and timing* are passed to each prior, in the form of a start token and positional embedding for the timing data.  The hidden states are mapped to the closest codebook vector from the VQVAE in order to convert them to raw audio.\n "
        },
        {
            "sha": "d9105da5d15787c3b2693285c84e9cb85142f504",
            "filename": "docs/source/en/model_doc/kosmos-2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos-2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The KOSMOS-2 model was proposed in [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei.\n+The KOSMOS-2 model was proposed in [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://huggingface.co/papers/2306.14824) by Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Furu Wei.\n \n KOSMOS-2 is a Transformer-based causal language model and is trained using the next-word prediction task on a web-scale\n dataset of grounded image-text pairs [GRIT](https://huggingface.co/datasets/zzliang/GRIT). The spatial coordinates of\n@@ -37,7 +37,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/kosmos_2_overview.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Overview of tasks that KOSMOS-2 can handle. Taken from the <a href=\"https://arxiv.org/abs/2306.14824\">original paper</a>. </small>\n+<small> Overview of tasks that KOSMOS-2 can handle. Taken from the <a href=\"https://huggingface.co/papers/2306.14824\">original paper</a>. </small>\n \n ## Example\n "
        },
        {
            "sha": "86c5c7c1fcb616848f49da70d26e03687344c862",
            "filename": "docs/source/en/model_doc/layoutlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The LayoutLM model was proposed in the paper [LayoutLM: Pre-training of Text and Layout for Document Image\n-Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and\n+Understanding](https://huggingface.co/papers/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and\n Ming Zhou. It's a simple but effective pretraining method of text and layout for document image understanding and\n information extraction tasks, such as form understanding and receipt understanding. It obtains state-of-the-art results\n on several downstream tasks:"
        },
        {
            "sha": "b6c6242e4545f11e730e75f78438e744ac51812e",
            "filename": "docs/source/en/model_doc/layoutlmv2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LayoutLMV2 model was proposed in [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu,\n+The LayoutLMV2 model was proposed in [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://huggingface.co/papers/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu,\n Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou. LayoutLMV2 improves [LayoutLM](layoutlm) to obtain\n state-of-the-art results across several document image understanding benchmarks:\n \n@@ -34,7 +34,7 @@ state-of-the-art results across several document image understanding benchmarks:\n   documents for testing).\n - document image classification: the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset (a collection of\n   400,000 images belonging to one of 16 classes).\n-- document visual question answering: the [DocVQA](https://arxiv.org/abs/2007.00398) dataset (a collection of 50,000\n+- document visual question answering: the [DocVQA](https://huggingface.co/papers/2007.00398) dataset (a collection of 50,000\n   questions defined on 12,000+ document images).\n \n The abstract from the paper is the following:\n@@ -65,7 +65,7 @@ python -m pip install torchvision tesseract\n - The main difference between LayoutLMv1 and LayoutLMv2 is that the latter incorporates visual embeddings during\n   pre-training (while LayoutLMv1 only adds visual embeddings during fine-tuning).\n - LayoutLMv2 adds both a relative 1D attention bias as well as a spatial 2D attention bias to the attention scores in\n-  the self-attention layers. Details can be found on page 5 of the [paper](https://arxiv.org/abs/2012.14740).\n+  the self-attention layers. Details can be found on page 5 of the [paper](https://huggingface.co/papers/2012.14740).\n - Demo notebooks on how to use the LayoutLMv2 model on RVL-CDIP, FUNSD, DocVQA, CORD can be found [here](https://github.com/NielsRogge/Transformers-Tutorials).\n - LayoutLMv2 uses Facebook AI's [Detectron2](https://github.com/facebookresearch/detectron2/) package for its visual\n   backbone. See [this link](https://detectron2.readthedocs.io/en/latest/tutorials/install.html) for installation"
        },
        {
            "sha": "cbf6709727fdb63a43c9b741cc0f2c887f8db6ba",
            "filename": "docs/source/en/model_doc/layoutlmv3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv3.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LayoutLMv3 model was proposed in [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.\n+The LayoutLMv3 model was proposed in [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://huggingface.co/papers/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.\n LayoutLMv3 simplifies [LayoutLMv2](layoutlmv2) by using patch embeddings (as in [ViT](vit)) instead of leveraging a CNN backbone, and pre-trains the model on 3 objectives: masked language modeling (MLM), masked image modeling (MIM)\n and word-patch alignment (WPA).\n \n@@ -29,7 +29,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/layoutlmv3_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> LayoutLMv3 architecture. Taken from the <a href=\"https://arxiv.org/abs/2204.08387\">original paper</a>. </small>\n+<small> LayoutLMv3 architecture. Taken from the <a href=\"https://huggingface.co/papers/2204.08387\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The TensorFlow version of this model was added by [chriskoo](https://huggingface.co/chriskoo), [tokec](https://huggingface.co/tokec), and [lre](https://huggingface.co/lre). The original code can be found [here](https://github.com/microsoft/unilm/tree/master/layoutlmv3).\n "
        },
        {
            "sha": "32f453fb6fa289517d73dcb48211a8ae3e98ecee",
            "filename": "docs/source/en/model_doc/layoutxlm.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutxlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutxlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutxlm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,8 +22,8 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-LayoutXLM was proposed in [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\n-Zhang, Furu Wei. It's a multilingual extension of the [LayoutLMv2 model](https://arxiv.org/abs/2012.14740) trained\n+LayoutXLM was proposed in [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://huggingface.co/papers/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\n+Zhang, Furu Wei. It's a multilingual extension of the [LayoutLMv2 model](https://huggingface.co/papers/2012.14740) trained\n on 53 languages.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "e0d44107bc9b3729ad614a23499065f1dd60a6e2",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LED model was proposed in [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz\n+The LED model was proposed in [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) by Iz\n Beltagy, Matthew E. Peters, Arman Cohan.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "7596980ecd8536718ef1dd5d1c04889c886084ed",
            "filename": "docs/source/en/model_doc/levit.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flevit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LeViT model was proposed in [LeViT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervรฉ Jรฉgou, Matthijs Douze. LeViT improves the [Vision Transformer (ViT)](vit) in performance and efficiency by a few architectural differences such as activation maps with decreasing resolutions in Transformers and the introduction of an attention bias to integrate positional information.\n+The LeViT model was proposed in [LeViT: Introducing Convolutions to Vision Transformers](https://huggingface.co/papers/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervรฉ Jรฉgou, Matthijs Douze. LeViT improves the [Vision Transformer (ViT)](vit) in performance and efficiency by a few architectural differences such as activation maps with decreasing resolutions in Transformers and the introduction of an attention bias to integrate positional information.\n \n The abstract from the paper is the following:\n \n@@ -40,7 +40,7 @@ to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeV\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/levit_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> LeViT Architecture. Taken from the <a href=\"https://arxiv.org/abs/2104.01136\">original paper</a>.</small>\n+<small> LeViT Architecture. Taken from the <a href=\"https://huggingface.co/papers/2104.01136\">original paper</a>.</small>\n \n This model was contributed by [anugunj](https://huggingface.co/anugunj). The original code can be found [here](https://github.com/facebookresearch/LeViT).\n "
        },
        {
            "sha": "57e8cac28ff763e3488822dc23bcedb7ebb87d8f",
            "filename": "docs/source/en/model_doc/lilt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flilt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flilt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flilt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LiLT model was proposed in [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.\n+The LiLT model was proposed in [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://huggingface.co/papers/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.\n LiLT allows to combine any pre-trained RoBERTa text encoder with a lightweight Layout Transformer, to enable [LayoutLM](layoutlm)-like document understanding for many\n languages.\n \n@@ -33,7 +33,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/lilt_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> LiLT architecture. Taken from the <a href=\"https://arxiv.org/abs/2202.13669\">original paper</a>. </small>\n+<small> LiLT architecture. Taken from the <a href=\"https://huggingface.co/papers/2202.13669\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/jpwang/lilt)."
        },
        {
            "sha": "ae1d3c92b16115b8344a575a393bc77e118c242a",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n \n LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture. In other words, it is an multi-modal version of LLMs fine-tuned for chat / instructions.\n \n-The LLaVa model was proposed in [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) and improved in [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/pdf/2310.03744) by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.\n+The LLaVa model was proposed in [Visual Instruction Tuning](https://huggingface.co/papers/2304.08485) and improved in [Improved Baselines with Visual Instruction Tuning](https://huggingface.co/papers/2310.03744) by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.\n \n The abstract from the paper is the following:\n \n@@ -35,7 +35,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> LLaVa architecture. Taken from the <a href=\"https://arxiv.org/abs/2304.08485\">original paper.</a> </small>\n+<small> LLaVa architecture. Taken from the <a href=\"https://huggingface.co/papers/2304.08485\">original paper.</a> </small>\n \n This model was contributed by [ArthurZ](https://huggingface.co/ArthurZ) and [ybelkada](https://huggingface.co/ybelkada).\n The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/main/llava)."
        },
        {
            "sha": "e4bb26f9c0266168fc0ade81f731d2396b1f1ba1",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -43,7 +43,7 @@ Along with performance improvements, LLaVA-NeXT maintains the minimalist design\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_next_overview.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> LLaVa-NeXT incorporates a higher input resolution by encoding various patches of the input image. Taken from the <a href=\"https://arxiv.org/abs/2310.03744\">original paper.</a> </small>\n+<small> LLaVa-NeXT incorporates a higher input resolution by encoding various patches of the input image. Taken from the <a href=\"https://huggingface.co/papers/2310.03744\">original paper.</a> </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/main)."
        },
        {
            "sha": "b3e42698c6af69e936bc872716c7ab12fb597dd2",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n The LLaVa-NeXT-Video model was proposed in [LLaVA-NeXT: A Strong Zero-shot Video Understanding Model\n ](https://llava-vl.github.io/blog/2024-04-30-llava-next-video/) by Yuanhan Zhang, Bo Li, Haotian Liu, Yong Jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, Chunyuan Li. LLaVa-NeXT-Video improves upon [LLaVa-NeXT](llava_next) by fine-tuning on a mix if video and image dataset thus increasing the model's performance on videos.\n \n-[LLaVA-NeXT](llava_next) surprisingly has strong performance in understanding video content in zero-shot fashion with the AnyRes technique that it uses. The AnyRes technique naturally represents a high-resolution image into multiple images. This technique is naturally generalizable to represent videos because videos can be considered as a set of frames (similar to a set of images in LLaVa-NeXT). The current version of LLaVA-NeXT makes use of AnyRes and trains with supervised fine-tuning (SFT) on top of LLaVA-Next on video data to achieves better video understanding capabilities.The model is a current SOTA among open-source models on [VideoMME bench](https://arxiv.org/abs/2405.21075).\n+[LLaVA-NeXT](llava_next) surprisingly has strong performance in understanding video content in zero-shot fashion with the AnyRes technique that it uses. The AnyRes technique naturally represents a high-resolution image into multiple images. This technique is naturally generalizable to represent videos because videos can be considered as a set of frames (similar to a set of images in LLaVa-NeXT). The current version of LLaVA-NeXT makes use of AnyRes and trains with supervised fine-tuning (SFT) on top of LLaVA-Next on video data to achieves better video understanding capabilities.The model is a current SOTA among open-source models on [VideoMME bench](https://huggingface.co/papers/2405.21075).\n \n \n The introduction from the blog is the following:"
        },
        {
            "sha": "a8b63c9016d96b1479ec68036a54bb661ee9f780",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LLaVA-OneVision model was proposed in [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326) by <Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li\n+The LLaVA-OneVision model was proposed in [LLaVA-OneVision: Easy Visual Task Transfer](https://huggingface.co/papers/2408.03326) by <Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li\n \n LLaVA-OneVision is a Vision-Language Model that can generate text conditioned on one or several images/videos. The model consists of SigLIP vision encoder and a Qwen2 language backbone. The images are processed with anyres-9 technique where the image is split into 9 patches to better process high resolution images and capture as much details as possible. However, videos are pooled to a total sequence length of 196 tokens each frame for more memory efficient computation. LLaVA-OneVision is available in three sizes: 0.5B, 7B and 72B and achieves remarkable performance on benchmark evaluations.\n \n@@ -41,7 +41,7 @@ videos.*\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava-ov-acrhitecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> LLaVA-OneVision architecture. Taken from the <a href=\"https://arxiv.org/abs/2408.03326\">original paper.</a> </small>\n+<small> LLaVA-OneVision architecture. Taken from the <a href=\"https://huggingface.co/papers/2408.03326\">original paper.</a> </small>\n \n Tips:\n "
        },
        {
            "sha": "b73f408c461f254f29df9a1e0881d6355581d7c5",
            "filename": "docs/source/en/model_doc/longt5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flongt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flongt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flongt5.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\n+The LongT5 model was proposed in [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://huggingface.co/papers/2112.07916)\n by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung and Yinfei Yang. It's an\n encoder-decoder transformer pre-trained in a text-to-text denoising generative setting. LongT5 model is an extension of\n T5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2)"
        },
        {
            "sha": "6880d2f98ace23d564d67abd01cacee43fe13cad",
            "filename": "docs/source/en/model_doc/luke.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fluke.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fluke.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fluke.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LUKE model was proposed in [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda and Yuji Matsumoto.\n+The LUKE model was proposed in [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda and Yuji Matsumoto.\n It is based on RoBERTa and adds entity embeddings as well as an entity-aware self-attention mechanism, which helps\n improve performance on various downstream tasks involving reasoning about entities such as named entity recognition,\n extractive and cloze-style question answering, entity typing, and relation classification."
        },
        {
            "sha": "77edd6bf789aa8a4b01743befd741b7618639b4c",
            "filename": "docs/source/en/model_doc/lxmert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flxmert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Flxmert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flxmert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders\n+The LXMERT model was proposed in [LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://huggingface.co/papers/1908.07490) by Hao Tan & Mohit Bansal. It is a series of bidirectional transformer encoders\n (one for the vision modality, one for the language modality, and then one to fuse both modalities) pretrained using a\n combination of masked language modeling, visual-language text alignment, ROI-feature regression, masked\n visual-attribute modeling, masked visual-object modeling, and visual-question answering objectives. The pretraining"
        },
        {
            "sha": "6e7b216d7c10e1d6027a924578e47d99dece086b",
            "filename": "docs/source/en/model_doc/m2m_100.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The M2M100 model was proposed in [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,\n+The M2M100 model was proposed in [Beyond English-Centric Multilingual Machine Translation](https://huggingface.co/papers/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,\n Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy\n Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n "
        },
        {
            "sha": "07a7342781d2b52c9bcfe2364460b7430a820910",
            "filename": "docs/source/en/model_doc/markuplm.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmarkuplm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The MarkupLM model was proposed in [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document\n-Understanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\n+Understanding](https://huggingface.co/papers/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei. MarkupLM is BERT, but\n applied to HTML pages instead of raw text documents. The model incorporates additional embedding layers to improve\n performance, similar to [LayoutLM](layoutlm).\n \n@@ -55,7 +55,7 @@ These are the XPATH tags and subscripts respectively for each token in the input\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/markuplm_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/> \n \n-<small> MarkupLM architecture. Taken from the <a href=\"https://arxiv.org/abs/2110.08518\">original paper.</a> </small>\n+<small> MarkupLM architecture. Taken from the <a href=\"https://huggingface.co/papers/2110.08518\">original paper.</a> </small>\n \n ## Usage: MarkupLMProcessor\n "
        },
        {
            "sha": "f27fd5948fd19812e8fc372fad5015fbb541fe97",
            "filename": "docs/source/en/model_doc/mask2former.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmask2former.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Mask2Former model was proposed in [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar. Mask2Former is a unified framework for panoptic, instance and semantic segmentation and features significant performance and efficiency improvements over [MaskFormer](maskformer).\n+The Mask2Former model was proposed in [Masked-attention Mask Transformer for Universal Image Segmentation](https://huggingface.co/papers/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar. Mask2Former is a unified framework for panoptic, instance and semantic segmentation and features significant performance and efficiency improvements over [MaskFormer](maskformer).\n \n The abstract from the paper is the following:\n \n@@ -31,7 +31,7 @@ of semantics defines a task. While only the semantics of each task differ, curre\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/mask2former_architecture.jpg\" alt=\"drawing\" width=\"600\"/>\n \n-<small> Mask2Former architecture. Taken from the <a href=\"https://arxiv.org/abs/2112.01527\">original paper.</a> </small>\n+<small> Mask2Former architecture. Taken from the <a href=\"https://huggingface.co/papers/2112.01527\">original paper.</a> </small>\n \n This model was contributed by [Shivalika Singh](https://huggingface.co/shivi) and [Alara Dirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/facebookresearch/Mask2Former).\n "
        },
        {
            "sha": "fcfe11ec55f65294f9f2ebaa7598901eaeb5fbcf",
            "filename": "docs/source/en/model_doc/maskformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmaskformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -29,13 +29,13 @@ breaking changes to fix it in the future. If you see something strange, file a [\n \n ## Overview\n \n-The MaskFormer model was proposed in [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov. MaskFormer addresses semantic segmentation with a mask classification paradigm instead of performing classic pixel-level classification.\n+The MaskFormer model was proposed in [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://huggingface.co/papers/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov. MaskFormer addresses semantic segmentation with a mask classification paradigm instead of performing classic pixel-level classification.\n \n The abstract from the paper is the following:\n \n *Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.*\n \n-The figure below illustrates the architecture of MaskFormer. Taken from the [original paper](https://arxiv.org/abs/2107.06278).\n+The figure below illustrates the architecture of MaskFormer. Taken from the [original paper](https://huggingface.co/papers/2107.06278).\n \n <img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/maskformer_architecture.png\"/>\n "
        },
        {
            "sha": "7dc5660db6c7af893b1c70b3b62eb82a677475f8",
            "filename": "docs/source/en/model_doc/matcha.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmatcha.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-MatCha has been proposed in the paper [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://arxiv.org/abs/2212.09662), from Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.\n+MatCha has been proposed in the paper [MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering](https://huggingface.co/papers/2212.09662), from Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, Julian Martin Eisenschlos.\n \n The abstract of the paper states the following:\n "
        },
        {
            "sha": "beb381f6a0d4aae8fc87fe739d2ca51d667dffb6",
            "filename": "docs/source/en/model_doc/mctct.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmctct.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmctct.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmctct.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -31,7 +31,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The M-CTC-T model was proposed in [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert. The model is a 1B-param transformer encoder, with a CTC head over 8065 character labels and a language identification head over 60 language ID labels. It is trained on Common Voice (version 6.1, December 2020 release) and VoxPopuli. After training on Common Voice and VoxPopuli, the model is trained on Common Voice only. The labels are unnormalized character-level transcripts (punctuation and capitalization are not removed). The model takes as input Mel filterbank features from a 16Khz audio signal.\n+The M-CTC-T model was proposed in [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://huggingface.co/papers/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert. The model is a 1B-param transformer encoder, with a CTC head over 8065 character labels and a language identification head over 60 language ID labels. It is trained on Common Voice (version 6.1, December 2020 release) and VoxPopuli. After training on Common Voice and VoxPopuli, the model is trained on Common Voice only. The labels are unnormalized character-level transcripts (punctuation and capitalization are not removed). The model takes as input Mel filterbank features from a 16Khz audio signal.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "080d8de529252f9b3dab5ca6ae10c6fd74ee9fdc",
            "filename": "docs/source/en/model_doc/mega.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmega.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -30,7 +30,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The MEGA model was proposed in [Mega: Moving Average Equipped Gated Attention](https://arxiv.org/abs/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\n+The MEGA model was proposed in [Mega: Moving Average Equipped Gated Attention](https://huggingface.co/papers/2209.10655) by Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.\n MEGA proposes a new approach to self-attention with each encoder layer having a multi-headed exponential moving average in addition to a single head of standard dot-product attention, giving the attention mechanism\n stronger positional biases. This allows MEGA to perform competitively to Transformers on standard benchmarks including LRA\n while also having significantly fewer parameters. MEGA's compute efficiency allows it to scale to very long sequences, making it an"
        },
        {
            "sha": "8d3ba122958930c1a32e9b37117390242ebfd324",
            "filename": "docs/source/en/model_doc/megatron-bert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron-bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron-bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron-bert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The MegatronBERT model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model\n-Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\n+Parallelism](https://huggingface.co/papers/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\n Jared Casper and Bryan Catanzaro.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "fc904746638caecb8ba0c73d18d50cc07d11817c",
            "filename": "docs/source/en/model_doc/megatron_gpt2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron_gpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron_gpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmegatron_gpt2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The MegatronGPT2 model was proposed in [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model\n-Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\n+Parallelism](https://huggingface.co/papers/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\n Jared Casper and Bryan Catanzaro.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "b98a34874e5166996e764cbec6cfcec0745d5e15",
            "filename": "docs/source/en/model_doc/mgp-str.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmgp-str.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmgp-str.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmgp-str.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MGP-STR model was proposed in [Multi-Granularity Prediction for Scene Text Recognition](https://arxiv.org/abs/2209.03592) by Peng Wang, Cheng Da, and Cong Yao. MGP-STR is a conceptually **simple** yet **powerful** vision Scene Text Recognition (STR) model, which is built upon the [Vision Transformer (ViT)](vit). To integrate linguistic knowledge, Multi-Granularity Prediction (MGP) strategy is proposed to inject information from the language modality into the model in an implicit way.\n+The MGP-STR model was proposed in [Multi-Granularity Prediction for Scene Text Recognition](https://huggingface.co/papers/2209.03592) by Peng Wang, Cheng Da, and Cong Yao. MGP-STR is a conceptually **simple** yet **powerful** vision Scene Text Recognition (STR) model, which is built upon the [Vision Transformer (ViT)](vit). To integrate linguistic knowledge, Multi-Granularity Prediction (MGP) strategy is proposed to inject information from the language modality into the model in an implicit way.\n \n The abstract from the paper is the following:\n \n@@ -31,7 +31,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/mgp_str_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> MGP-STR architecture. Taken from the <a href=\"https://arxiv.org/abs/2209.03592\">original paper</a>. </small>\n+<small> MGP-STR architecture. Taken from the <a href=\"https://huggingface.co/papers/2209.03592\">original paper</a>. </small>\n \n MGP-STR is trained on two synthetic datasets [MJSynth]((http://www.robots.ox.ac.uk/~vgg/data/text/)) (MJ) and [SynthText](http://www.robots.ox.ac.uk/~vgg/data/scenetext/) (ST) without fine-tuning on other datasets. It achieves state-of-the-art results on six standard Latin scene text benchmarks, including 3 regular text datasets (IC13, SVT, IIIT) and 3 irregular ones (IC15, SVTP, CUTE).\n This model was contributed by [yuekun](https://huggingface.co/yuekun). The original code can be found [here](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR)."
        },
        {
            "sha": "d4b9e56f0bff56748749fd583486e95eb3431392",
            "filename": "docs/source/en/model_doc/minimax.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fminimax.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MiniMax-Text-01 model was proposed in [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://arxiv.org/abs/2501.08313) by MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu.\n+The MiniMax-Text-01 model was proposed in [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://huggingface.co/papers/2501.08313) by MiniMax, Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, Zijia Wu.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "3472ebc220fca69e926221debc33ed171c2bdc3c",
            "filename": "docs/source/en/model_doc/mluke.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmluke.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmluke.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmluke.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,8 +22,8 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The mLUKE model was proposed in [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka. It's a multilingual extension\n-of the [LUKE model](https://arxiv.org/abs/2010.01057) trained on the basis of XLM-RoBERTa.\n+The mLUKE model was proposed in [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://huggingface.co/papers/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka. It's a multilingual extension\n+of the [LUKE model](https://huggingface.co/papers/2010.01057) trained on the basis of XLM-RoBERTa.\n \n It is based on XLM-RoBERTa and adds entity embeddings, which helps improve performance on various downstream tasks\n involving reasoning about entities such as named entity recognition, extractive question answering, relation"
        },
        {
            "sha": "53b73f829566388210aeda9809857cd342ee68bc",
            "filename": "docs/source/en/model_doc/mms.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmms.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MMS model was proposed in [Scaling Speech Technology to 1,000+ Languages](https://arxiv.org/abs/2305.13516) \n+The MMS model was proposed in [Scaling Speech Technology to 1,000+ Languages](https://huggingface.co/papers/2305.13516) \n by Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, Michael Auli\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "6fb69649ee0dca12864a635044b93aeb79fab7e8",
            "filename": "docs/source/en/model_doc/mobilevit.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MobileViT model was proposed in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari. MobileViT introduces a new layer that replaces local processing in convolutions with global processing using transformers.\n+The MobileViT model was proposed in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://huggingface.co/papers/2110.02178) by Sachin Mehta and Mohammad Rastegari. MobileViT introduces a new layer that replaces local processing in convolutions with global processing using transformers.\n \n The abstract from the paper is the following:\n \n@@ -36,7 +36,7 @@ This model was contributed by [matthijs](https://huggingface.co/Matthijs). The T\n - MobileViT is more like a CNN than a Transformer model. It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map. You can follow [this tutorial](https://keras.io/examples/vision/mobilevit) for a lightweight introduction.\n - One can use [`MobileViTImageProcessor`] to prepare images for the model. Note that if you do your own preprocessing, the pretrained checkpoints expect images to be in BGR pixel order (not RGB).\n - The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes).\n-- The segmentation model uses a [DeepLabV3](https://arxiv.org/abs/1706.05587) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n+- The segmentation model uses a [DeepLabV3](https://huggingface.co/papers/1706.05587) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n - As the name suggests MobileViT was designed to be performant and efficient on mobile phones. The TensorFlow versions of the MobileViT models are fully compatible with [TensorFlow Lite](https://www.tensorflow.org/lite).\n \n   You can use the following code to convert a MobileViT checkpoint (be it image classification or semantic segmentation) to generate a"
        },
        {
            "sha": "9c20fb6e964a985d32e116c45cfb31d2331aeb58",
            "filename": "docs/source/en/model_doc/mobilevitv2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevitv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevitv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevitv2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MobileViTV2 model was proposed in [Separable Self-attention for Mobile Vision Transformers](https://arxiv.org/abs/2206.02680) by Sachin Mehta and Mohammad Rastegari.\n+The MobileViTV2 model was proposed in [Separable Self-attention for Mobile Vision Transformers](https://huggingface.co/papers/2206.02680) by Sachin Mehta and Mohammad Rastegari.\n \n MobileViTV2 is the second version of MobileViT, constructed by replacing the multi-headed self-attention in MobileViT with separable self-attention.\n \n@@ -38,7 +38,7 @@ The original code can be found [here](https://github.com/apple/ml-cvnets).\n - MobileViTV2 is more like a CNN than a Transformer model. It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map.\n - One can use [`MobileViTImageProcessor`] to prepare images for the model. Note that if you do your own preprocessing, the pretrained checkpoints expect images to be in BGR pixel order (not RGB).\n - The available image classification checkpoints are pre-trained on [ImageNet-1k](https://huggingface.co/datasets/imagenet-1k) (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes).\n-- The segmentation model uses a [DeepLabV3](https://arxiv.org/abs/1706.05587) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n+- The segmentation model uses a [DeepLabV3](https://huggingface.co/papers/1706.05587) head. The available semantic segmentation checkpoints are pre-trained on [PASCAL VOC](http://host.robots.ox.ac.uk/pascal/VOC/).\n \n ## MobileViTV2Config\n "
        },
        {
            "sha": "caddc635cba521492d670dd66323adf65e7dc6fd",
            "filename": "docs/source/en/model_doc/mpnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmpnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MPNet model was proposed in [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n+The MPNet model was proposed in [MPNet: Masked and Permuted Pre-training for Language Understanding](https://huggingface.co/papers/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n \n MPNet adopts a novel pre-training method, named masked and permuted language modeling, to inherit the advantages of\n masked language modeling and permuted language modeling for natural language understanding."
        },
        {
            "sha": "9faa9a26166e92507b642c0fd93c79578ed9e3a1",
            "filename": "docs/source/en/model_doc/mra.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmra.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmra.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmra.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MRA model was proposed in [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://arxiv.org/abs/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh.\n+The MRA model was proposed in [Multi Resolution Analysis (MRA) for Approximate Self-Attention](https://huggingface.co/papers/2207.10284) by Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn M Fung, and Vikas Singh.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "d6b9ef99cb662d4271e2bcc6e41b0a3d3e7a0ed6",
            "filename": "docs/source/en/model_doc/mt5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmt5.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The mT5 model was presented in [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya\n+The mT5 model was presented in [mT5: A massively multilingual pre-trained text-to-text transformer](https://huggingface.co/papers/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya\n Siddhant, Aditya Barua, Colin Raffel.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "ff7645bceada4dbe64d0981032bc73d842601d71",
            "filename": "docs/source/en/model_doc/musicgen.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MusicGen model was proposed in the paper [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)\n+The MusicGen model was proposed in the paper [Simple and Controllable Music Generation](https://huggingface.co/papers/2306.05284)\n by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Dรฉfossez.\n \n MusicGen is a single stage auto-regressive Transformer model capable of generating high-quality music samples conditioned"
        },
        {
            "sha": "3e4bbabc6c48aa48d994d7eeb51fa35f96e93194",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MusicGen Melody model was proposed in [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Dรฉfossez.\n+The MusicGen Melody model was proposed in [Simple and Controllable Music Generation](https://huggingface.co/papers/2306.05284) by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi and Alexandre Dรฉfossez.\n \n MusicGen Melody is a single stage auto-regressive Transformer model capable of generating high-quality music samples conditioned on text descriptions or audio prompts. The text descriptions are passed through a frozen text encoder model to obtain a sequence of hidden-state representations. MusicGen is then trained to predict discrete audio tokens, or *audio codes*, conditioned on these hidden-states. These audio tokens are then decoded using an audio compression model, such as EnCodec, to recover the audio waveform.\n "
        },
        {
            "sha": "d2dcdeb301f3d24950dda15b47c3466b5389f700",
            "filename": "docs/source/en/model_doc/mvp.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmvp.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The MVP model was proposed in [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n+The MVP model was proposed in [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://huggingface.co/papers/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n \n \n According to the abstract,\n@@ -39,7 +39,7 @@ This model was contributed by [Tianyi Tang](https://huggingface.co/StevenTang).\n - We have released a series of models [here](https://huggingface.co/models?filter=mvp), including MVP, MVP with task-specific prompts, and multi-task pre-trained variants.\n - If you want to use a model without prompts (standard Transformer), you can load it through `MvpForConditionalGeneration.from_pretrained('RUCAIBox/mvp')`.\n - If you want to use a model with task-specific prompts, such as summarization, you can load it through `MvpForConditionalGeneration.from_pretrained('RUCAIBox/mvp-summarization')`.\n-- Our model supports lightweight prompt tuning following [Prefix-tuning](https://arxiv.org/abs/2101.00190) with method `set_lightweight_tuning()`.\n+- Our model supports lightweight prompt tuning following [Prefix-tuning](https://huggingface.co/papers/2101.00190) with method `set_lightweight_tuning()`.\n \n ## Usage examples\n \n@@ -86,7 +86,7 @@ For data-to-text generation, it is an example to use MVP and multi-task pre-trai\n ['Iron Man is a fictional superhero appearing in American comic books published by Marvel Comics.']\n ```\n \n-For lightweight tuning, *i.e.*, fixing the model and only tuning prompts, you can load MVP with randomly initialized prompts or with task-specific prompts. Our code also supports Prefix-tuning with BART following the [original paper](https://arxiv.org/abs/2101.00190).\n+For lightweight tuning, *i.e.*, fixing the model and only tuning prompts, you can load MVP with randomly initialized prompts or with task-specific prompts. Our code also supports Prefix-tuning with BART following the [original paper](https://huggingface.co/papers/2101.00190).\n \n ```python\n >>> from transformers import MvpForConditionalGeneration"
        },
        {
            "sha": "cb406e9d7d4d7e5070aedb767ebbbed8ccf90ec5",
            "filename": "docs/source/en/model_doc/myt5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmyt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fmyt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmyt5.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The myt5 model was proposed in [MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling](https://arxiv.org/pdf/2403.10691.pdf) by Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, and Luke Zettlemoyer.\n+The myt5 model was proposed in [MYTE: Morphology-Driven Byte Encoding for Better and Fairer Multilingual Language Modeling](https://huggingface.co/papers/2403.10691) by Tomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, and Luke Zettlemoyer.\n MyT5 (**My**te **T5**) is a multilingual language model based on T5 architecture.\n The model uses a **m**orphologically-driven **byte** (**MYTE**) representation described in our paper.\n **MYTE** uses codepoints corresponding to morphemes in contrast to characters used in UTF-8 encoding."
        },
        {
            "sha": "86a935f9f611c6e0f4524c9f7d57747c934452fe",
            "filename": "docs/source/en/model_doc/nat.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnat.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -30,7 +30,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-NAT was proposed in [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143)\n+NAT was proposed in [Neighborhood Attention Transformer](https://huggingface.co/papers/2204.07143)\n by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n \n It is a hierarchical vision transformer based on Neighborhood Attention, a sliding-window self attention pattern.\n@@ -53,7 +53,7 @@ src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/ma\n alt=\"drawing\" width=\"600\"/>\n \n <small> Neighborhood Attention compared to other attention patterns.\n-Taken from the <a href=\"https://arxiv.org/abs/2204.07143\">original paper</a>.</small>\n+Taken from the <a href=\"https://huggingface.co/papers/2204.07143\">original paper</a>.</small>\n \n This model was contributed by [Ali Hassani](https://huggingface.co/alihassanijr).\n The original code can be found [here](https://github.com/SHI-Labs/Neighborhood-Attention-Transformer)."
        },
        {
            "sha": "761ad33fde4e0716e9bbcbd6b7ad3bd00819feb7",
            "filename": "docs/source/en/model_doc/nemotron.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -44,9 +44,9 @@ NVIDIA NeMo is an end-to-end, cloud-native platform to build, customize, and dep\n \n ### Minitron 4B Base\n \n-Minitron is a family of small language models (SLMs) obtained by pruning NVIDIA's [Nemotron-4 15B](https://arxiv.org/abs/2402.16819) model. We prune model embedding size, attention heads, and MLP intermediate dimension, following which, we perform continued training with distillation to arrive at the final models.\n+Minitron is a family of small language models (SLMs) obtained by pruning NVIDIA's [Nemotron-4 15B](https://huggingface.co/papers/2402.16819) model. We prune model embedding size, attention heads, and MLP intermediate dimension, following which, we perform continued training with distillation to arrive at the final models.\n \n-Deriving the Minitron 8B and 4B models from the base 15B model using our approach requires up to **40x fewer training tokens** per model compared to training from scratch; this results in **compute cost savings of 1.8x** for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. Please refer to our [arXiv paper](https://arxiv.org/abs/2407.14679) for more details.\n+Deriving the Minitron 8B and 4B models from the base 15B model using our approach requires up to **40x fewer training tokens** per model compared to training from scratch; this results in **compute cost savings of 1.8x** for training the full model family (15B, 8B, and 4B). Minitron models exhibit up to a 16% improvement in MMLU scores compared to training from scratch, perform comparably to other community models such as Mistral 7B, Gemma 7B and Llama-3 8B, and outperform state-of-the-art compression techniques from the literature. Please refer to our [arXiv paper](https://huggingface.co/papers/2407.14679) for more details.\n \n Minitron models are for research and development only.\n \n@@ -84,7 +84,7 @@ Minitron is released under the [NVIDIA Open Model License Agreement](https://dev\n \n ### Evaluation Results\n \n-*5-shot performance.* Language Understanding evaluated using [Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300):\n+*5-shot performance.* Language Understanding evaluated using [Massive Multitask Language Understanding](https://huggingface.co/papers/2009.03300):\n \n | Average |\n | :---- |\n@@ -103,7 +103,7 @@ Minitron is released under the [NVIDIA Open Model License Agreement](https://dev\n | :------------- |\n | 23.3 |\n \n-Please refer to our [paper](https://arxiv.org/abs/2407.14679) for the full set of results.\n+Please refer to our [paper](https://huggingface.co/papers/2407.14679) for the full set of results.\n \n ### Citation\n "
        },
        {
            "sha": "edbadcb2209f002c7c952d1f2225d33ab8c28d71",
            "filename": "docs/source/en/model_doc/nezha.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnezha.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnezha.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnezha.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -30,7 +30,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The Nezha model was proposed in [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei et al.\n+The Nezha model was proposed in [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://huggingface.co/papers/1909.00204) by Junqiu Wei et al.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "4e5af4fb18fd88f40ee153749611c37a63994d3e",
            "filename": "docs/source/en/model_doc/nllb-moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb-moe.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The NLLB model was presented in [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by Marta R. Costa-jussร, James Cross, Onur รelebi,\n+The NLLB model was presented in [No Language Left Behind: Scaling Human-Centered Machine Translation](https://huggingface.co/papers/2207.04672) by Marta R. Costa-jussร, James Cross, Onur รelebi,\n Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,\n Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,\n Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmรกn, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,"
        },
        {
            "sha": "483d590016e61def43fe7f485dc3a3dcc9e61ae1",
            "filename": "docs/source/en/model_doc/nllb.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -65,7 +65,7 @@ For more details, feel free to check the linked [PR](https://github.com/huggingf\n \n ## Overview\n \n-The NLLB model was presented in [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by Marta R. Costa-jussร, James Cross, Onur รelebi,\n+The NLLB model was presented in [No Language Left Behind: Scaling Human-Centered Machine Translation](https://huggingface.co/papers/2207.04672) by Marta R. Costa-jussร, James Cross, Onur รelebi,\n Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,\n Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,\n Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmรกn, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,"
        },
        {
            "sha": "c3d6ef54f4767205cffe98266b59832c5d0f832a",
            "filename": "docs/source/en/model_doc/nougat.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ specific language governing permissions and limitations under the License. -->\n \n ## Overview\n \n-The Nougat model was proposed in [Nougat: Neural Optical Understanding for Academic Documents](https://arxiv.org/abs/2308.13418) by\n+The Nougat model was proposed in [Nougat: Neural Optical Understanding for Academic Documents](https://huggingface.co/papers/2308.13418) by\n Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic. Nougat uses the same architecture as [Donut](donut), meaning an image Transformer\n encoder and an autoregressive text Transformer decoder to translate scientific PDFs to markdown, enabling easier access to them.\n \n@@ -35,7 +35,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/nougat_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Nougat high-level overview. Taken from the <a href=\"https://arxiv.org/abs/2308.13418\">original paper</a>. </small>\n+<small> Nougat high-level overview. Taken from the <a href=\"https://huggingface.co/papers/2308.13418\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n [here](https://github.com/facebookresearch/nougat)."
        },
        {
            "sha": "f368a77a3c1da7cc789d5003ead470cfc8774f6a",
            "filename": "docs/source/en/model_doc/nystromformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnystromformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fnystromformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnystromformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Nystrรถmformer model was proposed in [*Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention*](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\n+The Nystrรถmformer model was proposed in [*Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention*](https://huggingface.co/papers/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn\n Fung, Yin Li, and Vikas Singh.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "c0d227cb549564c1514e3f849a2916d2db262380",
            "filename": "docs/source/en/model_doc/olmo.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The OLMo model was proposed in [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) by Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi.\n+The OLMo model was proposed in [OLMo: Accelerating the Science of Language Models](https://huggingface.co/papers/2402.00838) by Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hannaneh Hajishirzi.\n \n OLMo is a series of **O**pen **L**anguage **Mo**dels designed to enable the science of language models. The OLMo models are trained on the Dolma dataset. We release all code, checkpoints, logs (coming soon), and details involved in training these models.\n "
        },
        {
            "sha": "701d1b7c2f400956718ecf1e6412827b950eaea6",
            "filename": "docs/source/en/model_doc/olmoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The OLMoE model was proposed in [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060) by Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi.\n+The OLMoE model was proposed in [OLMoE: Open Mixture-of-Experts Language Models](https://huggingface.co/papers/2409.02060) by Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi.\n \n OLMoE is a series of **O**pen **L**anguage **Mo**dels using sparse **M**ixture-**o**f-**E**xperts designed to enable the science of language models. We release all code, checkpoints, logs, and details involved in training these models.\n "
        },
        {
            "sha": "b4fc6adef385d091c05018f1c17b3bb79e4ea9ef",
            "filename": "docs/source/en/model_doc/omdet-turbo.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,15 +22,15 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The OmDet-Turbo model was proposed in [Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head](https://arxiv.org/abs/2403.06892) by Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee. OmDet-Turbo incorporates components from RT-DETR and introduces a swift multimodal fusion module to achieve real-time open-vocabulary object detection capabilities while maintaining high accuracy. The base model achieves performance of up to 100.2 FPS and 53.4 AP on COCO zero-shot.\n+The OmDet-Turbo model was proposed in [Real-time Transformer-based Open-Vocabulary Detection with Efficient Fusion Head](https://huggingface.co/papers/2403.06892) by Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, Kyusong Lee. OmDet-Turbo incorporates components from RT-DETR and introduces a swift multimodal fusion module to achieve real-time open-vocabulary object detection capabilities while maintaining high accuracy. The base model achieves performance of up to 100.2 FPS and 53.4 AP on COCO zero-shot.\n \n The abstract from the paper is the following:\n \n *End-to-end transformer-based detectors (DETRs) have shown exceptional performance in both closed-set and open-vocabulary object detection (OVD) tasks through the integration of language modalities. However, their demanding computational requirements have hindered their practical application in real-time object detection (OD) scenarios. In this paper, we scrutinize the limitations of two leading models in the OVDEval benchmark, OmDet and Grounding-DINO, and introduce OmDet-Turbo. This novel transformer-based real-time OVD model features an innovative Efficient Fusion Head (EFH) module designed to alleviate the bottlenecks observed in OmDet and Grounding-DINO. Notably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with TensorRT and language cache techniques applied. Notably, in zero-shot scenarios on COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on par with current state-of-the-art supervised models. Furthermore, it establishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an AP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of OmDet-Turbo in industrial applications is underscored by its exceptional performance on benchmark datasets and superior inference speed, positioning it as a compelling choice for real-time object detection tasks.*\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/omdet_turbo_architecture.jpeg\" alt=\"drawing\" width=\"600\"/>\n \n-<small> OmDet-Turbo architecture overview. Taken from the <a href=\"https://arxiv.org/abs/2403.06892\">original paper</a>. </small>\n+<small> OmDet-Turbo architecture overview. Taken from the <a href=\"https://huggingface.co/papers/2403.06892\">original paper</a>. </small>\n \n This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan).\n The original code can be found [here](https://github.com/om-ai-lab/OmDet)."
        },
        {
            "sha": "c0dcfd8800dcbef1800a1bc1084dae96f608cd6a",
            "filename": "docs/source/en/model_doc/oneformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,15 +22,15 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The OneFormer model was proposed in [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi. OneFormer is a universal image segmentation framework that can be trained on a single panoptic dataset to perform semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference.\n+The OneFormer model was proposed in [OneFormer: One Transformer to Rule Universal Image Segmentation](https://huggingface.co/papers/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi. OneFormer is a universal image segmentation framework that can be trained on a single panoptic dataset to perform semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference.\n \n <img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png\"/>\n \n The abstract from the paper is the following:\n \n *Universal Image Segmentation is not a new concept. Past attempts to unify image segmentation in the last decades include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architectures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best performance. Ideally, a truly universal framework should be trained only once and achieve SOTA performance across all three image segmentation tasks. To that end, we propose OneFormer, a universal image segmentation framework that unifies segmentation with a multi-task train-once design. We first propose a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each of the three tasks individually with three times the resources. With new ConvNeXt and DiNAT backbones, we observe even more performance improvement. We believe OneFormer is a significant step towards making image segmentation more universal and accessible.*\n \n-The figure below illustrates the architecture of OneFormer. Taken from the [original paper](https://arxiv.org/abs/2211.06220).\n+The figure below illustrates the architecture of OneFormer. Taken from the [original paper](https://huggingface.co/papers/2211.06220).\n \n <img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_architecture.png\"/>\n "
        },
        {
            "sha": "93db673065a8448bc5a203323b8dbdbc43402cd9",
            "filename": "docs/source/en/model_doc/opt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The OPT model was proposed in [Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068) by Meta AI.\n+The OPT model was proposed in [Open Pre-trained Transformer Language Models](https://huggingface.co/papers/2205.01068) by Meta AI.\n OPT is a series of open-sourced large causal language models which perform similar in performance to GPT3.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "b7ab61cc9890b30a607fa385177904057c4424e6",
            "filename": "docs/source/en/model_doc/owlv2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlv2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-OWLv2 was proposed in [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2 scales up [OWL-ViT](owlvit) using self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. This results in large gains over the previous state-of-the-art for zero-shot object detection.\n+OWLv2 was proposed in [Scaling Open-Vocabulary Object Detection](https://huggingface.co/papers/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby. OWLv2 scales up [OWL-ViT](owlvit) using self-training, which uses an existing detector to generate pseudo-box annotations on image-text pairs. This results in large gains over the previous state-of-the-art for zero-shot object detection.\n \n The abstract from the paper is the following:\n \n@@ -31,7 +31,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/owlv2_overview.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> OWLv2 high-level overview. Taken from the <a href=\"https://arxiv.org/abs/2306.09683\">original paper</a>. </small>\n+<small> OWLv2 high-level overview. Taken from the <a href=\"https://huggingface.co/papers/2306.09683\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit)."
        },
        {
            "sha": "a69eee88c19cca24a649d192370cc32291a4838e",
            "filename": "docs/source/en/model_doc/owlvit.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fowlvit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text.\n+The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in [Simple Open-Vocabulary Object Detection with Vision Transformers](https://huggingface.co/papers/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text.\n \n The abstract from the paper is the following:\n \n@@ -31,7 +31,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/owlvit_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> OWL-ViT architecture. Taken from the <a href=\"https://arxiv.org/abs/2205.06230\">original paper</a>. </small>\n+<small> OWL-ViT architecture. Taken from the <a href=\"https://huggingface.co/papers/2205.06230\">original paper</a>. </small>\n \n This model was contributed by [adirik](https://huggingface.co/adirik). The original code can be found [here](https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit).\n "
        },
        {
            "sha": "30932067939f45daed4309af96fc50a43facf50d",
            "filename": "docs/source/en/model_doc/patchtsmixer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtsmixer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.\n+The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://huggingface.co/papers/2306.09364) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.\n \n \n PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression."
        },
        {
            "sha": "5d9a2f402eb2d4351baf4e89a5bc40413b7a40c8",
            "filename": "docs/source/en/model_doc/patchtst.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtst.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtst.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpatchtst.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The PatchTST model was proposed in [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.\n+The PatchTST model was proposed in [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://huggingface.co/papers/2211.14730) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.\n \n At a high level the model vectorizes time series into patches of a given size and encodes the resulting sequence of vectors via a Transformer that then outputs the prediction length forecast via an appropriate head. The model is illustrated in the following figure:\n "
        },
        {
            "sha": "379e0362bb70baac06b72866fd927952c8098bfd",
            "filename": "docs/source/en/model_doc/pegasus_x.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpegasus_x.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The PEGASUS-X model was proposed in [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347)  by Jason Phang, Yao Zhao and Peter J. Liu.\n+The PEGASUS-X model was proposed in [Investigating Efficiently Extending Transformers for Long Input Summarization](https://huggingface.co/papers/2208.04347)  by Jason Phang, Yao Zhao and Peter J. Liu.\n \n PEGASUS-X (PEGASUS eXtended) extends the PEGASUS models for long input summarization through additional long input pretraining and using staggered block-local attention with global tokens in the encoder.\n "
        },
        {
            "sha": "eb930bd4bdbc8fab37eb1ce668777fa4913cb39e",
            "filename": "docs/source/en/model_doc/perceiver.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fperceiver.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fperceiver.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fperceiver.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,11 +23,11 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The Perceiver IO model was proposed in [Perceiver IO: A General Architecture for Structured Inputs &\n-Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch,\n+Outputs](https://huggingface.co/papers/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch,\n Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hรฉnaff, Matthew M.\n Botvinick, Andrew Zisserman, Oriol Vinyals, Joรฃo Carreira.\n \n-Perceiver IO is a generalization of [Perceiver](https://arxiv.org/abs/2103.03206) to handle arbitrary outputs in\n+Perceiver IO is a generalization of [Perceiver](https://huggingface.co/papers/2103.03206) to handle arbitrary outputs in\n addition to arbitrary inputs. The original Perceiver only produced a single classification label. In addition to\n classification labels, Perceiver IO can produce (for example) language, optical flow, and multimodal videos with audio.\n This is done using the same building blocks as the original Perceiver. The computational complexity of Perceiver IO is\n@@ -80,7 +80,7 @@ size of 262 byte IDs).\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/perceiver_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Perceiver IO architecture. Taken from the <a href=\"https://arxiv.org/abs/2105.15203\">original paper</a> </small>\n+<small> Perceiver IO architecture. Taken from the <a href=\"https://huggingface.co/papers/2105.15203\">original paper</a> </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found\n [here](https://github.com/deepmind/deepmind-research/tree/master/perceiver)."
        },
        {
            "sha": "41753bff5bc40e135b7d3a6413a13ef499c9f23c",
            "filename": "docs/source/en/model_doc/phi3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi3.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Phi-3 model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) by Microsoft.\n+The Phi-3 model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://huggingface.co/papers/2404.14219) by Microsoft.\n \n ### Summary\n "
        },
        {
            "sha": "8395021411d114957ccacda16b851eb014ba491a",
            "filename": "docs/source/en/model_doc/phimoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The PhiMoE model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) by Microsoft.\n+The PhiMoE model was proposed in [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://huggingface.co/papers/2404.14219) by Microsoft.\n \n ### Summary\n "
        },
        {
            "sha": "b03e73d246aca9d9f697e474b1194e9d06b0259d",
            "filename": "docs/source/en/model_doc/pix2struct.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Pix2Struct model was proposed in [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://arxiv.org/abs/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.\n+The Pix2Struct model was proposed in [Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding](https://huggingface.co/papers/2210.03347) by Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, Kristina Toutanova.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "a885924530298ea9bd0e04abf9f4521a59d0f46c",
            "filename": "docs/source/en/model_doc/plbart.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fplbart.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The PLBART model was proposed in [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n+The PLBART model was proposed in [Unified Pre-training for Program Understanding and Generation](https://huggingface.co/papers/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n This is a BART-like model which can be used to perform code-summarization, code-generation, and code-translation tasks. The pre-trained model `plbart-base` has been trained using multilingual denoising task\n on Java, Python and English.\n \n@@ -50,7 +50,7 @@ model is multilingual it expects the sequences in a different format. A special\n source and target text. The source text format is `X [eos, src_lang_code]` where `X` is the source text. The\n target text format is `[tgt_lang_code] X [eos]`. `bos` is never used.\n \n-However, for fine-tuning, in some cases no language token is provided in cases where a single language is used. Please refer to [the paper](https://arxiv.org/abs/2103.06333) to learn more about this.\n+However, for fine-tuning, in some cases no language token is provided in cases where a single language is used. Please refer to [the paper](https://huggingface.co/papers/2103.06333) to learn more about this.\n \n In cases where the language code is needed, the regular [`~PLBartTokenizer.__call__`] will encode source text format\n when you pass texts as the first argument or with the keyword argument `text`, and will encode target text format if"
        },
        {
            "sha": "46c84d04fa7b6a4fa9665b37afe0f0255e4e8920",
            "filename": "docs/source/en/model_doc/poolformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpoolformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpoolformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpoolformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,13 +22,13 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The PoolFormer model was proposed in [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418)  by Sea AI Labs. Instead of designing complicated token mixer to achieve SOTA performance, the target of this work is to demonstrate the competence of transformer models largely stem from the general architecture MetaFormer.\n+The PoolFormer model was proposed in [MetaFormer is Actually What You Need for Vision](https://huggingface.co/papers/2111.11418)  by Sea AI Labs. Instead of designing complicated token mixer to achieve SOTA performance, the target of this work is to demonstrate the competence of transformer models largely stem from the general architecture MetaFormer.\n \n The abstract from the paper is the following:\n \n *Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only the most basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3%/1.1% accuracy with 35%/52% fewer parameters and 48%/60% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of \"MetaFormer\", a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design.*\n \n-The figure below illustrates the architecture of PoolFormer. Taken from the [original paper](https://arxiv.org/abs/2111.11418).\n+The figure below illustrates the architecture of PoolFormer. Taken from the [original paper](https://huggingface.co/papers/2111.11418).\n \n <img width=\"600\" src=\"https://user-images.githubusercontent.com/15921929/142746124-1ab7635d-2536-4a0e-ad43-b4fe2c5a525d.png\"/>\n "
        },
        {
            "sha": "6f78233d2c37ac059c19b913c3bb6c8c39787f23",
            "filename": "docs/source/en/model_doc/pop2piano.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpop2piano.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,14 +18,14 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The Pop2Piano model was proposed in [Pop2Piano : Pop Audio-based Piano Cover Generation](https://arxiv.org/abs/2211.00895) by Jongho Choi and Kyogu Lee.\n+The Pop2Piano model was proposed in [Pop2Piano : Pop Audio-based Piano Cover Generation](https://huggingface.co/papers/2211.00895) by Jongho Choi and Kyogu Lee.\n \n Piano covers of pop music are widely enjoyed, but generating them from music is not a trivial task. It requires great \n expertise with playing piano as well as knowing different characteristics and melodies of a song. With Pop2Piano you \n can directly generate a cover from a song's audio waveform. It is the first model to directly generate a piano cover \n from pop audio without melody and chord extraction modules. \n \n-Pop2Piano is an encoder-decoder Transformer model based on [T5](https://arxiv.org/pdf/1910.10683.pdf). The input audio \n+Pop2Piano is an encoder-decoder Transformer model based on [T5](https://huggingface.co/papers/1910.10683). The input audio \n is transformed to its waveform and passed to the encoder, which transforms it to a latent representation. The decoder \n uses these latent representations to generate token ids in an autoregressive way. Each token id corresponds to one of four \n different token types: time, velocity, note and 'special'. The token ids are then decoded to their equivalent MIDI file."
        },
        {
            "sha": "271fc4e2c0f5f841207376ac13a72d5424c41b9a",
            "filename": "docs/source/en/model_doc/prompt_depth_anything.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fprompt_depth_anything.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Prompt Depth Anything model was introduced in [Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation](https://arxiv.org/abs/2412.14015) by Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, Bingyi Kang. \n+The Prompt Depth Anything model was introduced in [Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation](https://huggingface.co/papers/2412.14015) by Haotong Lin, Sida Peng, Jingxiao Chen, Songyou Peng, Jiaming Sun, Minghuan Liu, Hujun Bao, Jiashi Feng, Xiaowei Zhou, Bingyi Kang. \n \n \n The abstract from the paper is as follows:\n@@ -28,7 +28,7 @@ The abstract from the paper is as follows:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/prompt_depth_anything_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Prompt Depth Anything overview. Taken from the <a href=\"https://arxiv.org/pdf/2412.14015\">original paper</a>.</small>\n+<small> Prompt Depth Anything overview. Taken from the <a href=\"https://huggingface.co/papers/2412.14015\">original paper</a>.</small>\n \n ## Usage example\n "
        },
        {
            "sha": "9085886cde13e170a065d02734454eb9a53b8e13",
            "filename": "docs/source/en/model_doc/prophetnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fprophetnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fprophetnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fprophetnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\n+The ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\n Zhang, Ming Zhou on 13 Jan, 2020.\n \n ProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of just"
        },
        {
            "sha": "4b221c9791ff7fea3bddb187765f4e3703e791e0",
            "filename": "docs/source/en/model_doc/pvt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -19,7 +19,7 @@ specific language governing permissions and limitations under the License.\n ## Overview\n \n The PVT model was proposed in\n-[Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://arxiv.org/abs/2102.12122)\n+[Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions](https://huggingface.co/papers/2102.12122)\n by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao. The PVT is a type of\n vision transformer that utilizes a pyramid structure to make it an effective backbone for dense prediction tasks. Specifically\n it allows for more fine-grained inputs (4 x 4 pixels per patch) to be used, while simultaneously shrinking the sequence length"
        },
        {
            "sha": "b8ebe9198a1990ba3f066c8b9d7b3ffdb43cf502",
            "filename": "docs/source/en/model_doc/pvt_v2.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpvt_v2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -19,11 +19,11 @@ specific language governing permissions and limitations under the License.\n ## Overview\n \n The PVTv2 model was proposed in\n-[PVT v2: Improved Baselines with Pyramid Vision Transformer](https://arxiv.org/abs/2106.13797) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. As an improved variant of PVT, it eschews position embeddings, relying instead on positional information encoded through zero-padding and overlapping patch embeddings. This lack of reliance on position embeddings simplifies the architecture, and enables running inference at any resolution without needing to interpolate them.\n+[PVT v2: Improved Baselines with Pyramid Vision Transformer](https://huggingface.co/papers/2106.13797) by Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. As an improved variant of PVT, it eschews position embeddings, relying instead on positional information encoded through zero-padding and overlapping patch embeddings. This lack of reliance on position embeddings simplifies the architecture, and enables running inference at any resolution without needing to interpolate them.\n \n-The PVTv2 encoder structure has been successfully deployed to achieve state-of-the-art scores in [Segformer](https://arxiv.org/abs/2105.15203) for semantic segmentation, [GLPN](https://arxiv.org/abs/2201.07436) for monocular depth, and [Panoptic Segformer](https://arxiv.org/abs/2109.03814) for panoptic segmentation.\n+The PVTv2 encoder structure has been successfully deployed to achieve state-of-the-art scores in [Segformer](https://huggingface.co/papers/2105.15203) for semantic segmentation, [GLPN](https://huggingface.co/papers/2201.07436) for monocular depth, and [Panoptic Segformer](https://huggingface.co/papers/2109.03814) for panoptic segmentation.\n \n-PVTv2 belongs to a family of models called [hierarchical transformers](https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f) , which make adaptations to transformer layers in order to generate multi-scale feature maps. Unlike the columnal structure of Vision Transformer ([ViT](https://arxiv.org/abs/2010.11929)) which loses fine-grained detail, multi-scale feature maps are known preserve this detail and aid performance in dense prediction tasks. In the case of PVTv2, this is achieved by generating image patch tokens using 2D convolution with overlapping kernels in each encoder layer.\n+PVTv2 belongs to a family of models called [hierarchical transformers](https://natecibik.medium.com/the-rise-of-vision-transformers-f623c980419f) , which make adaptations to transformer layers in order to generate multi-scale feature maps. Unlike the columnal structure of Vision Transformer ([ViT](https://huggingface.co/papers/2010.11929)) which loses fine-grained detail, multi-scale feature maps are known preserve this detail and aid performance in dense prediction tasks. In the case of PVTv2, this is achieved by generating image patch tokens using 2D convolution with overlapping kernels in each encoder layer.\n \n The multi-scale features of hierarchical transformers allow them to be easily swapped in for traditional workhorse computer vision backbone models like ResNet in larger architectures. Both Segformer and Panoptic Segformer demonstrated that configurations using PVTv2 for a backbone consistently outperformed those with similarly sized ResNet backbones. \n \n@@ -39,8 +39,8 @@ This model was contributed by [FoamoftheSea](https://huggingface.co/FoamoftheSea\n \n ## Usage tips\n \n-- [PVTv2](https://arxiv.org/abs/2106.13797) is a hierarchical transformer model which has demonstrated powerful performance in image classification and multiple other tasks, used as a backbone for semantic segmentation in [Segformer](https://arxiv.org/abs/2105.15203), monocular depth estimation in [GLPN](https://arxiv.org/abs/2201.07436), and panoptic segmentation in [Panoptic Segformer](https://arxiv.org/abs/2109.03814), consistently showing higher performance than similar ResNet configurations.\n-- Hierarchical transformers like PVTv2 achieve superior data and parameter efficiency on image data compared with pure transformer architectures by incorporating design elements of convolutional neural networks (CNNs) into their encoders. This creates a best-of-both-worlds architecture that infuses the useful inductive biases of CNNs like translation equivariance and locality into the network while still enjoying the benefits of dynamic data response and global relationship modeling provided by the self-attention mechanism of [transformers](https://arxiv.org/abs/1706.03762).\n+- [PVTv2](https://huggingface.co/papers/2106.13797) is a hierarchical transformer model which has demonstrated powerful performance in image classification and multiple other tasks, used as a backbone for semantic segmentation in [Segformer](https://huggingface.co/papers/2105.15203), monocular depth estimation in [GLPN](https://huggingface.co/papers/2201.07436), and panoptic segmentation in [Panoptic Segformer](https://huggingface.co/papers/2109.03814), consistently showing higher performance than similar ResNet configurations.\n+- Hierarchical transformers like PVTv2 achieve superior data and parameter efficiency on image data compared with pure transformer architectures by incorporating design elements of convolutional neural networks (CNNs) into their encoders. This creates a best-of-both-worlds architecture that infuses the useful inductive biases of CNNs like translation equivariance and locality into the network while still enjoying the benefits of dynamic data response and global relationship modeling provided by the self-attention mechanism of [transformers](https://huggingface.co/papers/1706.03762).\n - PVTv2 uses overlapping patch embeddings to create multi-scale feature maps, which are infused with location information using zero-padding and depth-wise convolutions.\n - To reduce the complexity in the attention layers, PVTv2 performs a spatial reduction on the hidden states using either strided 2D convolution (SRA) or fixed-size average pooling (Linear SRA). Although inherently more lossy, Linear SRA provides impressive performance with a linear complexity with respect to image size. To use Linear SRA in the self-attention layers, set `linear_attention=True` in the `PvtV2Config`.\n - [`PvtV2Model`] is the hierarchical transformer encoder (which is also often referred to as Mix Transformer or MiT in the literature). [`PvtV2ForImageClassification`] adds a simple classifier head on top to perform Image Classification. [`PvtV2Backbone`] can be used with the [`AutoBackbone`] system in larger architectures like Deformable DETR."
        },
        {
            "sha": "64e00d6a430540e306f1c68ca50a78ce57eacbad",
            "filename": "docs/source/en/model_doc/qdqbert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -31,7 +31,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n ## Overview\n \n The QDQBERT model can be referenced in [Integer Quantization for Deep Learning Inference: Principles and Empirical\n-Evaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius\n+Evaluation](https://huggingface.co/papers/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius\n Micikevicius.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "22e1effd271d794f24d3f1620bb77c090118997d",
            "filename": "docs/source/en/model_doc/qwen2_audio.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_audio.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -29,7 +29,7 @@ The Qwen2-Audio is the new model series of large audio-language models from the\n * voice chat: users can freely engage in voice interactions with Qwen2-Audio without text input\n * audio analysis: users could provide audio and text instructions for analysis during the interaction\n \n-It was proposed in [Qwen2-Audio Technical Report](https://arxiv.org/abs/2407.10759) by Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou.\n+It was proposed in [Qwen2-Audio Technical Report](https://huggingface.co/papers/2407.10759) by Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, Jingren Zhou.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "39ddbdc006aaf290aebaf2468a9714924b6de71f",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The [Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/) model is a major update to [Qwen-VL](https://arxiv.org/pdf/2308.12966) from the Qwen team at Alibaba Research. \n+The [Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/) model is a major update to [Qwen-VL](https://huggingface.co/papers/2308.12966) from the Qwen team at Alibaba Research. \n \n The abstract from the blog is the following:\n "
        },
        {
            "sha": "425d5c70d1dc22ce0baffb4bf9ba09b99d429d0a",
            "filename": "docs/source/en/model_doc/rag.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frag.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -29,7 +29,7 @@ sequence-to-sequence models. RAG models retrieve documents, pass them to a seq2s\n outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\n both retrieval and generation to adapt to downstream tasks.\n \n-It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\n+It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://huggingface.co/papers/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir\n Karpukhin, Naman Goyal, Heinrich Kรผttler, Mike Lewis, Wen-tau Yih, Tim Rocktรคschel, Sebastian Riedel, Douwe Kiela.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "efff6717d8984d7e11a9addde629ad315197862d",
            "filename": "docs/source/en/model_doc/realm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frealm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frealm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frealm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -30,7 +30,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The REALM model was proposed in [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It's a\n+The REALM model was proposed in [REALM: Retrieval-Augmented Language Model Pre-Training](https://huggingface.co/papers/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It's a\n retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then\n utilizes retrieved documents to process question answering tasks.\n "
        },
        {
            "sha": "e65c725d905228c1ca9f9c795d9334cf2cf80ba7",
            "filename": "docs/source/en/model_doc/reformer.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf) by Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n+The Reformer model was proposed in the paper [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451.pdf) by Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n \n The abstract from the paper is the following:\n \n@@ -93,7 +93,7 @@ length* of the `input_ids`.\n \n In Locality sensitive hashing (LSH) self attention the key and query projection weights are tied. Therefore, the key\n query embedding vectors are also tied. LSH self attention uses the locality sensitive hashing mechanism proposed in\n-[Practical and Optimal LSH for Angular Distance](https://arxiv.org/abs/1509.02897) to assign each of the tied key\n+[Practical and Optimal LSH for Angular Distance](https://huggingface.co/papers/1509.02897) to assign each of the tied key\n query embedding vectors to one of `config.num_buckets` possible buckets. The premise is that the more \"similar\"\n key query embedding vectors (in terms of *cosine similarity*) are to each other, the more likely they are assigned to\n the same bucket.\n@@ -105,7 +105,7 @@ each of length `config.lsh_chunk_length`. For each chunk, the query embedding ve\n (which are tied to themselves) and to the key embedding vectors of `config.lsh_num_chunks_before` previous\n neighboring chunks and `config.lsh_num_chunks_after` following neighboring chunks.\n \n-For more information, see the [original Paper](https://arxiv.org/abs/2001.04451) or this great [blog post](https://www.pragmatic.ml/reformer-deep-dive/).\n+For more information, see the [original Paper](https://huggingface.co/papers/2001.04451) or this great [blog post](https://www.pragmatic.ml/reformer-deep-dive/).\n \n Note that `config.num_buckets` can also be factorized into a list \\\\((n_{\\text{buckets}}^1,\n n_{\\text{buckets}}^2)\\\\). This way instead of assigning the query key embedding vectors to one of \\\\((1,\\ldots,"
        },
        {
            "sha": "a86176bcf2f1fe0a696122dc01c712786c1cd1a8",
            "filename": "docs/source/en/model_doc/regnet.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fregnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fregnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fregnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The RegNet model was proposed in [Designing Network Design Spaces](https://arxiv.org/abs/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n+The RegNet model was proposed in [Designing Network Design Spaces](https://huggingface.co/papers/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n \n The authors design search spaces to perform Neural Architecture Search (NAS). They first start from a high dimensional search space and iteratively reduce the search space by empirically applying constraints based on the best-performing models sampled by the current search space.\n \n@@ -37,7 +37,7 @@ This model was contributed by [Francesco](https://huggingface.co/Francesco). The\n was contributed by [sayakpaul](https://huggingface.co/sayakpaul) and [ariG23498](https://huggingface.co/ariG23498).\n The original code can be found [here](https://github.com/facebookresearch/pycls).\n \n-The huge 10B model from [Self-supervised Pretraining of Visual Features in the Wild](https://arxiv.org/abs/2103.01988), \n+The huge 10B model from [Self-supervised Pretraining of Visual Features in the Wild](https://huggingface.co/papers/2103.01988), \n trained on  one billion Instagram images, is available on the [hub](https://huggingface.co/facebook/regnet-y-10b-seer)\n \n ## Resources"
        },
        {
            "sha": "6cf0e35c2aae060a0343f63234f4244fc5b6ae22",
            "filename": "docs/source/en/model_doc/rembert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frembert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The RemBERT model was proposed in [Rethinking Embedding Coupling in Pre-trained Language Models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, Melvin Johnson, Sebastian Ruder.\n+The RemBERT model was proposed in [Rethinking Embedding Coupling in Pre-trained Language Models](https://huggingface.co/papers/2010.12821) by Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, Melvin Johnson, Sebastian Ruder.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "03ad0b0c32ed71068ea9acb5ebd2c0ac61e95500",
            "filename": "docs/source/en/model_doc/resnet.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fresnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fresnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fresnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The ResNet model was proposed in [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. Our implementation follows the small changes made by [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch), we apply the `stride=2` for downsampling in bottleneck's `3x3` conv and not in the first `1x1`. This is generally known as \"ResNet v1.5\".\n+The ResNet model was proposed in [Deep Residual Learning for Image Recognition](https://huggingface.co/papers/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun. Our implementation follows the small changes made by [Nvidia](https://catalog.ngc.nvidia.com/orgs/nvidia/resources/resnet_50_v1_5_for_pytorch), we apply the `stride=2` for downsampling in bottleneck's `3x3` conv and not in the first `1x1`. This is generally known as \"ResNet v1.5\".\n \n ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.\n \n@@ -34,7 +34,7 @@ The abstract from the paper is the following:\n *Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.\n The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.*\n \n-The figure below illustrates the architecture of ResNet. Taken from the [original paper](https://arxiv.org/abs/1512.03385).\n+The figure below illustrates the architecture of ResNet. Taken from the [original paper](https://huggingface.co/papers/1512.03385).\n \n <img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/resnet_architecture.png\"/>\n "
        },
        {
            "sha": "81b52fec02fe18efe87f193676dc5360afb99a07",
            "filename": "docs/source/en/model_doc/roberta-prelayernorm.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta-prelayernorm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta-prelayernorm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta-prelayernorm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The RoBERTa-PreLayerNorm model was proposed in [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.\n+The RoBERTa-PreLayerNorm model was proposed in [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://huggingface.co/papers/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.\n It is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/).\n \n The abstract from the paper is the following:\n@@ -37,7 +37,7 @@ The original code can be found [here](https://github.com/princeton-nlp/DinkyTrai\n \n ## Usage tips\n \n-- The implementation is the same as [Roberta](roberta) except instead of using _Add and Norm_ it does _Norm and Add_. _Add_ and _Norm_ refers to the Addition and LayerNormalization as described in [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n+- The implementation is the same as [Roberta](roberta) except instead of using _Add and Norm_ it does _Norm and Add_. _Add_ and _Norm_ refers to the Addition and LayerNormalization as described in [Attention Is All You Need](https://huggingface.co/papers/1706.03762).\n - This is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/).\n \n ## Resources"
        },
        {
            "sha": "cfbf31fceed845f0fd9538557954d788124dc082",
            "filename": "docs/source/en/model_doc/roberta.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The RoBERTa model was proposed in [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, [Myle Ott](https://huggingface.co/myleott), Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n+The RoBERTa model was proposed in [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://huggingface.co/papers/1907.11692) by Yinhan Liu, [Myle Ott](https://huggingface.co/myleott), Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\n Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google's BERT model released in 2018.\n \n It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with"
        },
        {
            "sha": "aeee1f4c035ccc249b82034211499161ce92f472",
            "filename": "docs/source/en/model_doc/rt_detr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n \n-The RT-DETR model was proposed in [DETRs Beat YOLOs on Real-time Object Detection](https://arxiv.org/abs/2304.08069) by Wenyu Lv, Yian Zhao, Shangliang Xu, Jinman Wei, Guanzhong Wang, Cheng Cui, Yuning Du, Qingqing Dang, Yi Liu.\n+The RT-DETR model was proposed in [DETRs Beat YOLOs on Real-time Object Detection](https://huggingface.co/papers/2304.08069) by Wenyu Lv, Yian Zhao, Shangliang Xu, Jinman Wei, Guanzhong Wang, Cheng Cui, Yuning Du, Qingqing Dang, Yi Liu.\n \n RT-DETR is an object detection model that stands for \"Real-Time DEtection Transformer.\" This model is designed to perform object detection tasks with a focus on achieving real-time performance while maintaining high accuracy. Leveraging the transformer architecture, which has gained significant popularity in various fields of deep learning, RT-DETR processes images to identify and locate multiple objects within them.\n \n@@ -34,7 +34,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/rt_detr_overview.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> RT-DETR performance relative to YOLO models. Taken from the <a href=\"https://arxiv.org/abs/2304.08069\">original paper.</a> </small>\n+<small> RT-DETR performance relative to YOLO models. Taken from the <a href=\"https://huggingface.co/papers/2304.08069\">original paper.</a> </small>\n \n The model version was contributed by [rafaelpadilla](https://huggingface.co/rafaelpadilla) and [sangbumchoi](https://github.com/SangbumChoi). The original code can be found [here](https://github.com/lyuwenyu/RT-DETR/).\n "
        },
        {
            "sha": "6390d36b073844b09198563af93412add1423096",
            "filename": "docs/source/en/model_doc/rt_detr_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The RT-DETRv2 model was proposed in [RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer](https://arxiv.org/abs/2407.17140) by Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, Yi Liu.\n+The RT-DETRv2 model was proposed in [RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer](https://huggingface.co/papers/2407.17140) by Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, Yi Liu.\n \n RT-DETRv2 refines RT-DETR by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. These changes enhance flexibility and practicality while maintaining real-time performance.\n "
        },
        {
            "sha": "cf5273e0894da7f437e90c1c0ffca6a9a24fb02f",
            "filename": "docs/source/en/model_doc/sam.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-SAM (Segment Anything Model) was proposed in [Segment Anything](https://arxiv.org/pdf/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n+SAM (Segment Anything Model) was proposed in [Segment Anything](https://huggingface.co/papers/2304.02643v1.pdf) by Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick.\n \n The model can be used to predict segmentation masks of any object of interest given an input image. \n \n@@ -109,13 +109,13 @@ A list of official Hugging Face and community (indicated by ๐) resources to h\n \n ## SlimSAM\n \n-SlimSAM, a pruned version of SAM, was proposed in [0.1% Data Makes Segment Anything Slim](https://arxiv.org/abs/2312.05284) by Zigeng Chen et al. SlimSAM reduces the size of the SAM models considerably while maintaining the same performance.\n+SlimSAM, a pruned version of SAM, was proposed in [0.1% Data Makes Segment Anything Slim](https://huggingface.co/papers/2312.05284) by Zigeng Chen et al. SlimSAM reduces the size of the SAM models considerably while maintaining the same performance.\n \n Checkpoints can be found on the [hub](https://huggingface.co/models?other=slimsam), and they can be used as a drop-in replacement of SAM.\n \n ## Grounded SAM\n \n-One can combine [Grounding DINO](grounding-dino) with SAM for text-based mask generation as introduced in [Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks](https://arxiv.org/abs/2401.14159). You can refer to this [demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Grounding%20DINO/GroundingDINO_with_Segment_Anything.ipynb) ๐ for details.\n+One can combine [Grounding DINO](grounding-dino) with SAM for text-based mask generation as introduced in [Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks](https://huggingface.co/papers/2401.14159). You can refer to this [demo notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Grounding%20DINO/GroundingDINO_with_Segment_Anything.ipynb) ๐ for details.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/grounded_sam.png\"\n alt=\"drawing\" width=\"900\"/>"
        },
        {
            "sha": "8e8e4e559fa23e1d205bbdbe1d42cd0e5049c0f4",
            "filename": "docs/source/en/model_doc/sam_hq.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -2,7 +2,7 @@\n \n ## Overview\n \n-SAM-HQ (High-Quality Segment Anything Model) was proposed in [Segment Anything in High Quality](https://arxiv.org/pdf/2306.01567.pdf) by Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu.\n+SAM-HQ (High-Quality Segment Anything Model) was proposed in [Segment Anything in High Quality](https://huggingface.co/papers/2306.01567) by Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu.\n \n The model is an enhancement to the original SAM model that produces significantly higher quality segmentation masks while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability.\n "
        },
        {
            "sha": "1d42de0a544b5c39b002026b6e0eea5efb6fbee4",
            "filename": "docs/source/en/model_doc/seamless_m4t.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -132,7 +132,7 @@ Use `return_intermediate_token_ids=True` with [`SeamlessM4TModel`] to return bot\n \n SeamlessM4T features a versatile architecture that smoothly handles the sequential generation of text and speech. This setup comprises two sequence-to-sequence (seq2seq) models. The first model translates the input modality into translated text, while the second model generates speech tokens, known as \"unit tokens,\" from the translated text.\n \n-Each modality has its own dedicated encoder with a unique architecture. Additionally, for speech output, a vocoder inspired by the [HiFi-GAN](https://arxiv.org/abs/2010.05646) architecture is placed on top of the second seq2seq model.\n+Each modality has its own dedicated encoder with a unique architecture. Additionally, for speech output, a vocoder inspired by the [HiFi-GAN](https://huggingface.co/papers/2010.05646) architecture is placed on top of the second seq2seq model.\n \n Here's how the generation process works:\n "
        },
        {
            "sha": "7898799ee44fb4769fd2940d6b97dfe8b8c81997",
            "filename": "docs/source/en/model_doc/seamless_m4t_v2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -131,7 +131,7 @@ Use `return_intermediate_token_ids=True` with [`SeamlessM4Tv2Model`] to return b\n \n SeamlessM4T-v2 features a versatile architecture that smoothly handles the sequential generation of text and speech. This setup comprises two sequence-to-sequence (seq2seq) models. The first model translates the input modality into translated text, while the second model generates speech tokens, known as \"unit tokens,\" from the translated text.\n \n-Each modality has its own dedicated encoder with a unique architecture. Additionally, for speech output, a vocoder inspired by the [HiFi-GAN](https://arxiv.org/abs/2010.05646) architecture is placed on top of the second seq2seq model.\n+Each modality has its own dedicated encoder with a unique architecture. Additionally, for speech output, a vocoder inspired by the [HiFi-GAN](https://huggingface.co/papers/2010.05646) architecture is placed on top of the second seq2seq model.\n \n ### Difference with SeamlessM4T-v1\n \n@@ -148,7 +148,7 @@ The second seq2seq model, named text-to-unit model, is now non-auto regressive,\n \n The speech encoder, which is used during the first-pass generation process to predict the translated text, differs mainly from the previous speech encoder through these mechanisms:\n - the use of chunked attention mask to prevent attention across chunks, ensuring that each position attends only to positions within its own chunk and a fixed number of previous chunks.\n-- the use of relative position embeddings which only considers distance between sequence elements rather than absolute positions. Please refer to [Self-Attentionwith Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155) for more details.\n+- the use of relative position embeddings which only considers distance between sequence elements rather than absolute positions. Please refer to [Self-Attentionwith Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155) for more details.\n - the use of a causal depth-wise convolution instead of a non-causal one.\n \n ### Generation process"
        },
        {
            "sha": "5bcb8ca2fc5bd1bd86194ba5845970f43d71956a",
            "filename": "docs/source/en/model_doc/segformer.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsegformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The SegFormer model was proposed in [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping\n+The SegFormer model was proposed in [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://huggingface.co/papers/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping\n Luo. The model consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great\n results on image segmentation benchmarks such as ADE20K and Cityscapes.\n \n@@ -41,7 +41,7 @@ and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50\n being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on\n Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.*\n \n-The figure below illustrates the architecture of SegFormer. Taken from the [original paper](https://arxiv.org/abs/2105.15203).\n+The figure below illustrates the architecture of SegFormer. Taken from the [original paper](https://huggingface.co/papers/2105.15203).\n \n <img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/segformer_architecture.png\"/>\n \n@@ -79,7 +79,7 @@ of the model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). T\n   background class and include this class as part of all labels. In that case, `do_reduce_labels` should be set to\n   `False`, as loss should also be computed for the background class.\n - As most models, SegFormer comes in different sizes, the details of which can be found in the table below\n-  (taken from Table 7 of the [original paper](https://arxiv.org/abs/2105.15203)).\n+  (taken from Table 7 of the [original paper](https://huggingface.co/papers/2105.15203)).\n \n | **Model variant** | **Depths**    | **Hidden sizes**    | **Decoder hidden size** | **Params (M)** | **ImageNet-1k Top 1** |\n | :---------------: | ------------- | ------------------- | :---------------------: | :------------: | :-------------------: |\n@@ -91,7 +91,7 @@ of the model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). T\n | MiT-b5            | [3, 6, 40, 3] | [64, 128, 320, 512] | 768                     | 82.0           | 83.8                  |\n \n Note that MiT in the above table refers to the Mix Transformer encoder backbone introduced in SegFormer. For\n-SegFormer's results on the segmentation datasets like ADE20k, refer to the [paper](https://arxiv.org/abs/2105.15203).\n+SegFormer's results on the segmentation datasets like ADE20k, refer to the [paper](https://huggingface.co/papers/2105.15203).\n \n ## Resources\n "
        },
        {
            "sha": "89f80871acf728e229a886eb14e4d287e187d710",
            "filename": "docs/source/en/model_doc/seggpt.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseggpt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The SegGPT model was proposed in [SegGPT: Segmenting Everything In Context](https://arxiv.org/abs/2304.03284) by Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. SegGPT employs a decoder-only Transformer that can generate a segmentation mask given an input image, a prompt image and its corresponding prompt mask. The model achieves remarkable one-shot results with 56.1 mIoU on COCO-20 and 85.6 mIoU on FSS-1000.\n+The SegGPT model was proposed in [SegGPT: Segmenting Everything In Context](https://huggingface.co/papers/2304.03284) by Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, Tiejun Huang. SegGPT employs a decoder-only Transformer that can generate a segmentation mask given an input image, a prompt image and its corresponding prompt mask. The model achieves remarkable one-shot results with 56.1 mIoU on COCO-20 and 85.6 mIoU on FSS-1000.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "a6648d2980e573047be6f39146373f8eceb807e7",
            "filename": "docs/source/en/model_doc/sew-d.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew-d.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew-d.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew-d.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n SEW-D (Squeezed and Efficient Wav2Vec with Disentangled attention) was proposed in [Performance-Efficiency Trade-offs\n-in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim,\n+in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim,\n Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "865b4943c3ee8d780693fba62ccd1bae7b4cabe8",
            "filename": "docs/source/en/model_doc/sew.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsew.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n SEW (Squeezed and Efficient Wav2Vec) was proposed in [Performance-Efficiency Trade-offs in Unsupervised Pre-training\n-for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q.\n+for Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q.\n Weinberger, Yoav Artzi.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "0e53418a7336d49288a7975572ee1b060aaa7b74",
            "filename": "docs/source/en/model_doc/shieldgemma2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -19,7 +19,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The ShieldGemma 2 model was proposed in a [technical report](https://arxiv.org/abs/2504.01081) by Google. ShieldGemma 2, built on [Gemma 3](https://ai.google.dev/gemma/docs/core/model_card_3), is a 4 billion (4B) parameter model that checks the safety of both synthetic and natural images against key categories to help you build robust datasets and models. With this addition to the Gemma family of models, researchers and developers can now easily minimize the risk of harmful content in their models across key areas of harm as defined below:\n+The ShieldGemma 2 model was proposed in a [technical report](https://huggingface.co/papers/2504.01081) by Google. ShieldGemma 2, built on [Gemma 3](https://ai.google.dev/gemma/docs/core/model_card_3), is a 4 billion (4B) parameter model that checks the safety of both synthetic and natural images against key categories to help you build robust datasets and models. With this addition to the Gemma family of models, researchers and developers can now easily minimize the risk of harmful content in their models across key areas of harm as defined below:\n \n -   No Sexually Explicit content: The image shall not contain content that depicts explicit or graphic sexual acts (e.g., pornography, erotic nudity, depictions of rape or sexual assault).\n -   No Dangerous Content: The image shall not contain content that facilitates or encourages activities that could cause real-world harm (e.g., building firearms and explosive devices, promotion of terrorism, instructions for suicide)."
        },
        {
            "sha": "52f6634f9fb8bfd01daea3eab529ff589d575459",
            "filename": "docs/source/en/model_doc/speech-encoder-decoder.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech-encoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech-encoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech-encoder-decoder.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -29,7 +29,7 @@ with any pretrained speech autoencoding model as the encoder (*e.g.* [Wav2Vec2](\n \n The effectiveness of initializing speech-sequence-to-text-sequence models with pretrained checkpoints for speech\n recognition and speech translation has *e.g.* been shown in [Large-Scale Self- and Semi-Supervised Learning for Speech\n-Translation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli,\n+Translation](https://huggingface.co/papers/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli,\n Alexis Conneau.\n \n An example of how to use a [`SpeechEncoderDecoderModel`] for inference can be seen in [Speech2Text2](speech_to_text_2)."
        },
        {
            "sha": "1b6c74892fae702334d6339399093fb7d7a121b4",
            "filename": "docs/source/en/model_doc/speech_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Speech2Text model was proposed in [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It's a\n+The Speech2Text model was proposed in [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://huggingface.co/papers/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It's a\n transformer-based seq2seq (encoder-decoder) model designed for end-to-end Automatic Speech Recognition (ASR) and Speech\n Translation (ST). It uses a convolutional downsampler to reduce the length of speech inputs by 3/4th before they are\n fed into the encoder. The model is trained with standard autoregressive cross-entropy loss and generates the"
        },
        {
            "sha": "8caf774e733db16cef929db72fc783992c22324e",
            "filename": "docs/source/en/model_doc/speech_to_text_2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text_2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text_2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text_2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The Speech2Text2 model is used together with [Wav2Vec2](wav2vec2) for Speech Translation models proposed in\n-[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by\n+[Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://huggingface.co/papers/2104.06678) by\n Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n \n Speech2Text2 is a *decoder-only* transformer model that can be used with any speech *encoder-only*, such as"
        },
        {
            "sha": "d41a583d7a6cd1de55f7178f347afa888979cc76",
            "filename": "docs/source/en/model_doc/speecht5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeecht5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeecht5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeecht5.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The SpeechT5 model was proposed in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n+The SpeechT5 model was proposed in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://huggingface.co/papers/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "74e9ffc25081b8ce8cbc162749002486ceb27b10",
            "filename": "docs/source/en/model_doc/splinter.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsplinter.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsplinter.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsplinter.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Splinter model was proposed in [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy. Splinter\n+The Splinter model was proposed in [Few-Shot Question Answering by Pretraining Span Selection](https://huggingface.co/papers/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy. Splinter\n is an encoder-only transformer (similar to BERT) pretrained using the recurring span selection task on a large corpus\n comprising Wikipedia and the Toronto Book Corpus.\n "
        },
        {
            "sha": "2b918782962818248a2a0a595468f3883fba55e1",
            "filename": "docs/source/en/model_doc/squeezebert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsqueezebert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsqueezebert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsqueezebert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\n+The SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://huggingface.co/papers/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\n bidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\n SqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\n instead of fully-connected layers for the Q, K, V and FFN layers."
        },
        {
            "sha": "61e70b18fd8b345406d969af4729c63387722fd6",
            "filename": "docs/source/en/model_doc/starcoder2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstarcoder2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-StarCoder2 is a family of open LLMs for code and comes in 3 different sizes with 3B, 7B and 15B parameters. The flagship StarCoder2-15B model is trained on over 4 trillion tokens and 600+ programming languages from The Stack v2. All models use Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and were trained using the Fill-in-the-Middle objective. The models have been released with the paper [StarCoder 2 and The Stack v2: The Next Generation](https://arxiv.org/abs/2402.19173) by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauร, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muรฑoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.\n+StarCoder2 is a family of open LLMs for code and comes in 3 different sizes with 3B, 7B and 15B parameters. The flagship StarCoder2-15B model is trained on over 4 trillion tokens and 600+ programming languages from The Stack v2. All models use Grouped Query Attention, a context window of 16,384 tokens with a sliding window attention of 4,096 tokens, and were trained using the Fill-in-the-Middle objective. The models have been released with the paper [StarCoder 2 and The Stack v2: The Next Generation](https://huggingface.co/papers/2402.19173) by Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauร, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muรฑoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries.\n \n The abstract of the paper is the following:\n "
        },
        {
            "sha": "38a5d2d888db79d650e0322ecc1928243ebda57f",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -21,7 +21,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The SuperGlue model was proposed in [SuperGlue: Learning Feature Matching with Graph Neural Networks](https://arxiv.org/abs/1911.11763) by Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich.\n+The SuperGlue model was proposed in [SuperGlue: Learning Feature Matching with Graph Neural Networks](https://huggingface.co/papers/1911.11763) by Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich.\n \n This model consists of matching two sets of interest points detected in an image. Paired with the \n [SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and "
        },
        {
            "sha": "aa22d30961ad88de644466c864f14704c2027b9e",
            "filename": "docs/source/en/model_doc/superpoint.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The SuperPoint model was proposed\n-in [SuperPoint: Self-Supervised Interest Point Detection and Description](https://arxiv.org/abs/1712.07629) by Daniel\n+in [SuperPoint: Self-Supervised Interest Point Detection and Description](https://huggingface.co/papers/1712.07629) by Daniel\n DeTone, Tomasz Malisiewicz and Andrew Rabinovich.\n \n This model is the result of a self-supervised training of a fully-convolutional network for interest point detection and\n@@ -45,7 +45,7 @@ when compared to LIFT, SIFT and ORB.*\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/superpoint_architecture.png\"\n alt=\"drawing\" width=\"500\"/>\n \n-<small> SuperPoint overview. Taken from the <a href=\"https://arxiv.org/abs/1712.07629v4\">original paper.</a> </small>\n+<small> SuperPoint overview. Taken from the <a href=\"https://huggingface.co/papers/1712.07629v4\">original paper.</a> </small>\n \n ## Usage tips\n "
        },
        {
            "sha": "5f9c38d614cd5f30d0fc6859c0a87729ae968eac",
            "filename": "docs/source/en/model_doc/swiftformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fswiftformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fswiftformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswiftformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The SwiftFormer model was proposed in [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/abs/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.\n+The SwiftFormer model was proposed in [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://huggingface.co/papers/2303.15446) by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.\n \n The SwiftFormer paper introduces a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations in the self-attention computation with linear element-wise multiplications. A series of models called 'SwiftFormer' is built based on this, which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Even their small variant achieves 78.5% top-1 ImageNet1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2ร faster compared to MobileViT-v2.\n "
        },
        {
            "sha": "340594b80ee56b15d729a4111c2275c5d1c7ae3b",
            "filename": "docs/source/en/model_doc/swin2sr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin2sr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin2sr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin2sr.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Swin2SR model was proposed in [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.\n+The Swin2SR model was proposed in [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://huggingface.co/papers/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.\n Swin2SR improves the [SwinIR](https://github.com/JingyunLiang/SwinIR/) model by incorporating [Swin Transformer v2](swinv2) layers which mitigates issues such as training instability, resolution gaps between pre-training\n and fine-tuning, and hunger on data.\n \n@@ -34,7 +34,7 @@ In this paper, we explore the novel Swin Transformer V2, to improve SwinIR for i\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/swin2sr_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Swin2SR architecture. Taken from the <a href=\"https://arxiv.org/abs/2209.11345\">original paper.</a> </small>\n+<small> Swin2SR architecture. Taken from the <a href=\"https://huggingface.co/papers/2209.11345\">original paper.</a> </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/mv-lab/swin2sr)."
        },
        {
            "sha": "8854bcc94154baf7e4043961750476073ad1b040",
            "filename": "docs/source/en/model_doc/switch_transformers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The SwitchTransformers model was proposed in [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\n+The SwitchTransformers model was proposed in [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://huggingface.co/papers/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\n \n The Switch Transformer model uses a sparse T5 encoder-decoder architecture, where the MLP are replaced by a Mixture of Experts (MoE). A routing mechanism (top 1 in this case) associates each token to one of the expert, where each expert is a dense MLP. While switch transformers have a lot more weights than their equivalent dense models, the sparsity allows better scaling and better finetuning performance at scale.\n During a forward pass, only a fraction of the weights are used. The routing mechanism allows the model to select relevant weights on the fly which increases the model capacity without increasing the number of operations."
        },
        {
            "sha": "7f10f30243b16aed9fa26fb56fcbb9b414cc5a68",
            "filename": "docs/source/en/model_doc/t5v1.1.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5v1.1.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5v1.1.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5v1.1.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -42,7 +42,7 @@ One can directly plug in the weights of T5v1.1 into a T5 model, like so:\n \n T5 Version 1.1 includes the following improvements compared to the original T5 model:\n \n-- GEGLU activation in the feed-forward hidden layer, rather than ReLU. See [this paper](https://arxiv.org/abs/2002.05202).\n+- GEGLU activation in the feed-forward hidden layer, rather than ReLU. See [this paper](https://huggingface.co/papers/2002.05202).\n \n - Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.\n "
        },
        {
            "sha": "534ab49c641d4745cb1f538c3f4269979b3150a7",
            "filename": "docs/source/en/model_doc/table-transformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftable-transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftable-transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftable-transformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Table Transformer model was proposed in [PubTables-1M: Towards comprehensive table extraction from unstructured documents](https://arxiv.org/abs/2110.00061) by\n+The Table Transformer model was proposed in [PubTables-1M: Towards comprehensive table extraction from unstructured documents](https://huggingface.co/papers/2110.00061) by\n Brandon Smock, Rohith Pesala, Robin Abraham. The authors introduce a new dataset, PubTables-1M, to benchmark progress in table extraction from unstructured documents,\n as well as table structure recognition and functional analysis. The authors train 2 [DETR](detr) models, one for table detection and one for table structure recognition, dubbed Table Transformers.\n \n@@ -40,7 +40,7 @@ special customization for these tasks.*\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/table_transformer_architecture.jpeg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Table detection and table structure recognition clarified. Taken from the <a href=\"https://arxiv.org/abs/2110.00061\">original paper</a>. </small>\n+<small> Table detection and table structure recognition clarified. Taken from the <a href=\"https://huggingface.co/papers/2110.00061\">original paper</a>. </small>\n \n The authors released 2 models, one for [table detection](https://huggingface.co/microsoft/table-transformer-detection) in \n documents, one for [table structure recognition](https://huggingface.co/microsoft/table-transformer-structure-recognition) "
        },
        {
            "sha": "9694b098eafd79b7c147535ac89dd5bc50957557",
            "filename": "docs/source/en/model_doc/tapex.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftapex.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -34,7 +34,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The TAPEX model was proposed in [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu,\n+The TAPEX model was proposed in [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://huggingface.co/papers/2107.07653) by Qian Liu,\n Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. TAPEX pre-trains a BART model to solve synthetic SQL queries, after\n which it can be fine-tuned to answer natural language questions related to tabular data, as well as performing table fact checking. \n "
        },
        {
            "sha": "f14cd2e94181bfdedbc19e13f9c4b82561c17bc8",
            "filename": "docs/source/en/model_doc/textnet.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,12 +22,12 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The TextNet model was proposed in [FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation](https://arxiv.org/abs/2111.02394) by Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu. TextNet is a vision backbone useful for text detection tasks. It is the result of neural architecture search (NAS) on backbones with reward function as text detection task (to provide powerful features for text detection).\n+The TextNet model was proposed in [FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation](https://huggingface.co/papers/2111.02394) by Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu. TextNet is a vision backbone useful for text detection tasks. It is the result of neural architecture search (NAS) on backbones with reward function as text detection task (to provide powerful features for text detection).\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/fast_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> TextNet backbone as part of FAST. Taken from the <a href=\"https://arxiv.org/abs/2111.02394\">original paper.</a> </small>\n+<small> TextNet backbone as part of FAST. Taken from the <a href=\"https://huggingface.co/papers/2111.02394\">original paper.</a> </small>\n \n This model was contributed by [Raghavan](https://huggingface.co/Raghavan), [jadechoghari](https://huggingface.co/jadechoghari) and [nielsr](https://huggingface.co/nielsr).\n "
        },
        {
            "sha": "c39a63a6687f878b6cafcb409ee7a7fd8c7a3314",
            "filename": "docs/source/en/model_doc/timesformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftimesformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The TimeSformer model was proposed in [TimeSformer: Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Facebook Research.\n+The TimeSformer model was proposed in [TimeSformer: Is Space-Time Attention All You Need for Video Understanding?](https://huggingface.co/papers/2102.05095) by Facebook Research.\n This work is a milestone in action-recognition field being the first video transformer. It inspired many transformer based video understanding and classification papers.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "a2353c94148e95534e99248836b90d2836b435db",
            "filename": "docs/source/en/model_doc/trajectory_transformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrajectory_transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrajectory_transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrajectory_transformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -31,7 +31,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The Trajectory Transformer model was proposed in [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039)  by Michael Janner, Qiyang Li, Sergey Levine.\n+The Trajectory Transformer model was proposed in [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://huggingface.co/papers/2106.02039)  by Michael Janner, Qiyang Li, Sergey Levine.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "66f249f24e0aa41964c45217e12c94835d409fc1",
            "filename": "docs/source/en/model_doc/transfo-xl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftransfo-xl.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -61,7 +61,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The Transformer-XL model was proposed in [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan\n+The Transformer-XL model was proposed in [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://huggingface.co/papers/1901.02860) by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan\n Salakhutdinov. It's a causal (uni-directional) transformer with relative positioning (sinusoรฏdal) embeddings which can\n reuse previously computed hidden-states to attend to longer context (memory). This model also uses adaptive softmax\n inputs and outputs (tied)."
        },
        {
            "sha": "420398376a5d6ec4c115b9d1d530da4680587c28",
            "filename": "docs/source/en/model_doc/trocr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrocr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrocr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftrocr.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ specific language governing permissions and limitations under the License. -->\n ## Overview\n \n The TrOCR model was proposed in [TrOCR: Transformer-based Optical Character Recognition with Pre-trained\n-Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\n+Models](https://huggingface.co/papers/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\n Zhoujun Li, Furu Wei. TrOCR consists of an image Transformer encoder and an autoregressive text Transformer decoder to\n perform [optical character recognition (OCR)](https://en.wikipedia.org/wiki/Optical_character_recognition).\n \n@@ -40,7 +40,7 @@ tasks.*\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/trocr_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> TrOCR architecture. Taken from the <a href=\"https://arxiv.org/abs/2109.10282\">original paper</a>. </small>\n+<small> TrOCR architecture. Taken from the <a href=\"https://huggingface.co/papers/2109.10282\">original paper</a>. </small>\n \n Please refer to the [`VisionEncoderDecoder`] class on how to use this model.\n "
        },
        {
            "sha": "949c8549f5a3e655e061d90c6077d86e2240a374",
            "filename": "docs/source/en/model_doc/tvlt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvlt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvlt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvlt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -30,7 +30,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The TVLT model was proposed in [TVLT: Textless Vision-Language Transformer](https://arxiv.org/abs/2209.14156)\n+The TVLT model was proposed in [TVLT: Textless Vision-Language Transformer](https://huggingface.co/papers/2209.14156)\n by Zineng Tang, Jaemin Cho, Yixin Nie, Mohit Bansal (the first three authors contributed equally). The Textless Vision-Language Transformer (TVLT) is a model that uses raw visual and audio inputs for vision-and-language representation learning, without using text-specific modules such as tokenization or automatic speech recognition (ASR). It can perform various audiovisual and vision-language tasks like retrieval, question answering, etc.\n \n The abstract from the paper is the following:\n@@ -42,7 +42,7 @@ The abstract from the paper is the following:\n alt=\"drawing\" width=\"600\"/>\n </p>\n \n-<small> TVLT architecture. Taken from the <a href=\"[https://arxiv.org/abs/2102.03334](https://arxiv.org/abs/2209.14156)\">original paper</a>. </small>\n+<small> TVLT architecture. Taken from the <a href=\"[https://huggingface.co/papers/2102.03334](https://huggingface.co/papers/2209.14156)\">original paper</a>. </small>\n \n The original code can be found [here](https://github.com/zinengtang/TVLT). This model was contributed by [Zineng Tang](https://huggingface.co/ZinengTang).\n "
        },
        {
            "sha": "1b83ebfa6d0ac2af1c4e386d7e7e0f5f691518d2",
            "filename": "docs/source/en/model_doc/tvp.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The text-visual prompting (TVP) framework was proposed in the paper [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://arxiv.org/abs/2303.04995) by Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding.\n+The text-visual prompting (TVP) framework was proposed in the paper [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://huggingface.co/papers/2303.04995) by Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Ding.\n \n The abstract from the paper is the following:\n \n@@ -29,7 +29,7 @@ This research addresses temporal video grounding (TVG), which is the process of\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/tvp_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> TVP architecture. Taken from the <a href=\"https://arxiv.org/abs/2303.04995\">original paper.</a> </small>\n+<small> TVP architecture. Taken from the <a href=\"https://huggingface.co/papers/2303.04995\">original paper.</a> </small>\n \n This model was contributed by [Jiqing Feng](https://huggingface.co/Jiqing). The original code can be found [here](https://github.com/intel/TVP).\n \n@@ -162,7 +162,7 @@ Tips:\n \n - This implementation of TVP uses [`BertTokenizer`] to generate text embeddings and Resnet-50 model to compute visual embeddings.\n - Checkpoints for pre-trained [tvp-base](https://huggingface.co/Intel/tvp-base) is released.\n-- Please refer to [Table 2](https://arxiv.org/pdf/2303.04995.pdf) for TVP's performance on Temporal Video Grounding task.\n+- Please refer to [Table 2](https://huggingface.co/papers/2303.04995) for TVP's performance on Temporal Video Grounding task.\n \n \n ## TvpConfig"
        },
        {
            "sha": "fd2a70d7ecfaf17d43a4afcaab853af214ab86a1",
            "filename": "docs/source/en/model_doc/udop.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fudop.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fudop.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fudop.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The UDOP model was proposed in [Unifying Vision, Text, and Layout for Universal Document Processing](https://arxiv.org/abs/2212.02623) by Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal.\n+The UDOP model was proposed in [Unifying Vision, Text, and Layout for Universal Document Processing](https://huggingface.co/papers/2212.02623) by Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, Mohit Bansal.\n UDOP adopts an encoder-decoder Transformer architecture based on [T5](t5) for document AI tasks like document image classification, document parsing and document visual question answering.\n \n The abstract from the paper is the following:\n@@ -28,7 +28,7 @@ We propose Universal Document Processing (UDOP), a foundation Document AI model\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/udop_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> UDOP architecture. Taken from the <a href=\"https://arxiv.org/abs/2212.02623\">original paper.</a> </small>\n+<small> UDOP architecture. Taken from the <a href=\"https://huggingface.co/papers/2212.02623\">original paper.</a> </small>\n \n ## Usage tips\n \n@@ -64,7 +64,7 @@ One can use [`UdopProcessor`] to prepare images and text for the model, which ta\n \n - If using an own OCR engine of choice, one recommendation is Azure's [Read API](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/how-to/call-read-api), which supports so-called line segments. Use of segment position embeddings typically results in better performance.\n - At inference time, it's recommended to use the `generate` method to autoregressively generate text given a document image.\n-- The model has been pre-trained on both self-supervised and supervised objectives. One can use the various task prefixes (prompts) used during pre-training to test out the out-of-the-box capabilities. For instance, the model can be prompted with \"Question answering. What is the date?\", as \"Question answering.\" is the task prefix used during pre-training for DocVQA. Refer to the [paper](https://arxiv.org/abs/2212.02623) (table 1) for all task prefixes.\n+- The model has been pre-trained on both self-supervised and supervised objectives. One can use the various task prefixes (prompts) used during pre-training to test out the out-of-the-box capabilities. For instance, the model can be prompted with \"Question answering. What is the date?\", as \"Question answering.\" is the task prefix used during pre-training for DocVQA. Refer to the [paper](https://huggingface.co/papers/2212.02623) (table 1) for all task prefixes.\n - One can also fine-tune [`UdopEncoderModel`], which is the encoder-only part of UDOP, which can be seen as a LayoutLMv3-like Transformer encoder. For discriminative tasks, one can just add a linear classifier on top of it and fine-tune it on a labeled dataset.\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr)."
        },
        {
            "sha": "b3c1a222606267f477fecb891adb98ff8c7f76fc",
            "filename": "docs/source/en/model_doc/ul2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ful2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Ful2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ful2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The T5 model was presented in [Unifying Language Learning Paradigms](https://arxiv.org/pdf/2205.05131v1.pdf) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.\n+The T5 model was presented in [Unifying Language Learning Paradigms](https://huggingface.co/papers/2205.05131) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "8d0adb8e7813ab26ecad5f7c52180121cf6626a2",
            "filename": "docs/source/en/model_doc/unispeech-sat.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The UniSpeech-SAT model was proposed in [UniSpeech-SAT: Universal Speech Representation Learning with Speaker Aware\n-Pre-Training](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen,\n+Pre-Training](https://huggingface.co/papers/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen,\n Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu .\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "a83f7600d51554c4cdb28349f648fac8b5b34fad",
            "filename": "docs/source/en/model_doc/unispeech.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The UniSpeech model was proposed in [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael\n+The UniSpeech model was proposed in [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://huggingface.co/papers/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael\n Zeng, Xuedong Huang .\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "57492dcd68c3e6c40a4665de3e3c8d80507e4808",
            "filename": "docs/source/en/model_doc/univnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Funivnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Funivnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funivnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The UnivNet model was proposed in [UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation](https://arxiv.org/abs/2106.07889) by Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kin, and Juntae Kim.\n+The UnivNet model was proposed in [UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation](https://huggingface.co/papers/2106.07889) by Won Jang, Dan Lim, Jaesam Yoon, Bongwan Kin, and Juntae Kim.\n The UnivNet model is a generative adversarial network (GAN) trained to synthesize high fidelity speech waveforms. The UnivNet model shared in `transformers` is the *generator*, which maps a conditioning log-mel spectrogram and optional noise sequence to a speech waveform (e.g. a vocoder). Only the generator is required for inference. The *discriminator* used to train the `generator` is not implemented.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "e215ec86210e10cbd14979f35f3a0399734eec1e",
            "filename": "docs/source/en/model_doc/upernet.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fupernet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fupernet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fupernet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The UPerNet model was proposed in [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221)\n+The UPerNet model was proposed in [Unified Perceptual Parsing for Scene Understanding](https://huggingface.co/papers/1807.10221)\n by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. UPerNet is a general framework to effectively segment\n a wide range of concepts from images, leveraging any vision backbone like [ConvNeXt](convnext) or [Swin](swin).\n \n@@ -33,7 +33,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/upernet_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> UPerNet framework. Taken from the <a href=\"https://arxiv.org/abs/1807.10221\">original paper</a>. </small>\n+<small> UPerNet framework. Taken from the <a href=\"https://huggingface.co/papers/1807.10221\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code is based on OpenMMLab's mmsegmentation [here](https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py).\n "
        },
        {
            "sha": "0a2569182376a2d3d35643bfc7ad41d187349b74",
            "filename": "docs/source/en/model_doc/van.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvan.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvan.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvan.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -31,7 +31,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The VAN model was proposed in [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n+The VAN model was proposed in [Visual Attention Network](https://huggingface.co/papers/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n \n This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations.\n \n@@ -43,7 +43,7 @@ Tips:\n \n - VAN does not have an embedding layer, thus the `hidden_states` will have a length equal to the number of stages.\n \n-The figure below illustrates the architecture of a Visual Attention Layer. Taken from the [original paper](https://arxiv.org/abs/2202.09741).\n+The figure below illustrates the architecture of a Visual Attention Layer. Taken from the [original paper](https://huggingface.co/papers/2202.09741).\n \n <img width=\"600\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/van_architecture.png\"/>\n "
        },
        {
            "sha": "a9282bdad0af808b9cba4ec6b8c5cd359d69f8f3",
            "filename": "docs/source/en/model_doc/video_llava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n Video-LLaVa is an open-source multimodal LLM trained by fine-tuning LlamA/Vicuna on multimodal instruction-following data generated by Llava1.5 and VideChat. It is an auto-regressive language model, based on the transformer architecture. Video-LLaVa unifies visual representations to the language feature space, and enables an LLM to perform visual reasoning capabilities on both images and videos simultaneously.\n \n \n-The Video-LLaVA model was proposed in [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://arxiv.org/abs/2311.10122) by Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munang Ning, Peng Jin, Li Yuan.\n+The Video-LLaVA model was proposed in [Video-LLaVA: Learning United Visual Representation by Alignment Before Projection](https://huggingface.co/papers/2311.10122) by Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munang Ning, Peng Jin, Li Yuan.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "ac3d6c044e641ef2ffffe0d2a5a1124c514cfae7",
            "filename": "docs/source/en/model_doc/videomae.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The VideoMAE model was proposed in [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\n+The VideoMAE model was proposed in [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://huggingface.co/papers/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\n VideoMAE extends masked auto encoders ([MAE](vit_mae)) to video, claiming state-of-the-art performance on several video classification benchmarks.\n \n The abstract from the paper is the following:\n@@ -34,7 +34,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/videomae_architecture.jpeg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> VideoMAE pre-training. Taken from the <a href=\"https://arxiv.org/abs/2203.12602\">original paper</a>. </small>\n+<small> VideoMAE pre-training. Taken from the <a href=\"https://huggingface.co/papers/2203.12602\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/MCG-NJU/VideoMAE)."
        },
        {
            "sha": "19146e3846dcab0c6da252e1df6d31890f9b56a4",
            "filename": "docs/source/en/model_doc/vilt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvilt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvilt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvilt.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The ViLT model was proposed in [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334)\n+The ViLT model was proposed in [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://huggingface.co/papers/2102.03334)\n by Wonjae Kim, Bokyung Son, Ildoo Kim. ViLT incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design\n for Vision-and-Language Pre-training (VLP).\n \n@@ -41,7 +41,7 @@ times faster than previous VLP models, yet with competitive or better downstream\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vilt_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> ViLT architecture. Taken from the <a href=\"https://arxiv.org/abs/2102.03334\">original paper</a>. </small>\n+<small> ViLT architecture. Taken from the <a href=\"https://huggingface.co/papers/2102.03334\">original paper</a>. </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/dandelin/ViLT).\n "
        },
        {
            "sha": "c60b17204585adb80ba6548b6e6a3b96f4c9dced",
            "filename": "docs/source/en/model_doc/vipllava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The VipLlava model was proposed in [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.\n+The VipLlava model was proposed in [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://huggingface.co/papers/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.\n \n VipLlava enhances the training protocol of Llava by marking images and interact with the model using natural cues like a \"red bounding box\" or \"pointed arrow\" during training.\n "
        },
        {
            "sha": "53c573be47e94abe272361edb4e2ebe2e38b0460",
            "filename": "docs/source/en/model_doc/vision-encoder-decoder.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-encoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-encoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-encoder-decoder.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -32,7 +32,7 @@ pretrained Transformer-based vision model as the encoder (*e.g.* [ViT](vit), [BE\n and any pretrained language model as the decoder (*e.g.* [RoBERTa](roberta), [GPT2](gpt2), [BERT](bert), [DistilBERT](distilbert)).\n \n The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for\n-example) [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\n+example) [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://huggingface.co/papers/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,\n Zhoujun Li, Furu Wei.\n \n After such a [`VisionEncoderDecoderModel`] has been trained/fine-tuned, it can be saved/loaded just like any other models (see the examples below"
        },
        {
            "sha": "3106cb0ac3e686648d14bb51346b44ab9e62dc11",
            "filename": "docs/source/en/model_doc/vision-text-dual-encoder.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-text-dual-encoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-text-dual-encoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-text-dual-encoder.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -33,7 +33,7 @@ to a shared latent space. The projection layers are randomly initialized so the\n downstream task. This model can be used to align the vision-text embeddings using CLIP like contrastive image-text\n training and then can be used for zero-shot vision tasks such image-classification or retrieval.\n \n-In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how\n+In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://huggingface.co/papers/2111.07991) it is shown how\n leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement on\n new zero-shot vision tasks such as image classification or retrieval.\n "
        },
        {
            "sha": "02c98ba3d957b6411ae7054945b77aefa6937d2d",
            "filename": "docs/source/en/model_doc/visual_bert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvisual_bert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The VisualBERT model was proposed in [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n+The VisualBERT model was proposed in [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://huggingface.co/papers/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n VisualBERT is a neural network trained on a variety of (image, text) pairs.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "c268c2fad370eb0d8306f133baa83873ce46c2c5",
            "filename": "docs/source/en/model_doc/vit_hybrid.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -32,7 +32,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n ## Overview\n \n The hybrid Vision Transformer (ViT) model was proposed in [An Image is Worth 16x16 Words: Transformers for Image Recognition\n-at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\n+at Scale](https://huggingface.co/papers/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\n Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\n Uszkoreit, Neil Houlsby. It's the first paper that successfully trains a Transformer encoder on ImageNet, attaining\n very good results compared to familiar convolutional architectures. ViT hybrid is a slight variant of the [plain Vision Transformer](vit),"
        },
        {
            "sha": "8835f01cd8e4d0031148e77d9148847c954475ce",
            "filename": "docs/source/en/model_doc/vit_msn.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The ViTMSN model was proposed in [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes,\n+The ViTMSN model was proposed in [Masked Siamese Networks for Label-Efficient Learning](https://huggingface.co/papers/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes,\n Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas. The paper presents a joint-embedding architecture to match the prototypes\n of masked patches with that of the unmasked patches. With this setup, their method yields excellent performance in the low-shot and extreme low-shot\n regimes.\n@@ -41,7 +41,7 @@ and with 1% of ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a ne\n \n <img src=\"https://i.ibb.co/W6PQMdC/Screenshot-2022-09-13-at-9-08-40-AM.png\" alt=\"drawing\" width=\"600\"/> \n \n-<small> MSN architecture. Taken from the <a href=\"https://arxiv.org/abs/2204.07141\">original paper.</a> </small>\n+<small> MSN architecture. Taken from the <a href=\"https://huggingface.co/papers/2204.07141\">original paper.</a> </small>\n \n This model was contributed by [sayakpaul](https://huggingface.co/sayakpaul). The original code can be found [here](https://github.com/facebookresearch/msn). \n "
        },
        {
            "sha": "738d83461b12e9e3035993abf660891383e0363a",
            "filename": "docs/source/en/model_doc/vitdet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitdet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitdet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitdet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The ViTDet model was proposed in [Exploring Plain Vision Transformer Backbones for Object Detection](https://arxiv.org/abs/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.\n+The ViTDet model was proposed in [Exploring Plain Vision Transformer Backbones for Object Detection](https://huggingface.co/papers/2203.16527) by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.\n VitDet leverages the plain [Vision Transformer](vit) for the task of object detection.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "f661de16225b78541580ddf29caffd7f35e8b724",
            "filename": "docs/source/en/model_doc/vitmatte.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitmatte.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitmatte.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitmatte.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The ViTMatte model was proposed in [Boosting Image Matting with Pretrained Plain Vision Transformers](https://arxiv.org/abs/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.\n+The ViTMatte model was proposed in [Boosting Image Matting with Pretrained Plain Vision Transformers](https://huggingface.co/papers/2305.15272) by Jingfeng Yao, Xinggang Wang, Shusheng Yang, Baoyuan Wang.\n ViTMatte leverages plain [Vision Transformers](vit) for the task of image matting, which is the process of accurately estimating the foreground object in images and videos.\n \n The abstract from the paper is the following:\n@@ -31,7 +31,7 @@ The original code can be found [here](https://github.com/hustvl/ViTMatte).\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitmatte_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> ViTMatte high-level overview. Taken from the <a href=\"https://arxiv.org/abs/2305.15272\">original paper.</a> </small>\n+<small> ViTMatte high-level overview. Taken from the <a href=\"https://huggingface.co/papers/2305.15272\">original paper.</a> </small>\n \n ## Resources\n "
        },
        {
            "sha": "7a417cc2138285153980c9dfc6b5247b8f0c8750",
            "filename": "docs/source/en/model_doc/vitpose.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The ViTPose model was proposed in [ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://arxiv.org/abs/2204.12484) by Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao. ViTPose employs a standard, non-hierarchical [Vision Transformer](vit) as backbone for the task of keypoint estimation. A simple decoder head is added on top to predict the heatmaps from a given image. Despite its simplicity, the model gets state-of-the-art results on the challenging MS COCO Keypoint Detection benchmark. The model was further improved in [ViTPose++: Vision Transformer for Generic Body Pose Estimation](https://arxiv.org/abs/2212.04246) where the authors employ\n+The ViTPose model was proposed in [ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://huggingface.co/papers/2204.12484) by Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao. ViTPose employs a standard, non-hierarchical [Vision Transformer](vit) as backbone for the task of keypoint estimation. A simple decoder head is added on top to predict the heatmaps from a given image. Despite its simplicity, the model gets state-of-the-art results on the challenging MS COCO Keypoint Detection benchmark. The model was further improved in [ViTPose++: Vision Transformer for Generic Body Pose Estimation](https://huggingface.co/papers/2212.04246) where the authors employ\n a mixture-of-experts (MoE) module in the ViT backbone along with pre-training on more data, which further enhances the performance.\n \n The abstract from the paper is the following:\n@@ -28,7 +28,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> ViTPose architecture. Taken from the <a href=\"https://arxiv.org/abs/2204.12484\">original paper.</a> </small>\n+<small> ViTPose architecture. Taken from the <a href=\"https://huggingface.co/papers/2204.12484\">original paper.</a> </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr) and [sangbumchoi](https://github.com/SangbumChoi).\n The original code can be found [here](https://github.com/ViTAE-Transformer/ViTPose).\n@@ -95,7 +95,7 @@ image_pose_result = pose_results[0]  # results for first image\n \n ### ViTPose++ models\n \n-The best [checkpoints](https://huggingface.co/collections/usyd-community/vitpose-677fcfd0a0b2b5c8f79c4335) are those of the [ViTPose++ paper](https://arxiv.org/abs/2212.04246). ViTPose++ models employ a so-called [Mixture-of-Experts (MoE)](https://huggingface.co/blog/moe) architecture for the ViT backbone, resulting in better performance.\n+The best [checkpoints](https://huggingface.co/collections/usyd-community/vitpose-677fcfd0a0b2b5c8f79c4335) are those of the [ViTPose++ paper](https://huggingface.co/papers/2212.04246). ViTPose++ models employ a so-called [Mixture-of-Experts (MoE)](https://huggingface.co/blog/moe) architecture for the ViT backbone, resulting in better performance.\n \n The ViTPose+ checkpoints use 6 experts, hence 6 different dataset indices can be passed. \n An overview of the various dataset indices is provided below:"
        },
        {
            "sha": "cf32c749e23ff8a6c4983e0af3c8ae383a49de26",
            "filename": "docs/source/en/model_doc/vivit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -20,7 +20,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-The Vivit model was proposed in [ViViT: A Video Vision Transformer](https://arxiv.org/abs/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luฤiฤ, Cordelia Schmid.\n+The Vivit model was proposed in [ViViT: A Video Vision Transformer](https://huggingface.co/papers/2103.15691) by Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luฤiฤ, Cordelia Schmid.\n The paper proposes one of the first successful pure-transformer based set of models for video understanding.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "fa304b3a86d0250f9f649f9971a2217eb5100516",
            "filename": "docs/source/en/model_doc/wav2vec2-conformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-conformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-conformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-conformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Wav2Vec2-Conformer was added to an updated version of [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.\n+The Wav2Vec2-Conformer was added to an updated version of [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://huggingface.co/papers/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.\n \n The official results of the model can be found in Table 3 and Table 4 of the paper.\n \n@@ -36,7 +36,7 @@ Note: Meta (FAIR) released a new version of [Wav2Vec2-BERT 2.0](https://huggingf\n ## Usage tips\n \n - Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Attention*-block with a *Conformer*-block\n-  as introduced in [Conformer: Convolution-augmented Transformer for Speech Recognition](https://arxiv.org/abs/2005.08100).\n+  as introduced in [Conformer: Convolution-augmented Transformer for Speech Recognition](https://huggingface.co/papers/2005.08100).\n - For the same number of layers, Wav2Vec2-Conformer requires more parameters than Wav2Vec2, but also yields \n an improved word error rate.\n - Wav2Vec2-Conformer uses the same tokenizer and feature extractor as Wav2Vec2."
        },
        {
            "sha": "d884f44b1e5b311832d69fdbbc75719ffe305270",
            "filename": "docs/source/en/model_doc/wav2vec2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n+The Wav2Vec2 model was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://huggingface.co/papers/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "863bdafca33e00e82d6dc00e827ef1da4256275b",
            "filename": "docs/source/en/model_doc/wav2vec2_phoneme.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2_phoneme.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2_phoneme.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2_phoneme.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The Wav2Vec2Phoneme model was proposed in [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al.,\n-2021](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n+2021)](https://huggingface.co/papers/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "7dfe6f26bb62dd17b7fd89f29d1863c77ebd0610",
            "filename": "docs/source/en/model_doc/wavlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fwavlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fwavlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwavlm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The WavLM model was proposed in [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,\n+The WavLM model was proposed in [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://huggingface.co/papers/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen,\n Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,\n Michael Zeng, Furu Wei.\n "
        },
        {
            "sha": "ca78a68ae26200b7a952ed8dcb5d47acf47545c9",
            "filename": "docs/source/en/model_doc/xclip.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxclip.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The X-CLIP model was proposed in [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\n+The X-CLIP model was proposed in [Expanding Language-Image Pretrained Models for General Video Recognition](https://huggingface.co/papers/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\n X-CLIP is a minimal extension of [CLIP](clip) for video. The model consists of a text encoder, a cross-frame vision encoder, a multi-frame integration Transformer, and a video-specific prompt generator.\n \n The abstract from the paper is the following:\n@@ -36,7 +36,7 @@ Tips:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/xclip_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> X-CLIP architecture. Taken from the <a href=\"https://arxiv.org/abs/2208.02816\">original paper.</a> </small>\n+<small> X-CLIP architecture. Taken from the <a href=\"https://huggingface.co/papers/2208.02816\">original paper.</a> </small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr).\n The original code can be found [here](https://github.com/microsoft/VideoX/tree/master/X-CLIP)."
        },
        {
            "sha": "6c0c180727d5e6e2ce18876dd8eeaa204537476e",
            "filename": "docs/source/en/model_doc/xglm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxglm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxglm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxglm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The XGLM model was proposed in [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668)\n+The XGLM model was proposed in [Few-shot Learning with Multilingual Language Models](https://huggingface.co/papers/2112.10668)\n by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, \n Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, \n Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li."
        },
        {
            "sha": "5d11a532f20636f13c4e89963aea981436381578",
            "filename": "docs/source/en/model_doc/xlm-prophetnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-prophetnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-prophetnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-prophetnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -43,7 +43,7 @@ You can do so by running the following command: `pip install -U transformers==4.\n \n ## Overview\n \n-The XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\n+The XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei\n Zhang, Ming Zhou on 13 Jan, 2020.\n \n XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \"ngram\" language modeling instead of"
        },
        {
            "sha": "05b4a425934dd18a20021f90e5882fbe14ad11f9",
            "filename": "docs/source/en/model_doc/xlm-v.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-v.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-v.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlm-v.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -26,7 +26,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n XLM-V is multilingual language model with a one million token vocabulary trained on 2.5TB of data from Common Crawl (same as XLM-R).\n-It was introduced in the [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://arxiv.org/abs/2301.10472)\n+It was introduced in the [XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models](https://huggingface.co/papers/2301.10472)\n paper by Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer and Madian Khabsa.\n \n From the abstract of the XLM-V paper:"
        },
        {
            "sha": "e35851d5d2f3f3897c18412ce2474eaae90c9d22",
            "filename": "docs/source/en/model_doc/xlnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The XLNet model was proposed in [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\n+The XLNet model was proposed in [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://huggingface.co/papers/1906.08237) by Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\n Quoc V. Le. XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method to learn\n bidirectional contexts by maximizing the expected likelihood over all permutations of the input sequence factorization\n order."
        },
        {
            "sha": "238c703f3ecc5a7bfc14e3756aff90f3cd031cd6",
            "filename": "docs/source/en/model_doc/xls_r.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxls_r.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxls_r.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxls_r.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The XLS-R model was proposed in [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman\n+The XLS-R model was proposed in [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://huggingface.co/papers/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman\n Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "eceea3be205331195d8ed668b1e2e8fc7209ecc7",
            "filename": "docs/source/en/model_doc/xlsr_wav2vec2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlsr_wav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlsr_wav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlsr_wav2vec2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The XLSR-Wav2Vec2 model was proposed in [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael\n+The XLSR-Wav2Vec2 model was proposed in [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://huggingface.co/papers/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael\n Auli.\n \n The abstract from the paper is the following:"
        },
        {
            "sha": "9eaf56d2be37ccfd68f8a4e8bd97e9851aa1a881",
            "filename": "docs/source/en/model_doc/yolos.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The YOLOS model was proposed in [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n+The YOLOS model was proposed in [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://huggingface.co/papers/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n YOLOS proposes to just leverage the plain [Vision Transformer (ViT)](vit) for object detection, inspired by DETR. It turns out that a base-sized encoder-only Transformer can also achieve 42 AP on COCO, similar to DETR and much more complex frameworks such as Faster R-CNN.\n \n The abstract from the paper is the following:\n@@ -34,7 +34,7 @@ The abstract from the paper is the following:\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/yolos_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> YOLOS architecture. Taken from the <a href=\"https://arxiv.org/abs/2106.00666\">original paper</a>.</small>\n+<small> YOLOS architecture. Taken from the <a href=\"https://huggingface.co/papers/2106.00666\">original paper</a>.</small>\n \n This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code can be found [here](https://github.com/hustvl/YOLOS).\n "
        },
        {
            "sha": "344fad9e12b174b3401ca382af007607fd3357da",
            "filename": "docs/source/en/model_doc/yoso.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fyoso.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_doc%2Fyoso.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyoso.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The YOSO model was proposed in [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714)  \n+The YOSO model was proposed in [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://huggingface.co/papers/2111.09714)  \n by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh. YOSO approximates standard softmax self-attention\n via a Bernoulli sampling scheme based on Locality Sensitive Hashing (LSH). In principle, all the Bernoulli random variables can be sampled with\n a single hash. \n@@ -56,7 +56,7 @@ does not require compiling CUDA kernels.\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/yoso_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/> \n \n-<small> YOSO Attention Algorithm. Taken from the <a href=\"https://arxiv.org/abs/2111.09714\">original paper</a>.</small>\n+<small> YOSO Attention Algorithm. Taken from the <a href=\"https://huggingface.co/papers/2111.09714\">original paper</a>.</small>\n \n ## Resources\n "
        },
        {
            "sha": "7ef53f40566ed55368bf3bd003bccb9d19c7223b",
            "filename": "docs/source/en/model_memory_anatomy.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -204,7 +204,7 @@ Transformers architecture includes 3 main groups of operations grouped below by\n \n This knowledge can be helpful to know when analyzing performance bottlenecks.\n \n-This summary is derived from [Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020](https://arxiv.org/abs/2007.00072)\n+This summary is derived from [Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020](https://huggingface.co/papers/2007.00072)\n \n \n ## Anatomy of Model's Memory"
        },
        {
            "sha": "0836f27283b3b4cd8f66f51b3253060185997ba0",
            "filename": "docs/source/en/model_summary.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fmodel_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_summary.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # The Transformer model family\n \n-Since its introduction in 2017, the [original Transformer](https://arxiv.org/abs/1706.03762) model (see the [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) blog post for a gentle technical introduction) has inspired many new and exciting models that extend beyond natural language processing (NLP) tasks. There are models for [predicting the folded structure of proteins](https://huggingface.co/blog/deep-learning-with-proteins), [training a cheetah to run](https://huggingface.co/blog/train-decision-transformers), and [time series forecasting](https://huggingface.co/blog/time-series-transformers). With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they're based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it'll help you understand Transformers you haven't encountered before.\n+Since its introduction in 2017, the [original Transformer](https://huggingface.co/papers/1706.03762) model (see the [Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) blog post for a gentle technical introduction) has inspired many new and exciting models that extend beyond natural language processing (NLP) tasks. There are models for [predicting the folded structure of proteins](https://huggingface.co/blog/deep-learning-with-proteins), [training a cheetah to run](https://huggingface.co/blog/train-decision-transformers), and [time series forecasting](https://huggingface.co/blog/time-series-transformers). With so many Transformer variants available, it can be easy to miss the bigger picture. What all these models have in common is they're based on the original Transformer architecture. Some models only use the encoder or decoder, while others use both. This provides a useful taxonomy to categorize and examine the high-level differences within models in the Transformer family, and it'll help you understand Transformers you haven't encountered before.\n \n If you aren't familiar with the original Transformer model or need a refresher, check out the [How do Transformers work](https://huggingface.co/course/chapter1/4?fw=pt) chapter from the Hugging Face course.\n \n@@ -32,7 +32,7 @@ If you aren't familiar with the original Transformer model or need a refresher,\n \n ### Convolutional network\n \n-For a long time, convolutional networks (CNNs) were the dominant paradigm for computer vision tasks until the [Vision Transformer](https://arxiv.org/abs/2010.11929) demonstrated its scalability and efficiency. Even then, some of a CNN's best qualities, like translation invariance, are so powerful (especially for certain tasks) that some Transformers incorporate convolutions in their architecture. [ConvNeXt](model_doc/convnext) flipped this exchange around and incorporated design choices from Transformers to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows to patchify an image and a larger kernel to increase its global receptive field. ConvNeXt also makes several layer design choices to be more memory-efficient and improve performance, so it competes favorably with Transformers!\n+For a long time, convolutional networks (CNNs) were the dominant paradigm for computer vision tasks until the [Vision Transformer](https://huggingface.co/papers/2010.11929) demonstrated its scalability and efficiency. Even then, some of a CNN's best qualities, like translation invariance, are so powerful (especially for certain tasks) that some Transformers incorporate convolutions in their architecture. [ConvNeXt](model_doc/convnext) flipped this exchange around and incorporated design choices from Transformers to modernize a CNN. For example, ConvNeXt uses non-overlapping sliding windows to patchify an image and a larger kernel to increase its global receptive field. ConvNeXt also makes several layer design choices to be more memory-efficient and improve performance, so it competes favorably with Transformers!\n \n ### Encoder[[cv-encoder]]\n \n@@ -58,7 +58,7 @@ Vision models commonly use an encoder (also known as a backbone) to extract impo\n \n [BERT](model_doc/bert) is an encoder-only Transformer that randomly masks certain tokens in the input to avoid seeing other tokens, which would allow it to \"cheat\". The pretraining objective is to predict the masked token based on the context. This allows BERT to fully use the left and right contexts to help it learn a deeper and richer representation of the inputs. However, there was still room for improvement in BERT's pretraining strategy. [RoBERTa](model_doc/roberta) improved upon this by introducing a new pretraining recipe that includes training for longer and on larger batches, randomly masking tokens at each epoch instead of just once during preprocessing, and removing the next-sentence prediction objective. \n \n-The dominant strategy to improve performance is to increase the model size. But training large models is computationally expensive. One way to reduce computational costs is using a smaller model like [DistilBERT](model_doc/distilbert). DistilBERT uses [knowledge distillation](https://arxiv.org/abs/1503.02531) - a compression technique - to create a smaller version of BERT while keeping nearly all of its language understanding capabilities. \n+The dominant strategy to improve performance is to increase the model size. But training large models is computationally expensive. One way to reduce computational costs is using a smaller model like [DistilBERT](model_doc/distilbert). DistilBERT uses [knowledge distillation](https://huggingface.co/papers/1503.02531) - a compression technique - to create a smaller version of BERT while keeping nearly all of its language understanding capabilities. \n \n However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix.\n "
        },
        {
            "sha": "49bd7f89bc92be0f51ed190ee8163e95060040aa",
            "filename": "docs/source/en/quantization/aqlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fquantization%2Faqlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fquantization%2Faqlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Faqlm.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # AQLM\n \n-Additive Quantization of Language Models ([AQLM](https://arxiv.org/abs/2401.06118)) quantizes multiple weights together and takes advantage of interdependencies between them. AQLM represents groups of 8-16 weights as a sum of multiple vector codes.\n+Additive Quantization of Language Models ([AQLM](https://huggingface.co/papers/2401.06118)) quantizes multiple weights together and takes advantage of interdependencies between them. AQLM represents groups of 8-16 weights as a sum of multiple vector codes.\n \n AQLM also supports fine-tuning with [LoRA](https://huggingface.co/docs/peft/package_reference/lora) with the [PEFT](https://huggingface.co/docs/peft) library, and is fully compatible with [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) for even faster inference and training.\n "
        },
        {
            "sha": "922210b2137b22d417ec4679f900c534f5d32d9b",
            "filename": "docs/source/en/quantization/bitnet.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fquantization%2Fbitnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fquantization%2Fbitnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fbitnet.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # BitNet\n \n-[BitNet](https://arxiv.org/abs/2402.17764) replaces traditional linear layers in Multi-Head Attention and feed-forward networks with specialized BitLinear layers. The BitLinear layers quantize the weights using ternary precision (with values of -1, 0, and 1) and quantize the activations to 8-bit precision.\n+[BitNet](https://huggingface.co/papers/2402.17764) replaces traditional linear layers in Multi-Head Attention and feed-forward networks with specialized BitLinear layers. The BitLinear layers quantize the weights using ternary precision (with values of -1, 0, and 1) and quantize the activations to 8-bit precision.\n \n <figure style=\"text-align: center;\">\n   <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/1.58llm_extreme_quantization/bitlinear.png\" alt=\"Alt Text\" />\n@@ -27,7 +27,7 @@ BitNet models can't be quantized on the fly. They need to be quantized during pr\n \n 1. Compute the average of the absolute values of the weight matrix and use as a scale.\n 2. Divide the weights by the scale, round the values, constrain them between -1 and 1, and rescale them to continue in full precision.\n-3. Activations are quantized to a specified bit-width (8-bit) using [absmax](https://arxiv.org/pdf/2208.07339) quantization (symmetric per channel quantization). This involves scaling the activations into a range of [โ128,127].\n+3. Activations are quantized to a specified bit-width (8-bit) using [absmax](https://huggingface.co/papers/2208.07339) quantization (symmetric per channel quantization). This involves scaling the activations into a range of [โ128,127].\n \n Refer to this [PR](https://github.com/huggingface/nanotron/pull/180) to pretrain or fine-tune a 1.58-bit model with [Nanotron](https://github.com/huggingface/nanotron). For fine-tuning, convert a model from the Hugging Face to Nanotron format. Find the conversion steps in this [PR](https://github.com/huggingface/nanotron/pull/174).\n "
        },
        {
            "sha": "07f6e2b31feddbedea704c69a0b468aa916f894f",
            "filename": "docs/source/en/quantization/higgs.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fquantization%2Fhiggs.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fquantization%2Fhiggs.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fhiggs.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # HIGGS\n \n-[HIGGS](https://arxiv.org/abs/2411.17525) is a zero-shot quantization algorithm that combines Hadamard preprocessing with MSE-Optimal quantization grids to achieve lower quantization error and state-of-the-art performance.\n+[HIGGS](https://huggingface.co/papers/2411.17525) is a zero-shot quantization algorithm that combines Hadamard preprocessing with MSE-Optimal quantization grids to achieve lower quantization error and state-of-the-art performance.\n \n Runtime support for HIGGS is implemented through the [FLUTE](https://github.com/HanGuo97/flute) library. Only the 70B and 405B variants of Llama 3 and Llama 3.0, and the 8B and 27B variants of Gemma 2 are currently supported. HIGGS also doesn't support quantized training and backward passes in general at the moment.\n "
        },
        {
            "sha": "392895e0ea14f54d6cb9bf8469f587d0f8a56e1d",
            "filename": "docs/source/en/quantization/vptq.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fquantization%2Fvptq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Fquantization%2Fvptq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fvptq.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -69,4 +69,4 @@ VPTQ achieves better accuracy and higher throughput with lower quantization over\n \n See an example demo of VPTQ on the VPTQ Online Demo [Space](https://huggingface.co/spaces/microsoft/VPTQ) or try running the VPTQ inference [notebook](https://colab.research.google.com/github/microsoft/VPTQ/blob/main/notebooks/vptq_example.ipynb).\n \n-For more information, read the VPTQ [paper](https://arxiv.org/pdf/2409.17066).\n+For more information, read the VPTQ [paper](https://huggingface.co/papers/2409.17066)."
        },
        {
            "sha": "4d1e735bd7d4730241fb4f55135594bc505888fd",
            "filename": "docs/source/en/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -17,7 +17,7 @@ rendered properly in your Markdown viewer.\n \n [[open-in-colab]]\n \n-Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). To distill knowledge from one model to another, we take a pre-trained teacher model trained on a certain task (image classification for this case) and randomly initialize a student model to be trained on image classification. Next, we train the student model to minimize the difference between its outputs and the teacher's outputs, thus making it mimic the behavior. It was first introduced in [Distilling the Knowledge in a Neural Network by Hinton et al](https://arxiv.org/abs/1503.02531). In this guide, we will do task-specific knowledge distillation. We will use the [beans dataset](https://huggingface.co/datasets/beans) for this.\n+Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). To distill knowledge from one model to another, we take a pre-trained teacher model trained on a certain task (image classification for this case) and randomly initialize a student model to be trained on image classification. Next, we train the student model to minimize the difference between its outputs and the teacher's outputs, thus making it mimic the behavior. It was first introduced in [Distilling the Knowledge in a Neural Network by Hinton et al](https://huggingface.co/papers/1503.02531). In this guide, we will do task-specific knowledge distillation. We will use the [beans dataset](https://huggingface.co/datasets/beans) for this.\n \n This guide demonstrates how you can distill a [fine-tuned ViT model](https://huggingface.co/merve/vit-mobilenet-beans-224) (teacher model) to a [MobileNet](https://huggingface.co/google/mobilenet_v2_1.4_224) (student model) using the [TrainerยAPI](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer) of ๐ค Transformers.\n "
        },
        {
            "sha": "7a8c6ba45d04d382fc01994e79e65f64326fca45",
            "filename": "docs/source/en/tasks/video_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -405,7 +405,7 @@ def compute_metrics(eval_pred):\n \n **A note on evaluation**:\n \n-In the [VideoMAE paper](https://arxiv.org/abs/2203.12602), the authors use the following evaluation strategy. They evaluate the model on several clips from test videos and apply different crops to those clips and report the aggregate score. However, in the interest of simplicity and brevity, we don't consider that in this tutorial.\n+In the [VideoMAE paper](https://huggingface.co/papers/2203.12602), the authors use the following evaluation strategy. They evaluate the model on several clips from test videos and apply different crops to those clips and report the aggregate score. However, in the interest of simplicity and brevity, we don't consider that in this tutorial.\n \n Also, define a `collate_fn`, which will be used to batch examples together. Each batch consists of 2 keys, namely `pixel_values` and `labels`.\n "
        },
        {
            "sha": "afdfb869899def17f0d115df5d9adf49bd34bbe5",
            "filename": "docs/source/en/tasks_explained.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Ftasks_explained.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Ftasks_explained.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks_explained.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -120,7 +120,7 @@ This section briefly explains convolutions, but it'd be helpful to have a prior\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\n </div>\n \n-<small>A basic convolution without padding or stride, taken from <a href=\"https://arxiv.org/abs/1603.07285\">A guide to convolution arithmetic for deep learning.</a></small>\n+<small>A basic convolution without padding or stride, taken from <a href=\"https://huggingface.co/papers/1603.07285\">A guide to convolution arithmetic for deep learning.</a></small>\n \n You can feed this output to another convolutional layer, and with each successive layer, the network learns more complex and abstract things like hotdogs or rockets. Between convolutional layers, it is common to add a pooling layer to reduce dimensionality and make the model more robust to variations of a feature's position.\n "
        },
        {
            "sha": "801948f35d87be78fb75394da93eb73ede104ab6",
            "filename": "docs/source/en/tokenizer_summary.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Ftokenizer_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fen%2Ftokenizer_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftokenizer_summary.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -140,7 +140,7 @@ on.\n ### Byte-Pair Encoding (BPE)\n \n Byte-Pair Encoding (BPE) was introduced in [Neural Machine Translation of Rare Words with Subword Units (Sennrich et\n-al., 2015)](https://arxiv.org/abs/1508.07909). BPE relies on a pre-tokenizer that splits the training data into\n+al., 2015)](https://huggingface.co/papers/1508.07909). BPE relies on a pre-tokenizer that splits the training data into\n words. Pretokenization can be as simple as space tokenization, e.g. [GPT-2](model_doc/gpt2), [RoBERTa](model_doc/roberta). More advanced pre-tokenization include rule-based tokenization, e.g. [XLM](model_doc/xlm),\n [FlauBERT](model_doc/flaubert) which uses Moses for most languages, or [GPT](model_doc/openai-gpt) which uses\n spaCy and ftfy, to count the frequency of each word in the training corpus.\n@@ -230,7 +230,7 @@ to ensure it's _worth it_.\n ### Unigram\n \n Unigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation\n-Models with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or\n+Models with Multiple Subword Candidates (Kudo, 2018)](https://huggingface.co/papers/1804.10959). In contrast to BPE or\n WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each\n symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and\n the most common substrings. Unigram is not used directly for any of the models in the transformers, but it's used in\n@@ -270,7 +270,7 @@ All tokenization algorithms described so far have the same problem: It is assume\n separate words. However, not all languages use spaces to separate words. One possible solution is to use language\n specific pre-tokenizers, *e.g.* [XLM](model_doc/xlm) uses a specific Chinese, Japanese, and Thai pre-tokenizer.\n To solve this problem more generally, [SentencePiece: A simple and language independent subword tokenizer and\n-detokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) treats the input\n+detokenizer for Neural Text Processing (Kudo et al., 2018)](https://huggingface.co/papers/1808.06226) treats the input\n as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram\n algorithm to construct the appropriate vocabulary.\n "
        },
        {
            "sha": "19ae1444ae44cc0c8ab458efb5ec9bc20e83f614",
            "filename": "docs/source/es/bertology.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fbertology.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -21,21 +21,21 @@ Hay un creciente campo de estudio empeรฑado en la investigaciรณn del funcionamie\n \n \n - BERT Rediscovers the Classical NLP Pipeline por Ian Tenney, Dipanjan Das, Ellie Pavlick:\n-  https://arxiv.org/abs/1905.05950\n-- Are Sixteen Heads Really Better than One? por Paul Michel, Omer Levy, Graham Neubig: https://arxiv.org/abs/1905.10650\n+  https://huggingface.co/papers/1905.05950\n+- Are Sixteen Heads Really Better than One? por Paul Michel, Omer Levy, Graham Neubig: https://huggingface.co/papers/1905.10650\n - What Does BERT Look At? An Analysis of BERT's Attention por Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D.\n-  Manning: https://arxiv.org/abs/1906.04341\n-- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: https://arxiv.org/abs/2210.04633\n+  Manning: https://huggingface.co/papers/1906.04341\n+- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: https://huggingface.co/papers/2210.04633\n \n Para asistir al desarrollo de este nuevo campo, hemos incluido algunas features adicionales en los modelos BERT/GPT/GPT-2 para\n ayudar a acceder a las representaciones internas, principalmente adaptado de la gran obra de Paul Michel\n-(https://arxiv.org/abs/1905.10650):\n+(https://huggingface.co/papers/1905.10650):\n \n \n - accediendo a todos los hidden-states de BERT/GPT/GPT-2,\n - accediendo a todos los pesos de atenciรณn para cada head de BERT/GPT/GPT-2,\n - adquiriendo los valores de salida y gradientes de las heads para poder computar la mรฉtrica de importancia de las heads y realizar la poda de heads como se explica\n-  en https://arxiv.org/abs/1905.10650.\n+  en https://huggingface.co/papers/1905.10650.\n \n Para ayudarte a entender y usar estas features, hemos aรฑadido un script especรญfico de ejemplo: [bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py) mientras extraes informaciรณn y cortas un modelo pre-entrenado en\n GLUE."
        },
        {
            "sha": "3debcdbd354580c243ef2afab449f84a632c4d27",
            "filename": "docs/source/es/glossary.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Fglossary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Fglossary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fglossary.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -147,7 +147,7 @@ El proceso de seleccionar y transformar datos crudos en un conjunto de caracter\n \n En cada bloque de atenciรณn residual en los transformadores, la capa de autoatenciรณn suele ir seguida de 2 capas de avance. El tamaรฑo de embedding intermedio de las capas de avance suele ser mayor que el tamaรฑo oculto del modelo (por ejemplo, para `google-bert/bert-base-uncased`).\n \n-Para una entrada de tamaรฑo `[batch_size, sequence_length]`, la memoria requerida para almacenar los embeddings intermedios de avance `[batch_size, sequence_length, config.intermediate_size]` puede representar una gran fracciรณn del uso de memoria. Los autores de [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) observaron que, dado que el cรกlculo es independiente de la dimensiรณn `sequence_length`, es matemรกticamente equivalente calcular los embeddings de salida de ambas capas de avance  `[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n` individualmente y concatenarlos despuรฉs a `[batch_size, sequence_length, config.hidden_size]` con `n = sequence_length`, lo que intercambia el aumento del tiempo de cรกlculo por una reducciรณn en el uso de memoria, pero produce un resultado matemรกticamente **equivalente**.\n+Para una entrada de tamaรฑo `[batch_size, sequence_length]`, la memoria requerida para almacenar los embeddings intermedios de avance `[batch_size, sequence_length, config.intermediate_size]` puede representar una gran fracciรณn del uso de memoria. Los autores de [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) observaron que, dado que el cรกlculo es independiente de la dimensiรณn `sequence_length`, es matemรกticamente equivalente calcular los embeddings de salida de ambas capas de avance  `[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n` individualmente y concatenarlos despuรฉs a `[batch_size, sequence_length, config.hidden_size]` con `n = sequence_length`, lo que intercambia el aumento del tiempo de cรกlculo por una reducciรณn en el uso de memoria, pero produce un resultado matemรกticamente **equivalente**.\n \n Para modelos que utilizan la funciรณn [`apply_chunking_to_forward`], el `chunk_size` define el nรบmero de embeddings de salida que se calculan en paralelo y, por lo tanto, define el equilibrio entre la complejidad de memoria y tiempo. Si `chunk_size` se establece en 0, no se realiza ninguna fragmentaciรณn de avance.\n \n@@ -183,7 +183,7 @@ Los IDs de entrada a menudo son los รบnicos parรกmetros necesarios que se deben\n \n <Youtube id=\"VFp38yj8h3A\"/>\n \n-Cada tokenizador funciona de manera diferente, pero el mecanismo subyacente sigue siendo el mismo. Aquรญ tienes un ejemplo utilizando el tokenizador BERT, que es un tokenizador [WordPiece](https://arxiv.org/pdf/1609.08144.pdf):\n+Cada tokenizador funciona de manera diferente, pero el mecanismo subyacente sigue siendo el mismo. Aquรญ tienes un ejemplo utilizando el tokenizador BERT, que es un tokenizador [WordPiece](https://huggingface.co/papers/1609.08144):\n \n ```python\n >>> from transformers import BertTokenizer"
        },
        {
            "sha": "428a7294475e546d2803702a8cb1f8e71d36e2db",
            "filename": "docs/source/es/index.md",
            "status": "modified",
            "additions": 106,
            "deletions": 106,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Findex.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -49,122 +49,122 @@ La biblioteca actualmente contiene implementaciones de JAX, PyTorch y TensorFlow\n \n <!--This list is updated automatically from the README with _make fix-copies_. Do not update manually! -->\n \n-1. **[ALBERT](model_doc/albert)** (de Google Research y el Instituto Tecnolรณgico de Toyota en Chicago) publicado con el paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), por Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n-1. **[ALIGN](model_doc/align)** (de Google Research) publicado con el paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) por Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.\n-1. **[BART](model_doc/bart)** (de Facebook) publicado con el paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) por Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov y Luke Zettlemoyer.\n-1. **[BARThez](model_doc/barthez)** (de รcole polytechnique) publicado con el paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) por Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n-1. **[BARTpho](model_doc/bartpho)** (de VinAI Research) publicado con el paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) por Nguyen Luong Tran, Duong Minh Le y Dat Quoc Nguyen.\n-1. **[BEiT](model_doc/beit)** (de Microsoft) publicado con el paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) por Hangbo Bao, Li Dong, Furu Wei.\n-1. **[BERT](model_doc/bert)** (de Google) publicado con el paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) por Jacob Devlin, Ming-Wei Chang, Kenton Lee y Kristina Toutanova.\n+1. **[ALBERT](model_doc/albert)** (de Google Research y el Instituto Tecnolรณgico de Toyota en Chicago) publicado con el paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://huggingface.co/papers/1909.11942), por Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n+1. **[ALIGN](model_doc/align)** (de Google Research) publicado con el paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://huggingface.co/papers/2102.05918) por Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.\n+1. **[BART](model_doc/bart)** (de Facebook) publicado con el paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://huggingface.co/papers/1910.13461) por Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov y Luke Zettlemoyer.\n+1. **[BARThez](model_doc/barthez)** (de รcole polytechnique) publicado con el paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://huggingface.co/papers/2010.12321) por Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n+1. **[BARTpho](model_doc/bartpho)** (de VinAI Research) publicado con el paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://huggingface.co/papers/2109.09701) por Nguyen Luong Tran, Duong Minh Le y Dat Quoc Nguyen.\n+1. **[BEiT](model_doc/beit)** (de Microsoft) publicado con el paper [BEiT: BERT Pre-Training of Image Transformers](https://huggingface.co/papers/2106.08254) por Hangbo Bao, Li Dong, Furu Wei.\n+1. **[BERT](model_doc/bert)** (de Google) publicado con el paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://huggingface.co/papers/1810.04805) por Jacob Devlin, Ming-Wei Chang, Kenton Lee y Kristina Toutanova.\n 1. **[BERTweet](model_doc/bertweet)** (de VinAI Research) publicado con el paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) por Dat Quoc Nguyen, Thanh Vu y Anh Tuan Nguyen.\n-1. **[BERT For Sequence Generation](model_doc/bert-generation)** (de Google) publicado con el paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) por Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n-1. **[BigBird-RoBERTa](model_doc/big_bird)** (de Google Research) publicado con el paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) por Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n-1. **[BigBird-Pegasus](model_doc/bigbird_pegasus)** (de Google Research) publicado con el paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) por Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n-1. **[Blenderbot](model_doc/blenderbot)** (de Facebook) publicado con el paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) por Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n-1. **[BlenderbotSmall](model_doc/blenderbot-small)** (de Facebook) publicado con el paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) por Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n-1. **[BORT](model_doc/bort)** (de Alexa) publicado con el paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) por Adrian de Wynter y Daniel J. Perry.\n-1. **[ByT5](model_doc/byt5)** (de Google Research) publicado con el paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) por Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n-1. **[CamemBERT](model_doc/camembert)** (de Inria/Facebook/Sorbonne) publicado con el paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) por Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suรกrez*, Yoann Dupont, Laurent Romary, รric Villemonte de la Clergerie, Djamรฉ Seddah y Benoรฎt Sagot.\n-1. **[CANINE](model_doc/canine)** (de Google Research) publicado con el paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) por Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n-1. **[ConvNeXT](model_doc/convnext)** (de Facebook AI) publicado con el paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) por Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n-1. **[ConvNeXTV2](model_doc/convnextv2)** (de Facebook AI) publicado con el paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) por Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n-1. **[CLIP](model_doc/clip)** (de OpenAI) publicado con el paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) por Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n-1. **[ConvBERT](model_doc/convbert)** (de YituTech) publicado con el paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) por Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n-1. **[CPM](model_doc/cpm)** (de Universidad de Tsinghua) publicado con el paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) por Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n-1. **[CTRL](model_doc/ctrl)** (de Salesforce) publicado con el paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) por Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong y Richard Socher.\n-1. **[Data2Vec](model_doc/data2vec)** (de Facebook) publicado con el paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) por Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n-1. **[DeBERTa](model_doc/deberta)** (de Microsoft) publicado con el paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) por Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n-1. **[DeBERTa-v2](model_doc/deberta-v2)** (de Microsoft) publicado con el paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) por Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n-1. **[Decision Transformer](model_doc/decision_transformer)** (de Berkeley/Facebook/Google) publicado con el paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) por Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n-1. **[DiT](model_doc/dit)** (de Microsoft Research) publicado con el paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) por Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n-1. **[DeiT](model_doc/deit)** (de Facebook) publicado con el paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) por Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervรฉ Jรฉgou.\n-1. **[DETR](model_doc/detr)** (de Facebook) publicado con el paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) por Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n-1. **[DialoGPT](model_doc/dialogpt)** (de Microsoft Research) publicado con el paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) por Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n-1. **[DistilBERT](model_doc/distilbert)** (de HuggingFace), publicado junto con el paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) por Victor Sanh, Lysandre Debut y Thomas Wolf. Se ha aplicado el mismo mรฉtodo para comprimir GPT2 en [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa en [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), BERT multilingรผe en [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) y una versiรณn alemana de DistilBERT.\n-1. **[DPR](model_doc/dpr)** (de Facebook) publicado con el paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) por Vladimir Karpukhin, Barlas Oฤuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, y Wen-tau Yih.\n-1. **[DPT](master/model_doc/dpt)** (de Intel Labs) publicado con el paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) por Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n-1. **[EfficientNet](model_doc/efficientnet)** (from Google Research) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)  by Mingxing Tan and Quoc V. Le.\n-1. **[EncoderDecoder](model_doc/encoder-decoder)** (de Google Research) publicado con el paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) por Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n-1. **[ELECTRA](model_doc/electra)** (de Google Research/Universidad de Stanford) publicado con el paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) por Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n-1. **[FlauBERT](model_doc/flaubert)** (de CNRS) publicado con el paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) por Hang Le, Loรฏc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoรฎt Crabbรฉ, Laurent Besacier, Didier Schwab.\n-1. **[FNet](model_doc/fnet)** (de Google Research) publicado con el paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) por James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n-1. **[Funnel Transformer](model_doc/funnel)** (de CMU/Google Brain) publicado con el paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) por Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n-1. **[GLPN](model_doc/glpn)** (de KAIST) publicado con el paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) por Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n+1. **[BERT For Sequence Generation](model_doc/bert-generation)** (de Google) publicado con el paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) por Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+1. **[BigBird-RoBERTa](model_doc/big_bird)** (de Google Research) publicado con el paper [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) por Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n+1. **[BigBird-Pegasus](model_doc/bigbird_pegasus)** (de Google Research) publicado con el paper [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) por Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n+1. **[Blenderbot](model_doc/blenderbot)** (de Facebook) publicado con el paper [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) por Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n+1. **[BlenderbotSmall](model_doc/blenderbot-small)** (de Facebook) publicado con el paper [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) por Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n+1. **[BORT](model_doc/bort)** (de Alexa) publicado con el paper [Optimal Subarchitecture Extraction For BERT](https://huggingface.co/papers/2010.10499) por Adrian de Wynter y Daniel J. Perry.\n+1. **[ByT5](model_doc/byt5)** (de Google Research) publicado con el paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://huggingface.co/papers/2105.13626) por Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n+1. **[CamemBERT](model_doc/camembert)** (de Inria/Facebook/Sorbonne) publicado con el paper [CamemBERT: a Tasty French Language Model](https://huggingface.co/papers/1911.03894) por Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suรกrez*, Yoann Dupont, Laurent Romary, รric Villemonte de la Clergerie, Djamรฉ Seddah y Benoรฎt Sagot.\n+1. **[CANINE](model_doc/canine)** (de Google Research) publicado con el paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://huggingface.co/papers/2103.06874) por Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n+1. **[ConvNeXT](model_doc/convnext)** (de Facebook AI) publicado con el paper [A ConvNet for the 2020s](https://huggingface.co/papers/2201.03545) por Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n+1. **[ConvNeXTV2](model_doc/convnextv2)** (de Facebook AI) publicado con el paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://huggingface.co/papers/2301.00808) por Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n+1. **[CLIP](model_doc/clip)** (de OpenAI) publicado con el paper [Learning Transferable Visual Models From Natural Language Supervision](https://huggingface.co/papers/2103.00020) por Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n+1. **[ConvBERT](model_doc/convbert)** (de YituTech) publicado con el paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://huggingface.co/papers/2008.02496) por Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n+1. **[CPM](model_doc/cpm)** (de Universidad de Tsinghua) publicado con el paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://huggingface.co/papers/2012.00413) por Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n+1. **[CTRL](model_doc/ctrl)** (de Salesforce) publicado con el paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://huggingface.co/papers/1909.05858) por Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong y Richard Socher.\n+1. **[Data2Vec](model_doc/data2vec)** (de Facebook) publicado con el paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://huggingface.co/papers/2202.03555) por Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n+1. **[DeBERTa](model_doc/deberta)** (de Microsoft) publicado con el paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) por Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n+1. **[DeBERTa-v2](model_doc/deberta-v2)** (de Microsoft) publicado con el paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) por Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n+1. **[Decision Transformer](model_doc/decision_transformer)** (de Berkeley/Facebook/Google) publicado con el paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://huggingface.co/papers/2106.01345) por Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n+1. **[DiT](model_doc/dit)** (de Microsoft Research) publicado con el paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://huggingface.co/papers/2203.02378) por Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n+1. **[DeiT](model_doc/deit)** (de Facebook) publicado con el paper [Training data-efficient image transformers & distillation through attention](https://huggingface.co/papers/2012.12877) por Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervรฉ Jรฉgou.\n+1. **[DETR](model_doc/detr)** (de Facebook) publicado con el paper [End-to-End Object Detection with Transformers](https://huggingface.co/papers/2005.12872) por Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n+1. **[DialoGPT](model_doc/dialogpt)** (de Microsoft Research) publicado con el paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://huggingface.co/papers/1911.00536) por Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n+1. **[DistilBERT](model_doc/distilbert)** (de HuggingFace), publicado junto con el paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://huggingface.co/papers/1910.01108) por Victor Sanh, Lysandre Debut y Thomas Wolf. Se ha aplicado el mismo mรฉtodo para comprimir GPT2 en [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa en [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), BERT multilingรผe en [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) y una versiรณn alemana de DistilBERT.\n+1. **[DPR](model_doc/dpr)** (de Facebook) publicado con el paper [Dense Passage Retrieval for Open-Domain Question Answering](https://huggingface.co/papers/2004.04906) por Vladimir Karpukhin, Barlas Oฤuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, y Wen-tau Yih.\n+1. **[DPT](master/model_doc/dpt)** (de Intel Labs) publicado con el paper [Vision Transformers for Dense Prediction](https://huggingface.co/papers/2103.13413) por Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n+1. **[EfficientNet](model_doc/efficientnet)** (from Google Research) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://huggingface.co/papers/1905.11946)  by Mingxing Tan and Quoc V. Le.\n+1. **[EncoderDecoder](model_doc/encoder-decoder)** (de Google Research) publicado con el paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) por Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+1. **[ELECTRA](model_doc/electra)** (de Google Research/Universidad de Stanford) publicado con el paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://huggingface.co/papers/2003.10555) por Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n+1. **[FlauBERT](model_doc/flaubert)** (de CNRS) publicado con el paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://huggingface.co/papers/1912.05372) por Hang Le, Loรฏc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoรฎt Crabbรฉ, Laurent Besacier, Didier Schwab.\n+1. **[FNet](model_doc/fnet)** (de Google Research) publicado con el paper [FNet: Mixing Tokens with Fourier Transforms](https://huggingface.co/papers/2105.03824) por James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n+1. **[Funnel Transformer](model_doc/funnel)** (de CMU/Google Brain) publicado con el paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://huggingface.co/papers/2006.03236) por Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n+1. **[GLPN](model_doc/glpn)** (de KAIST) publicado con el paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://huggingface.co/papers/2201.07436) por Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n 1. **[GPT](model_doc/openai-gpt)** (de OpenAI) publicado con el paper [Improving Language Understanding by Generative Pre-Training](https://openai.com/research/language-unsupervised/) por Alec Radford, Karthik Narasimhan, Tim Salimans y Ilya Sutskever.\n 1. **[GPT-2](model_doc/gpt2)** (de OpenAI) publicado con el paper [Language Models are Unsupervised Multitask Learners](https://openai.com/research/better-language-models/) por Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei y Ilya Sutskever.\n 1. **[GPT-J](model_doc/gptj)** (de EleutherAI) publicado con el repositorio [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) por Ben Wang y Aran Komatsuzaki.\n 1. **[GPT Neo](model_doc/gpt_neo)** (de EleutherAI) publicado en el paper [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) por Sid Black, Stella Biderman, Leo Gao, Phil Wang y Connor Leahy.\n 1. **[GPTSAN-japanese](model_doc/gptsan-japanese)** released with [GPTSAN](https://github.com/tanreinama/GPTSAN) by Toshiyuki Sakamoto (tanreinama).\n-1. **[Hubert](model_doc/hubert)** (de Facebook) publicado con el paper [HuBERT: Self-Supervised Speech Representation Learning por Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) por Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n-1. **[I-BERT](model_doc/ibert)** (de Berkeley) publicado con el paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) por Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n+1. **[Hubert](model_doc/hubert)** (de Facebook) publicado con el paper [HuBERT: Self-Supervised Speech Representation Learning por Masked Prediction of Hidden Units](https://huggingface.co/papers/2106.07447) por Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n+1. **[I-BERT](model_doc/ibert)** (de Berkeley) publicado con el paper [I-BERT: Integer-only BERT Quantization](https://huggingface.co/papers/2101.01321) por Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n 1. **[ImageGPT](model_doc/imagegpt)** (de OpenAI) publicado con el paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) por Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.\n-1. **[LayoutLM](model_doc/layoutlm)** (de Microsoft Research Asia) publicado con el paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) por Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n-1. **[LayoutLMv2](model_doc/layoutlmv2)** (de Microsoft Research Asia) publicado con el paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) por Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n-1. **[LayoutXLM](model_doc/layoutxlm)** (de Microsoft Research Asia) publicado con el paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) por Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n-1. **[LED](model_doc/led)** (de AllenAI) publicado con el paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) por Iz Beltagy, Matthew E. Peters, Arman Cohan.\n-1. **[Longformer](model_doc/longformer)** (de AllenAI) publicado con el paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) por Iz Beltagy, Matthew E. Peters, Arman Cohan.\n-1. **[LUKE](model_doc/luke)** (de Studio Ousia) publicado con el paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) por Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n-1. **[mLUKE](model_doc/mluke)** (de Studio Ousia) publicado con el paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) por Ryokan Ri, Ikuya Yamada, y Yoshimasa Tsuruoka.\n-1. **[LXMERT](model_doc/lxmert)** (de UNC Chapel Hill) publicado con el paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) por Hao Tan y Mohit Bansal.\n-1. **[M2M100](model_doc/m2m_100)** (de Facebook) publicado con el paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) por Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n+1. **[LayoutLM](model_doc/layoutlm)** (de Microsoft Research Asia) publicado con el paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://huggingface.co/papers/1912.13318) por Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n+1. **[LayoutLMv2](model_doc/layoutlmv2)** (de Microsoft Research Asia) publicado con el paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://huggingface.co/papers/2012.14740) por Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n+1. **[LayoutXLM](model_doc/layoutxlm)** (de Microsoft Research Asia) publicado con el paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://huggingface.co/papers/2104.08836) por Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n+1. **[LED](model_doc/led)** (de AllenAI) publicado con el paper [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) por Iz Beltagy, Matthew E. Peters, Arman Cohan.\n+1. **[Longformer](model_doc/longformer)** (de AllenAI) publicado con el paper [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) por Iz Beltagy, Matthew E. Peters, Arman Cohan.\n+1. **[LUKE](model_doc/luke)** (de Studio Ousia) publicado con el paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057) por Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n+1. **[mLUKE](model_doc/mluke)** (de Studio Ousia) publicado con el paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://huggingface.co/papers/2110.08151) por Ryokan Ri, Ikuya Yamada, y Yoshimasa Tsuruoka.\n+1. **[LXMERT](model_doc/lxmert)** (de UNC Chapel Hill) publicado con el paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) por Hao Tan y Mohit Bansal.\n+1. **[M2M100](model_doc/m2m_100)** (de Facebook) publicado con el paper [Beyond English-Centric Multilingual Machine Translation](https://huggingface.co/papers/2010.11125) por Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n 1. **[MarianMT](model_doc/marian)** Modelos de traducciรณn automรกtica entrenados usando [OPUS](http://opus.nlpl.eu/) data por Jรถrg Tiedemann. El [Marian Framework](https://marian-nmt.github.io/) estรก siendo desarrollado por el equipo de traductores de Microsoft.\n-1. **[Mask2Former](model_doc/mask2former)** (de FAIR y UIUC) publicado con el paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) por Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.\n-1. **[MaskFormer](model_doc/maskformer)** (de Meta y UIUC) publicado con el paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) por Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n-1. **[MBart](model_doc/mbart)** (de Facebook) publicado con el paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) por Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n-1. **[MBart-50](model_doc/mbart)** (de Facebook) publicado con el paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) por Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n-1. **[Megatron-BERT](model_doc/megatron-bert)** (de NVIDIA) publicado con el paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) por Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper y Bryan Catanzaro.\n-1. **[Megatron-GPT2](model_doc/megatron_gpt2)** (de NVIDIA) publicado con el paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) por Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper y Bryan Catanzaro.\n-1. **[MPNet](model_doc/mpnet)** (de Microsoft Research) publicado con el paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) por Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n-1. **[MT5](model_doc/mt5)** (de Google AI) publicado con el paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) por Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n-1. **[Nystrรถmformer](model_doc/nystromformer)** (de la Universidad de Wisconsin - Madison) publicado con el paper [Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) por Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n-1. **[OneFormer](model_doc/oneformer)** (de la SHI Labs) publicado con el paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) por Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n-1. **[Pegasus](model_doc/pegasus)** (de Google) publicado con el paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) por Jingqing Zhang, Yao Zhao, Mohammad Saleh y Peter J. Liu.\n-1. **[Perceiver IO](model_doc/perceiver)** (de Deepmind) publicado con el paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) por Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hรฉnaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Joรฃo Carreira.\n+1. **[Mask2Former](model_doc/mask2former)** (de FAIR y UIUC) publicado con el paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://huggingface.co/papers/2112.01527) por Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.\n+1. **[MaskFormer](model_doc/maskformer)** (de Meta y UIUC) publicado con el paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://huggingface.co/papers/2107.06278) por Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n+1. **[MBart](model_doc/mbart)** (de Facebook) publicado con el paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://huggingface.co/papers/2001.08210) por Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n+1. **[MBart-50](model_doc/mbart)** (de Facebook) publicado con el paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://huggingface.co/papers/2008.00401) por Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n+1. **[Megatron-BERT](model_doc/megatron-bert)** (de NVIDIA) publicado con el paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://huggingface.co/papers/1909.08053) por Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper y Bryan Catanzaro.\n+1. **[Megatron-GPT2](model_doc/megatron_gpt2)** (de NVIDIA) publicado con el paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://huggingface.co/papers/1909.08053) por Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper y Bryan Catanzaro.\n+1. **[MPNet](model_doc/mpnet)** (de Microsoft Research) publicado con el paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://huggingface.co/papers/2004.09297) por Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n+1. **[MT5](model_doc/mt5)** (de Google AI) publicado con el paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://huggingface.co/papers/2010.11934) por Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n+1. **[Nystrรถmformer](model_doc/nystromformer)** (de la Universidad de Wisconsin - Madison) publicado con el paper [Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention](https://huggingface.co/papers/2102.03902) por Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n+1. **[OneFormer](model_doc/oneformer)** (de la SHI Labs) publicado con el paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://huggingface.co/papers/2211.06220) por Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n+1. **[Pegasus](model_doc/pegasus)** (de Google) publicado con el paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://huggingface.co/papers/1912.08777) por Jingqing Zhang, Yao Zhao, Mohammad Saleh y Peter J. Liu.\n+1. **[Perceiver IO](model_doc/perceiver)** (de Deepmind) publicado con el paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://huggingface.co/papers/2107.14795) por Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hรฉnaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Joรฃo Carreira.\n 1. **[PhoBERT](model_doc/phobert)** (de VinAI Research) publicado con el paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) por Dat Quoc Nguyen y Anh Tuan Nguyen.\n-1. **[PLBart](model_doc/plbart)** (de UCLA NLP) publicado con el paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) por Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n-1. **[PoolFormer](model_doc/poolformer)** (de Sea AI Labs) publicado con el paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) por Yu, Weihao y Luo, Mi y Zhou, Pan y Si, Chenyang y Zhou, Yichen y Wang, Xinchao y Feng, Jiashi y Yan, Shuicheng.\n-1. **[ProphetNet](model_doc/prophetnet)** (de Microsoft Research) publicado con el paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) por Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang y Ming Zhou.\n-1. **[QDQBert](model_doc/qdqbert)** (de NVIDIA) publicado con el paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) por Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev y Paulius Micikevicius.\n-1. **[REALM](model_doc/realm.html)** (de Google Research) publicado con el paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) por Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat y Ming-Wei Chang.\n-1. **[Reformer](model_doc/reformer)** (de Google Research) publicado con el paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) por Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n-1. **[RemBERT](model_doc/rembert)** (de Google Research) publicado con el paper [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) por Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, M. Johnson, Sebastian Ruder.\n-1. **[RegNet](model_doc/regnet)** (de META Platforms) publicado con el paper [Designing Network Design Space](https://arxiv.org/abs/2003.13678) por Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n-1. **[ResNet](model_doc/resnet)** (de Microsoft Research) publicado con el paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) por Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n-1. **[RoBERTa](model_doc/roberta)** (de Facebook), publicado junto con el paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) por Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n-1. **[RoFormer](model_doc/roformer)** (de ZhuiyiTechnology), publicado junto con el paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) por Jianlin Su y Yu Lu y Shengfeng Pan y Bo Wen y Yunfeng Liu.\n-1. **[SegFormer](model_doc/segformer)** (de NVIDIA) publicado con el paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) por Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n-1. **[SEW](model_doc/sew)** (de ASAPP) publicado con el paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) por Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n-1. **[SEW-D](model_doc/sew_d)** (de ASAPP) publicado con el paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) por Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n-1. **[SpeechToTextTransformer](model_doc/speech_to_text)** (de Facebook), publicado junto con el paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) por Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n-1. **[SpeechToTextTransformer2](model_doc/speech_to_text_2)** (de Facebook), publicado junto con el paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) por Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n-1. **[Splinter](model_doc/splinter)** (de Universidad de Tel Aviv), publicado junto con el paper [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) pory Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n-1. **[SqueezeBert](model_doc/squeezebert)** (de Berkeley) publicado con el paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) por Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, y Kurt W. Keutzer.\n-1. **[Swin Transformer](model_doc/swin)** (de Microsoft) publicado con el paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) por Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n-1. **[T5](model_doc/t5)** (de Google AI) publicado con el paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) por Colin Raffel y Noam Shazeer y Adam Roberts y Katherine Lee y Sharan Narang y Michael Matena y Yanqi Zhou y Wei Li y Peter J. Liu.\n+1. **[PLBart](model_doc/plbart)** (de UCLA NLP) publicado con el paper [Unified Pre-training for Program Understanding and Generation](https://huggingface.co/papers/2103.06333) por Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n+1. **[PoolFormer](model_doc/poolformer)** (de Sea AI Labs) publicado con el paper [MetaFormer is Actually What You Need for Vision](https://huggingface.co/papers/2111.11418) por Yu, Weihao y Luo, Mi y Zhou, Pan y Si, Chenyang y Zhou, Yichen y Wang, Xinchao y Feng, Jiashi y Yan, Shuicheng.\n+1. **[ProphetNet](model_doc/prophetnet)** (de Microsoft Research) publicado con el paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://huggingface.co/papers/2001.04063) por Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang y Ming Zhou.\n+1. **[QDQBert](model_doc/qdqbert)** (de NVIDIA) publicado con el paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://huggingface.co/papers/2004.09602) por Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev y Paulius Micikevicius.\n+1. **[REALM](model_doc/realm.html)** (de Google Research) publicado con el paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://huggingface.co/papers/2002.08909) por Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat y Ming-Wei Chang.\n+1. **[Reformer](model_doc/reformer)** (de Google Research) publicado con el paper [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) por Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n+1. **[RemBERT](model_doc/rembert)** (de Google Research) publicado con el paper [Rethinking embedding coupling in pre-trained language models](https://huggingface.co/papers/2010.12821) por Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, M. Johnson, Sebastian Ruder.\n+1. **[RegNet](model_doc/regnet)** (de META Platforms) publicado con el paper [Designing Network Design Space](https://huggingface.co/papers/2003.13678) por Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n+1. **[ResNet](model_doc/resnet)** (de Microsoft Research) publicado con el paper [Deep Residual Learning for Image Recognition](https://huggingface.co/papers/1512.03385) por Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n+1. **[RoBERTa](model_doc/roberta)** (de Facebook), publicado junto con el paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://huggingface.co/papers/1907.11692) por Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n+1. **[RoFormer](model_doc/roformer)** (de ZhuiyiTechnology), publicado junto con el paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://huggingface.co/papers/2104.09864) por Jianlin Su y Yu Lu y Shengfeng Pan y Bo Wen y Yunfeng Liu.\n+1. **[SegFormer](model_doc/segformer)** (de NVIDIA) publicado con el paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://huggingface.co/papers/2105.15203) por Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n+1. **[SEW](model_doc/sew)** (de ASAPP) publicado con el paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) por Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n+1. **[SEW-D](model_doc/sew_d)** (de ASAPP) publicado con el paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) por Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n+1. **[SpeechToTextTransformer](model_doc/speech_to_text)** (de Facebook), publicado junto con el paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://huggingface.co/papers/2010.05171) por Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n+1. **[SpeechToTextTransformer2](model_doc/speech_to_text_2)** (de Facebook), publicado junto con el paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://huggingface.co/papers/2104.06678) por Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n+1. **[Splinter](model_doc/splinter)** (de Universidad de Tel Aviv), publicado junto con el paper [Few-Shot Question Answering by Pretraining Span Selection](https://huggingface.co/papers/2101.00438) pory Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n+1. **[SqueezeBert](model_doc/squeezebert)** (de Berkeley) publicado con el paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://huggingface.co/papers/2006.11316) por Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, y Kurt W. Keutzer.\n+1. **[Swin Transformer](model_doc/swin)** (de Microsoft) publicado con el paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://huggingface.co/papers/2103.14030) por Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n+1. **[T5](model_doc/t5)** (de Google AI) publicado con el paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://huggingface.co/papers/1910.10683) por Colin Raffel y Noam Shazeer y Adam Roberts y Katherine Lee y Sharan Narang y Michael Matena y Yanqi Zhou y Wei Li y Peter J. Liu.\n 1. **[T5v1.1](model_doc/t5v1.1)** (de Google AI) publicado en el repositorio [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) por Colin Raffel y Noam Shazeer y Adam Roberts y Katherine Lee y Sharan Narang y Michael Matena y Yanqi Zhou y Wei Li y Peter J. Liu.\n-1. **[TAPAS](model_doc/tapas)** (de Google AI) publicado con el paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) por Jonathan Herzig, Paweล Krzysztof Nowak, Thomas Mรผller, Francesco Piccinno y Julian Martin Eisenschlos.\n-1. **[TAPEX](model_doc/tapex)** (de Microsoft Research) publicado con el paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) por Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\n-1. **[Transformer-XL](model_doc/transfo-xl)** (de Google/CMU) publicado con el paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) por Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n-1. **[TrOCR](model_doc/trocr)** (de Microsoft), publicado junto con el paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) por Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n-1. **[UniSpeech](model_doc/unispeech)** (de Microsoft Research) publicado con el paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) por Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n-1. **[UniSpeechSat](model_doc/unispeech-sat)** (de Microsoft Research) publicado con el paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) por Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n-1. **[VAN](model_doc/van)** (de la Universidad de Tsinghua y la Universidad de Nankai) publicado con el paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) por Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n-1. **[ViLT](model_doc/vilt)** (de NAVER AI Lab/Kakao Enterprise/Kakao Brain) publicado con el paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) por Wonjae Kim, Bokyung Son, Ildoo Kim.\n-1. **[Vision Transformer (ViT)](model_doc/vit)** (de Google AI) publicado con el paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) por Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n-1. **[ViTMAE](model_doc/vit_mae)** (de Meta AI) publicado con el paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) por Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollรกr, Ross Girshick.\n-1. **[VisualBERT](model_doc/visual_bert)** (de UCLA NLP) publicado con el paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) por Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n-1. **[WavLM](model_doc/wavlm)** (de Microsoft Research) publicado con el paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) por Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n-1. **[Wav2Vec2](model_doc/wav2vec2)** (de Facebook AI) publicado con el paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) por Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n-1. **[Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)** (de Facebook AI) publicado con el paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) por Qiantong Xu, Alexei Baevski, Michael Auli.\n-1. **[XGLM](model_doc/xglm)** (de Facebook AI) publicado con el paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) por Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n-1. **[XLM](model_doc/xlm)** (de Facebook) publicado junto con el paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) por Guillaume Lample y Alexis Conneau.\n-1. **[XLM-ProphetNet](model_doc/xlm-prophetnet)** (de Microsoft Research) publicado con el paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) por Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang y Ming Zhou.\n-1. **[XLM-RoBERTa](model_doc/xlm-roberta)** (de Facebook AI), publicado junto con el paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) por Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmรกn, Edouard Grave, Myle Ott, Luke Zettlemoyer y Veselin Stoyanov.\n-1. **[XLM-RoBERTa-XL](model_doc/xlm-roberta-xl)** (de Facebook AI), publicado junto con el paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) por Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n-1. **[XLNet](model_doc/xlnet)** (de Google/CMU) publicado con el paper [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) por Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n-1. **[XLSR-Wav2Vec2](model_doc/xlsr_wav2vec2)** (de Facebook AI) publicado con el paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) por Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n-1. **[XLS-R](model_doc/xls_r)** (de Facebook AI) publicado con el paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) por Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n-1. **[YOSO](model_doc/yoso)** (de la Universidad de Wisconsin-Madison) publicado con el paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) por Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n+1. **[TAPAS](model_doc/tapas)** (de Google AI) publicado con el paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://huggingface.co/papers/2004.02349) por Jonathan Herzig, Paweล Krzysztof Nowak, Thomas Mรผller, Francesco Piccinno y Julian Martin Eisenschlos.\n+1. **[TAPEX](model_doc/tapex)** (de Microsoft Research) publicado con el paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://huggingface.co/papers/2107.07653) por Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\n+1. **[Transformer-XL](model_doc/transfo-xl)** (de Google/CMU) publicado con el paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://huggingface.co/papers/1901.02860) por Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n+1. **[TrOCR](model_doc/trocr)** (de Microsoft), publicado junto con el paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://huggingface.co/papers/2109.10282) por Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n+1. **[UniSpeech](model_doc/unispeech)** (de Microsoft Research) publicado con el paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://huggingface.co/papers/2101.07597) por Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n+1. **[UniSpeechSat](model_doc/unispeech-sat)** (de Microsoft Research) publicado con el paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://huggingface.co/papers/2110.05752) por Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n+1. **[VAN](model_doc/van)** (de la Universidad de Tsinghua y la Universidad de Nankai) publicado con el paper [Visual Attention Network](https://huggingface.co/papers/2202.09741) por Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n+1. **[ViLT](model_doc/vilt)** (de NAVER AI Lab/Kakao Enterprise/Kakao Brain) publicado con el paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://huggingface.co/papers/2102.03334) por Wonjae Kim, Bokyung Son, Ildoo Kim.\n+1. **[Vision Transformer (ViT)](model_doc/vit)** (de Google AI) publicado con el paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://huggingface.co/papers/2010.11929) por Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n+1. **[ViTMAE](model_doc/vit_mae)** (de Meta AI) publicado con el paper [Masked Autoencoders Are Scalable Vision Learners](https://huggingface.co/papers/2111.06377) por Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollรกr, Ross Girshick.\n+1. **[VisualBERT](model_doc/visual_bert)** (de UCLA NLP) publicado con el paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://huggingface.co/papers/1908.03557) por Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n+1. **[WavLM](model_doc/wavlm)** (de Microsoft Research) publicado con el paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://huggingface.co/papers/2110.13900) por Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n+1. **[Wav2Vec2](model_doc/wav2vec2)** (de Facebook AI) publicado con el paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://huggingface.co/papers/2006.11477) por Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n+1. **[Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)** (de Facebook AI) publicado con el paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://huggingface.co/papers/2109.11680) por Qiantong Xu, Alexei Baevski, Michael Auli.\n+1. **[XGLM](model_doc/xglm)** (de Facebook AI) publicado con el paper [Few-shot Learning with Multilingual Language Models](https://huggingface.co/papers/2112.10668) por Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n+1. **[XLM](model_doc/xlm)** (de Facebook) publicado junto con el paper [Cross-lingual Language Model Pretraining](https://huggingface.co/papers/1901.07291) por Guillaume Lample y Alexis Conneau.\n+1. **[XLM-ProphetNet](model_doc/xlm-prophetnet)** (de Microsoft Research) publicado con el paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://huggingface.co/papers/2001.04063) por Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang y Ming Zhou.\n+1. **[XLM-RoBERTa](model_doc/xlm-roberta)** (de Facebook AI), publicado junto con el paper [Unsupervised Cross-lingual Representation Learning at Scale](https://huggingface.co/papers/1911.02116) por Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmรกn, Edouard Grave, Myle Ott, Luke Zettlemoyer y Veselin Stoyanov.\n+1. **[XLM-RoBERTa-XL](model_doc/xlm-roberta-xl)** (de Facebook AI), publicado junto con el paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://huggingface.co/papers/2105.00572) por Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n+1. **[XLNet](model_doc/xlnet)** (de Google/CMU) publicado con el paper [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://huggingface.co/papers/1906.08237) por Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n+1. **[XLSR-Wav2Vec2](model_doc/xlsr_wav2vec2)** (de Facebook AI) publicado con el paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://huggingface.co/papers/2006.13979) por Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n+1. **[XLS-R](model_doc/xls_r)** (de Facebook AI) publicado con el paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://huggingface.co/papers/2111.09296) por Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n+1. **[YOSO](model_doc/yoso)** (de la Universidad de Wisconsin-Madison) publicado con el paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://huggingface.co/papers/2111.09714) por Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n \n \n ### Frameworks compatibles"
        },
        {
            "sha": "54609a1c1e7959376c057d1664992cfb9a3eaa42",
            "filename": "docs/source/es/model_memory_anatomy.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Fmodel_memory_anatomy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Fmodel_memory_anatomy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fmodel_memory_anatomy.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -185,7 +185,7 @@ La arquitectura de los transformers incluye 3 grupos principales de operaciones\n \n Este conocimiento puede ser รบtil al analizar cuellos de botella de rendimiento.\n \n-Este resumen se deriva de [Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020](https://arxiv.org/abs/2007.00072)\n+Este resumen se deriva de [Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020](https://huggingface.co/papers/2007.00072)\n \n \n ## Anatomรญa de la Memoria del Modelo"
        },
        {
            "sha": "69d822e82ac67001d0668fff8adb0e6041e2b46a",
            "filename": "docs/source/es/tasks_explained.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Ftasks_explained.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Ftasks_explained.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks_explained.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -120,7 +120,7 @@ Esta secciรณn explica brevemente las convoluciones, pero serรญa รบtil tener un e\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\n </div>\n \n-<small>Una convoluciรณn bรกsica sin relleno ni paso, tomada de <a href=\"https://arxiv.org/abs/1603.07285\">Una guรญa para la aritmรฉtica de convoluciones para el aprendizaje profundo.</a></small>\n+<small>Una convoluciรณn bรกsica sin relleno ni paso, tomada de <a href=\"https://huggingface.co/papers/1603.07285\">Una guรญa para la aritmรฉtica de convoluciones para el aprendizaje profundo.</a></small>\n \n Puedes alimentar esta salida a otra capa convolucional, y con cada capa sucesiva, la red aprende cosas mรกs complejas y abstractas como perros calientes o cohetes. Entre capas convolucionales, es comรบn aรฑadir una capa de agrupaciรณn para reducir la dimensionalidad y hacer que el modelo sea mรกs robusto a las variaciones de la posiciรณn de una caracterรญstica.\n "
        },
        {
            "sha": "731f16b3fedd8579438c3cc0fd4cd5bfabfc0e68",
            "filename": "docs/source/es/tokenizer_summary.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Ftokenizer_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fes%2Ftokenizer_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftokenizer_summary.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -98,7 +98,7 @@ Ahora, veamos cรณmo funcionan los diferentes algoritmos de tokenizaciรณn de subp\n \n ### Byte-Pair Encoding (BPE)\n \n-La Codificaciรณn por Pares de Bytes (BPE por sus siglas en inglรฉs) fue introducida en [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909). BPE se basa en un pre-tokenizador que divide los datos de entrenamiento en palabras. La pre-tokenizaciรณn puede ser tan simple como la tokenizaciรณn por espacio, por ejemplo, [GPT-2](https://huggingface.co/docs/transformers/en/model_doc/gpt2), [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta). La pre-tokenizaciรณn mรกs avanzada incluye la tokenizaciรณn basada en reglas, por ejemplo, [XLM](https://huggingface.co/docs/transformers/en/model_doc/xlm), [FlauBERT](https://huggingface.co/docs/transformers/en/model_doc/flaubert) que utiliza Moses para la mayorรญa de los idiomas, o [GPT](https://huggingface.co/docs/transformers/en/model_doc/openai-gpt) que utiliza spaCy y ftfy, para contar la frecuencia de cada palabra en el corpus de entrenamiento.\n+La Codificaciรณn por Pares de Bytes (BPE por sus siglas en inglรฉs) fue introducida en [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://huggingface.co/papers/1508.07909). BPE se basa en un pre-tokenizador que divide los datos de entrenamiento en palabras. La pre-tokenizaciรณn puede ser tan simple como la tokenizaciรณn por espacio, por ejemplo, [GPT-2](https://huggingface.co/docs/transformers/en/model_doc/gpt2), [RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta). La pre-tokenizaciรณn mรกs avanzada incluye la tokenizaciรณn basada en reglas, por ejemplo, [XLM](https://huggingface.co/docs/transformers/en/model_doc/xlm), [FlauBERT](https://huggingface.co/docs/transformers/en/model_doc/flaubert) que utiliza Moses para la mayorรญa de los idiomas, o [GPT](https://huggingface.co/docs/transformers/en/model_doc/openai-gpt) que utiliza spaCy y ftfy, para contar la frecuencia de cada palabra en el corpus de entrenamiento.\n \n Despuรฉs de la pre-tokenizaciรณn, se ha creado un conjunto de palabras รบnicas y ha determinado la frecuencia con la que cada palabra apareciรณ en los datos de entrenamiento. A continuaciรณn, BPE crea un vocabulario base que consiste en todos los sรญmbolos que aparecen en el conjunto de palabras รบnicas y aprende reglas de fusiรณn para formar un nuevo sรญmbolo a partir de dos sรญmbolos del vocabulario base. Lo hace hasta que el vocabulario ha alcanzado el tamaรฑo de vocabulario deseado. Tenga en cuenta que el tamaรฑo de vocabulario deseado es un hiperparรกmetro que se debe definir antes de entrenar el tokenizador.\n \n@@ -148,7 +148,7 @@ WordPiece es el algoritmo de tokenizaciรณn de subpalabras utilizado por [BERT](h\n \n ### Unigram\n \n-Unigram es un algoritmo de tokenizaciรณn de subpalabras introducido en [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). A diferencia de BPE o WordPiece, Unigram inicializa su vocabulario base con un gran nรบmero de sรญmbolos y progresivamente recorta cada sรญmbolo para obtener un vocabulario mรกs pequeรฑo. El vocabulario base podrรญa corresponder, por ejemplo, a todas las palabras pre-tokenizadas y las subcadenas mรกs comunes. Unigram no se utiliza directamente para ninguno de los modelos transformers, pero se utiliza en conjunto con [SentencePiece](#sentencepiece).\n+Unigram es un algoritmo de tokenizaciรณn de subpalabras introducido en [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)](https://huggingface.co/papers/1804.10959). A diferencia de BPE o WordPiece, Unigram inicializa su vocabulario base con un gran nรบmero de sรญmbolos y progresivamente recorta cada sรญmbolo para obtener un vocabulario mรกs pequeรฑo. El vocabulario base podrรญa corresponder, por ejemplo, a todas las palabras pre-tokenizadas y las subcadenas mรกs comunes. Unigram no se utiliza directamente para ninguno de los modelos transformers, pero se utiliza en conjunto con [SentencePiece](#sentencepiece).\n \n En cada paso de entrenamiento, el algoritmo Unigram define una pรฉrdida (a menudo definida como la probabilidad logarรญtmica) sobre los datos de entrenamiento dados el vocabulario actual y un modelo de lenguaje unigram. Luego, para cada sรญmbolo en el vocabulario, el algoritmo calcula cuรกnto aumentarรญa la pรฉrdida general si el sรญmbolo se eliminara del vocabulario. Luego, Unigram elimina un porcentaje `p` de los sรญmbolos cuyo aumento de pรฉrdida es el mรกs bajo (siendo `p` generalmente 10% o 20%), es decir, aquellos sรญmbolos que menos afectan la pรฉrdida general sobre los datos de entrenamiento. Este proceso se repite hasta que el vocabulario haya alcanzado el tamaรฑo deseado. El algoritmo Unigram siempre mantiene los caracteres base para que cualquier palabra pueda ser tokenizada.\n \n@@ -168,7 +168,7 @@ $$\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )\n \n ### SentencePiece\n \n-Todos los algoritmos de tokenizaciรณn descritos hasta ahora tienen el mismo problema: se asume que el texto de entrada utiliza espacios para separar palabras. Sin embargo, no todos los idiomas utilizan espacios para separar palabras. Una posible soluciรณn es utilizar pre-tokenizadores especรญficos del idioma, *ej.* [XLM](https://huggingface.co/docs/transformers/en/model_doc/xlm) utiliza un pre-tokenizador especรญfico para chino, japonรฉs y tailandรฉs. Para resolver este problema de manera mรกs general, [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)](https://arxiv.org/pdf/1808.06226.pdf) trata el texto de entrada como una corriente de entrada bruta, por lo que incluye el espacio en el conjunto de caracteres para utilizar. Luego utiliza el algoritmo BPE o unigram para construir el vocabulario apropiado.\n+Todos los algoritmos de tokenizaciรณn descritos hasta ahora tienen el mismo problema: se asume que el texto de entrada utiliza espacios para separar palabras. Sin embargo, no todos los idiomas utilizan espacios para separar palabras. Una posible soluciรณn es utilizar pre-tokenizadores especรญficos del idioma, *ej.* [XLM](https://huggingface.co/docs/transformers/en/model_doc/xlm) utiliza un pre-tokenizador especรญfico para chino, japonรฉs y tailandรฉs. Para resolver este problema de manera mรกs general, [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)](https://huggingface.co/papers/1808.06226) trata el texto de entrada como una corriente de entrada bruta, por lo que incluye el espacio en el conjunto de caracteres para utilizar. Luego utiliza el algoritmo BPE o unigram para construir el vocabulario apropiado.\n \n Por ejemplo, [`XLNetTokenizer`](https://huggingface.co/docs/transformers/en/model_doc/xlnet#transformers.XLNetTokenizer) utiliza SentencePiece, razรณn por la cual en el ejemplo anterior se incluyรณ el carรกcter `\"โ\"` en el vocabulario. Decodificar con SentencePiece es muy fรกcil, ya que todos los tokens pueden simplemente concatenarse y `\"โ\"` se reemplaza por un espacio.\n "
        },
        {
            "sha": "61ca795cf99530ab54bf42f67420eb97db913a0f",
            "filename": "docs/source/fr/index.md",
            "status": "modified",
            "additions": 162,
            "deletions": 162,
            "changes": 324,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Ffr%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Ffr%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Findex.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -53,186 +53,186 @@ La documentation est organisรฉe en 5 parties:\n \n <!--This list is updated automatically from the README with _make fix-copies_. Do not update manually! -->\n \n-1. **[ALBERT](model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n-1. **[ALIGN](model_doc/align)** (from Google Research) released with the paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.\n-1. **[AltCLIP](model_doc/altclip)** (from BAAI) released with the paper [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679) by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.\n-1. **[Audio Spectrogram Transformer](model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\n-1. **[BART](model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n-1. **[BARThez](model_doc/barthez)** (from รcole polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n-1. **[BARTpho](model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n-1. **[BEiT](model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\n-1. **[BERT](model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n-1. **[BERT For Sequence Generation](model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+1. **[ALBERT](model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://huggingface.co/papers/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n+1. **[ALIGN](model_doc/align)** (from Google Research) released with the paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://huggingface.co/papers/2102.05918) by Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.\n+1. **[AltCLIP](model_doc/altclip)** (from BAAI) released with the paper [AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://huggingface.co/papers/2211.06679) by Chen, Zhongzhi and Liu, Guang and Zhang, Bo-Wen and Ye, Fulong and Yang, Qinghong and Wu, Ledell.\n+1. **[Audio Spectrogram Transformer](model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://huggingface.co/papers/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\n+1. **[BART](model_doc/bart)** (from Facebook) released with the paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://huggingface.co/papers/1910.13461) by Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov and Luke Zettlemoyer.\n+1. **[BARThez](model_doc/barthez)** (from รcole polytechnique) released with the paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://huggingface.co/papers/2010.12321) by Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n+1. **[BARTpho](model_doc/bartpho)** (from VinAI Research) released with the paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://huggingface.co/papers/2109.09701) by Nguyen Luong Tran, Duong Minh Le and Dat Quoc Nguyen.\n+1. **[BEiT](model_doc/beit)** (from Microsoft) released with the paper [BEiT: BERT Pre-Training of Image Transformers](https://huggingface.co/papers/2106.08254) by Hangbo Bao, Li Dong, Furu Wei.\n+1. **[BERT](model_doc/bert)** (from Google) released with the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://huggingface.co/papers/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova.\n+1. **[BERT For Sequence Generation](model_doc/bert-generation)** (from Google) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n 1. **[BERTweet](model_doc/bertweet)** (from VinAI Research) released with the paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) by Dat Quoc Nguyen, Thanh Vu and Anh Tuan Nguyen.\n-1. **[BigBird-Pegasus](model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n-1. **[BigBird-RoBERTa](model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n+1. **[BigBird-Pegasus](model_doc/bigbird_pegasus)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n+1. **[BigBird-RoBERTa](model_doc/big_bird)** (from Google Research) released with the paper [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) by Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n 1. **[BioGpt](model_doc/biogpt)** (from Microsoft Research AI4Science) released with the paper [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu.\n-1. **[BiT](model_doc/bit)** (from Google AI) released with the paper [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.\n-1. **[Blenderbot](model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n-1. **[BlenderbotSmall](model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n-1. **[BLIP](model_doc/blip)** (from Salesforce) released with the paper [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\n+1. **[BiT](model_doc/bit)** (from Google AI) released with the paper [Big Transfer (BiT): General Visual Representation Learning](https://huggingface.co/papers/1912.11370) by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.\n+1. **[Blenderbot](model_doc/blenderbot)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n+1. **[BlenderbotSmall](model_doc/blenderbot-small)** (from Facebook) released with the paper [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) by Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n+1. **[BLIP](model_doc/blip)** (from Salesforce) released with the paper [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://huggingface.co/papers/2201.12086) by Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi.\n 1. **[BLOOM](model_doc/bloom)** (from BigScience workshop) released by the [BigScience Workshop](https://bigscience.huggingface.co/).\n-1. **[BORT](model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) by Adrian de Wynter and Daniel J. Perry.\n-1. **[BridgeTower](model_doc/bridgetower)** (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://arxiv.org/abs/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.\n-1. **[ByT5](model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n-1. **[CamemBERT](model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suรกrez*, Yoann Dupont, Laurent Romary, รric Villemonte de la Clergerie, Djamรฉ Seddah and Benoรฎt Sagot.\n-1. **[CANINE](model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n-1. **[Chinese-CLIP](model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.\n-1. **[CLIP](model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n-1. **[CLIPSeg](model_doc/clipseg)** (from University of Gรถttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo Lรผddecke and Alexander Ecker.\n-1. **[CodeGen](model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://arxiv.org/abs/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.\n-1. **[Conditional DETR](model_doc/conditional_detr)** (from Microsoft Research Asia) released with the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.\n-1. **[ConvBERT](model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n-1. **[ConvNeXT](model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n-1. **[ConvNeXTV2](model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n-1. **[CPM](model_doc/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n-1. **[CTRL](model_doc/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\n-1. **[CvT](model_doc/cvt)** (from Microsoft) released with the paper [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\n-1. **[Data2Vec](model_doc/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n-1. **[DeBERTa](model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n-1. **[DeBERTa-v2](model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n-1. **[Decision Transformer](model_doc/decision_transformer)** (from Berkeley/Facebook/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n-1. **[Deformable DETR](model_doc/deformable_detr)** (from SenseTime Research) released with the paper [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.\n-1. **[DeiT](model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervรฉ Jรฉgou.\n-1. **[DETA](model_doc/deta)** (from The University of Texas at Austin) released with the paper [NMS Strikes Back](https://arxiv.org/abs/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krรคhenbรผhl.\n-1. **[DETR](model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n-1. **[DialoGPT](model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n-1. **[DiNAT](model_doc/dinat)** (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://arxiv.org/abs/2209.15001) by Ali Hassani and Humphrey Shi.\n-1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n-1. **[DiT](model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n-1. **[Donut](model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\n-1. **[DPR](model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) by Vladimir Karpukhin, Barlas Oฤuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n-1. **[DPT](master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) by Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n-1. **[EfficientFormer](model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://arxiv.org/abs/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.\n-1. **[ELECTRA](model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n-1. **[EncoderDecoder](model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n-1. **[ERNIE](model_doc/ernie)** (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.\n+1. **[BORT](model_doc/bort)** (from Alexa) released with the paper [Optimal Subarchitecture Extraction For BERT](https://huggingface.co/papers/2010.10499) by Adrian de Wynter and Daniel J. Perry.\n+1. **[BridgeTower](model_doc/bridgetower)** (from Harbin Institute of Technology/Microsoft Research Asia/Intel Labs) released with the paper [BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning](https://huggingface.co/papers/2206.08657) by Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan.\n+1. **[ByT5](model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://huggingface.co/papers/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n+1. **[CamemBERT](model_doc/camembert)** (from Inria/Facebook/Sorbonne) released with the paper [CamemBERT: a Tasty French Language Model](https://huggingface.co/papers/1911.03894) by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suรกrez*, Yoann Dupont, Laurent Romary, รric Villemonte de la Clergerie, Djamรฉ Seddah and Benoรฎt Sagot.\n+1. **[CANINE](model_doc/canine)** (from Google Research) released with the paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://huggingface.co/papers/2103.06874) by Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n+1. **[Chinese-CLIP](model_doc/chinese_clip)** (from OFA-Sys) released with the paper [Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://huggingface.co/papers/2211.01335) by An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, Chang Zhou.\n+1. **[CLIP](model_doc/clip)** (from OpenAI) released with the paper [Learning Transferable Visual Models From Natural Language Supervision](https://huggingface.co/papers/2103.00020) by Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n+1. **[CLIPSeg](model_doc/clipseg)** (from University of Gรถttingen) released with the paper [Image Segmentation Using Text and Image Prompts](https://huggingface.co/papers/2112.10003) by Timo Lรผddecke and Alexander Ecker.\n+1. **[CodeGen](model_doc/codegen)** (from Salesforce) released with the paper [A Conversational Paradigm for Program Synthesis](https://huggingface.co/papers/2203.13474) by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong.\n+1. **[Conditional DETR](model_doc/conditional_detr)** (from Microsoft Research Asia) released with the paper [Conditional DETR for Fast Training Convergence](https://huggingface.co/papers/2108.06152) by Depu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, Jingdong Wang.\n+1. **[ConvBERT](model_doc/convbert)** (from YituTech) released with the paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://huggingface.co/papers/2008.02496) by Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n+1. **[ConvNeXT](model_doc/convnext)** (from Facebook AI) released with the paper [A ConvNet for the 2020s](https://huggingface.co/papers/2201.03545) by Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n+1. **[ConvNeXTV2](model_doc/convnextv2)** (from Facebook AI) released with the paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://huggingface.co/papers/2301.00808) by Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n+1. **[CPM](model_doc/cpm)** (from Tsinghua University) released with the paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://huggingface.co/papers/2012.00413) by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n+1. **[CTRL](model_doc/ctrl)** (from Salesforce) released with the paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://huggingface.co/papers/1909.05858) by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\n+1. **[CvT](model_doc/cvt)** (from Microsoft) released with the paper [CvT: Introducing Convolutions to Vision Transformers](https://huggingface.co/papers/2103.15808) by Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\n+1. **[Data2Vec](model_doc/data2vec)** (from Facebook) released with the paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://huggingface.co/papers/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n+1. **[DeBERTa](model_doc/deberta)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n+1. **[DeBERTa-v2](model_doc/deberta-v2)** (from Microsoft) released with the paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n+1. **[Decision Transformer](model_doc/decision_transformer)** (from Berkeley/Facebook/Google) released with the paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://huggingface.co/papers/2106.01345) by Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n+1. **[Deformable DETR](model_doc/deformable_detr)** (from SenseTime Research) released with the paper [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://huggingface.co/papers/2010.04159) by Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai.\n+1. **[DeiT](model_doc/deit)** (from Facebook) released with the paper [Training data-efficient image transformers & distillation through attention](https://huggingface.co/papers/2012.12877) by Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervรฉ Jรฉgou.\n+1. **[DETA](model_doc/deta)** (from The University of Texas at Austin) released with the paper [NMS Strikes Back](https://huggingface.co/papers/2212.06137) by Jeffrey Ouyang-Zhang, Jang Hyun Cho, Xingyi Zhou, Philipp Krรคhenbรผhl.\n+1. **[DETR](model_doc/detr)** (from Facebook) released with the paper [End-to-End Object Detection with Transformers](https://huggingface.co/papers/2005.12872) by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n+1. **[DialoGPT](model_doc/dialogpt)** (from Microsoft Research) released with the paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://huggingface.co/papers/1911.00536) by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n+1. **[DiNAT](model_doc/dinat)** (from SHI Labs) released with the paper [Dilated Neighborhood Attention Transformer](https://huggingface.co/papers/2209.15001) by Ali Hassani and Humphrey Shi.\n+1. **[DistilBERT](model_doc/distilbert)** (from HuggingFace), released together with the paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://huggingface.co/papers/1910.01108) by Victor Sanh, Lysandre Debut and Thomas Wolf. The same method has been applied to compress GPT2 into [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa into [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT into [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n+1. **[DiT](model_doc/dit)** (from Microsoft Research) released with the paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://huggingface.co/papers/2203.02378) by Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n+1. **[Donut](model_doc/donut)** (from NAVER), released together with the paper [OCR-free Document Understanding Transformer](https://huggingface.co/papers/2111.15664) by Geewook Kim, Teakgyu Hong, Moonbin Yim, Jeongyeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, Seunghyun Park.\n+1. **[DPR](model_doc/dpr)** (from Facebook) released with the paper [Dense Passage Retrieval for Open-Domain Question Answering](https://huggingface.co/papers/2004.04906) by Vladimir Karpukhin, Barlas Oฤuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n+1. **[DPT](master/model_doc/dpt)** (from Intel Labs) released with the paper [Vision Transformers for Dense Prediction](https://huggingface.co/papers/2103.13413) by Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n+1. **[EfficientFormer](model_doc/efficientformer)** (from Snap Research) released with the paper [EfficientFormer: Vision Transformers at MobileNetSpeed](https://huggingface.co/papers/2206.01191) by Yanyu Li, Geng Yuan, Yang Wen, Ju Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, Jian Ren.\n+1. **[ELECTRA](model_doc/electra)** (from Google Research/Stanford University) released with the paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://huggingface.co/papers/2003.10555) by Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n+1. **[EncoderDecoder](model_doc/encoder-decoder)** (from Google Research) released with the paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+1. **[ERNIE](model_doc/ernie)** (from Baidu) released with the paper [ERNIE: Enhanced Representation through Knowledge Integration](https://huggingface.co/papers/1904.09223) by Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, Hua Wu.\n 1. **[ESM](model_doc/esm)** (from Meta AI) are transformer protein language models.  **ESM-1b** was released with the paper [Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences](https://www.pnas.org/content/118/15/e2016239118) by Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. **ESM-1v** was released with the paper [Language models enable zero-shot prediction of the effects of mutations on protein function](https://doi.org/10.1101/2021.07.09.450648) by Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu and Alexander Rives. **ESM-2 and ESMFold** were released with the paper [Language models of protein sequences at the scale of evolution enable accurate structure prediction](https://doi.org/10.1101/2022.07.20.500902) by Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Allan dos Santos Costa, Maryam Fazel-Zarandi, Tom Sercu, Sal Candido, Alexander Rives.\n-1. **[FastSpeech2Conformer](model_doc/fastspeech2_conformer)** (from ESPnet) released with the paper [Recent Developments On Espnet Toolkit Boosted By Conformer](https://arxiv.org/abs/2010.13956) by Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun Wei, Wangyou Zhang, and Yuekai Zhang.\n+1. **[FastSpeech2Conformer](model_doc/fastspeech2_conformer)** (from ESPnet) released with the paper [Recent Developments On Espnet Toolkit Boosted By Conformer](https://huggingface.co/papers/2010.13956) by Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun Wei, Wangyou Zhang, and Yuekai Zhang.\n 1. **[FLAN-T5](model_doc/flan-t5)** (from Google AI) released in the repository [google-research/t5x](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) by Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei\n-1. **[FlauBERT](model_doc/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) by Hang Le, Loรฏc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoรฎt Crabbรฉ, Laurent Besacier, Didier Schwab.\n-1. **[FLAVA](model_doc/flava)** (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.\n-1. **[FNet](model_doc/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n-1. **[Funnel Transformer](model_doc/funnel)** (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n-1. **[GIT](model_doc/git)** (from Microsoft Research) released with the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100) by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.\n-1. **[GLPN](model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n+1. **[FlauBERT](model_doc/flaubert)** (from CNRS) released with the paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://huggingface.co/papers/1912.05372) by Hang Le, Loรฏc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoรฎt Crabbรฉ, Laurent Besacier, Didier Schwab.\n+1. **[FLAVA](model_doc/flava)** (from Facebook AI) released with the paper [FLAVA: A Foundational Language And Vision Alignment Model](https://huggingface.co/papers/2112.04482) by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela.\n+1. **[FNet](model_doc/fnet)** (from Google Research) released with the paper [FNet: Mixing Tokens with Fourier Transforms](https://huggingface.co/papers/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n+1. **[Funnel Transformer](model_doc/funnel)** (from CMU/Google Brain) released with the paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://huggingface.co/papers/2006.03236) by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n+1. **[GIT](model_doc/git)** (from Microsoft Research) released with the paper [GIT: A Generative Image-to-text Transformer for Vision and Language](https://huggingface.co/papers/2205.14100) by Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang.\n+1. **[GLPN](model_doc/glpn)** (from KAIST) released with the paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://huggingface.co/papers/2201.07436) by Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n 1. **[GPT](model_doc/openai-gpt)** (from OpenAI) released with the paper [Improving Language Understanding by Generative Pre-Training](https://openai.com/research/language-unsupervised/) by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n 1. **[GPT Neo](model_doc/gpt_neo)** (from EleutherAI) released in the repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy.\n-1. **[GPT NeoX](model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\n+1. **[GPT NeoX](model_doc/gpt_neox)** (from EleutherAI) released with the paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://huggingface.co/papers/2204.06745) by Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\n 1. **[GPT NeoX Japanese](model_doc/gpt_neox_japanese)** (from ABEJA) released by Shinya Otani, Takayoshi Makabe, Anuj Arora, and Kyo Hattori.\n 1. **[GPT-2](model_doc/gpt2)** (from OpenAI) released with the paper [Language Models are Unsupervised Multitask Learners](https://openai.com/research/better-language-models/) by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever.\n 1. **[GPT-J](model_doc/gptj)** (from EleutherAI) released in the repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) by Ben Wang and Aran Komatsuzaki.\n 1. **[GPT-Sw3](model_doc/gpt-sw3)** (from AI-Sweden) released with the paper [Lessons Learned from GPT-SW3: Building the First Large-Scale Generative Language Model for Swedish](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.376.pdf) by Ariel Ekgren, Amaru Cuba Gyllensten, Evangelia Gogoulou, Alice Heiman, Severine Verlinden, Joey รhman, Fredrik Carlsson, Magnus Sahlgren.\n-1. **[Graphormer](model_doc/graphormer)** (from Microsoft) released with the paper [Do Transformers Really Perform Bad for Graph Representation?](https://arxiv.org/abs/2106.05234) by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.\n-1. **[GroupViT](model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://arxiv.org/abs/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\n-1. **[Hubert](model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n-1. **[I-BERT](model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n+1. **[Graphormer](model_doc/graphormer)** (from Microsoft) released with the paper [Do Transformers Really Perform Bad for Graph Representation?](https://huggingface.co/papers/2106.05234) by Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu.\n+1. **[GroupViT](model_doc/groupvit)** (from UCSD, NVIDIA) released with the paper [GroupViT: Semantic Segmentation Emerges from Text Supervision](https://huggingface.co/papers/2202.11094) by Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, Xiaolong Wang.\n+1. **[Hubert](model_doc/hubert)** (from Facebook) released with the paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://huggingface.co/papers/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n+1. **[I-BERT](model_doc/ibert)** (from Berkeley) released with the paper [I-BERT: Integer-only BERT Quantization](https://huggingface.co/papers/2101.01321) by Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n 1. **[ImageGPT](model_doc/imagegpt)** (from OpenAI) released with the paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) by Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.\n-1. **[Jukebox](model_doc/jukebox)** (from OpenAI) released with the paper [Jukebox: A Generative Model for Music](https://arxiv.org/pdf/2005.00341.pdf) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.\n-1. **[LayoutLM](model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n-1. **[LayoutLMv2](model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n-1. **[LayoutLMv3](model_doc/layoutlmv3)** (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.\n-1. **[LayoutXLM](model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n-1. **[LED](model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n-1. **[LeViT](model_doc/levit)** (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervรฉ Jรฉgou, Matthijs Douze.\n-1. **[LiLT](model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.\n-1. **[Longformer](model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n-1. **[LongT5](model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.\n-1. **[LUKE](model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n-1. **[LXMERT](model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) by Hao Tan and Mohit Bansal.\n-1. **[M-CTC-T](model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://arxiv.org/abs/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.\n-1. **[M2M100](model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n+1. **[Jukebox](model_doc/jukebox)** (from OpenAI) released with the paper [Jukebox: A Generative Model for Music](https://huggingface.co/papers/2005.00341) by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever.\n+1. **[LayoutLM](model_doc/layoutlm)** (from Microsoft Research Asia) released with the paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://huggingface.co/papers/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n+1. **[LayoutLMv2](model_doc/layoutlmv2)** (from Microsoft Research Asia) released with the paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://huggingface.co/papers/2012.14740) by Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n+1. **[LayoutLMv3](model_doc/layoutlmv3)** (from Microsoft Research Asia) released with the paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://huggingface.co/papers/2204.08387) by Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.\n+1. **[LayoutXLM](model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://huggingface.co/papers/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n+1. **[LED](model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n+1. **[LeViT](model_doc/levit)** (from Meta AI) released with the paper [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://huggingface.co/papers/2104.01136) by Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Hervรฉ Jรฉgou, Matthijs Douze.\n+1. **[LiLT](model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://huggingface.co/papers/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.\n+1. **[Longformer](model_doc/longformer)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.\n+1. **[LongT5](model_doc/longt5)** (from Google AI) released with the paper [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://huggingface.co/papers/2112.07916) by Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang.\n+1. **[LUKE](model_doc/luke)** (from Studio Ousia) released with the paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057) by Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n+1. **[LXMERT](model_doc/lxmert)** (from UNC Chapel Hill) released with the paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) by Hao Tan and Mohit Bansal.\n+1. **[M-CTC-T](model_doc/mctct)** (from Facebook) released with the paper [Pseudo-Labeling For Massively Multilingual Speech Recognition](https://huggingface.co/papers/2111.00161) by Loren Lugosch, Tatiana Likhomanenko, Gabriel Synnaeve, and Ronan Collobert.\n+1. **[M2M100](model_doc/m2m_100)** (from Facebook) released with the paper [Beyond English-Centric Multilingual Machine Translation](https://huggingface.co/papers/2010.11125) by Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n 1. **[MarianMT](model_doc/marian)** Machine translation models trained using [OPUS](http://opus.nlpl.eu/) data by Jรถrg Tiedemann. The [Marian Framework](https://marian-nmt.github.io/) is being developed by the Microsoft Translator Team.\n-1. **[MarkupLM](model_doc/markuplm)** (from Microsoft Research Asia) released with the paper [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://arxiv.org/abs/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.\n-1. **[Mask2Former](model_doc/mask2former)** (from FAIR and UIUC) released with the paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.\n-1. **[MaskFormer](model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n-1. **[mBART](model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n-1. **[mBART-50](model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n-1. **[Megatron-BERT](model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n-1. **[Megatron-GPT2](model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n-1. **[mLUKE](model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\n-1. **[MobileBERT](model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\n-1. **[MobileNetV1](model_doc/mobilenet_v1)** (from Google Inc.) released with the paper [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\n-1. **[MobileNetV2](model_doc/mobilenet_v2)** (from Google Inc.) released with the paper [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.\n-1. **[MobileViT](model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari.\n-1. **[MPNet](model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n-1. **[MT5](model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n-1. **[MVP](model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://arxiv.org/abs/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n-1. **[NAT](model_doc/nat)** (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://arxiv.org/abs/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n-1. **[Nezha](model_doc/nezha)** (from Huawei Noahโs Ark Lab) released with the paper [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://arxiv.org/abs/1909.00204) by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.\n-1. **[NLLB](model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://arxiv.org/abs/2207.04672) by the NLLB team.\n-1. **[Nystrรถmformer](model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n-1. **[OneFormer](model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n-1. **[OPT](master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\n-1. **[OWL-ViT](model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.\n-1. **[Pegasus](model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.\n-1. **[PEGASUS-X](model_doc/pegasus_x)** (from Google) released with the paper [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) by Jason Phang, Yao Zhao, and Peter J. Liu.\n-1. **[Perceiver IO](model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hรฉnaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Joรฃo Carreira.\n+1. **[MarkupLM](model_doc/markuplm)** (from Microsoft Research Asia) released with the paper [MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding](https://huggingface.co/papers/2110.08518) by Junlong Li, Yiheng Xu, Lei Cui, Furu Wei.\n+1. **[Mask2Former](model_doc/mask2former)** (from FAIR and UIUC) released with the paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://huggingface.co/papers/2112.01527) by Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.\n+1. **[MaskFormer](model_doc/maskformer)** (from Meta and UIUC) released with the paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://huggingface.co/papers/2107.06278) by Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n+1. **[mBART](model_doc/mbart)** (from Facebook) released with the paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://huggingface.co/papers/2001.08210) by Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n+1. **[mBART-50](model_doc/mbart)** (from Facebook) released with the paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://huggingface.co/papers/2008.00401) by Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n+1. **[Megatron-BERT](model_doc/megatron-bert)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://huggingface.co/papers/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n+1. **[Megatron-GPT2](model_doc/megatron_gpt2)** (from NVIDIA) released with the paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://huggingface.co/papers/1909.08053) by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper and Bryan Catanzaro.\n+1. **[mLUKE](model_doc/mluke)** (from Studio Ousia) released with the paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://huggingface.co/papers/2110.08151) by Ryokan Ri, Ikuya Yamada, and Yoshimasa Tsuruoka.\n+1. **[MobileBERT](model_doc/mobilebert)** (from CMU/Google Brain) released with the paper [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://huggingface.co/papers/2004.02984) by Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.\n+1. **[MobileNetV1](model_doc/mobilenet_v1)** (from Google Inc.) released with the paper [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://huggingface.co/papers/1704.04861) by Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam.\n+1. **[MobileNetV2](model_doc/mobilenet_v2)** (from Google Inc.) released with the paper [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://huggingface.co/papers/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen.\n+1. **[MobileViT](model_doc/mobilevit)** (from Apple) released with the paper [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://huggingface.co/papers/2110.02178) by Sachin Mehta and Mohammad Rastegari.\n+1. **[MPNet](model_doc/mpnet)** (from Microsoft Research) released with the paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://huggingface.co/papers/2004.09297) by Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n+1. **[MT5](model_doc/mt5)** (from Google AI) released with the paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://huggingface.co/papers/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n+1. **[MVP](model_doc/mvp)** (from RUC AI Box) released with the paper [MVP: Multi-task Supervised Pre-training for Natural Language Generation](https://huggingface.co/papers/2206.12131) by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.\n+1. **[NAT](model_doc/nat)** (from SHI Labs) released with the paper [Neighborhood Attention Transformer](https://huggingface.co/papers/2204.07143) by Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi.\n+1. **[Nezha](model_doc/nezha)** (from Huawei Noahโs Ark Lab) released with the paper [NEZHA: Neural Contextualized Representation for Chinese Language Understanding](https://huggingface.co/papers/1909.00204) by Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen and Qun Liu.\n+1. **[NLLB](model_doc/nllb)** (from Meta) released with the paper [No Language Left Behind: Scaling Human-Centered Machine Translation](https://huggingface.co/papers/2207.04672) by the NLLB team.\n+1. **[Nystrรถmformer](model_doc/nystromformer)** (from the University of Wisconsin - Madison) released with the paper [Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention](https://huggingface.co/papers/2102.03902) by Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n+1. **[OneFormer](model_doc/oneformer)** (from SHI Labs) released with the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://huggingface.co/papers/2211.06220) by Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n+1. **[OPT](master/model_doc/opt)** (from Meta AI) released with the paper [OPT: Open Pre-trained Transformer Language Models](https://huggingface.co/papers/2205.01068) by Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\n+1. **[OWL-ViT](model_doc/owlvit)** (from Google AI) released with the paper [Simple Open-Vocabulary Object Detection with Vision Transformers](https://huggingface.co/papers/2205.06230) by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby.\n+1. **[Pegasus](model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://huggingface.co/papers/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.\n+1. **[PEGASUS-X](model_doc/pegasus_x)** (from Google) released with the paper [Investigating Efficiently Extending Transformers for Long Input Summarization](https://huggingface.co/papers/2208.04347) by Jason Phang, Yao Zhao, and Peter J. Liu.\n+1. **[Perceiver IO](model_doc/perceiver)** (from Deepmind) released with the paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://huggingface.co/papers/2107.14795) by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hรฉnaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Joรฃo Carreira.\n 1. **[PhoBERT](model_doc/phobert)** (from VinAI Research) released with the paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) by Dat Quoc Nguyen and Anh Tuan Nguyen.\n-1. **[PLBart](model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n-1. **[PoolFormer](model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.\n-1. **[ProphetNet](model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n-1. **[QDQBert](model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.\n-1. **[RAG](model_doc/rag)** (from Facebook) released with the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kรผttler, Mike Lewis, Wen-tau Yih, Tim Rocktรคschel, Sebastian Riedel, Douwe Kiela.\n-1. **[REALM](model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.\n-1. **[Reformer](model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) by Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n-1. **[RegNet](model_doc/regnet)** (from META Platforms) released with the paper [Designing Network Design Space](https://arxiv.org/abs/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n-1. **[RemBERT](model_doc/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) by Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, M. Johnson, Sebastian Ruder.\n-1. **[ResNet](model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n-1. **[RoBERTa](model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n-1. **[RoBERTa-PreLayerNorm](model_doc/roberta-prelayernorm)** (from Facebook) released with the paper [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://arxiv.org/abs/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.\n+1. **[PLBart](model_doc/plbart)** (from UCLA NLP) released with the paper [Unified Pre-training for Program Understanding and Generation](https://huggingface.co/papers/2103.06333) by Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n+1. **[PoolFormer](model_doc/poolformer)** (from Sea AI Labs) released with the paper [MetaFormer is Actually What You Need for Vision](https://huggingface.co/papers/2111.11418) by Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng.\n+1. **[ProphetNet](model_doc/prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n+1. **[QDQBert](model_doc/qdqbert)** (from NVIDIA) released with the paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://huggingface.co/papers/2004.09602) by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius Micikevicius.\n+1. **[RAG](model_doc/rag)** (from Facebook) released with the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://huggingface.co/papers/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kรผttler, Mike Lewis, Wen-tau Yih, Tim Rocktรคschel, Sebastian Riedel, Douwe Kiela.\n+1. **[REALM](model_doc/realm.html)** (from Google Research) released with the paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://huggingface.co/papers/2002.08909) by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang.\n+1. **[Reformer](model_doc/reformer)** (from Google Research) released with the paper [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) by Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n+1. **[RegNet](model_doc/regnet)** (from META Platforms) released with the paper [Designing Network Design Space](https://huggingface.co/papers/2003.13678) by Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n+1. **[RemBERT](model_doc/rembert)** (from Google Research) released with the paper [Rethinking embedding coupling in pre-trained language models](https://huggingface.co/papers/2010.12821) by Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, M. Johnson, Sebastian Ruder.\n+1. **[ResNet](model_doc/resnet)** (from Microsoft Research) released with the paper [Deep Residual Learning for Image Recognition](https://huggingface.co/papers/1512.03385) by Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n+1. **[RoBERTa](model_doc/roberta)** (from Facebook), released together with the paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://huggingface.co/papers/1907.11692) by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n+1. **[RoBERTa-PreLayerNorm](model_doc/roberta-prelayernorm)** (from Facebook) released with the paper [fairseq: A Fast, Extensible Toolkit for Sequence Modeling](https://huggingface.co/papers/1904.01038) by Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli.\n 1. **[RoCBert](model_doc/roc_bert)** (from WeChatAI) released with the paper [RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining](https://aclanthology.org/2022.acl-long.65.pdf) by HuiSu, WeiweiShi, XiaoyuShen, XiaoZhou, TuoJi, JiaruiFang, JieZhou.\n-1. **[RoFormer](model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n-1. **[SegFormer](model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n-1. **[SEW](model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n-1. **[SEW-D](model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n-1. **[SpeechT5](model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n-1. **[SpeechToTextTransformer](model_doc/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n-1. **[SpeechToTextTransformer2](model_doc/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n-1. **[Splinter](model_doc/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n-1. **[SqueezeBERT](model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n-1. **[Swin Transformer](model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n-1. **[Swin Transformer V2](model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://arxiv.org/abs/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\n-1. **[Swin2SR](model_doc/swin2sr)** (from University of Wรผrzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.\n-1. **[SwitchTransformers](model_doc/switch_transformers)** (from Google) released with the paper [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\n-1. **[T5](model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n+1. **[RoFormer](model_doc/roformer)** (from ZhuiyiTechnology), released together with the paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://huggingface.co/papers/2104.09864) by Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu.\n+1. **[SegFormer](model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://huggingface.co/papers/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n+1. **[SEW](model_doc/sew)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n+1. **[SEW-D](model_doc/sew_d)** (from ASAPP) released with the paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) by Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n+1. **[SpeechT5](model_doc/speecht5)** (from Microsoft Research) released with the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://huggingface.co/papers/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n+1. **[SpeechToTextTransformer](model_doc/speech_to_text)** (from Facebook), released together with the paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://huggingface.co/papers/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n+1. **[SpeechToTextTransformer2](model_doc/speech_to_text_2)** (from Facebook), released together with the paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://huggingface.co/papers/2104.06678) by Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n+1. **[Splinter](model_doc/splinter)** (from Tel Aviv University), released together with the paper [Few-Shot Question Answering by Pretraining Span Selection](https://huggingface.co/papers/2101.00438) by Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n+1. **[SqueezeBERT](model_doc/squeezebert)** (from Berkeley) released with the paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://huggingface.co/papers/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer.\n+1. **[Swin Transformer](model_doc/swin)** (from Microsoft) released with the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://huggingface.co/papers/2103.14030) by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n+1. **[Swin Transformer V2](model_doc/swinv2)** (from Microsoft) released with the paper [Swin Transformer V2: Scaling Up Capacity and Resolution](https://huggingface.co/papers/2111.09883) by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.\n+1. **[Swin2SR](model_doc/swin2sr)** (from University of Wรผrzburg) released with the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://huggingface.co/papers/2209.11345) by Marcos V. Conde, Ui-Jin Choi, Maxime Burchi, Radu Timofte.\n+1. **[SwitchTransformers](model_doc/switch_transformers)** (from Google) released with the paper [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://huggingface.co/papers/2101.03961) by William Fedus, Barret Zoph, Noam Shazeer.\n+1. **[T5](model_doc/t5)** (from Google AI) released with the paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://huggingface.co/papers/1910.10683) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n 1. **[T5v1.1](model_doc/t5v1.1)** (from Google AI) released in the repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) by Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu.\n-1. **[Table Transformer](model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://arxiv.org/abs/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.\n-1. **[TAPAS](model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) by Jonathan Herzig, Paweล Krzysztof Nowak, Thomas Mรผller, Francesco Piccinno and Julian Martin Eisenschlos.\n-1. **[TAPEX](model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\n+1. **[Table Transformer](model_doc/table-transformer)** (from Microsoft Research) released with the paper [PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents](https://huggingface.co/papers/2110.00061) by Brandon Smock, Rohith Pesala, Robin Abraham.\n+1. **[TAPAS](model_doc/tapas)** (from Google AI) released with the paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://huggingface.co/papers/2004.02349) by Jonathan Herzig, Paweล Krzysztof Nowak, Thomas Mรผller, Francesco Piccinno and Julian Martin Eisenschlos.\n+1. **[TAPEX](model_doc/tapex)** (from Microsoft Research) released with the paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://huggingface.co/papers/2107.07653) by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\n 1. **[Time Series Transformer](model_doc/time_series_transformer)** (from HuggingFace).\n-1. **[TimeSformer](model_doc/timesformer)** (from Facebook) released with the paper [Is Space-Time Attention All You Need for Video Understanding?](https://arxiv.org/abs/2102.05095) by Gedas Bertasius, Heng Wang, Lorenzo Torresani.\n-1. **[Trajectory Transformer](model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine\n-1. **[Transformer-XL](model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n-1. **[TrOCR](model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n-1. **[UL2](model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler\n-1. **[UniSpeech](model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n-1. **[UniSpeechSat](model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n-1. **[UPerNet](model_doc/upernet)** (from Peking University) released with the paper [Unified Perceptual Parsing for Scene Understanding](https://arxiv.org/abs/1807.10221) by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.\n-1. **[VAN](model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n-1. **[VideoMAE](model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\n-1. **[ViLT](model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.\n-1. **[Vision Transformer (ViT)](model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n-1. **[VisualBERT](model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n-1. **[ViT Hybrid](model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n-1. **[ViTMAE](model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollรกr, Ross Girshick.\n-1. **[ViTMSN](model_doc/vit_msn)** (from Meta AI) released with the paper [Masked Siamese Networks for Label-Efficient Learning](https://arxiv.org/abs/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.\n-1. **[Wav2Vec2](model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n-1. **[Wav2Vec2-Conformer](model_doc/wav2vec2-conformer)** (from Facebook AI) released with the paper [FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ](https://arxiv.org/abs/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.\n-1. **[Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n-1. **[WavLM](model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n+1. **[TimeSformer](model_doc/timesformer)** (from Facebook) released with the paper [Is Space-Time Attention All You Need for Video Understanding?](https://huggingface.co/papers/2102.05095) by Gedas Bertasius, Heng Wang, Lorenzo Torresani.\n+1. **[Trajectory Transformer](model_doc/trajectory_transformers)** (from the University of California at Berkeley) released with the paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://huggingface.co/papers/2106.02039) by Michael Janner, Qiyang Li, Sergey Levine\n+1. **[Transformer-XL](model_doc/transfo-xl)** (from Google/CMU) released with the paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://huggingface.co/papers/1901.02860) by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n+1. **[TrOCR](model_doc/trocr)** (from Microsoft), released together with the paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://huggingface.co/papers/2109.10282) by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n+1. **[UL2](model_doc/ul2)** (from Google Research) released with the paper [Unifying Language Learning Paradigms](https://huggingface.co/papers/2205.05131v1) by Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler\n+1. **[UniSpeech](model_doc/unispeech)** (from Microsoft Research) released with the paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://huggingface.co/papers/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n+1. **[UniSpeechSat](model_doc/unispeech-sat)** (from Microsoft Research) released with the paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://huggingface.co/papers/2110.05752) by Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n+1. **[UPerNet](model_doc/upernet)** (from Peking University) released with the paper [Unified Perceptual Parsing for Scene Understanding](https://huggingface.co/papers/1807.10221) by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun.\n+1. **[VAN](model_doc/van)** (from Tsinghua University and Nankai University) released with the paper [Visual Attention Network](https://huggingface.co/papers/2202.09741) by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n+1. **[VideoMAE](model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://huggingface.co/papers/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.\n+1. **[ViLT](model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://huggingface.co/papers/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.\n+1. **[Vision Transformer (ViT)](model_doc/vit)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://huggingface.co/papers/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n+1. **[VisualBERT](model_doc/visual_bert)** (from UCLA NLP) released with the paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://huggingface.co/papers/1908.03557) by Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n+1. **[ViT Hybrid](model_doc/vit_hybrid)** (from Google AI) released with the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://huggingface.co/papers/2010.11929) by Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n+1. **[ViTMAE](model_doc/vit_mae)** (from Meta AI) released with the paper [Masked Autoencoders Are Scalable Vision Learners](https://huggingface.co/papers/2111.06377) by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollรกr, Ross Girshick.\n+1. **[ViTMSN](model_doc/vit_msn)** (from Meta AI) released with the paper [Masked Siamese Networks for Label-Efficient Learning](https://huggingface.co/papers/2204.07141) by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas.\n+1. **[Wav2Vec2](model_doc/wav2vec2)** (from Facebook AI) released with the paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://huggingface.co/papers/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n+1. **[Wav2Vec2-Conformer](model_doc/wav2vec2-conformer)** (from Facebook AI) released with the paper [FAIRSEQ S2T: Fast Speech-to-Text Modeling with FAIRSEQ](https://huggingface.co/papers/2010.05171) by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, Juan Pino.\n+1. **[Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)** (from Facebook AI) released with the paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://huggingface.co/papers/2109.11680) by Qiantong Xu, Alexei Baevski, Michael Auli.\n+1. **[WavLM](model_doc/wavlm)** (from Microsoft Research) released with the paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://huggingface.co/papers/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n 1. **[Whisper](model_doc/whisper)** (from OpenAI) released with the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://cdn.openai.com/papers/whisper.pdf) by Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, Ilya Sutskever.\n-1. **[X-CLIP](model_doc/xclip)** (from Microsoft Research) released with the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\n-1. **[XGLM](model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n-1. **[XLM](model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) by Guillaume Lample and Alexis Conneau.\n-1. **[XLM-ProphetNet](model_doc/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n-1. **[XLM-RoBERTa](model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmรกn, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.\n-1. **[XLM-RoBERTa-XL](model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n-1. **[XLNet](model_doc/xlnet)** (from Google/CMU) released with the paper [โXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n-1. **[XLS-R](model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n-1. **[XLSR-Wav2Vec2](model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n-1. **[YOLOS](model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n-1. **[YOSO](model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n+1. **[X-CLIP](model_doc/xclip)** (from Microsoft Research) released with the paper [Expanding Language-Image Pretrained Models for General Video Recognition](https://huggingface.co/papers/2208.02816) by Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling.\n+1. **[XGLM](model_doc/xglm)** (From Facebook AI) released with the paper [Few-shot Learning with Multilingual Language Models](https://huggingface.co/papers/2112.10668) by Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n+1. **[XLM](model_doc/xlm)** (from Facebook) released together with the paper [Cross-lingual Language Model Pretraining](https://huggingface.co/papers/1901.07291) by Guillaume Lample and Alexis Conneau.\n+1. **[XLM-ProphetNet](model_doc/xlm-prophetnet)** (from Microsoft Research) released with the paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://huggingface.co/papers/2001.04063) by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang and Ming Zhou.\n+1. **[XLM-RoBERTa](model_doc/xlm-roberta)** (from Facebook AI), released together with the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://huggingface.co/papers/1911.02116) by Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmรกn, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.\n+1. **[XLM-RoBERTa-XL](model_doc/xlm-roberta-xl)** (from Facebook AI), released together with the paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://huggingface.co/papers/2105.00572) by Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n+1. **[XLNet](model_doc/xlnet)** (from Google/CMU) released with the paper [โXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://huggingface.co/papers/1906.08237) by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n+1. **[XLS-R](model_doc/xls_r)** (from Facebook AI) released with the paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://huggingface.co/papers/2111.09296) by Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n+1. **[XLSR-Wav2Vec2](model_doc/xlsr_wav2vec2)** (from Facebook AI) released with the paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://huggingface.co/papers/2006.13979) by Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n+1. **[YOLOS](model_doc/yolos)** (from Huazhong University of Science & Technology) released with the paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://huggingface.co/papers/2106.00666) by Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n+1. **[YOSO](model_doc/yoso)** (from the University of Wisconsin - Madison) released with the paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://huggingface.co/papers/2111.09714) by Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n \n \n ### Frameworks compatibles"
        },
        {
            "sha": "775f8f4ff7fb22152781d7f85a40ae45c3fb8984",
            "filename": "docs/source/fr/tasks_explained.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Ffr%2Ftasks_explained.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Ffr%2Ftasks_explained.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Ftasks_explained.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -120,7 +120,7 @@ Cette section explique briรจvement les convolutions, mais il serait utile d'avoi\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif\"/>\n </div>\n \n-<small>Une convolution de base sans padding ni stride, tirรฉe de <a href=\"https://arxiv.org/abs/1603.07285\">Un guide des calculs de convolution pour l'apprentissage profond.</a></small>\n+<small>Une convolution de base sans padding ni stride, tirรฉe de <a href=\"https://huggingface.co/papers/1603.07285\">Un guide des calculs de convolution pour l'apprentissage profond.</a></small>\n \n Vous pouvez alimenter la sortie d'une couche convolutionnelle ร une autre couche convolutionnelle. ร chaque couche successive, le rรฉseau apprend des caractรฉristiques de plus en plus complexes et abstraites, telles que des objets spรฉcifiques comme des hot-dogs ou des fusรฉes. Entre les couches convolutionnelles, il est courant d'ajouter des couches de pooling pour rรฉduire la dimensionnalitรฉ et rendre le modรจle plus robuste aux variations de position des caractรฉristiques.\n "
        },
        {
            "sha": "a0c2523966140a2a0c3175f19da8693bff1475a8",
            "filename": "docs/source/it/index.md",
            "status": "modified",
            "additions": 113,
            "deletions": 113,
            "changes": 226,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fit%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fit%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Findex.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -54,128 +54,128 @@ La libreria attualmente contiene implementazioni in JAX, PyTorch e TensorFlow, p\n \n <!--This list is updated automatically from the README with _make fix-copies_. Do not update manually! -->\n \n-1. **[ALBERT](model_doc/albert)** (da Google Research e l'Istituto Tecnologico di Chicago) rilasciato con il paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), da Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n-1. **[ALIGN](model_doc/align)** (from Google Research) rilasciato con il paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918) da Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.\n-1. **[BART](model_doc/bart)** (da Facebook) rilasciato con il paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) da Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov e Luke Zettlemoyer.\n-1. **[BARThez](model_doc/barthez)** (da politecnico di รcole) rilasciato con il paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) da Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n-1. **[BARTpho](model_doc/bartpho)** (da VinAI Research) rilasciato con il paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://arxiv.org/abs/2109.09701) da Nguyen Luong Tran, Duong Minh Le e Dat Quoc Nguyen.\n-1. **[BEiT](model_doc/beit)** (da Microsoft) rilasciato con il paper [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) da Hangbo Bao, Li Dong, Furu Wei.\n-1. **[BERT](model_doc/bert)** (da Google) rilasciato con il paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) da Jacob Devlin, Ming-Wei Chang, Kenton Lee e Kristina Toutanova.\n+1. **[ALBERT](model_doc/albert)** (da Google Research e l'Istituto Tecnologico di Chicago) rilasciato con il paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://huggingface.co/papers/1909.11942), da Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n+1. **[ALIGN](model_doc/align)** (from Google Research) rilasciato con il paper [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://huggingface.co/papers/2102.05918) da Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig.\n+1. **[BART](model_doc/bart)** (da Facebook) rilasciato con il paper [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://huggingface.co/papers/1910.13461) da Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov e Luke Zettlemoyer.\n+1. **[BARThez](model_doc/barthez)** (da politecnico di รcole) rilasciato con il paper [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://huggingface.co/papers/2010.12321) da Moussa Kamal Eddine, Antoine J.-P. Tixier, Michalis Vazirgiannis.\n+1. **[BARTpho](model_doc/bartpho)** (da VinAI Research) rilasciato con il paper [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese](https://huggingface.co/papers/2109.09701) da Nguyen Luong Tran, Duong Minh Le e Dat Quoc Nguyen.\n+1. **[BEiT](model_doc/beit)** (da Microsoft) rilasciato con il paper [BEiT: BERT Pre-Training of Image Transformers](https://huggingface.co/papers/2106.08254) da Hangbo Bao, Li Dong, Furu Wei.\n+1. **[BERT](model_doc/bert)** (da Google) rilasciato con il paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://huggingface.co/papers/1810.04805) da Jacob Devlin, Ming-Wei Chang, Kenton Lee e Kristina Toutanova.\n 1. **[BERTweet](model_doc/bertweet)** (da VinAI Research) rilasciato con il paper [BERTweet: A pre-trained language model for English Tweets](https://aclanthology.org/2020.emnlp-demos.2/) da Dat Quoc Nguyen, Thanh Vu e Anh Tuan Nguyen.\n-1. **[BERT For Sequence Generation](model_doc/bert-generation)** (da Google) rilasciato con il paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) da Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n-1. **[BigBird-RoBERTa](model_doc/big_bird)** (da Google Research) rilasciato con il paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) da Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n-1. **[BigBird-Pegasus](model_doc/bigbird_pegasus)** (v Google Research) rilasciato con il paper [Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) da Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n-1. **[Blenderbot](model_doc/blenderbot)** (da Facebook) rilasciato con il paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) da Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n-1. **[BlenderbotSmall](model_doc/blenderbot-small)** (da Facebook) rilasciato con il paper [Recipes for building an open-domain chatbot](https://arxiv.org/abs/2004.13637) da Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n-1. **[BORT](model_doc/bort)** (da Alexa) rilasciato con il paper [Optimal Subarchitecture Extraction For BERT](https://arxiv.org/abs/2010.10499) da Adrian de Wynter e Daniel J. Perry.\n-1. **[ByT5](model_doc/byt5)** (da Google Research) rilasciato con il paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) da Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n-1. **[CamemBERT](model_doc/camembert)** (da Inria/Facebook/Sorbonne) rilasciato con il paper [CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) da Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suรกrez*, Yoann Dupont, Laurent Romary, รric Villemonte de la Clergerie, Djamรฉ Seddah e Benoรฎt Sagot.\n-1. **[CANINE](model_doc/canine)** (da Google Research) rilasciato con il paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874) da Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n-1. **[ConvNeXT](model_doc/convnext)** (da Facebook AI) rilasciato con il paper [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545) da Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n-1. **[ConvNeXTV2](model_doc/convnextv2)** (da Facebook AI) rilasciato con il paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://arxiv.org/abs/2301.00808) da Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n-1. **[CLIP](model_doc/clip)** (da OpenAI) rilasciato con il paper [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) da Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n-1. **[ConvBERT](model_doc/convbert)** (da YituTech) rilasciato con il paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/abs/2008.02496) da Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n-1. **[CPM](model_doc/cpm)** (dalla Universitร di Tsinghua) rilasciato con il paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://arxiv.org/abs/2012.00413) da Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n-1. **[CTRL](model_doc/ctrl)** (da Salesforce) rilasciato con il paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) da Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong e Richard Socher.\n-1. **[CvT](model_doc/cvt)** (da Microsoft) rilasciato con il paper [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/abs/2103.15808) da Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\n-1. **[Data2Vec](model_doc/data2vec)** (da Facebook) rilasciato con il paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://arxiv.org/abs/2202.03555) da Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n-1. **[DeBERTa](model_doc/deberta)** (da Microsoft) rilasciato con il paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) da Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n-1. **[DeBERTa-v2](model_doc/deberta-v2)** (da Microsoft) rilasciato con il paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) da Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n-1. **[Decision Transformer](model_doc/decision_transformer)** (da Berkeley/Facebook/Google) rilasciato con il paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345) da Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n-1. **[DiT](model_doc/dit)** (da Microsoft Research) rilasciato con il paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://arxiv.org/abs/2203.02378) da Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n-1. **[DeiT](model_doc/deit)** (da Facebook) rilasciato con il paper [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877) da Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervรฉ Jรฉgou.\n-1. **[DETR](model_doc/detr)** (da Facebook) rilasciato con il paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) da Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n-1. **[DialoGPT](model_doc/dialogpt)** (da Microsoft Research) rilasciato con il paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://arxiv.org/abs/1911.00536) da Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n-1. **[DistilBERT](model_doc/distilbert)** (da HuggingFace), rilasciato assieme al paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108) da Victor Sanh, Lysandre Debut e Thomas Wolf. La stessa tecnica รจ stata applicata per comprimere GPT2 in [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa in [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT in [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n-1. **[DPR](model_doc/dpr)** (da Facebook) rilasciato con il paper [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906) da Vladimir Karpukhin, Barlas Oฤuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, e Wen-tau Yih.\n-1. **[DPT](master/model_doc/dpt)** (da Intel Labs) rilasciato con il paper [Vision Transformers for Dense Prediction](https://arxiv.org/abs/2103.13413) da Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n-1. **[EfficientNet](model_doc/efficientnet)** (from Google Research) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)  by Mingxing Tan and Quoc V. Le.\n-1. **[EncoderDecoder](model_doc/encoder-decoder)** (da Google Research) rilasciato con il paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) da Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n-1. **[ELECTRA](model_doc/electra)** (da Google Research/Stanford University) rilasciato con il paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://arxiv.org/abs/2003.10555) da Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n-1. **[FlauBERT](model_doc/flaubert)** (da CNRS) rilasciato con il paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://arxiv.org/abs/1912.05372) da Hang Le, Loรฏc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoรฎt Crabbรฉ, Laurent Besacier, Didier Schwab.\n-1. **[FLAVA](model_doc/flava)** (da Facebook AI) rilasciato con il paper [FLAVA: A Foundational Language And Vision Alignment Model](https://arxiv.org/abs/2112.04482) da Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, e Douwe Kiela.\n-1. **[FNet](model_doc/fnet)** (da Google Research) rilasciato con il paper [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) da James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n-1. **[Funnel Transformer](model_doc/funnel)** (da CMU/Google Brain) rilasciato con il paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236) da Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n-1. **[GLPN](model_doc/glpn)** (da KAIST) rilasciato con il paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://arxiv.org/abs/2201.07436) da Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n+1. **[BERT For Sequence Generation](model_doc/bert-generation)** (da Google) rilasciato con il paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) da Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+1. **[BigBird-RoBERTa](model_doc/big_bird)** (da Google Research) rilasciato con il paper [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) da Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n+1. **[BigBird-Pegasus](model_doc/bigbird_pegasus)** (v Google Research) rilasciato con il paper [Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) da Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed.\n+1. **[Blenderbot](model_doc/blenderbot)** (da Facebook) rilasciato con il paper [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) da Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n+1. **[BlenderbotSmall](model_doc/blenderbot-small)** (da Facebook) rilasciato con il paper [Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) da Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M. Smith, Y-Lan Boureau, Jason Weston.\n+1. **[BORT](model_doc/bort)** (da Alexa) rilasciato con il paper [Optimal Subarchitecture Extraction For BERT](https://huggingface.co/papers/2010.10499) da Adrian de Wynter e Daniel J. Perry.\n+1. **[ByT5](model_doc/byt5)** (da Google Research) rilasciato con il paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://huggingface.co/papers/2105.13626) da Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.\n+1. **[CamemBERT](model_doc/camembert)** (da Inria/Facebook/Sorbonne) rilasciato con il paper [CamemBERT: a Tasty French Language Model](https://huggingface.co/papers/1911.03894) da Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suรกrez*, Yoann Dupont, Laurent Romary, รric Villemonte de la Clergerie, Djamรฉ Seddah e Benoรฎt Sagot.\n+1. **[CANINE](model_doc/canine)** (da Google Research) rilasciato con il paper [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://huggingface.co/papers/2103.06874) da Jonathan H. Clark, Dan Garrette, Iulia Turc, John Wieting.\n+1. **[ConvNeXT](model_doc/convnext)** (da Facebook AI) rilasciato con il paper [A ConvNet for the 2020s](https://huggingface.co/papers/2201.03545) da Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie.\n+1. **[ConvNeXTV2](model_doc/convnextv2)** (da Facebook AI) rilasciato con il paper [ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders](https://huggingface.co/papers/2301.00808) da Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, Saining Xie.\n+1. **[CLIP](model_doc/clip)** (da OpenAI) rilasciato con il paper [Learning Transferable Visual Models From Natural Language Supervision](https://huggingface.co/papers/2103.00020) da Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever.\n+1. **[ConvBERT](model_doc/convbert)** (da YituTech) rilasciato con il paper [ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://huggingface.co/papers/2008.02496) da Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan.\n+1. **[CPM](model_doc/cpm)** (dalla Universitร di Tsinghua) rilasciato con il paper [CPM: A Large-scale Generative Chinese Pre-trained Language Model](https://huggingface.co/papers/2012.00413) da Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin, Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen, Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.\n+1. **[CTRL](model_doc/ctrl)** (da Salesforce) rilasciato con il paper [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://huggingface.co/papers/1909.05858) da Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong e Richard Socher.\n+1. **[CvT](model_doc/cvt)** (da Microsoft) rilasciato con il paper [CvT: Introducing Convolutions to Vision Transformers](https://huggingface.co/papers/2103.15808) da Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, Lei Zhang.\n+1. **[Data2Vec](model_doc/data2vec)** (da Facebook) rilasciato con il paper [Data2Vec:  A General Framework for Self-supervised Learning in Speech, Vision and Language](https://huggingface.co/papers/2202.03555) da Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, Michael Auli.\n+1. **[DeBERTa](model_doc/deberta)** (da Microsoft) rilasciato con il paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) da Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n+1. **[DeBERTa-v2](model_doc/deberta-v2)** (da Microsoft) rilasciato con il paper [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://huggingface.co/papers/2006.03654) da Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen.\n+1. **[Decision Transformer](model_doc/decision_transformer)** (da Berkeley/Facebook/Google) rilasciato con il paper [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://huggingface.co/papers/2106.01345) da Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch.\n+1. **[DiT](model_doc/dit)** (da Microsoft Research) rilasciato con il paper [DiT: Self-supervised Pre-training for Document Image Transformer](https://huggingface.co/papers/2203.02378) da Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei.\n+1. **[DeiT](model_doc/deit)** (da Facebook) rilasciato con il paper [Training data-efficient image transformers & distillation through attention](https://huggingface.co/papers/2012.12877) da Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Hervรฉ Jรฉgou.\n+1. **[DETR](model_doc/detr)** (da Facebook) rilasciato con il paper [End-to-End Object Detection with Transformers](https://huggingface.co/papers/2005.12872) da Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko.\n+1. **[DialoGPT](model_doc/dialogpt)** (da Microsoft Research) rilasciato con il paper [DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation](https://huggingface.co/papers/1911.00536) da Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan.\n+1. **[DistilBERT](model_doc/distilbert)** (da HuggingFace), rilasciato assieme al paper [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://huggingface.co/papers/1910.01108) da Victor Sanh, Lysandre Debut e Thomas Wolf. La stessa tecnica รจ stata applicata per comprimere GPT2 in [DistilGPT2](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), RoBERTa in [DistilRoBERTa](https://github.com/huggingface/transformers-research-projects/tree/main/distillation), Multilingual BERT in [DistilmBERT](https://github.com/huggingface/transformers-research-projects/tree/main/distillation) and a German version of DistilBERT.\n+1. **[DPR](model_doc/dpr)** (da Facebook) rilasciato con il paper [Dense Passage Retrieval for Open-Domain Question Answering](https://huggingface.co/papers/2004.04906) da Vladimir Karpukhin, Barlas Oฤuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, e Wen-tau Yih.\n+1. **[DPT](master/model_doc/dpt)** (da Intel Labs) rilasciato con il paper [Vision Transformers for Dense Prediction](https://huggingface.co/papers/2103.13413) da Renรฉ Ranftl, Alexey Bochkovskiy, Vladlen Koltun.\n+1. **[EfficientNet](model_doc/efficientnet)** (from Google Research) released with the paper [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://huggingface.co/papers/1905.11946)  by Mingxing Tan and Quoc V. Le.\n+1. **[EncoderDecoder](model_doc/encoder-decoder)** (da Google Research) rilasciato con il paper [Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) da Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+1. **[ELECTRA](model_doc/electra)** (da Google Research/Stanford University) rilasciato con il paper [ELECTRA: Pre-training text encoders as discriminators rather than generators](https://huggingface.co/papers/2003.10555) da Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning.\n+1. **[FlauBERT](model_doc/flaubert)** (da CNRS) rilasciato con il paper [FlauBERT: Unsupervised Language Model Pre-training for French](https://huggingface.co/papers/1912.05372) da Hang Le, Loรฏc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Benoรฎt Crabbรฉ, Laurent Besacier, Didier Schwab.\n+1. **[FLAVA](model_doc/flava)** (da Facebook AI) rilasciato con il paper [FLAVA: A Foundational Language And Vision Alignment Model](https://huggingface.co/papers/2112.04482) da Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, e Douwe Kiela.\n+1. **[FNet](model_doc/fnet)** (da Google Research) rilasciato con il paper [FNet: Mixing Tokens with Fourier Transforms](https://huggingface.co/papers/2105.03824) da James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.\n+1. **[Funnel Transformer](model_doc/funnel)** (da CMU/Google Brain) rilasciato con il paper [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://huggingface.co/papers/2006.03236) da Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le.\n+1. **[GLPN](model_doc/glpn)** (da KAIST) rilasciato con il paper [Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth](https://huggingface.co/papers/2201.07436) da Doyeon Kim, Woonghyun Ga, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, Junmo Kim.\n 1. **[GPT](model_doc/openai-gpt)** (da OpenAI) rilasciato con il paper [Improving Language Understanding by Generative Pre-Training](https://openai.com/research/language-unsupervised/) da Alec Radford, Karthik Narasimhan, Tim Salimans e Ilya Sutskever.\n 1. **[GPT-2](model_doc/gpt2)** (da OpenAI) rilasciato con il paper [Language Models are Unsupervised Multitask Learners](https://openai.com/research/better-language-models/) da Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei e Ilya Sutskever.\n 1. **[GPT-J](model_doc/gptj)** (da EleutherAI) rilasciato nel repository [kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax/) da Ben Wang e Aran Komatsuzaki.\n 1. **[GPT Neo](model_doc/gpt_neo)** (da EleutherAI) rilasciato nel repository [EleutherAI/gpt-neo](https://github.com/EleutherAI/gpt-neo) da Sid Black, Stella Biderman, Leo Gao, Phil Wang e Connor Leahy.\n-1. **[GPT NeoX](model_doc/gpt_neox)** (da EleutherAI) rilasciato con il paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) da Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\n-1. **[Hubert](model_doc/hubert)** (da Facebook) rilasciato con il paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://arxiv.org/abs/2106.07447) da Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n-1. **[I-BERT](model_doc/ibert)** (da Berkeley) rilasciato con il paper [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321) da Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n+1. **[GPT NeoX](model_doc/gpt_neox)** (da EleutherAI) rilasciato con il paper [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://huggingface.co/papers/2204.06745) da Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach\n+1. **[Hubert](model_doc/hubert)** (da Facebook) rilasciato con il paper [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units](https://huggingface.co/papers/2106.07447) da Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.\n+1. **[I-BERT](model_doc/ibert)** (da Berkeley) rilasciato con il paper [I-BERT: Integer-only BERT Quantization](https://huggingface.co/papers/2101.01321) da Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer.\n 1. **[ImageGPT](model_doc/imagegpt)** (da OpenAI) rilasciato con il paper [Generative Pretraining from Pixels](https://openai.com/blog/image-gpt/) da Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever.\n-1. **[LayoutLM](model_doc/layoutlm)** (da Microsoft Research Asia) rilasciato con il paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/abs/1912.13318) da Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n-1. **[LayoutLMv2](model_doc/layoutlmv2)** (da Microsoft Research Asia) rilasciato con il paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://arxiv.org/abs/2012.14740) da Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n-1. **[LayoutLMv3](model_doc/layoutlmv3)** (da Microsoft Research Asia) rilasciato con il paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://arxiv.org/abs/2204.08387) da Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.\n-1. **[LayoutXLM](model_doc/layoutlxlm)** (da Microsoft Research Asia) rilasciato con il paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) da Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n-1. **[LED](model_doc/led)** (da AllenAI) rilasciato con il paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) da Iz Beltagy, Matthew E. Peters, Arman Cohan.\n-1. **[Longformer](model_doc/longformer)** (da AllenAI) rilasciato con il paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) da Iz Beltagy, Matthew E. Peters, Arman Cohan.\n-1. **[LUKE](model_doc/luke)** (da Studio Ousia) rilasciato con il paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://arxiv.org/abs/2010.01057) da Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n-1. **[mLUKE](model_doc/mluke)** (da Studio Ousia) rilasciato con il paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://arxiv.org/abs/2110.08151) da Ryokan Ri, Ikuya Yamada, e Yoshimasa Tsuruoka.\n-1. **[LXMERT](model_doc/lxmert)** (da UNC Chapel Hill) rilasciato con il paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://arxiv.org/abs/1908.07490) da Hao Tan e Mohit Bansal.\n-1. **[M2M100](model_doc/m2m_100)** (da Facebook) rilasciato con il paper [Beyond English-Centric Multilingual Machine Translation](https://arxiv.org/abs/2010.11125) da Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n+1. **[LayoutLM](model_doc/layoutlm)** (da Microsoft Research Asia) rilasciato con il paper [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://huggingface.co/papers/1912.13318) da Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou.\n+1. **[LayoutLMv2](model_doc/layoutlmv2)** (da Microsoft Research Asia) rilasciato con il paper [LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding](https://huggingface.co/papers/2012.14740) da Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Wanxiang Che, Min Zhang, Lidong Zhou.\n+1. **[LayoutLMv3](model_doc/layoutlmv3)** (da Microsoft Research Asia) rilasciato con il paper [LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking](https://huggingface.co/papers/2204.08387) da Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei.\n+1. **[LayoutXLM](model_doc/layoutlxlm)** (da Microsoft Research Asia) rilasciato con il paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://huggingface.co/papers/2104.08836) da Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.\n+1. **[LED](model_doc/led)** (da AllenAI) rilasciato con il paper [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) da Iz Beltagy, Matthew E. Peters, Arman Cohan.\n+1. **[Longformer](model_doc/longformer)** (da AllenAI) rilasciato con il paper [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) da Iz Beltagy, Matthew E. Peters, Arman Cohan.\n+1. **[LUKE](model_doc/luke)** (da Studio Ousia) rilasciato con il paper [LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention](https://huggingface.co/papers/2010.01057) da Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto.\n+1. **[mLUKE](model_doc/mluke)** (da Studio Ousia) rilasciato con il paper [mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models](https://huggingface.co/papers/2110.08151) da Ryokan Ri, Ikuya Yamada, e Yoshimasa Tsuruoka.\n+1. **[LXMERT](model_doc/lxmert)** (da UNC Chapel Hill) rilasciato con il paper [LXMERT: Learning Cross-Modality Encoder Representations from Transformers for Open-Domain Question Answering](https://huggingface.co/papers/1908.07490) da Hao Tan e Mohit Bansal.\n+1. **[M2M100](model_doc/m2m_100)** (da Facebook) rilasciato con il paper [Beyond English-Centric Multilingual Machine Translation](https://huggingface.co/papers/2010.11125) da Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, Armand Joulin.\n 1. **[MarianMT](model_doc/marian)** Modello di machine learning per le traduzioni allenato utilizzando i dati [OPUS](http://opus.nlpl.eu/) di Jรถrg Tiedemann. Il [Framework Marian](https://marian-nmt.github.io/) รจ stato sviluppato dal Microsoft Translator Team.\n-1. **[Mask2Former](model_doc/mask2former)** (da FAIR e UIUC) rilasciato con il paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) da Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.\n-1. **[MaskFormer](model_doc/maskformer)** (da Meta e UIUC) rilasciato con il paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://arxiv.org/abs/2107.06278) da Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n-1. **[MBart](model_doc/mbart)** (da Facebook) rilasciato con il paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210) da Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n-1. **[MBart-50](model_doc/mbart)** (da Facebook) rilasciato con il paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://arxiv.org/abs/2008.00401) da Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n-1. **[Megatron-BERT](model_doc/megatron-bert)** (da NVIDIA) rilasciato con il paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) da Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper e Bryan Catanzaro.\n-1. **[Megatron-GPT2](model_doc/megatron_gpt2)** (da NVIDIA) rilasciato con il paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) da Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper e Bryan Catanzaro.\n-1. **[MPNet](model_doc/mpnet)** (da Microsoft Research) rilasciato con il paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) da Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n-1. **[MT5](model_doc/mt5)** (da Google AI) rilasciato con il paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) da Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n-1. **[Nystrรถmformer](model_doc/nystromformer)** (dalla Universitร del Wisconsin - Madison) rilasciato con il paper [Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention](https://arxiv.org/abs/2102.03902) da Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n-1. **[OneFormer](model_doc/oneformer)** (da SHI Labs) rilasciato con il paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) da Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n-1. **[OPT](master/model_doc/opt)** (da Meta AI) rilasciato con il paper [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) da Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\n-1. **[Pegasus](model_doc/pegasus)** (da Google) rilasciato con il paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) da Jingqing Zhang, Yao Zhao, Mohammad Saleh e Peter J. Liu.\n-1. **[Perceiver IO](model_doc/perceiver)** (da Deepmind) rilasciato con il paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/abs/2107.14795) da Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hรฉnaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Joรฃo Carreira.\n+1. **[Mask2Former](model_doc/mask2former)** (da FAIR e UIUC) rilasciato con il paper [Masked-attention Mask Transformer for Universal Image Segmentation](https://huggingface.co/papers/2112.01527) da Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, Rohit Girdhar.\n+1. **[MaskFormer](model_doc/maskformer)** (da Meta e UIUC) rilasciato con il paper [Per-Pixel Classification is Not All You Need for Semantic Segmentation](https://huggingface.co/papers/2107.06278) da Bowen Cheng, Alexander G. Schwing, Alexander Kirillov.\n+1. **[MBart](model_doc/mbart)** (da Facebook) rilasciato con il paper [Multilingual Denoising Pre-training for Neural Machine Translation](https://huggingface.co/papers/2001.08210) da Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer.\n+1. **[MBart-50](model_doc/mbart)** (da Facebook) rilasciato con il paper [Multilingual Translation with Extensible Multilingual Pretraining and Finetuning](https://huggingface.co/papers/2008.00401) da Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, Angela Fan.\n+1. **[Megatron-BERT](model_doc/megatron-bert)** (da NVIDIA) rilasciato con il paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://huggingface.co/papers/1909.08053) da Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper e Bryan Catanzaro.\n+1. **[Megatron-GPT2](model_doc/megatron_gpt2)** (da NVIDIA) rilasciato con il paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://huggingface.co/papers/1909.08053) da Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper e Bryan Catanzaro.\n+1. **[MPNet](model_doc/mpnet)** (da Microsoft Research) rilasciato con il paper [MPNet: Masked and Permuted Pre-training for Language Understanding](https://huggingface.co/papers/2004.09297) da Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu.\n+1. **[MT5](model_doc/mt5)** (da Google AI) rilasciato con il paper [mT5: A massively multilingual pre-trained text-to-text transformer](https://huggingface.co/papers/2010.11934) da Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, Colin Raffel.\n+1. **[Nystrรถmformer](model_doc/nystromformer)** (dalla Universitร del Wisconsin - Madison) rilasciato con il paper [Nystrรถmformer: A Nystrรถm-Based Algorithm for Approximating Self-Attention](https://huggingface.co/papers/2102.03902) da Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, Vikas Singh.\n+1. **[OneFormer](model_doc/oneformer)** (da SHI Labs) rilasciato con il paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://huggingface.co/papers/2211.06220) da Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, Humphrey Shi.\n+1. **[OPT](master/model_doc/opt)** (da Meta AI) rilasciato con il paper [OPT: Open Pre-trained Transformer Language Models](https://huggingface.co/papers/2205.01068) da Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen et al.\n+1. **[Pegasus](model_doc/pegasus)** (da Google) rilasciato con il paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://huggingface.co/papers/1912.08777) da Jingqing Zhang, Yao Zhao, Mohammad Saleh e Peter J. Liu.\n+1. **[Perceiver IO](model_doc/perceiver)** (da Deepmind) rilasciato con il paper [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://huggingface.co/papers/2107.14795) da Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Hรฉnaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, Joรฃo Carreira.\n 1. **[PhoBERT](model_doc/phobert)** (da VinAI Research) rilasciato con il paper [PhoBERT: Pre-trained language models for Vietnamese](https://www.aclweb.org/anthology/2020.findings-emnlp.92/) da Dat Quoc Nguyen e Anh Tuan Nguyen.\n-1. **[PLBart](model_doc/plbart)** (da UCLA NLP) rilasciato con il paper [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) da Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n-1. **[PoolFormer](model_doc/poolformer)** (da Sea AI Labs) rilasciato con il paper [MetaFormer is Actually What You Need for Vision](https://arxiv.org/abs/2111.11418) da Yu, Weihao e Luo, Mi e Zhou, Pan e Si, Chenyang e Zhou, Yichen e Wang, Xinchao e Feng, Jiashi e Yan, Shuicheng.\n-1. **[ProphetNet](model_doc/prophetnet)** (da Microsoft Research) rilasciato con il paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) da Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang e Ming Zhou.\n-1. **[QDQBert](model_doc/qdqbert)** (da NVIDIA) rilasciato con il paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://arxiv.org/abs/2004.09602) da Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev e Paulius Micikevicius.\n-1. **[REALM](model_doc/realm.html)** (da Google Research) rilasciato con il paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909) da Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat e Ming-Wei Chang.\n-1. **[Reformer](model_doc/reformer)** (da Google Research) rilasciato con il paper [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451) da Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n-1. **[RemBERT](model_doc/rembert)** (da Google Research) rilasciato con il paper [Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821) da Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, M. Johnson, Sebastian Ruder.\n-1. **[RegNet](model_doc/regnet)** (da META Platforms) rilasciato con il paper [Designing Network Design Space](https://arxiv.org/abs/2003.13678) da Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n-1. **[ResNet](model_doc/resnet)** (da Microsoft Research) rilasciato con il paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) da Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n-1. **[RoBERTa](model_doc/roberta)** (da Facebook), rilasciato assieme al paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) da Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n-1. **[RoFormer](model_doc/roformer)** (da ZhuiyiTechnology), rilasciato assieme al paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) da Jianlin Su e Yu Lu e Shengfeng Pan e Bo Wen e Yunfeng Liu.\n-1. **[SegFormer](model_doc/segformer)** (da NVIDIA) rilasciato con il paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) da Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n-1. **[SEW](model_doc/sew)** (da ASAPP) rilasciato con il paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) da Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n-1. **[SEW-D](model_doc/sew_d)** (da ASAPP) rilasciato con il paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://arxiv.org/abs/2109.06870) da Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n-1. **[SpeechToTextTransformer](model_doc/speech_to_text)** (da Facebook), rilasciato assieme al paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://arxiv.org/abs/2010.05171) da Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n-1. **[SpeechToTextTransformer2](model_doc/speech_to_text_2)** (da Facebook), rilasciato assieme al paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://arxiv.org/abs/2104.06678) da Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n-1. **[Splinter](model_doc/splinter)** (dalla Universitร di Tel Aviv), rilasciato assieme al paper [Few-Shot Question Answering by Pretraining Span Selection](https://arxiv.org/abs/2101.00438) da Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n-1. **[SqueezeBert](model_doc/squeezebert)** (da Berkeley) rilasciato con il paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) da Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, e Kurt W. Keutzer.\n-1. **[Swin Transformer](model_doc/swin)** (da Microsoft) rilasciato con il paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) da Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n-1. **[T5](model_doc/t5)** (da Google AI) rilasciato con il paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) da Colin Raffel e Noam Shazeer e Adam Roberts e Katherine Lee e Sharan Narang e Michael Matena e Yanqi Zhou e Wei Li e Peter J. Liu.\n+1. **[PLBart](model_doc/plbart)** (da UCLA NLP) rilasciato con il paper [Unified Pre-training for Program Understanding and Generation](https://huggingface.co/papers/2103.06333) da Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, Kai-Wei Chang.\n+1. **[PoolFormer](model_doc/poolformer)** (da Sea AI Labs) rilasciato con il paper [MetaFormer is Actually What You Need for Vision](https://huggingface.co/papers/2111.11418) da Yu, Weihao e Luo, Mi e Zhou, Pan e Si, Chenyang e Zhou, Yichen e Wang, Xinchao e Feng, Jiashi e Yan, Shuicheng.\n+1. **[ProphetNet](model_doc/prophetnet)** (da Microsoft Research) rilasciato con il paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://huggingface.co/papers/2001.04063) da Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang e Ming Zhou.\n+1. **[QDQBert](model_doc/qdqbert)** (da NVIDIA) rilasciato con il paper [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation](https://huggingface.co/papers/2004.09602) da Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev e Paulius Micikevicius.\n+1. **[REALM](model_doc/realm.html)** (da Google Research) rilasciato con il paper [REALM: Retrieval-Augmented Language Model Pre-Training](https://huggingface.co/papers/2002.08909) da Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat e Ming-Wei Chang.\n+1. **[Reformer](model_doc/reformer)** (da Google Research) rilasciato con il paper [Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451) da Nikita Kitaev, ลukasz Kaiser, Anselm Levskaya.\n+1. **[RemBERT](model_doc/rembert)** (da Google Research) rilasciato con il paper [Rethinking embedding coupling in pre-trained language models](https://huggingface.co/papers/2010.12821) da Hyung Won Chung, Thibault Fรฉvry, Henry Tsai, M. Johnson, Sebastian Ruder.\n+1. **[RegNet](model_doc/regnet)** (da META Platforms) rilasciato con il paper [Designing Network Design Space](https://huggingface.co/papers/2003.13678) da Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, Piotr Dollรกr.\n+1. **[ResNet](model_doc/resnet)** (da Microsoft Research) rilasciato con il paper [Deep Residual Learning for Image Recognition](https://huggingface.co/papers/1512.03385) da Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n+1. **[RoBERTa](model_doc/roberta)** (da Facebook), rilasciato assieme al paper [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://huggingface.co/papers/1907.11692) da Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.\n+1. **[RoFormer](model_doc/roformer)** (da ZhuiyiTechnology), rilasciato assieme al paper [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://huggingface.co/papers/2104.09864) da Jianlin Su e Yu Lu e Shengfeng Pan e Bo Wen e Yunfeng Liu.\n+1. **[SegFormer](model_doc/segformer)** (da NVIDIA) rilasciato con il paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://huggingface.co/papers/2105.15203) da Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.\n+1. **[SEW](model_doc/sew)** (da ASAPP) rilasciato con il paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) da Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n+1. **[SEW-D](model_doc/sew_d)** (da ASAPP) rilasciato con il paper [Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition](https://huggingface.co/papers/2109.06870) da Felix Wu, Kwangyoun Kim, Jing Pan, Kyu Han, Kilian Q. Weinberger, Yoav Artzi.\n+1. **[SpeechToTextTransformer](model_doc/speech_to_text)** (da Facebook), rilasciato assieme al paper [fairseq S2T: Fast Speech-to-Text Modeling with fairseq](https://huggingface.co/papers/2010.05171) da Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino.\n+1. **[SpeechToTextTransformer2](model_doc/speech_to_text_2)** (da Facebook), rilasciato assieme al paper [Large-Scale Self- and Semi-Supervised Learning for Speech Translation](https://huggingface.co/papers/2104.06678) da Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.\n+1. **[Splinter](model_doc/splinter)** (dalla Universitร di Tel Aviv), rilasciato assieme al paper [Few-Shot Question Answering by Pretraining Span Selection](https://huggingface.co/papers/2101.00438) da Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, Omer Levy.\n+1. **[SqueezeBert](model_doc/squeezebert)** (da Berkeley) rilasciato con il paper [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://huggingface.co/papers/2006.11316) da Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, e Kurt W. Keutzer.\n+1. **[Swin Transformer](model_doc/swin)** (da Microsoft) rilasciato con il paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://huggingface.co/papers/2103.14030) da Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo.\n+1. **[T5](model_doc/t5)** (da Google AI) rilasciato con il paper [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://huggingface.co/papers/1910.10683) da Colin Raffel e Noam Shazeer e Adam Roberts e Katherine Lee e Sharan Narang e Michael Matena e Yanqi Zhou e Wei Li e Peter J. Liu.\n 1. **[T5v1.1](model_doc/t5v1.1)** (da Google AI) rilasciato nel repository [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511) da Colin Raffel e Noam Shazeer e Adam Roberts e Katherine Lee e Sharan Narang e Michael Matena e Yanqi Zhou e Wei Li e Peter J. Liu.\n-1. **[TAPAS](model_doc/tapas)** (da Google AI) rilasciato con il paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://arxiv.org/abs/2004.02349) da Jonathan Herzig, Paweล Krzysztof Nowak, Thomas Mรผller, Francesco Piccinno e Julian Martin Eisenschlos.\n-1. **[TAPEX](model_doc/tapex)** (da Microsoft Research) rilasciato con il paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://arxiv.org/abs/2107.07653) da Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\n-1. **[Trajectory Transformer](model_doc/trajectory_transformers)** (dall'Universitร della California a Berkeley) rilasciato con il paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://arxiv.org/abs/2106.02039) da Michael Janner, Qiyang Li, Sergey Levine\n-1. **[Transformer-XL](model_doc/transfo-xl)** (da Google/CMU) rilasciato con il paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) da Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n-1. **[TrOCR](model_doc/trocr)** (da Microsoft), rilasciato assieme al paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://arxiv.org/abs/2109.10282) da Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n-1. **[UniSpeech](model_doc/unispeech)** (da Microsoft Research) rilasciato con il paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://arxiv.org/abs/2101.07597) da Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n-1. **[UniSpeechSat](model_doc/unispeech-sat)** (da Microsoft Research) rilasciato con il paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://arxiv.org/abs/2110.05752) da Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n-1. **[VAN](model_doc/van)** (dalle Universitร di Tsinghua e Nankai) rilasciato con il paper [Visual Attention Network](https://arxiv.org/abs/2202.09741) da Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n-1. **[ViLT](model_doc/vilt)** (da NAVER AI Lab/Kakao Enterprise/Kakao Brain) rilasciato con il paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) da Wonjae Kim, Bokyung Son, Ildoo Kim.\n-1. **[Vision Transformer (ViT)](model_doc/vit)** (da Google AI) rilasciato con il paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) da Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n-1. **[ViTMAE](model_doc/vit_mae)** (da Meta AI) rilasciato con il paper [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/abs/2111.06377) da Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollรกr, Ross Girshick.\n-1. **[VisualBERT](model_doc/visual_bert)** (da UCLA NLP) rilasciato con il paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/pdf/1908.03557) da Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n-1. **[WavLM](model_doc/wavlm)** (da Microsoft Research) rilasciato con il paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://arxiv.org/abs/2110.13900) da Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n-1. **[Wav2Vec2](model_doc/wav2vec2)** (da Facebook AI) rilasciato con il paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477) da Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n-1. **[Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)** (da Facebook AI) rilasciato con il paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://arxiv.org/abs/2109.11680) da Qiantong Xu, Alexei Baevski, Michael Auli.\n-1. **[XGLM](model_doc/xglm)** (da Facebook AI) rilasciato con il paper [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) da Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n-1. **[XLM](model_doc/xlm)** (v Facebook) rilasciato assieme al paper [Cross-lingual Language Model Pretraining](https://arxiv.org/abs/1901.07291) da Guillaume Lample e Alexis Conneau.\n-1. **[XLM-ProphetNet](model_doc/xlm-prophetnet)** (da Microsoft Research) rilasciato con il paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://arxiv.org/abs/2001.04063) da Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang e Ming Zhou.\n-1. **[XLM-RoBERTa](model_doc/xlm-roberta)** (da Facebook AI), rilasciato assieme al paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) da Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmรกn, Edouard Grave, Myle Ott, Luke Zettlemoyer e Veselin Stoyanov.\n-1. **[XLM-RoBERTa-XL](model_doc/xlm-roberta-xl)** (da Facebook AI), rilasciato assieme al paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://arxiv.org/abs/2105.00572) da Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n-1. **[XLNet](model_doc/xlnet)** (da Google/CMU) rilasciato con il paper [โXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) da Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n-1. **[XLSR-Wav2Vec2](model_doc/xlsr_wav2vec2)** (da Facebook AI) rilasciato con il paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://arxiv.org/abs/2006.13979) da Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n-1. **[XLS-R](model_doc/xls_r)** (da Facebook AI) rilasciato con il paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://arxiv.org/abs/2111.09296) da Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n-1. **[YOLOS](model_doc/yolos)** (dalla Universitร della scienza e tecnologia di Huazhong) rilasciato con il paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://arxiv.org/abs/2106.00666) da Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n-1. **[YOSO](model_doc/yoso)** (dall'Universitร del Wisconsin - Madison) rilasciato con il paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://arxiv.org/abs/2111.09714) da Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n+1. **[TAPAS](model_doc/tapas)** (da Google AI) rilasciato con il paper [TAPAS: Weakly Supervised Table Parsing via Pre-training](https://huggingface.co/papers/2004.02349) da Jonathan Herzig, Paweล Krzysztof Nowak, Thomas Mรผller, Francesco Piccinno e Julian Martin Eisenschlos.\n+1. **[TAPEX](model_doc/tapex)** (da Microsoft Research) rilasciato con il paper [TAPEX: Table Pre-training via Learning a Neural SQL Executor](https://huggingface.co/papers/2107.07653) da Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou.\n+1. **[Trajectory Transformer](model_doc/trajectory_transformers)** (dall'Universitร della California a Berkeley) rilasciato con il paper [Offline Reinforcement Learning as One Big Sequence Modeling Problem](https://huggingface.co/papers/2106.02039) da Michael Janner, Qiyang Li, Sergey Levine\n+1. **[Transformer-XL](model_doc/transfo-xl)** (da Google/CMU) rilasciato con il paper [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://huggingface.co/papers/1901.02860) da Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n+1. **[TrOCR](model_doc/trocr)** (da Microsoft), rilasciato assieme al paper [TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models](https://huggingface.co/papers/2109.10282) da Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei.\n+1. **[UniSpeech](model_doc/unispeech)** (da Microsoft Research) rilasciato con il paper [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data](https://huggingface.co/papers/2101.07597) da Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael Zeng, Xuedong Huang.\n+1. **[UniSpeechSat](model_doc/unispeech-sat)** (da Microsoft Research) rilasciato con il paper [UNISPEECH-SAT: UNIVERSAL SPEECH REPRESENTATION LEARNING WITH SPEAKER AWARE PRE-TRAINING](https://huggingface.co/papers/2110.05752) da Sanyuan Chen, Yu Wu, Chengyi Wang, Zhengyang Chen, Zhuo Chen, Shujie Liu, Jian Wu, Yao Qian, Furu Wei, Jinyu Li, Xiangzhan Yu.\n+1. **[VAN](model_doc/van)** (dalle Universitร di Tsinghua e Nankai) rilasciato con il paper [Visual Attention Network](https://huggingface.co/papers/2202.09741) da Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu.\n+1. **[ViLT](model_doc/vilt)** (da NAVER AI Lab/Kakao Enterprise/Kakao Brain) rilasciato con il paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://huggingface.co/papers/2102.03334) da Wonjae Kim, Bokyung Son, Ildoo Kim.\n+1. **[Vision Transformer (ViT)](model_doc/vit)** (da Google AI) rilasciato con il paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://huggingface.co/papers/2010.11929) da Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby.\n+1. **[ViTMAE](model_doc/vit_mae)** (da Meta AI) rilasciato con il paper [Masked Autoencoders Are Scalable Vision Learners](https://huggingface.co/papers/2111.06377) da Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollรกr, Ross Girshick.\n+1. **[VisualBERT](model_doc/visual_bert)** (da UCLA NLP) rilasciato con il paper [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://huggingface.co/papers/1908.03557) da Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang.\n+1. **[WavLM](model_doc/wavlm)** (da Microsoft Research) rilasciato con il paper [WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing](https://huggingface.co/papers/2110.13900) da Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Furu Wei.\n+1. **[Wav2Vec2](model_doc/wav2vec2)** (da Facebook AI) rilasciato con il paper [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://huggingface.co/papers/2006.11477) da Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli.\n+1. **[Wav2Vec2Phoneme](model_doc/wav2vec2_phoneme)** (da Facebook AI) rilasciato con il paper [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition](https://huggingface.co/papers/2109.11680) da Qiantong Xu, Alexei Baevski, Michael Auli.\n+1. **[XGLM](model_doc/xglm)** (da Facebook AI) rilasciato con il paper [Few-shot Learning with Multilingual Language Models](https://huggingface.co/papers/2112.10668) da Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, Xian Li.\n+1. **[XLM](model_doc/xlm)** (v Facebook) rilasciato assieme al paper [Cross-lingual Language Model Pretraining](https://huggingface.co/papers/1901.07291) da Guillaume Lample e Alexis Conneau.\n+1. **[XLM-ProphetNet](model_doc/xlm-prophetnet)** (da Microsoft Research) rilasciato con il paper [ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training](https://huggingface.co/papers/2001.04063) da Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang e Ming Zhou.\n+1. **[XLM-RoBERTa](model_doc/xlm-roberta)** (da Facebook AI), rilasciato assieme al paper [Unsupervised Cross-lingual Representation Learning at Scale](https://huggingface.co/papers/1911.02116) da Alexis Conneau*, Kartikay Khandelwal*, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmรกn, Edouard Grave, Myle Ott, Luke Zettlemoyer e Veselin Stoyanov.\n+1. **[XLM-RoBERTa-XL](model_doc/xlm-roberta-xl)** (da Facebook AI), rilasciato assieme al paper [Larger-Scale Transformers for Multilingual Masked Language Modeling](https://huggingface.co/papers/2105.00572) da Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, Alexis Conneau.\n+1. **[XLNet](model_doc/xlnet)** (da Google/CMU) rilasciato con il paper [โXLNet: Generalized Autoregressive Pretraining for Language Understanding](https://huggingface.co/papers/1906.08237) da Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n+1. **[XLSR-Wav2Vec2](model_doc/xlsr_wav2vec2)** (da Facebook AI) rilasciato con il paper [Unsupervised Cross-Lingual Representation Learning For Speech Recognition](https://huggingface.co/papers/2006.13979) da Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli.\n+1. **[XLS-R](model_doc/xls_r)** (da Facebook AI) rilasciato con il paper [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale](https://huggingface.co/papers/2111.09296) da Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, Michael Auli.\n+1. **[YOLOS](model_doc/yolos)** (dalla Universitร della scienza e tecnologia di Huazhong) rilasciato con il paper [You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection](https://huggingface.co/papers/2106.00666) da Yuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, Wenyu Liu.\n+1. **[YOSO](model_doc/yoso)** (dall'Universitร del Wisconsin - Madison) rilasciato con il paper [You Only Sample (Almost) Once: Linear Cost Self-Attention Via Bernoulli Sampling](https://huggingface.co/papers/2111.09714) da Zhanpeng Zeng, Yunyang Xiong, Sathya N. Ravi, Shailesh Acharya, Glenn Fung, Vikas Singh.\n \n \n ### Framework supportati"
        },
        {
            "sha": "5339d72d4c9d23e0c0ed5521de94863c8a15a02a",
            "filename": "docs/source/it/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fit%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fit%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fperf_infer_gpu_one.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -29,13 +29,13 @@ Nota che questa funzione puรฒ essere utilizzata anche nelle configurazioni multi\n \n </Tip>\n \n-Dal paper [`LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale`](https://arxiv.org/abs/2208.07339), noi supportiamo l'integrazione di Hugging Face per tutti i modelli dell'Hub con poche righe di codice.\n+Dal paper [`LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale`](https://huggingface.co/papers/2208.07339), noi supportiamo l'integrazione di Hugging Face per tutti i modelli dell'Hub con poche righe di codice.\n Il metodo `nn.Linear` riduce la dimensione di 2 per i pesi `float16` e `bfloat16` e di 4 per i pesi `float32`, con un impatto quasi nullo sulla qualitร, operando sugli outlier in half-precision.\n \n ![HFxbitsandbytes.png](https://cdn-uploads.huggingface.co/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png)\n \n Il metodo Int8 mixed-precision matrix decomposition funziona separando la moltiplicazione tra matrici in due flussi: (1) una matrice di flusso di outlier di caratteristiche sistematiche moltiplicata in fp16, (2) in flusso regolare di moltiplicazione di matrici int8 (99,9%). Con questo metodo, รจ possibile effettutare inferenza int8 per modelli molto grandi senza degrado predittivo.\n-Per maggiori dettagli sul metodo, consultare il [paper](https://arxiv.org/abs/2208.07339) o il nostro [blogpost sull'integrazione](https://huggingface.co/blog/hf-bitsandbytes-integration).\n+Per maggiori dettagli sul metodo, consultare il [paper](https://huggingface.co/papers/2208.07339) o il nostro [blogpost sull'integrazione](https://huggingface.co/blog/hf-bitsandbytes-integration).\n \n ![MixedInt8.gif](https://cdn-uploads.huggingface.co/production/uploads/1660567469965-62441d1d9fdefb55a0b7d12c.gif)\n "
        },
        {
            "sha": "5287354cb2f148fe4b1133816b19b85e43f93061",
            "filename": "docs/source/ja/bertology.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fbertology.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fbertology.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fbertology.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -20,15 +20,15 @@ rendered properly in your Markdown viewer.\n ๅคง่ฆๆจกใชใใฉใณในใใฉใผใใผใไพใใฐBERTใฎๅ้จๅไฝใ่ชฟๆปใใ็็ฉถ้ๅใๆฅๆ้ทใใฆใใพใ๏ผใใใใBERTologyใใจใๅผใณใพใ๏ผใใใฎๅ้ใฎ่ฏใไพใฏไปฅไธใงใ๏ผ\n \n - BERT Rediscovers the Classical NLP Pipeline by Ian Tenney, Dipanjan Das, Ellie Pavlick:\n-  [่ซๆใชใณใฏ](https://arxiv.org/abs/1905.05950)\n-- Are Sixteen Heads Really Better than One? by Paul Michel, Omer Levy, Graham Neubig: [่ซๆใชใณใฏ](https://arxiv.org/abs/1905.10650)\n-- What Does BERT Look At? An Analysis of BERT's Attention by Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning: [่ซๆใชใณใฏ](https://arxiv.org/abs/1906.04341)\n-- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: [่ซๆใชใณใฏ](https://arxiv.org/abs/2210.04633)\n+  [่ซๆใชใณใฏ](https://huggingface.co/papers/1905.05950)\n+- Are Sixteen Heads Really Better than One? by Paul Michel, Omer Levy, Graham Neubig: [่ซๆใชใณใฏ](https://huggingface.co/papers/1905.10650)\n+- What Does BERT Look At? An Analysis of BERT's Attention by Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning: [่ซๆใชใณใฏ](https://huggingface.co/papers/1906.04341)\n+- CAT-probing: A Metric-based Approach to Interpret How Pre-trained Models for Programming Language Attend Code Structure: [่ซๆใชใณใฏ](https://huggingface.co/papers/2210.04633)\n \n-ใใฎๆฐใใๅ้ใฎ็บๅฑใๆฏๆดใใใใใซใBERT/GPT/GPT-2ใขใใซใซใใใคใใฎ่ฟฝๅๆฉ่ฝใ็ตใฟ่พผใฟใไบบใใๅ้จ่กจ็พใซใขใฏใปในใงใใใใใซใใพใใใใใใใฎๆฉ่ฝใฏใไธปใซPaul Michelๆฐใฎๅชใใ็็ฉถ๏ผ[่ซๆใชใณใฏ](https://arxiv.org/abs/1905.10650)๏ผใซๅบใฅใใฆใใพใใๅทไฝ็ใซใฏใไปฅไธใฎๆฉ่ฝใๅซใพใใฆใใพใ๏ผ\n+ใใฎๆฐใใๅ้ใฎ็บๅฑใๆฏๆดใใใใใซใBERT/GPT/GPT-2ใขใใซใซใใใคใใฎ่ฟฝๅๆฉ่ฝใ็ตใฟ่พผใฟใไบบใใๅ้จ่กจ็พใซใขใฏใปในใงใใใใใซใใพใใใใใใใฎๆฉ่ฝใฏใไธปใซPaul Michelๆฐใฎๅชใใ็็ฉถ๏ผ[่ซๆใชใณใฏ](https://huggingface.co/papers/1905.10650)๏ผใซๅบใฅใใฆใใพใใๅทไฝ็ใซใฏใไปฅไธใฎๆฉ่ฝใๅซใพใใฆใใพใ๏ผ\n \n - BERT/GPT/GPT-2ใฎใในใฆใฎ้ใ็ถๆใซใขใฏใปในใใใใจใใงใใพใใ\n - BERT/GPT/GPT-2ใฎๅใใใใฎๆณจๆ้ใฟใซใขใฏใปในใงใใพใใ\n-- ใใใใฎๅบๅๅคใจๅพ้ใๅๅพใใใใใใฎ้่ฆๆงในใณใขใ่จ็ฎใใ[่ซๆใชใณใฏ](https://arxiv.org/abs/1905.10650)ใง่ชฌๆใใใฆใใใใใซใใใใๅๆธใงใใพใใ\n+- ใใใใฎๅบๅๅคใจๅพ้ใๅๅพใใใใใใฎ้่ฆๆงในใณใขใ่จ็ฎใใ[่ซๆใชใณใฏ](https://huggingface.co/papers/1905.10650)ใง่ชฌๆใใใฆใใใใใซใใใใๅๆธใงใใพใใ\n \n ใใใใฎๆฉ่ฝใ็่งฃใใไฝฟ็จใใใฎใๆฏๆดใใใใใซใ็นๅฎใฎใตใณใใซในใฏใชใใใ[bertology.py](https://github.com/huggingface/transformers-research-projects/tree/main/bertology/run_bertology.py)ใใ่ฟฝๅใใพใใใใใฎในใฏใชใใใฏใGLUEใงไบๅใใฌใผใใณใฐใใใใขใใซใใๆๅฑใๆฝๅบใใใใใใๅๆธใใๅฝนๅฒใๆใใใพใใ"
        },
        {
            "sha": "9fca784d0096beef3b326faa7cae82a8c8cab7fe",
            "filename": "docs/source/ja/generation_strategies.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fgeneration_strategies.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -170,7 +170,7 @@ An increasing sequence: one, two, three, four, five, six, seven, eight, nine, te\n \n ### Contrastive search\n \n-ใณใณใใฉในใใฃใๆค็ดขใใณใผใใฃใณใฐๆฆ็ฅใฏใ2022ๅนดใฎ่ซๆ[A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417)ใงๆๆกใใใพใใใ\n+ใณใณใใฉในใใฃใๆค็ดขใใณใผใใฃใณใฐๆฆ็ฅใฏใ2022ๅนดใฎ่ซๆ[A Contrastive Framework for Neural Text Generation](https://huggingface.co/papers/2202.06417)ใงๆๆกใใใพใใใ\n ใใใฏใ้ๅๅพฉ็ใงใใใชใใไธ่ฒซๆงใฎใใ้ทใๅบๅใ็ๆใใใใใซๅชใใ็ตๆใ็คบใใฆใใพใใใณใณใใฉในใใฃใๆค็ดขใฎๅไฝๅ็ใๅญฆใถใซใฏใ[ใใฎใใญใฐใในใ](https://huggingface.co/blog/introducing-csearch)ใใ่ฆงใใใใใ\n ใณใณใใฉในใใฃใๆค็ดขใฎๅไฝใๆๅนใซใใๅถๅพกใใ2ใคใฎไธป่ฆใชใใฉใกใผใฟใฏใpenalty_alphaใใจใtop_kใใงใ๏ผ\n \n@@ -266,7 +266,7 @@ time.\"\\n\\nHe added: \"I am very proud of the work I have been able to do in the l\n \n ### Diverse beam search decoding\n \n-ๅคๆงใชใใผใใตใผใใใณใผใใฃใณใฐๆฆ็ฅใฏใใใผใใตใผใๆฆ็ฅใฎๆกๅผตใงใใใ้ธๆ่ขใใใใๅคๆงใชใใผใใทใผใฑใณในใ็ๆใงใใใใใซใใพใใใใฎไป็ตใฟใฎ่ฉณ็ดฐใซใคใใฆใฏใ[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf) ใใๅ็งใใใใใใใฎใขใใญใผใใซใฏใ`num_beams`ใ`num_beam_groups`ใใใใณ `diversity_penalty` ใจใใ3ใคใฎไธป่ฆใชใใฉใกใผใฟใใใใพใใๅคๆงๆงใใใซใใฃใฏใๅบๅใใฐใซใผใใใจใซ็ฐใชใใใจใไฟ่จผใใใใผใใตใผใใฏๅใฐใซใผใๅใงไฝฟ็จใใใพใใ\n+ๅคๆงใชใใผใใตใผใใใณใผใใฃใณใฐๆฆ็ฅใฏใใใผใใตใผใๆฆ็ฅใฎๆกๅผตใงใใใ้ธๆ่ขใใใใๅคๆงใชใใผใใทใผใฑใณในใ็ๆใงใใใใใซใใพใใใใฎไป็ตใฟใฎ่ฉณ็ดฐใซใคใใฆใฏใ[Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://huggingface.co/papers/1610.02424) ใใๅ็งใใใใใใใฎใขใใญใผใใซใฏใ`num_beams`ใ`num_beam_groups`ใใใใณ `diversity_penalty` ใจใใ3ใคใฎไธป่ฆใชใใฉใกใผใฟใใใใพใใๅคๆงๆงใใใซใใฃใฏใๅบๅใใฐใซใผใใใจใซ็ฐใชใใใจใไฟ่จผใใใใผใใตใผใใฏๅใฐใซใผใๅใงไฝฟ็จใใใพใใ\n \n \n ```python"
        },
        {
            "sha": "775bffdd0c694e01976a8ec427452dce75610f5a",
            "filename": "docs/source/ja/glossary.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fglossary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fglossary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fglossary.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -149,7 +149,7 @@ The encoded versions have different lengths:\n ใใฉใณในใใฉใผใใผๅใฎๅๆฎๅทฎๆณจๆใใญใใฏใงใฏใ้ๅธธใ่ชๅทฑๆณจๆๅฑคใฎๅพใซ2ใคใฎใใฃใผใใใฉใฏใผใๅฑคใ็ถใใพใใ\n ใใฃใผใใใฉใฏใผใๅฑคใฎไธญ้ๅใ่พผใฟใตใคใบใฏใใขใใซใฎ้ใใใตใคใบใใใๅคงใใใใจใใใใใใพใ๏ผใใจใใฐใ`google-bert/bert-base-uncased`ใฎๅดๅ๏ผใ\n \n-ๅฅๅใตใคใบใ `[batch_sizeใsequence_length]` ใฎๅดๅใไธญ้ใใฃใผใใใฉใฏใผใๅใ่พผใฟ `[batch_sizeใsequence_lengthใconfig.intermediate_size]` ใไฟๅญใใใใใซๅฟ่ฆใชใกใขใชใฏใใกใขใชใฎๅคง้จๅใๅใใใใจใใใใพใใ[Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451)ใฎ่่ใฏใ่จ็ฎใ `sequence_length` ๆฌกๅใซไพๅญใใชใใใใไธกๆนใฎใใฃใผใใใฉใฏใผใๅฑคใฎๅบๅๅใ่พผใฟ `[batch_sizeใconfig.hidden_size]_0ใ...ใ[batch_sizeใconfig.hidden_size]_n` ใๅๅฅใซ่จ็ฎใใๅพใง `[batch_sizeใsequence_lengthใconfig.hidden_size]` ใซ้ฃ็ตใใใใจใฏๆฐๅญฆ็ใซ็ญไพกใงใใใจๆฐไปใใพใใใใใใซใใใๅขๅใใ่จ็ฎๆ้ใจใกใขใชไฝฟ็จ้ใฎใใฌใผใใชใใ็ใใพใใใๆฐๅญฆ็ใซ็ญไพกใช็ตๆใๅพใใใพใใ\n+ๅฅๅใตใคใบใ `[batch_sizeใsequence_length]` ใฎๅดๅใไธญ้ใใฃใผใใใฉใฏใผใๅใ่พผใฟ `[batch_sizeใsequence_lengthใconfig.intermediate_size]` ใไฟๅญใใใใใซๅฟ่ฆใชใกใขใชใฏใใกใขใชใฎๅคง้จๅใๅใใใใจใใใใพใใ[Reformer: The Efficient Transformer](https://huggingface.co/papers/2001.04451)ใฎ่่ใฏใ่จ็ฎใ `sequence_length` ๆฌกๅใซไพๅญใใชใใใใไธกๆนใฎใใฃใผใใใฉใฏใผใๅฑคใฎๅบๅๅใ่พผใฟ `[batch_sizeใconfig.hidden_size]_0ใ...ใ[batch_sizeใconfig.hidden_size]_n` ใๅๅฅใซ่จ็ฎใใๅพใง `[batch_sizeใsequence_lengthใconfig.hidden_size]` ใซ้ฃ็ตใใใใจใฏๆฐๅญฆ็ใซ็ญไพกใงใใใจๆฐไปใใพใใใใใใซใใใๅขๅใใ่จ็ฎๆ้ใจใกใขใชไฝฟ็จ้ใฎใใฌใผใใชใใ็ใใพใใใๆฐๅญฆ็ใซ็ญไพกใช็ตๆใๅพใใใพใใ\n \n [`apply_chunking_to_forward`] ้ขๆฐใไฝฟ็จใใใขใใซใฎๅดๅใ`chunk_size` ใฏไธฆๅใซ่จ็ฎใใใๅบๅๅใ่พผใฟใฎๆฐใๅฎ็พฉใใใกใขใชใจๆ้ใฎ่ค้ใใฎใใฌใผใใชใใๅฎ็พฉใใพใใ`chunk_size` ใ 0 ใซ่จญๅฎใใใฆใใๅดๅใใใฃใผใใใฉใฏใผใใฎใใฃใณใญใณใฐใฏ่กใใใพใใใ\n \n@@ -185,7 +185,7 @@ The encoded versions have different lengths:\n \n <Youtube id=\"VFp38yj8h3A\"/>\n \n-ๅใใผใฏใใคใถใผใฏ็ฐใชใๆนๆณใงๅไฝใใพใใใๅบๆฌ็ใชใกใซใใบใใฏๅใใงใใไปฅไธใฏBERTใใผใฏใใคใถใผใไฝฟ็จใใไพใงใใBERTใใผใฏใใคใถใผใฏ[WordPiece](https://arxiv.org/pdf/1609.08144.pdf)ใใผใฏใใคใถใผใงใใ\n+ๅใใผใฏใใคใถใผใฏ็ฐใชใๆนๆณใงๅไฝใใพใใใๅบๆฌ็ใชใกใซใใบใใฏๅใใงใใไปฅไธใฏBERTใใผใฏใใคใถใผใไฝฟ็จใใไพใงใใBERTใใผใฏใใคใถใผใฏ[WordPiece](https://huggingface.co/papers/1609.08144)ใใผใฏใใคใถใผใงใใ\n \n \n ```python"
        },
        {
            "sha": "1fdd300b7682c0c02a733fa01b45537fe2cec36d",
            "filename": "docs/source/ja/index.md",
            "status": "modified",
            "additions": 156,
            "deletions": 156,
            "changes": 312,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Findex.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82"
        },
        {
            "sha": "b0bc6610876baef0d40251e6b34a06ad773e29d4",
            "filename": "docs/source/ja/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # DeepSpeed Integration\n \n-[DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ใฏใ[ZeRO ่ซๆ](https://arxiv.org/abs/1910.02054) ใง่ชฌๆใใใฆใใใในใฆใๅฎ่ฃใใพใใ็พๅจใๆฌกใฎใใฎใๅฎๅจใซใตใใผใใใฆใใพใใ\n+[DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ใฏใ[ZeRO ่ซๆ](https://huggingface.co/papers/1910.02054) ใง่ชฌๆใใใฆใใใในใฆใๅฎ่ฃใใพใใ็พๅจใๆฌกใฎใใฎใๅฎๅจใซใตใใผใใใฆใใพใใ\n \n 1. ใชใใใฃใใคใถใผใฎ็ถๆๅๅฒ (ZeRO ในใใผใธ 1)\n 2. ๅพ้ๅๅฒ (ZeRO ในใใผใธ 2)\n@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n 5. ไธ้ฃใฎ้ซ้ CUDA ๆกๅผตใใผในใฎใชใใใฃใใคใถใผ\n 6. CPU ใใใณ NVMe ใธใฎ ZeRO ใชใใญใผใ\n \n-ZeRO-Offload ใซใฏ็ฌ่ชใฎๅฐ็จใใผใใผใใใใพใ: [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)ใ NVMe ใตใใผใใซใคใใฆใฏใ่ซๆ [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)ใ\n+ZeRO-Offload ใซใฏ็ฌ่ชใฎๅฐ็จใใผใใผใใใใพใ: [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://huggingface.co/papers/2101.06840)ใ NVMe ใตใใผใใซใคใใฆใฏใ่ซๆ [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://huggingface.co/papers/2104.07857)ใ\n \n DeepSpeed ZeRO-2 ใฏใใใฎๆฉ่ฝใๆจ่ซใซใฏๅฝนใซ็ซใใชใใใใไธปใซใใฌใผใใณใฐใฎใฟใซไฝฟ็จใใใพใใ\n \n@@ -2246,9 +2246,9 @@ RUN_SLOW=1 pytest tests/deepspeed\n \n ่ซๆ:\n \n-- [ZeRO: ๅใใฉใกใผใฟ ใขใใซใฎใใฌใผใใณใฐใซๅใใใกใขใชใฎๆ้ฉๅ](https://arxiv.org/abs/1910.02054)\n-- [ZeRO-Offload: 10 ๅ่ฆๆจกใฎใขใใซ ใใฌใผใใณใฐใฎๆฐไธปๅ](https://arxiv.org/abs/2101.06840)\n-- [ZeRO-Infinity: ๆฅต้ในใฑใผใซใฎๆทฑๅฑคๅญฆ็ฟใฎใใใฎ GPU ใกใขใชใฎๅฃใๆใก็ดใ](https://arxiv.org/abs/2104.07857)\n+- [ZeRO: ๅใใฉใกใผใฟ ใขใใซใฎใใฌใผใใณใฐใซๅใใใกใขใชใฎๆ้ฉๅ](https://huggingface.co/papers/1910.02054)\n+- [ZeRO-Offload: 10 ๅ่ฆๆจกใฎใขใใซ ใใฌใผใใณใฐใฎๆฐไธปๅ](https://huggingface.co/papers/2101.06840)\n+- [ZeRO-Infinity: ๆฅต้ในใฑใผใซใฎๆทฑๅฑคๅญฆ็ฟใฎใใใฎ GPU ใกใขใชใฎๅฃใๆใก็ดใ](https://huggingface.co/papers/2104.07857)\n \n ๆๅพใซใHuggingFace [`Trainer`] ใฏ DeepSpeed ใฎใฟใ็ตฑๅใใฆใใใใจใ่ฆใใฆใใใฆใใใใใ\n DeepSpeed ใฎไฝฟ็จใซ้ขใใฆๅ้กใ่ณชๅใใใๅดๅใฏใ[DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/issues) ใซๅ้กใๆๅบใใฆใใใใใ"
        },
        {
            "sha": "45333859e8fdf61b7973b1e8c03389a5509c5946",
            "filename": "docs/source/ja/main_classes/processors.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmain_classes%2Fprocessors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmain_classes%2Fprocessors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fprocessors.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -78,7 +78,7 @@ QQPใQNLIใRTEใWNLIใ\n ่จ่ชใ่ถใใใใญในใ่กจ็พใฎๅ่ณชใ XNLI ใฏใ[*MultiNLI*](http://www.nyu.edu/projects/bowman/multinli/) ใซๅบใฅใใฏใฉใฆใใฝใผในใฎใใผใฟใปใใใงใใใใญในใใฎใใขใซใฏใ15 ๅใฎใใญในใๅซๆใขใใใผใทใงใณใใฉใใซไปใใใใฆใใพใใ\n ใใพใใพใช่จ่ช (่ฑ่ชใชใฉใฎ้ซใชใฝใผใน่จ่ชใจในใฏใใช่ชใชใฉใฎไฝใชใฝใผใน่จ่ชใฎไธกๆนใๅซใ)ใ\n \n-่ซๆ [XNLI: Evaluating Cross-lingual Sentence Representations](https://arxiv.org/abs/1809.05053) ใจๅๆใซใชใชใผในใใใพใใใ\n+่ซๆ [XNLI: Evaluating Cross-lingual Sentence Representations](https://huggingface.co/papers/1809.05053) ใจๅๆใซใชใชใผในใใใพใใใ\n \n ใใฎใฉใคใใฉใชใฏใXNLI ใใผใฟใใญใผใใใใใญใปใใตใใในใใใพใใ\n \n@@ -92,8 +92,8 @@ QQPใQNLIใRTEใWNLIใ\n \n [The Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer//) ใฏใๆฌกใฎใใณใใใผใฏใงใใ\n ่ณชๅๅฟ็ญใซ้ขใใใขใใซใฎใใใฉใผใใณในใ่ฉไพกใใพใใ v1.1 ใจ v2.0 ใฎ 2 ใคใฎใใผใธใงใณใๅฉ็จๅฏ่ฝใงใใๆๅใฎใใผใธใงใณ\n-(v1.1) ใฏใ่ซๆ [SQuAD: 100,000+ question for Machine Comprehension of Text](https://arxiv.org/abs/1606.05250) ใจใจใใซใชใชใผในใใใพใใใ 2 ็ช็ฎใฎใใผใธใงใณ (v2.0) ใฏใ่ซๆ [Know What You Don't ใจๅๆใซใชใชใผในใใใพใใใ\n-็ฅใฃใฆใใในใ: SQuAD ใฎ็ญใใใใชใ่ณชๅ](https://arxiv.org/abs/1806.03822)ใ\n+(v1.1) ใฏใ่ซๆ [SQuAD: 100,000+ question for Machine Comprehension of Text](https://huggingface.co/papers/1606.05250) ใจใจใใซใชใชใผในใใใพใใใ 2 ็ช็ฎใฎใใผใธใงใณ (v2.0) ใฏใ่ซๆ [Know What You Don't ใจๅๆใซใชใชใผในใใใพใใใ\n+็ฅใฃใฆใใในใ: SQuAD ใฎ็ญใใใใชใ่ณชๅ](https://huggingface.co/papers/1806.03822)ใ\n \n ใใฎใฉใคใใฉใชใฏใๆฌกใฎ 2 ใคใฎใใผใธใงใณใฎใใใใใฎใใญใปใใตใใในใใใพใใ\n "
        },
        {
            "sha": "b511b9dbfc6bc82215bcefc473b1c38630b6c971",
            "filename": "docs/source/ja/main_classes/quantization.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fquantization.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -22,7 +22,7 @@ rendered properly in your Markdown viewer.\n ๐ค Transformers ใซใฏใ่จ่ชใขใใซใง GPTQ ้ๅญๅใๅฎ่กใใใใใฎ `optimum` API ใ็ตฑๅใใใฆใใพใใใใใฉใผใใณในใๅคงๅนใซไฝไธใใใใใจใชใใๆจ่ซ้ๅบฆใ้ซ้ๅใใใใจใชใใใขใใซใ 8ใ4ใ3ใใใใซใฏ 2 ใใใใงใญใผใใใใณ้ๅญๅใงใใพใใใใใฏใใปใจใใฉใฎ GPU ใใผใใฆใงใขใงใตใใผใใใใฆใใพใใ\n \n ้ๅญๅใขใใซใฎ่ฉณ็ดฐใซใคใใฆใฏใไปฅไธใ็ขบ่ชใใฆใใใใใ\n-- [GPTQ](https://arxiv.org/pdf/2210.17323.pdf) ่ซๆ\n+- [GPTQ](https://huggingface.co/papers/2210.17323) ่ซๆ\n - GPTQ ้ๅญๅใซ้ขใใ `optimum` [ใฌใคใ](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization)\n - ใใใฏใจใณใใจใใฆไฝฟ็จใใใ [`AutoGPTQ`](https://github.com/PanQiWei/AutoGPTQ) ใฉใคใใฉใช\n \n@@ -163,7 +163,7 @@ GPTQ ใไฝฟ็จใใฆใขใใซใ้ๅญๅใใๆนๆณใจใpeft ใไฝฟ็จใใฆ\n ๐ค Transformers ใฏใ`bitsandbytes` ใงๆใใใไฝฟ็จใใใใขใธใฅใผใซใจ็ทๅฏใซ็ตฑๅใใใฆใใพใใๆฐ่กใฎใณใผใใงใขใใซใ 8 ใใใ็ฒพๅบฆใงใญใผใใงใใพใใ\n ใใใฏใ`bitsandbytes`ใฎ `0.37.0`ใชใชใผในไปฅ้ใใปใจใใฉใฎ GPU ใใผใใฆใงใขใงใตใใผใใใใฆใใพใใ\n \n-้ๅญๅๆนๆณใฎ่ฉณ็ดฐใซใคใใฆใฏใ[LLM.int8()](https://arxiv.org/abs/2208.07339) ่ซๆใใพใใฏ [ใใญใฐๆ็จฟ](https://huggingface.co/blog/hf-bitsandbytes-) ใใ่ฆงใใใใใ็ตฑๅ๏ผใณใฉใใฌใผใทใงใณใซใคใใฆใ\n+้ๅญๅๆนๆณใฎ่ฉณ็ดฐใซใคใใฆใฏใ[LLM.int8()](https://huggingface.co/papers/2208.07339) ่ซๆใใพใใฏ [ใใญใฐๆ็จฟ](https://huggingface.co/blog/hf-bitsandbytes-) ใใ่ฆงใใใใใ็ตฑๅ๏ผใณใฉใใฌใผใทใงใณใซใคใใฆใ\n \n `0.39.0`ใชใชใผในไปฅ้ใFP4 ใใผใฟๅใๆดป็จใใ4 ใใใ้ๅญๅใไฝฟ็จใใฆ`device_map`ใใตใใผใใใไปปๆใฎใขใใซใใญใผใใงใใพใใ\n \n@@ -214,7 +214,7 @@ torch.float32\n \n - **`batch_size=1` ใซใใ้ซ้ๆจ่ซ :** bitsandbytes ใฎ `0.40.0` ใชใชใผในไปฅ้ใ`batch_size=1` ใงใฏ้ซ้ๆจ่ซใฎๆฉๆตใๅใใใใจใใงใใพใใ [ใใใใฎใชใชใผใน ใใผใ](https://github.com/TimDettmers/bitsandbytes/releases/tag/0.40.0) ใ็ขบ่ชใใใใฎๆฉ่ฝใๆดป็จใใใซใฏ`0.40.0`ไปฅ้ใฎใใผใธใงใณใไฝฟ็จใใฆใใใใจใ็ขบ่ชใใฆใใใใใ็ฎฑใฎใ\n \n-- **ใใฌใผใใณใฐ:** [QLoRA ่ซๆ](https://arxiv.org/abs/2305.14314) ใซใใใจใ4 ใใใๅบๆฌใขใใซใใใฌใผใใณใฐใใๅดๅ (ไพ: LoRA ใขใใใฟใผใไฝฟ็จ)ใ`bnb_4bit_quant_type='nf4'` ใไฝฟ็จใใๅฟ่ฆใใใใพใใ ใ\n+- **ใใฌใผใใณใฐ:** [QLoRA ่ซๆ](https://huggingface.co/papers/2305.14314) ใซใใใจใ4 ใใใๅบๆฌใขใใซใใใฌใผใใณใฐใใๅดๅ (ไพ: LoRA ใขใใใฟใผใไฝฟ็จ)ใ`bnb_4bit_quant_type='nf4'` ใไฝฟ็จใใๅฟ่ฆใใใใพใใ ใ\n \n - **ๆจ่ซ:** ๆจ่ซใฎๅดๅใ`bnb_4bit_quant_type` ใฏใใใฉใผใใณในใซๅคงใใชๅฝฑ้ฟใไธใใพใใใใใใใใขใใซใฎ้ใฟใจใฎไธ่ฒซๆงใไฟใคใใใซใๅฟใๅใ `bnb_4bit_compute_dtype` ใใใณ `torch_dtype` ๅผๆฐใไฝฟ็จใใฆใใใใใ\n "
        },
        {
            "sha": "57f0a12a7efbbc6336031dbcb62f0165a37e30a1",
            "filename": "docs/source/ja/main_classes/trainer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -291,7 +291,7 @@ export CUDA_VISIBLE_DEVICES=1,0\n [`Trainer`] ใฏใใใฌใผใใณใฐใๅ็ใซๆนๅใใๅฏ่ฝๆงใฎใใใฉใคใใฉใชใใตใใผใใใใใใซๆกๅผตใใใพใใใ\n ๆ้ใจใฏใใใซๅคงใใชใขใใซใซ้ฉๅใใพใใ\n \n-็พๅจใใตใผใใใผใใฃใฎใฝใชใฅใผใทใงใณ [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ใใใณ [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html) ใใตใใผใใใฆใใพใใ่ซๆ [ZeRO: ใกใขใชใฎๆ้ฉๅๅใใฉใกใผใฟ ใขใใซใฎใใฌใผใใณใฐใซๅใใฆใSamyam RajbhandariใJeff RasleyใOlatunji RuwaseใYuxiong He ่](https://arxiv.org/abs/1910.02054)ใ\n+็พๅจใใตใผใใใผใใฃใฎใฝใชใฅใผใทใงใณ [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ใใใณ [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html) ใใตใใผใใใฆใใพใใ่ซๆ [ZeRO: ใกใขใชใฎๆ้ฉๅๅใใฉใกใผใฟ ใขใใซใฎใใฌใผใใณใฐใซๅใใฆใSamyam RajbhandariใJeff RasleyใOlatunji RuwaseใYuxiong He ่](https://huggingface.co/papers/1910.02054)ใ\n \n ใใฎๆไพใใใใตใใผใใฏใใใฎ่จไบใฎๅท็ญๆ็นใงใฏๆฐใใใฆๅฎ้จ็ใชใใฎใงใใ DeepSpeed ใจ PyTorch FSDP ใฎใตใใผใใฏใขใฏใใฃใใงใใใใใใซ้ขใใๅ้กใฏๆญ่ฟใใพใใใFairScale ็ตฑๅใฏ PyTorch ใกใคใณใซ็ตฑๅใใใฆใใใใใใใใตใใผใใใฆใใพใใ ([PyTorch FSDP ็ตฑๅ](#pytorch-fully-sharded-data-parallel))\n "
        },
        {
            "sha": "7824f24599912e79315cfdc9166bbe691960e2ac",
            "filename": "docs/source/ja/model_doc/albert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Falbert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n \n ## ๆฆ่ฆ\n \n-ALBERTใขใใซใฏใใ[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)ใใจใใ่ซๆใงZhenzhong LanใMingda ChenใSebastian GoodmanใKevin GimpelใPiyush SharmaใRadu SoricutใซใใฃใฆๆๆกใใใพใใใBERTใฎใกใขใชๆถ่ฒปใๆธใใใใฌใผใใณใฐใ้ซ้ๅใใใใใฎใใฉใกใผใฟๅๆธๆ่กใ2ใค็คบใใฆใใพใ๏ผ\n+ALBERTใขใใซใฏใใ[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://huggingface.co/papers/1909.11942)ใใจใใ่ซๆใงZhenzhong LanใMingda ChenใSebastian GoodmanใKevin GimpelใPiyush SharmaใRadu SoricutใซใใฃใฆๆๆกใใใพใใใBERTใฎใกใขใชๆถ่ฒปใๆธใใใใฌใผใใณใฐใ้ซ้ๅใใใใใฎใใฉใกใผใฟๅๆธๆ่กใ2ใค็คบใใฆใใพใ๏ผ\n \n - ๅใ่พผใฟ่กๅใ2ใคใฎๅฐใใช่กๅใซๅๅฒใใใ\n - ใฐใซใผใ้ใงๅๅฒใใใ็นฐใ่ฟใๅฑคใไฝฟ็จใใใ"
        },
        {
            "sha": "d1ff4d918a64ffd61c41cd9f139f5797962dd217",
            "filename": "docs/source/ja/model_doc/align.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Falign.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Falign.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Falign.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## ๆฆ่ฆ\n \n-ALIGNใขใใซใฏใใ[Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/abs/2102.05918)ใใจใใ่ซๆใงChao JiaใYinfei YangใYe XiaใYi-Ting ChenใZarana ParekhใHieu PhamใQuoc V. LeใYunhsuan SungใZhen LiใTom DuerigใซใใฃใฆๆๆกใใใพใใใALIGNใฏใใซใใขใผใใซใช่ฆ่ฆ่จ่ชใขใใซใงใใใใใฏ็ปๅใจใใญในใใฎ้กไผผๅบฆใใใผใญใทใงใใ็ปๅๅ้กใซไฝฟ็จใงใใพใใALIGNใฏ[EfficientNet](efficientnet)ใ่ฆ่ฆใจใณใณใผใใผใจใใฆใ[BERT](bert)ใใใญในใใจใณใณใผใใผใจใใฆๆญ่ผใใใใฅใขใซใจใณใณใผใใผๆง้ใ็นๅพดใจใใๅฏพ็งๅญฆ็ฟใซใใฃใฆ่ฆ่ฆใจใใญในใใฎ่กจ็พใๆดๅใใใใใจใๅญฆใณใพใใใใใพใงใฎ็็ฉถใจใฏ็ฐใชใใALIGNใฏๅทจๅคงใงใใคใธใผใชใใผใฟใปใใใๆดป็จใใใณใผใในใฎในใฑใผใซใๅฉ็จใใฆๅ็ดใชๆนๆณใชใใๆๅ็ซฏใฎ่กจ็พใ้ๆใงใใใใจใ็คบใใฆใใพใใ\n+ALIGNใขใใซใฏใใ[Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://huggingface.co/papers/2102.05918)ใใจใใ่ซๆใงChao JiaใYinfei YangใYe XiaใYi-Ting ChenใZarana ParekhใHieu PhamใQuoc V. LeใYunhsuan SungใZhen LiใTom DuerigใซใใฃใฆๆๆกใใใพใใใALIGNใฏใใซใใขใผใใซใช่ฆ่ฆ่จ่ชใขใใซใงใใใใใฏ็ปๅใจใใญในใใฎ้กไผผๅบฆใใใผใญใทใงใใ็ปๅๅ้กใซไฝฟ็จใงใใพใใALIGNใฏ[EfficientNet](efficientnet)ใ่ฆ่ฆใจใณใณใผใใผใจใใฆใ[BERT](bert)ใใใญในใใจใณใณใผใใผใจใใฆๆญ่ผใใใใฅใขใซใจใณใณใผใใผๆง้ใ็นๅพดใจใใๅฏพ็งๅญฆ็ฟใซใใฃใฆ่ฆ่ฆใจใใญในใใฎ่กจ็พใๆดๅใใใใใจใๅญฆใณใพใใใใใพใงใฎ็็ฉถใจใฏ็ฐใชใใALIGNใฏๅทจๅคงใงใใคใธใผใชใใผใฟใปใใใๆดป็จใใใณใผใในใฎในใฑใผใซใๅฉ็จใใฆๅ็ดใชๆนๆณใชใใๆๅ็ซฏใฎ่กจ็พใ้ๆใงใใใใจใ็คบใใฆใใพใใ\n \n ่ซๆใฎ่ฆๆจใฏไปฅไธใฎ้ใใงใ๏ผ\n "
        },
        {
            "sha": "fe721d29bfe5370f03c424cb94077f97d2571e06",
            "filename": "docs/source/ja/model_doc/altclip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Faltclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Faltclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Faltclip.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -19,7 +19,7 @@ rendered properly in your Markdown viewer.\n ## ๆฆ่ฆ\n \n \n-AltCLIPใขใใซใฏใใ[AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://arxiv.org/abs/2211.06679v2)ใใจใใ่ซๆใงZhongzhi ChenใGuang LiuใBo-Wen ZhangใFulong YeใQinghong YangใLedell WuใซใใฃใฆๆๆกใใใพใใใAltCLIP๏ผCLIPใฎ่จ่ชใจใณใณใผใใผใฎไปฃๆฟ๏ผใฏใๆงใใช็ปๅ-ใใญในใใใขใใใณใใญในใ-ใใญในใใใขใงใใฌใผใใณใฐใใใใใฅใผใฉใซใใใใฏใผใฏใงใใCLIPใฎใใญในใใจใณใณใผใใผใไบๅๅญฆ็ฟๆธใฟใฎๅค่จ่ชใใญในใใจใณใณใผใใผXLM-Rใซ็ฝฎใๆใใใใจใงใใปใผๅจใฆใฎใฟในใฏใงCLIPใซ้ๅธธใซ่ฟใๆง่ฝใๅพใใใใชใชใธใใซใฎCLIPใฎ่ฝๅใๅค่จ่ช็่งฃใชใฉใซๆกๅผตใใพใใใ\n+AltCLIPใขใใซใฏใใ[AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities](https://huggingface.co/papers/2211.06679)ใใจใใ่ซๆใงZhongzhi ChenใGuang LiuใBo-Wen ZhangใFulong YeใQinghong YangใLedell WuใซใใฃใฆๆๆกใใใพใใใAltCLIP๏ผCLIPใฎ่จ่ชใจใณใณใผใใผใฎไปฃๆฟ๏ผใฏใๆงใใช็ปๅ-ใใญในใใใขใใใณใใญในใ-ใใญในใใใขใงใใฌใผใใณใฐใใใใใฅใผใฉใซใใใใฏใผใฏใงใใCLIPใฎใใญในใใจใณใณใผใใผใไบๅๅญฆ็ฟๆธใฟใฎๅค่จ่ชใใญในใใจใณใณใผใใผXLM-Rใซ็ฝฎใๆใใใใจใงใใปใผๅจใฆใฎใฟในใฏใงCLIPใซ้ๅธธใซ่ฟใๆง่ฝใๅพใใใใชใชใธใใซใฎCLIPใฎ่ฝๅใๅค่จ่ช็่งฃใชใฉใซๆกๅผตใใพใใใ\n \n ่ซๆใฎ่ฆๆจใฏไปฅไธใฎ้ใใงใ๏ผ\n "
        },
        {
            "sha": "eb0c3cf6e36eca8ea8774f9467ddaf02fe17310c",
            "filename": "docs/source/ja/model_doc/audio-spectrogram-transformer.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Faudio-spectrogram-transformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## ๆฆ่ฆ\n \n-Audio Spectrogram Transformerใขใใซใฏใ[AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778)ใจใใ่ซๆใงYuan GongใYu-An ChungใJames Glassใซใใฃใฆๆๆกใใใพใใใใใใฏใ้ณๅฃฐใ็ปๅ๏ผในใใฏใใญใฐใฉใ๏ผใซๅคๆใใใใจใงใ้ณๅฃฐใซ[Vision Transformer](vit)ใ้ฉ็จใใพใใใใฎใขใใซใฏ้ณๅฃฐๅ้กใซใใใฆๆๅ็ซฏใฎ็ตๆใๅพใฆใใพใใ\n+Audio Spectrogram Transformerใขใใซใฏใ[AST: Audio Spectrogram Transformer](https://huggingface.co/papers/2104.01778)ใจใใ่ซๆใงYuan GongใYu-An ChungใJames Glassใซใใฃใฆๆๆกใใใพใใใใใใฏใ้ณๅฃฐใ็ปๅ๏ผในใใฏใใญใฐใฉใ๏ผใซๅคๆใใใใจใงใ้ณๅฃฐใซ[Vision Transformer](vit)ใ้ฉ็จใใพใใใใฎใขใใซใฏ้ณๅฃฐๅ้กใซใใใฆๆๅ็ซฏใฎ็ตๆใๅพใฆใใพใใ\n \n ่ซๆใฎ่ฆๆจใฏไปฅไธใฎ้ใใงใ๏ผ\n \n@@ -27,15 +27,15 @@ Audio Spectrogram Transformerใขใใซใฏใ[AST: Audio Spectrogram Transformer]\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/audio_spectogram_transformer_architecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> Audio Spectrogram Transformerใฎใขใผใญใใฏใใฃใ<a href=\"https://arxiv.org/abs/2104.01778\">ๅ่ซๆ</a>ใใๆ็ฒใ</small>\n+<small> Audio Spectrogram Transformerใฎใขใผใญใใฏใใฃใ<a href=\"https://huggingface.co/papers/2104.01778\">ๅ่ซๆ</a>ใใๆ็ฒใ</small>\n \n ใใฎใขใใซใฏ[nielsr](https://huggingface.co/nielsr)ใใๆไพใใใพใใใ\n ใชใชใธใใซใฎใณใผใใฏ[ใใกใ](https://github.com/YuanGongND/ast)ใง่ฆใใใจใใงใใพใใ\n \n ## ไฝฟ็จไธใฎใใณใ\n \n - ็ฌ่ชใฎใใผใฟใปใใใงAudio Spectrogram Transformer๏ผAST๏ผใใใกใคใณใใฅใผใใณใฐใใๅดๅใๅฅๅใฎๆญฃ่ฆๅ๏ผๅฅๅใฎๅนณๅใ0ใๆจๆบๅๅทฎใ0.5ใซใใใใจ๏ผๅฆ็ใใใใจใๆจๅฅจใใใพใใ[`ASTFeatureExtractor`]ใฏใใใๅฆ็ใใพใใใใใฉใซใใงใฏAudioSetใฎๅนณๅใจๆจๆบๅๅทฎใไฝฟ็จใใฆใใใใจใซๆณจๆใใฆใใใใใ่่ใไธๆตใฎใใผใฟใปใใใฎ็ตฑ่จใใฉใฎใใใซ่จ็ฎใใฆใใใใฏใ[`ast/src/get_norm_stats.py`](https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py)ใง็ขบ่ชใใใใจใใงใใพใใ\n-- ASTใฏไฝใๅญฆ็ฟ็ใๅฟ่ฆใงใใ ่่ใฏ[PSLA่ซๆ](https://arxiv.org/abs/2102.01243)ใงๆๆกใใใCNNใขใใซใซๆฏในใฆ10ๅๅฐใใๅญฆ็ฟ็ใไฝฟ็จใใฆใใพใ๏ผใ็ดๆฉใๅๆใใใใใใฟในใฏใซ้ฉใใๅญฆ็ฟ็ใจๅญฆ็ฟ็ในใฑใธใฅใผใฉใผใๆขใใใจใใๅงใใใพใใ\n+- ASTใฏไฝใๅญฆ็ฟ็ใๅฟ่ฆใงใใ ่่ใฏ[PSLA่ซๆ](https://huggingface.co/papers/2102.01243)ใงๆๆกใใใCNNใขใใซใซๆฏในใฆ10ๅๅฐใใๅญฆ็ฟ็ใไฝฟ็จใใฆใใพใ๏ผใ็ดๆฉใๅๆใใใใใใฟในใฏใซ้ฉใใๅญฆ็ฟ็ใจๅญฆ็ฟ็ในใฑใธใฅใผใฉใผใๆขใใใจใใๅงใใใพใใ\n \n ## ๅ่่ณๆ\n "
        },
        {
            "sha": "65c20bfa60f5de15b351f2d079b2e74afcdd335f",
            "filename": "docs/source/ja/model_doc/autoformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fautoformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fautoformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fautoformer.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## ๆฆ่ฆ\n \n-Autoformerใขใใซใฏใใ[Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008)ใใจใใ่ซๆใงHaixu WuใJiehui XuใJianmin WangใMingsheng Longใซใใฃใฆๆๆกใใใพใใใ\n+Autoformerใขใใซใฏใใ[Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting](https://huggingface.co/papers/2106.13008)ใใจใใ่ซๆใงHaixu WuใJiehui XuใJianmin WangใMingsheng Longใซใใฃใฆๆๆกใใใพใใใ\n \n ใใฎใขใใซใฏใไบๆธฌใใญใปในไธญใซใใฌใณใใจๅญฃ็ฏๆงๆๅใ้ๆฌก็ใซๅ่งฃใงใใๆทฑๅฑคๅ่งฃใขใผใญใใฏใใฃใจใใฆTransformerใๅขๅผทใใพใใ\n "
        },
        {
            "sha": "6d11c122d9ec7021a215c241028c61f0e21a736d",
            "filename": "docs/source/ja/model_doc/bart.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbart.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -31,7 +31,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n Bart ใขใใซใฏใ[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generationใ\n-็ฟป่จณใจ็่งฃ](https://arxiv.org/abs/1910.13461) Mike LewisใYinhan LiuใNaman GoyalใMarjan ่\n+็ฟป่จณใจ็่งฃ](https://huggingface.co/papers/1910.13461) Mike LewisใYinhan LiuใNaman GoyalใMarjan ่\n ใฌใบใใใใธใฃใใใขใใใซใฉใใใณใปใขใใกใใใชใกใซใปใฌใดใฃใใในใปในใใคใใใใซใผใฏใปใผใใซใขใคใคใผใ2019ๅนด10ๆ29ๆฅใ\n \n ่ฆ็ดใซใใใจใ\n@@ -65,7 +65,7 @@ Bart ใขใใซใฏใ[BART: Denoising Sequence-to-Sequence Pre-training for Natur\n   [examples/pytorch/summarization/](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization/README.md)ใ\n - Hugging Face `datasets` ใไฝฟ็จใใฆ [`BartForConditionalGeneration`] ใใใฌใผใใณใฐใใๆนๆณใฎไพ\n   ใชใใธใงใฏใใฏใใใฎ [ใใฉใผใฉใ ใใฃในใซใใทใงใณ](https://discuss.huggingface.co/t/train-bart-for-conditional-generation-e-g-summarization/1904) ใง่ฆใคใใใใจใใงใใพใใ\n-- [ๆฝๅบใใใใใงใใฏใใคใณใ](https://huggingface.co/models?search=distilbart) ใฏใใใฎ [่ซๆ](https://arxiv.org/abs/2010.13002) ใง่ชฌๆใใใฆใใพใใ\n+- [ๆฝๅบใใใใใงใใฏใใคใณใ](https://huggingface.co/models?search=distilbart) ใฏใใใฎ [่ซๆ](https://huggingface.co/papers/2010.13002) ใง่ชฌๆใใใฆใใพใใ\n \n ## Implementation Notes\n \n@@ -132,7 +132,7 @@ BART ใๅงใใใฎใซๅฝน็ซใคๅฌๅผ Hugging Face ใใใณใณใใฅใใใฃ\n - [ใใญในใๅ้กใฟในใฏใฌใคใ(่ฑ่ช็)](../../en/tasks/sequence_classification)\n - [่ณชๅๅ็ญใฟในใฏ ใฌใคใ](../tasks/question_answering)\n - [ๅๆ่จ่ชใขใใชใณใฐ ใฟในใฏ ใฌใคใ](../tasks/language_modeling)\n-- [ๆฝๅบใใใใใงใใฏใใคใณใ](https://huggingface.co/models?search=distilbart) ใฏใใใฎ [่ซๆ](https://arxiv.org/abs/2010.13002) ใง่ชฌๆใใใฆใใพใใ\n+- [ๆฝๅบใใใใใงใใฏใใคใณใ](https://huggingface.co/models?search=distilbart) ใฏใใใฎ [่ซๆ](https://huggingface.co/papers/2010.13002) ใง่ชฌๆใใใฆใใพใใ\n \n ## BartConfig\n "
        },
        {
            "sha": "5668772c2636763f8c546f1c11846af4f2737cb2",
            "filename": "docs/source/ja/model_doc/barthez.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbarthez.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbarthez.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbarthez.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BARThez ใขใใซใฏใMoussa Kamal EddineใAntoine J.-P ใซใใฃใฆ [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://arxiv.org/abs/2010.12321) ใงๆๆกใใใพใใใใใฃใฏใทใจใใใซใชในใปใดใกใธใซใธใฃใณใในใ10ๆ23ๆฅใ\n+BARThez ใขใใซใฏใMoussa Kamal EddineใAntoine J.-P ใซใใฃใฆ [BARThez: a Skilled Pretrained French Sequence-to-Sequence Model](https://huggingface.co/papers/2010.12321) ใงๆๆกใใใพใใใใใฃใฏใทใจใใใซใชในใปใดใกใธใซใธใฃใณใในใ10ๆ23ๆฅใ\n 2020ๅนดใ\n \n ่ซๆใฎ่ฆ็ด:"
        },
        {
            "sha": "3596a91fe1acc23f01887390746a3a948eaf4074",
            "filename": "docs/source/ja/model_doc/bartpho.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbartpho.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbartpho.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbartpho.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BARTpho ใขใใซใฏใNguyen Luong TranใDuong Minh LeใDat Quoc Nguyen ใซใใฃใฆ [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnam](https://arxiv.org/abs/2109.09701) ใงๆๆกใใใพใใใ\n+BARTpho ใขใใซใฏใNguyen Luong TranใDuong Minh LeใDat Quoc Nguyen ใซใใฃใฆ [BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnam](https://huggingface.co/papers/2109.09701) ใงๆๆกใใใพใใใ\n \n ่ซๆใฎ่ฆ็ดใฏๆฌกใฎใจใใใงใใ\n "
        },
        {
            "sha": "21ccc28c68e298288cdbf4b1f7860ac0cb75c23f",
            "filename": "docs/source/ja/model_doc/beit.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbeit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,11 +18,11 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BEiT ใขใใซใฏใ[BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254) ใงๆๆกใใใพใใใ\n+BEiT ใขใใซใฏใ[BEiT: BERT Pre-Training of Image Transformers](https://huggingface.co/papers/2106.08254) ใงๆๆกใใใพใใใ\n ใใณใใปใใชใใชใผใปใใณใใใซใปใฆใงใคใ BERT ใซ่งฆ็บใใใ BEiT ใฏใ่ชๅทฑๆๅธซใใใฎไบๅใใฌใผใใณใฐใไฝๆใใๆๅใฎ่ซๆใงใใ\n ใใธใงใณ ใใฉใณในใใฉใผใใผ (ViT) ใฏใๆๅธซไปใไบๅใใฌใผใใณใฐใใใๅชใใใใใฉใผใใณในใ็บๆฎใใพใใใฏใฉในใไบๆธฌใใใใใซใขใใซใไบๅใใฌใผใใณใฐใใใฎใงใฏใชใ\n-([ใชใชใธใใซใฎ ViT ่ซๆ](https://arxiv.org/abs/2010.11929) ใง่กใใใใใใซ) ็ปๅใฎ BEiT ใขใใซใฏใๆฌกใฎใใใซไบๅใใฌใผใใณใฐใใใฆใใพใใ\n-ใในใฏใใใ OpenAI ใฎ [DALL-E ใขใใซ](https://arxiv.org/abs/2102.12092) ใฎใณใผใใใใฏใใใใธใฅใขใซ ใใผใฏใณใไบๆธฌใใพใ\n+([ใชใชใธใใซใฎ ViT ่ซๆ](https://huggingface.co/papers/2010.11929) ใง่กใใใใใใซ) ็ปๅใฎ BEiT ใขใใซใฏใๆฌกใฎใใใซไบๅใใฌใผใใณใฐใใใฆใใพใใ\n+ใในใฏใใใ OpenAI ใฎ [DALL-E ใขใใซ](https://huggingface.co/papers/2102.12092) ใฎใณใผใใใใฏใใใใธใฅใขใซ ใใผใฏใณใไบๆธฌใใพใ\n ใใใใ\n \n ่ซๆใฎ่ฆ็ดใฏๆฌกใฎใจใใใงใใ\n@@ -66,7 +66,7 @@ BEiT ใขใใซใฏใ[BEiT: BERT Pre-Training of Image Transformers](https://arxi\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/beit_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> BEiT ใฎไบๅใใฌใผใใณใฐใ <a href=\"https://arxiv.org/abs/2106.08254\">ๅใฎ่ซๆใใๆ็ฒใ</a> </small>\n+<small> BEiT ใฎไบๅใใฌใผใใณใฐใ <a href=\"https://huggingface.co/papers/2106.08254\">ๅใฎ่ซๆใใๆ็ฒใ</a> </small>\n \n ใใฎใขใใซใฏใ[nielsr](https://huggingface.co/nielsr) ใซใใฃใฆๆไพใใใพใใใใใฎใขใใซใฎ JAX/FLAX ใใผใธใงใณใฏใ\n [kamalkraj](https://huggingface.co/kamalkraj) ใซใใๆ็จฟใๅใฎใณใผใใฏ [ใใ](https://github.com/microsoft/unilm/tree/master/beit) ใซใใใพใใ"
        },
        {
            "sha": "bf3b93296851e16f051887bf3f7fdf546d2f8844",
            "filename": "docs/source/ja/model_doc/bert-generation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert-generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert-generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert-generation.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -19,7 +19,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n BertGeneration ใขใใซใฏใๆฌกใไฝฟ็จใใฆใทใผใฑใณใน้ใฎใฟในใฏใซๅฉ็จใงใใ BERT ใขใใซใงใใ\n-[Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://arxiv.org/abs/1907.12461) ใงๆๆกใใใฆใใ [`EncoderDecoderModel`]\n+[Leveraging Pre-trained Checkpoints for Sequence Generation Tasks](https://huggingface.co/papers/1907.12461) ใงๆๆกใใใฆใใ [`EncoderDecoderModel`]\n ใฟในใฏใSascha RotheใSishi NagayanใAliaksei Severyn ่ใ\n \n ่ซๆใฎ่ฆ็ดใฏๆฌกใฎใจใใใงใใ"
        },
        {
            "sha": "e0367dcd46ebb28e9e2de72baf80b97c0f8395a8",
            "filename": "docs/source/ja/model_doc/bert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BERT ใขใใซใฏใJacob DevlinใMing-Wei ChangใKenton LeeใKristina Toutanova ใซใใฃใฆ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) ใงๆๆกใใใพใใใใใใฏ\n+BERT ใขใใซใฏใJacob DevlinใMing-Wei ChangใKenton LeeใKristina Toutanova ใซใใฃใฆ [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://huggingface.co/papers/1810.04805) ใงๆๆกใใใพใใใใใใฏ\n ใในใฏใใใ่จ่ชใขใใชใณใฐ็ฎๆจใจๆฌกใฎๆใฎ็ตใฟๅใใใไฝฟ็จใใฆไบๅใใฌใผใใณใฐใใใๅๆนๅใใฉใณในใใฉใผใใผ\n Toronto Book Corpus ใจ Wikipedia ใใใชใๅคง่ฆๆจกใชใณใผใในใงใฎไบๆธฌใ\n "
        },
        {
            "sha": "d5f4f9d282eac46ca0109f56ea46c70da7d87877",
            "filename": "docs/source/ja/model_doc/big_bird.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbig_bird.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BigBird ใขใใซใฏใ[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) ใงๆๆกใใใพใใใ\n+BigBird ใขใใซใฏใ[Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) ใงๆๆกใใใพใใใ\n ใถใใผใซใใใณใธใซใจใฐใซใฌใใทใฅใใฐใซใจใใใคใใฏใใผใซใปใขใดใฃใใดใกใจใจใคใณใบใชใผใใธใงใทใฅใขใจใขใซใใซใใฃใใฏใชในใจใชใณใฟใใณใ\n ใตใณใใฃใขใดใจใใกใใใใฃใชใใใจใฉใใฉใใขใใซใผใใจใฏใณใใญใผใใกใณใจใคใณใใชใผใชใฉใ BigBird ใฏๆณจ็ฎๅบฆใไฝใ\n BERT ใชใฉใฎ Transformer ใใผในใฎใขใใซใใใใซ้ทใใทใผใฑใณในใซๆกๅผตใใใTransformer ใใผในใฎใขใใซใใพใฐใใซๅใใฆ"
        },
        {
            "sha": "ecee19d6a5190a1969ebfa9e704d2159bc0bce50",
            "filename": "docs/source/ja/model_doc/bigbird_pegasus.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbigbird_pegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbigbird_pegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbigbird_pegasus.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BigBird ใขใใซใฏใ[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062) ใงๆๆกใใใพใใใ\n+BigBird ใขใใซใฏใ[Big Bird: Transformers for Longer Sequences](https://huggingface.co/papers/2007.14062) ใงๆๆกใใใพใใใ\n ใถใใผใซใใใณใธใซใจใฐใซใฌใใทใฅใใฐใซใจใใใคใใฏใใผใซใปใขใดใฃใใดใกใจใจใคใณใบใชใผใใธใงใทใฅใขใจใขใซใใซใใฃใใฏใชในใจใชใณใฟใใณใ\n ใตใณใใฃใขใดใจใใกใใใใฃใชใใใจใฉใใฉใใขใใซใผใใจใฏใณใใญใผใใกใณใจใคใณใใชใผใชใฉใ BigBird ใฏๆณจ็ฎๅบฆใไฝใ\n BERT ใชใฉใฎ Transformer ใใผในใฎใขใใซใใใใซ้ทใใทใผใฑใณในใซๆกๅผตใใใTransformer ใใผในใฎใขใใซใใพใฐใใซๅใใฆ"
        },
        {
            "sha": "a9e13e270aaf9690b042ebfab9d23836350cb142",
            "filename": "docs/source/ja/model_doc/bit.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbit.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BiT ใขใใซใฏใAlexander KolesnikovใLucas BeyerใXiaohua ZhaiใJoan PuigcerverใJessica YungใSylvain Gelly ใซใใฃใฆ [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) ใงๆๆกใใใพใใใใใผใซใปใใผใซใบใใผใ\n+BiT ใขใใซใฏใAlexander KolesnikovใLucas BeyerใXiaohua ZhaiใJoan PuigcerverใJessica YungใSylvain Gelly ใซใใฃใฆ [Big Transfer (BiT): General Visual Representation Learning](https://huggingface.co/papers/1912.11370) ใงๆๆกใใใพใใใใใผใซใปใใผใซใบใใผใ\n BiT ใฏใ[ResNet](resnet) ใฎใใใชใขใผใญใใฏใใฃ (ๅทไฝ็ใซใฏ ResNetv2) ใฎไบๅใใฌใผใใณใฐใในใฑใผใซใขใใใใใใใฎ็ฐกๅใชใฌใทใใงใใใใฎๆนๆณใซใใใ่ปข็งปๅญฆ็ฟใๅคงๅนใซๆนๅใใใพใใ\n \n ่ซๆใฎ่ฆ็ดใฏๆฌกใฎใจใใใงใใ\n@@ -27,8 +27,8 @@ BiT ใฏใ[ResNet](resnet) ใฎใใใชใขใผใญใใฏใใฃ (ๅทไฝ็ใซใฏ Re\n \n ## Usage tips\n \n-- BiT ใขใใซใฏใใขใผใญใใฏใใฃใฎ็นใง ResNetv2 ใจๅ็ญใงใใใๆฌกใฎ็นใ็ฐใชใใพใ: 1) ใในใฆใฎใใใๆญฃ่ฆๅๅฑคใ [ใฐใซใผใๆญฃ่ฆๅ](https://arxiv.org/abs/1803.08494) ใซ็ฝฎใๆใใใใพใใ\n-2) [้ใฟใฎๆจๆบๅ](https://arxiv.org/abs/1903.10520) ใฏ็ณใฟ่พผใฟๅฑคใซไฝฟ็จใใใพใใ่่ใใฏใไธกๆนใฎ็ตใฟๅใใใๅคงใใชใใใใตใคใบใงใฎใใฌใผใใณใฐใซๅฝน็ซใกใ้่ฆใชๅนๆใใใใใจใ็คบใใฆใใพใใ\n+- BiT ใขใใซใฏใใขใผใญใใฏใใฃใฎ็นใง ResNetv2 ใจๅ็ญใงใใใๆฌกใฎ็นใ็ฐใชใใพใ: 1) ใในใฆใฎใใใๆญฃ่ฆๅๅฑคใ [ใฐใซใผใๆญฃ่ฆๅ](https://huggingface.co/papers/1803.08494) ใซ็ฝฎใๆใใใใพใใ\n+2) [้ใฟใฎๆจๆบๅ](https://huggingface.co/papers/1903.10520) ใฏ็ณใฟ่พผใฟๅฑคใซไฝฟ็จใใใพใใ่่ใใฏใไธกๆนใฎ็ตใฟๅใใใๅคงใใชใใใใตใคใบใงใฎใใฌใผใใณใฐใซๅฝน็ซใกใ้่ฆใชๅนๆใใใใใจใ็คบใใฆใใพใใ\n ่ปข็งปๅญฆ็ฟใธใฎๅฝฑ้ฟใ\n \n ใใฎใขใใซใฏใ[nielsr](https://huggingface.co/nielsr) ใซใใฃใฆๆไพใใใพใใใ"
        },
        {
            "sha": "97455bddf806d5c29a7e2ad409152448c73c3eef",
            "filename": "docs/source/ja/model_doc/blenderbot-small.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot-small.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot-small.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot-small.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-Blender ใใฃใใใใใ ใขใใซใฏใ[Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen RollerใEmily DinanใNaman GoyalใDa JuใMary Williamsonใyinghan Liuใใงๆๆกใใใพใใใ\n+Blender ใใฃใใใใใ ใขใใซใฏใ[Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) Stephen RollerใEmily DinanใNaman GoyalใDa JuใMary Williamsonใyinghan Liuใใงๆๆกใใใพใใใ\n ใธใณใปใทใฅใผใใใคใซใปใชใใใใซใผใใปใทใฃในใฟใผใใจใชใใฏใปMใปในใในใY-ใฉใณใปใใผใญใผใใธใงใคใฝใณใปใฆใงในใใณใ2020ๅนด4ๆ30ๆฅใ\n \n ่ซๆใฎ่ฆๆจใฏๆฌกใฎใจใใใงใใ"
        },
        {
            "sha": "f2a03e69c9875c5db7f76cc0d68535da435c5554",
            "filename": "docs/source/ja/model_doc/blenderbot.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-Blender ใใฃใใใใใ ใขใใซใฏใ[Recipes for building an open-domain chatbot](https://arxiv.org/pdf/2004.13637.pdf) Stephen RollerใEmily DinanใNaman GoyalใDa JuใMary Williamsonใyinghan Liuใใงๆๆกใใใพใใใ\n+Blender ใใฃใใใใใ ใขใใซใฏใ[Recipes for building an open-domain chatbot](https://huggingface.co/papers/2004.13637) Stephen RollerใEmily DinanใNaman GoyalใDa JuใMary Williamsonใyinghan Liuใใงๆๆกใใใพใใใ\n ใธใณใปใทใฅใผใใใคใซใปใชใใใใซใผใใปใทใฃในใฟใผใใจใชใใฏใปMใปในใในใY-ใฉใณใปใใผใญใผใใธใงใคใฝใณใปใฆใงในใใณใ2020ๅนด4ๆ30ๆฅใ\n \n ่ซๆใฎ่ฆๆจใฏๆฌกใฎใจใใใงใใ\n@@ -45,7 +45,7 @@ Blender ใใฃใใใใใ ใขใใซใฏใ[Recipes for building an open-domai\n \n ## Implementation Notes\n \n-- Blenderbot ใฏใๆจๆบใฎ [seq2seq ใขใใซ ใใฉใณในใใฉใผใใผ](https://arxiv.org/pdf/1706.03762.pdf) ใใผในใฎใขใผใญใใฏใใฃใไฝฟ็จใใพใใ\n+- Blenderbot ใฏใๆจๆบใฎ [seq2seq ใขใใซ ใใฉใณในใใฉใผใใผ](https://huggingface.co/papers/1706.03762) ใใผในใฎใขใผใญใใฏใใฃใไฝฟ็จใใพใใ\n - ๅฉ็จๅฏ่ฝใชใใงใใฏใใคใณใใฏใ[ใขใใซ ใใ](https://huggingface.co/models?search=blenderbot) ใง่ฆใคใใใใจใใงใใพใใ\n - ใใใฏ *ใใใฉใซใ* Blenderbot ใขใใซ ใฏใฉในใงใใใใใใๆฌกใฎใใใชๅฐใใชใใงใใฏใใคใณใใใใใคใใใใพใใ\n   `facebook/blenderbot_small_90M` ใฏใขใผใญใใฏใใฃใ็ฐใชใใใใไธ็ทใซไฝฟ็จใใๅฟ่ฆใใใใพใใ"
        },
        {
            "sha": "52a092ac9ae650256c6252659ab66aaeedee5264",
            "filename": "docs/source/ja/model_doc/blip-2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip-2.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,9 +18,9 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BLIP-2 ใขใใซใฏใ[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) ใงๆๆกใใใพใใใ\n+BLIP-2 ใขใใซใฏใ[BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://huggingface.co/papers/2301.12597) ใงๆๆกใใใพใใใ\n Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.ใปใตใใฌใผใผใในใใฃใผใใณใปใใคใ BLIP-2 ใฏใ่ปฝ้ใฎ 12 ๅฑค Transformer ใใใฌใผใใณใฐใใใใจใงใใใชใผใบใใใไบๅใใฌใผใใณใฐๆธใฟ็ปๅใจใณใณใผใใผใจๅคง่ฆๆจก่จ่ชใขใใซ (LLM) ใๆดป็จใใพใใ\n-ใใใใฎ้ใซใจใณใณใผใใผใ้็ฝฎใใใใพใใพใช่ฆ่ฆ่จ่ชใฟในใฏใงๆๅ็ซฏใฎใใใฉใผใใณในใๅฎ็พใใพใใๆใๆณจ็ฎใในใ็นใฏใBLIP-2 ใ 800 ๅใใฉใกใผใฟ ใขใใซใงใใ [Flamingo](https://arxiv.org/abs/2204.14198) ใ 8.7% ๆนๅใใฆใใใใจใงใใ\n+ใใใใฎ้ใซใจใณใณใผใใผใ้็ฝฎใใใใพใใพใช่ฆ่ฆ่จ่ชใฟในใฏใงๆๅ็ซฏใฎใใใฉใผใใณในใๅฎ็พใใพใใๆใๆณจ็ฎใในใ็นใฏใBLIP-2 ใ 800 ๅใใฉใกใผใฟ ใขใใซใงใใ [Flamingo](https://huggingface.co/papers/2204.14198) ใ 8.7% ๆนๅใใฆใใใใจใงใใ\n ใผใญใทใงใใ VQAv2 ใงใฏใใฌใผใใณใฐๅฏ่ฝใชใใฉใกใผใฟใผใ 54 ๅใฎ 1 ใซๆธๅฐใใพใใ\n \n ่ซๆใฎ่ฆ็ดใฏๆฌกใฎใจใใใงใใ\n@@ -30,7 +30,7 @@ Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi.ใปใตใใฌใผใผใในใใฃ\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/blip2_architecture.jpg\"\n alt=\"drawing\" width=\"600\"/> \n \n-<small> BLIP-2 ใขใผใญใใฏใใฃใ <a href=\"https://arxiv.org/abs/2301.12597\">ๅใฎ่ซๆใใๆ็ฒใ</a> </small>\n+<small> BLIP-2 ใขใผใญใใฏใใฃใ <a href=\"https://huggingface.co/papers/2301.12597\">ๅใฎ่ซๆใใๆ็ฒใ</a> </small>\n \n ใใฎใขใใซใฏใ[nielsr](https://huggingface.co/nielsr) ใซใใฃใฆๆไพใใใพใใใ\n ๅใฎใณใผใใฏ [ใใ](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207) ใซใใใพใใ"
        },
        {
            "sha": "e93c740883ab90d778629ac8b886f5cd1b1e4c03",
            "filename": "docs/source/ja/model_doc/blip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fblip.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BLIP ใขใใซใฏใ[BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086) ใง Junnan LiใDongxu LiใCaiming XiongใSteven Hoi ใซใใฃใฆๆๆกใใใพใใใ ใ\n+BLIP ใขใใซใฏใ[BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://huggingface.co/papers/2201.12086) ใง Junnan LiใDongxu LiใCaiming XiongใSteven Hoi ใซใใฃใฆๆๆกใใใพใใใ ใ\n \n BLIP ใฏใๆฌกใฎใใใชใใพใใพใชใใซใใขใผใใซ ใฟในใฏใๅฎ่กใงใใใขใใซใงใใ\n - ่ฆ่ฆ็ใช่ณชๅๅฟ็ญ"
        },
        {
            "sha": "185187219e74f6e036e50c20297b37750230b74d",
            "filename": "docs/source/ja/model_doc/bort.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbort.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbort.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbort.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -27,7 +27,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BORT ใขใใซใฏใ[Optimal Subarchitecture Extraction for BERT](https://arxiv.org/abs/2010.10499) ใงๆๆกใใใพใใใ\n+BORT ใขใใซใฏใ[Optimal Subarchitecture Extraction for BERT](https://huggingface.co/papers/2010.10499) ใงๆๆกใใใพใใใ\n Adrian de Wynter and Daniel J. Perry.ใใใฏใBERT ใฎใขใผใญใใฏใใฃ ใใฉใกใผใฟใฎๆ้ฉใชใตใใปใใใงใใ\n ่่ใฏใใใซใใใจๅผใใงใใพใใ\n "
        },
        {
            "sha": "116d87caa5f400c6fc19dee9c1cd43577b1b7096",
            "filename": "docs/source/ja/model_doc/bridgetower.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbridgetower.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbridgetower.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbridgetower.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-BridgeTower ใขใใซใฏใXiao XuใChenfei WuใShachar RosenmanใVasudev LalใWanxiang CheใNan Duan [BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning](https://arxiv.org/abs/2206.08657) ใงๆๆกใใใพใใใใใฅใขใณใใใฎใขใใซใฎ็ฎๆจใฏใ\n+BridgeTower ใขใใซใฏใXiao XuใChenfei WuใShachar RosenmanใVasudev LalใWanxiang CheใNan Duan [BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning](https://huggingface.co/papers/2206.08657) ใงๆๆกใใใพใใใใใฅใขใณใใใฎใขใใซใฎ็ฎๆจใฏใ\n ๅใฆใใขใผใใซ ใจใณใณใผใใจใฏใญในใขใผใใซ ใจใณใณใผใใฎ้ใฎใใชใใธใซใใใใฏใญในใขใผใใซ ใจใณใณใผใใฎๅๅฑคใงใฎๅๆฌ็ใใค่ฉณ็ดฐใชๅฏพ่ฉฑใๅฏ่ฝใซใชใใ่ฟฝๅใฎใใใฉใผใใณในใจ่จ็ฎใณในใใใปใจใใฉ็ก่ฆใงใใ็จๅบฆใงใใใพใใพใชไธๆตใฟในใฏใงๅชใใใใใฉใผใใณในใๅฎ็พใใพใใ\n \n ใใฎ่ซๆใฏ [AAAI'23](https://aaai.org/Conferences/AAAI-23/) ไผ่ญฐใซๆกๆใใใพใใใ\n@@ -35,7 +35,7 @@ BridgeTower ใขใใซใฏใXiao XuใChenfei WuใShachar RosenmanใVasudev Lal\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/bridgetower_architecture%20.jpg\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> ใใชใใธใฟใฏใผ ใขใผใญใใฏใใฃใ <a href=\"https://arxiv.org/abs/2206.08657\">ๅใฎ่ซๆใใๆ็ฒใ</a> </small>\n+<small> ใใชใใธใฟใฏใผ ใขใผใญใใฏใใฃใ <a href=\"https://huggingface.co/papers/2206.08657\">ๅใฎ่ซๆใใๆ็ฒใ</a> </small>\n \n ใใฎใขใใซใฏใ[Anahita Bhiwandiwalla](https://huggingface.co/anahita-b)ใ[Tiep Le](https://huggingface.co/Tile)ใ[Shaoyen Tseng](https://huggingface.co/shaoyent) ใๅใฎใณใผใใฏ [ใใ](https://github.com/microsoft/BridgeTower) ใซใใใพใใ\n \n@@ -124,7 +124,7 @@ BridgeTower ใฏใใใธใฅใขใซ ใจใณใณใผใใผใใใญในใ ใจใณใณใผ\n \n - BridgeTower ใฎใใฎๅฎ่ฃใงใฏใ[`RobertaTokenizer`] ใไฝฟ็จใใฆใใญในใๅใ่พผใฟใ็ๆใใOpenAI ใฎ CLIP/ViT ใขใใซใไฝฟ็จใใฆ่ฆ่ฆ็ๅใ่พผใฟใ่จ็ฎใใพใใ\n - ไบๅใใฌใผใใณใฐใใใ [bridgeTower-base](https://huggingface.co/BridgeTower/bridgetower-base) ใใใณ [bridgetower ใในใฏใใใ่จ่ชใขใใชใณใฐใจ็ปๅใใญในใ ใใใใณใฐ](https://huggingface.co/BridgeTower/bridgetower--base-itm-mlm) ใฎใใงใใฏใใคใณใ ใใชใชใผในใใใพใใใ\n-- ็ปๅๆค็ดขใใใณใใฎไปใฎไธๆตใฟในใฏใซใใใ BridgeTower ใฎใใใฉใผใใณในใซใคใใฆใฏใ[่กจ 5](https://arxiv.org/pdf/2206.08657.pdf) ใๅ็งใใฆใใใใใ\n+- ็ปๅๆค็ดขใใใณใใฎไปใฎไธๆตใฟในใฏใซใใใ BridgeTower ใฎใใใฉใผใใณในใซใคใใฆใฏใ[่กจ 5](https://huggingface.co/papers/2206.08657) ใๅ็งใใฆใใใใใ\n - ใใฎใขใใซใฎ PyTorch ใใผใธใงใณใฏใtorch 1.10 ไปฅ้ใงใฎใฟไฝฟ็จใงใใพใใ\n \n ## BridgeTowerConfig"
        },
        {
            "sha": "def3539585209df564fb05ad7e19407d20ab86e0",
            "filename": "docs/source/ja/model_doc/bros.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbros.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbros.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbros.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -14,7 +14,7 @@ specific language governing permissions and limitations under the License.\n \n ## Overview\n \n-BROS ใขใใซใฏใTeakgyu HonใDonghyun KimใMingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park ใซใใฃใฆ [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://arxiv.org/abs/2108.04539) ใงๆๆกใใใพใใใ \n+BROS ใขใใซใฏใTeakgyu HonใDonghyun KimใMingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park ใซใใฃใฆ [BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents](https://huggingface.co/papers/2108.04539) ใงๆๆกใใใพใใใ \n \n BROS ใฏ *BERT Relying On Spatality* ใฎ็ฅใงใใใใใฏใไธ้ฃใฎใใผใฏใณใจใใฎๅข็ใใใฏในใๅฅๅใจใใฆๅใๅใใไธ้ฃใฎ้ใ็ถๆใๅบๅใใใจใณใณใผใใผๅฐ็จใฎ Transformer ใขใใซใงใใ BROS ใฏใ็ตถๅฏพ็ใช็ฉบ้ๆๅฑใไฝฟ็จใใไปฃใใใซใ็ธๅฏพ็ใช็ฉบ้ๆๅฑใใจใณใณใผใใใพใใ\n "
        },
        {
            "sha": "83f7f0b4ac57c79717fd2fc96c58514b7f2895b7",
            "filename": "docs/source/ja/model_doc/byt5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbyt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fbyt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbyt5.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-ByT5 ใขใใซใฏใ[ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\n+ByT5 ใขใใซใฏใ[ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://huggingface.co/papers/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\n Kale, Adam Roberts, Colin Raffel.\n \n ่ซๆใฎ่ฆ็ดใฏๆฌกใฎใจใใใงใใ"
        },
        {
            "sha": "382077613dd1637fe99d7017061f172b3b1704a6",
            "filename": "docs/source/ja/model_doc/camembert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/de24fb63ed3f151e1c962ffffbcc217982addd82/docs%2Fsource%2Fja%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fcamembert.md?ref=de24fb63ed3f151e1c962ffffbcc217982addd82",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-CamemBERT ใขใใซใฏใ[CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894) ใงๆๆกใใใพใใใ\n+CamemBERT ใขใใซใฏใ[CamemBERT: a Tasty French Language Model](https://huggingface.co/papers/1911.03894) ใงๆๆกใใใพใใใ\n Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suรกrez, Yoann Dupont, Laurent Romary, รric Villemonte de la\n Clergerie, Djamรฉ Seddah, and Benoรฎt Sagot. 2019ๅนดใซใชใชใผในใใใFacebookใฎRoBERTaใขใใซใใใผในใซใใใขใใซใงใใ\n 138GBใฎใใฉใณใน่ชใใญในใใงใใฌใผใใณใฐใใใพใใใ"
        }
    ],
    "stats": {
        "total": 5239,
        "additions": 2622,
        "deletions": 2617
    }
}