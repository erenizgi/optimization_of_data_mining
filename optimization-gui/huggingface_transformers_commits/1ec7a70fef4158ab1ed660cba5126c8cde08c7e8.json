{
    "author": "Wang-Xiaodong1899",
    "message": "fix trainer tr_loss add error (#33651)",
    "sha": "1ec7a70fef4158ab1ed660cba5126c8cde08c7e8",
    "files": [
        {
            "sha": "8367e2e413c13e78d8fb3afd1283e74d3e61b240",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ec7a70fef4158ab1ed660cba5126c8cde08c7e8/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ec7a70fef4158ab1ed660cba5126c8cde08c7e8/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1ec7a70fef4158ab1ed660cba5126c8cde08c7e8",
            "patch": "@@ -2393,13 +2393,13 @@ def _inner_training_loop(\n                     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n                 ):\n                     # if loss is nan or inf simply add the average of previous logged losses\n-                    tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n+                    tr_loss = tr_loss + tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n                 else:\n                     if tr_loss.device != tr_loss_step.device:\n                         raise ValueError(\n                             f\"Calculated loss must be on the original device: {tr_loss.device} but device in use is {tr_loss_step.device}\"\n                         )\n-                    tr_loss += tr_loss_step\n+                    tr_loss = tr_loss + tr_loss_step\n \n                 self.current_flos += float(self.floating_point_ops(inputs))\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}