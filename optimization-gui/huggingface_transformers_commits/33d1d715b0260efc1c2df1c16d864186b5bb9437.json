{
    "author": "Isotr0py",
    "message": "Add ImageProcessorFast to Qwen2.5-VL processor (#36164)\n\n* add qwen2 fast image processor to modular file\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* fix modular\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* fix circle import\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* add docs\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* fix typo\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* add modular generated files\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* revert qwen2vl fast image processor\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* remove qwen2.5-vl image processor from modular\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* re-generate qwen2.5-vl files\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* remove unnecessary test\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* fix auto map\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* cleanup\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* fix model_input_names\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* remove import\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n* make fix-copies\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>\r\n\r\n---------\r\n\r\nSigned-off-by: isotr0py <2037008807@qq.com>",
    "sha": "33d1d715b0260efc1c2df1c16d864186b5bb9437",
    "files": [
        {
            "sha": "f08343506b6ade8da93ebf204b3c8955318ab39c",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/33d1d715b0260efc1c2df1c16d864186b5bb9437/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/33d1d715b0260efc1c2df1c16d864186b5bb9437/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=33d1d715b0260efc1c2df1c16d864186b5bb9437",
            "patch": "@@ -264,11 +264,6 @@ model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n \n [[autodoc]] Qwen2_5_VLConfig\n \n-## Qwen2_5_VLImageProcessor\n-\n-[[autodoc]] Qwen2_5_VLImageProcessor\n-    - preprocess\n-\n ## Qwen2_5_VLProcessor\n \n [[autodoc]] Qwen2_5_VLProcessor"
        },
        {
            "sha": "e9c752b854662320cb9791339a6cbf95e05cc2f1",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=33d1d715b0260efc1c2df1c16d864186b5bb9437",
            "patch": "@@ -1281,7 +1281,6 @@\n     _import_structure[\"models.pixtral\"].append(\"PixtralImageProcessor\")\n     _import_structure[\"models.poolformer\"].extend([\"PoolFormerFeatureExtractor\", \"PoolFormerImageProcessor\"])\n     _import_structure[\"models.pvt\"].extend([\"PvtImageProcessor\"])\n-    _import_structure[\"models.qwen2_5_vl\"].extend([\"Qwen2_5_VLImageProcessor\"])\n     _import_structure[\"models.qwen2_vl\"].extend([\"Qwen2VLImageProcessor\"])\n     _import_structure[\"models.rt_detr\"].extend([\"RTDetrImageProcessor\"])\n     _import_structure[\"models.sam\"].extend([\"SamImageProcessor\"])\n@@ -6444,7 +6443,6 @@\n             PoolFormerImageProcessor,\n         )\n         from .models.pvt import PvtImageProcessor\n-        from .models.qwen2_5_vl import Qwen2_5_VLImageProcessor\n         from .models.qwen2_vl import Qwen2VLImageProcessor\n         from .models.rt_detr import RTDetrImageProcessor\n         from .models.sam import SamImageProcessor"
        },
        {
            "sha": "ef4d9b25d1d2c2513a64d960e75ba1852beb6c6c",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=33d1d715b0260efc1c2df1c16d864186b5bb9437",
            "patch": "@@ -127,6 +127,7 @@\n             (\"poolformer\", (\"PoolFormerImageProcessor\",)),\n             (\"pvt\", (\"PvtImageProcessor\",)),\n             (\"pvt_v2\", (\"PvtImageProcessor\",)),\n+            (\"qwen2_5_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"qwen2_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"regnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"resnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),"
        },
        {
            "sha": "7a9f44a7a05fc65136552948be9acef53e5ea106",
            "filename": "src/transformers/models/qwen2_5_vl/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2F__init__.py?ref=33d1d715b0260efc1c2df1c16d864186b5bb9437",
            "patch": "@@ -19,7 +19,6 @@\n \n if TYPE_CHECKING:\n     from .configuration_qwen2_5_vl import *\n-    from .image_processing_qwen2_5_vl import *\n     from .modeling_qwen2_5_vl import *\n     from .processing_qwen2_5_vl import *\n else:"
        },
        {
            "sha": "17afed7d6d391f81f38108658d8c2d64a174ea0d",
            "filename": "src/transformers/models/qwen2_5_vl/image_processing_qwen2_5_vl.py",
            "status": "removed",
            "additions": 0,
            "deletions": 426,
            "changes": 426,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fimage_processing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fimage_processing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fimage_processing_qwen2_5_vl.py?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -1,426 +0,0 @@\n-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#           This file was automatically generated from src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py.\n-#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#             the file from the modular. If any change should be done, please apply the change to the\n-#                          modular_qwen2_5_vl.py file directly. One of our CI enforces this.\n-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-# coding=utf-8\n-# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n-#\n-# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n-# and OPT implementations in this library. It has been modified from its\n-# original forms to accommodate minor architectural differences compared\n-# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import math\n-from typing import Dict, List, Optional, Union\n-\n-import numpy as np\n-\n-from ...feature_extraction_utils import BatchFeature\n-from ...image_processing_utils import BaseImageProcessor\n-from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    VideoInput,\n-    get_image_size,\n-    infer_channel_dimension_format,\n-    is_scaled_image,\n-    make_batched_videos,\n-    make_flat_list_of_images,\n-    make_list_of_images,\n-    to_numpy_array,\n-    valid_images,\n-    validate_preprocess_arguments,\n-)\n-from ...utils import TensorType, logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-def smart_resize(\n-    height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n-):\n-    \"\"\"Rescales the image so that the following conditions are met:\n-\n-    1. Both dimensions (height and width) are divisible by 'factor'.\n-\n-    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n-\n-    3. The aspect ratio of the image is maintained as closely as possible.\n-\n-    \"\"\"\n-    if height < factor or width < factor:\n-        raise ValueError(f\"height:{height} or width:{width} must be larger than factor:{factor}\")\n-    elif max(height, width) / min(height, width) > 200:\n-        raise ValueError(\n-            f\"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}\"\n-        )\n-    h_bar = round(height / factor) * factor\n-    w_bar = round(width / factor) * factor\n-    if h_bar * w_bar > max_pixels:\n-        beta = math.sqrt((height * width) / max_pixels)\n-        h_bar = math.floor(height / beta / factor) * factor\n-        w_bar = math.floor(width / beta / factor) * factor\n-    elif h_bar * w_bar < min_pixels:\n-        beta = math.sqrt(min_pixels / (height * width))\n-        h_bar = math.ceil(height * beta / factor) * factor\n-        w_bar = math.ceil(width * beta / factor) * factor\n-    return h_bar, w_bar\n-\n-\n-class Qwen2_5_VLImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a Qwen2.5-VL image processor that dynamically resizes images based on the original images.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use when resizing the image.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n-            Mean to use if normalizing the image. This is a float or list of floats for each channel in the image.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats for each channel in the image.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-        min_pixels (`int`, *optional*, defaults to `56 * 56`):\n-            The min pixels of the image to resize the image.\n-        max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n-            The max pixels of the image to resize the image.\n-        patch_size (`int`, *optional*, defaults to 14):\n-            The spacial patch size of the vision encoder.\n-        temporal_patch_size (`int`, *optional*, defaults to 2):\n-            The temporal patch size of the vision encoder.\n-        merge_size (`int`, *optional*, defaults to 2):\n-            The merge size of the vision encoder to llm encoder.\n-    \"\"\"\n-\n-    model_input_names = [\n-        \"pixel_values\",\n-        \"image_grid_thw\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"second_per_grid_ts\",\n-    ]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = True,\n-        min_pixels: int = 56 * 56,\n-        max_pixels: int = 28 * 28 * 1280,\n-        patch_size: int = 14,\n-        temporal_patch_size: int = 2,\n-        merge_size: int = 2,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        self.do_resize = do_resize\n-        self.resample = resample\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n-        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.min_pixels = min_pixels\n-        self.max_pixels = max_pixels\n-        self.patch_size = patch_size\n-        self.temporal_patch_size = temporal_patch_size\n-        self.merge_size = merge_size\n-        self.size = {\"shortest_edge\": min_pixels, \"longest_edge\": max_pixels}\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    def _preprocess(\n-        self,\n-        images: Union[ImageInput, VideoInput],\n-        do_resize: bool = None,\n-        resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ):\n-        \"\"\"\n-        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n-            vision_info (`List[Dict]`, *optional*):\n-                Optional list of dictionaries containing additional information about vision inputs.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` enums.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Scale factor to use if rescaling the image.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n-                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n-                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the image to RGB.\n-            data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        images = make_list_of_images(images)\n-\n-        if do_convert_rgb:\n-            images = [convert_to_rgb(image) for image in images]\n-\n-        # All transformations expect numpy arrays.\n-        images = [to_numpy_array(image) for image in images]\n-\n-        if do_rescale and is_scaled_image(images[0]):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled images. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images[0])\n-\n-        height, width = get_image_size(images[0], channel_dim=input_data_format)\n-        resized_height, resized_width = height, width\n-        processed_images = []\n-        for image in images:\n-            if do_resize:\n-                resized_height, resized_width = smart_resize(\n-                    height,\n-                    width,\n-                    factor=self.patch_size * self.merge_size,\n-                    min_pixels=self.min_pixels,\n-                    max_pixels=self.max_pixels,\n-                )\n-                image = resize(\n-                    image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n-                )\n-\n-            if do_rescale:\n-                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-            if do_normalize:\n-                image = self.normalize(\n-                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n-                )\n-\n-            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-            processed_images.append(image)\n-\n-        patches = np.array(processed_images)\n-        if data_format == ChannelDimension.LAST:\n-            patches = patches.transpose(0, 3, 1, 2)\n-        if patches.shape[0] % self.temporal_patch_size != 0:\n-            repeats = np.repeat(patches[-1][np.newaxis], self.temporal_patch_size - 1, axis=0)\n-            patches = np.concatenate([patches, repeats], axis=0)\n-        channel = patches.shape[1]\n-        grid_t = patches.shape[0] // self.temporal_patch_size\n-        grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n-        patches = patches.reshape(\n-            grid_t,\n-            self.temporal_patch_size,\n-            channel,\n-            grid_h // self.merge_size,\n-            self.merge_size,\n-            self.patch_size,\n-            grid_w // self.merge_size,\n-            self.merge_size,\n-            self.patch_size,\n-        )\n-        patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n-        flatten_patches = patches.reshape(\n-            grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size\n-        )\n-\n-        return flatten_patches, (grid_t, grid_h, grid_w)\n-\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        videos: VideoInput = None,\n-        do_resize: bool = None,\n-        size: Dict[str, int] = None,\n-        resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n-        image_mean: Optional[Union[float, List[float]]] = None,\n-        image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ):\n-        \"\"\"\n-        Args:\n-            images (`ImageInput`):\n-                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`):\n-                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the image to RGB.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                - Unset: Return a list of `np.ndarray`.\n-                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n-                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        size = size if size is not None else self.size\n-        resample = resample if resample is not None else self.resample\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        if images is not None:\n-            images = make_flat_list_of_images(images)\n-        if videos is not None:\n-            videos = make_batched_videos(videos)\n-\n-        if images is not None and not valid_images(images):\n-            raise ValueError(\n-                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n-                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n-            )\n-\n-        validate_preprocess_arguments(\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        if images is not None:\n-            pixel_values, vision_grid_thws = [], []\n-            for image in images:\n-                patches, image_grid_thw = self._preprocess(\n-                    image,\n-                    do_resize=do_resize,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values.extend(patches)\n-                vision_grid_thws.append(image_grid_thw)\n-            pixel_values = np.array(pixel_values)\n-            vision_grid_thws = np.array(vision_grid_thws)\n-            data = {\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws}\n-\n-        if videos is not None:\n-            pixel_values, vision_grid_thws = [], []\n-            for images in videos:\n-                patches, video_grid_thw = self._preprocess(\n-                    images,\n-                    do_resize=do_resize,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values.extend(patches)\n-                vision_grid_thws.append(video_grid_thw)\n-            pixel_values = np.array(pixel_values)\n-            vision_grid_thws = np.array(vision_grid_thws)\n-            data = {\"pixel_values_videos\": pixel_values, \"video_grid_thw\": vision_grid_thws}\n-\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n-\n-\n-__all__ = [\"Qwen2_5_VLImageProcessor\"]"
        },
        {
            "sha": "0740de2e217ed1420dde5a08bf9a354d43b61ff4",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 48,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=33d1d715b0260efc1c2df1c16d864186b5bb9437",
            "patch": "@@ -29,7 +29,6 @@\n from torch.nn import CrossEntropyLoss\n \n from transformers.models.qwen2_vl.configuration_qwen2_vl import Qwen2VLConfig\n-from transformers.models.qwen2_vl.image_processing_qwen2_vl import Qwen2VLImageProcessor\n from transformers.models.qwen2_vl.modeling_qwen2_vl import (\n     PatchEmbed,\n     PatchMerger,\n@@ -854,48 +853,6 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n \n-class Qwen2_5_VLImageProcessor(Qwen2VLImageProcessor):\n-    r\"\"\"\n-    Constructs a Qwen2.5-VL image processor that dynamically resizes images based on the original images.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use when resizing the image.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n-            Mean to use if normalizing the image. This is a float or list of floats for each channel in the image.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats for each channel in the image.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-        min_pixels (`int`, *optional*, defaults to `56 * 56`):\n-            The min pixels of the image to resize the image.\n-        max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n-            The max pixels of the image to resize the image.\n-        patch_size (`int`, *optional*, defaults to 14):\n-            The spacial patch size of the vision encoder.\n-        temporal_patch_size (`int`, *optional*, defaults to 2):\n-            The temporal patch size of the vision encoder.\n-        merge_size (`int`, *optional*, defaults to 2):\n-            The merge size of the vision encoder to llm encoder.\n-    \"\"\"\n-\n-    model_input_names = [\n-        \"pixel_values\",\n-        \"image_grid_thw\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"second_per_grid_ts\",\n-    ]\n-\n-\n class Qwen2_5_VLVideosProcessorKwargs(VideosKwargs, total=False):\n     fps: Union[List[float], float]\n \n@@ -913,18 +870,25 @@ class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n class Qwen2_5_VLProcessor(Qwen2VLProcessor):\n     r\"\"\"\n     Constructs a Qwen2.5-VL processor which wraps a Qwen2.5-VL image processor and a Qwen2 tokenizer into a single processor.\n-    [`Qwen2_5_VLProcessor`] offers all the functionalities of [`Qwen2_5_VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n+    [`Qwen2_5_VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n     [`~Qwen2_5_VLProcessor.__call__`] and [`~Qwen2_5_VLProcessor.decode`] for more information.\n     Args:\n-        image_processor ([`Qwen2_5_VLImageProcessor`], *optional*):\n+        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n             The image processor is a required input.\n         tokenizer ([`Qwen2TokenizerFast`], *optional*):\n             The tokenizer is a required input.\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n     \"\"\"\n \n-    image_processor_class = \"Qwen2_5_VLImageProcessor\"\n+    image_processor_class = \"AutoImageProcessor\"\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        return names_from_processor + [\"second_per_grid_ts\"]\n \n     def __call__(\n         self,\n@@ -937,7 +901,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n-        Qwen2_5_VLImageProcessor's [`~Qwen2_5_VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n+        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n \n         Args:\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n@@ -1040,6 +1004,5 @@ def __call__(\n     \"Qwen2_5_VLForConditionalGeneration\",\n     \"Qwen2_5_VLModel\",\n     \"Qwen2_5_VLPreTrainedModel\",\n-    \"Qwen2_5_VLImageProcessor\",\n     \"Qwen2_5_VLProcessor\",\n ]"
        },
        {
            "sha": "a11010f6c91c3a0a5777729365d3b6fab7eae90f",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=33d1d715b0260efc1c2df1c16d864186b5bb9437",
            "patch": "@@ -48,10 +48,10 @@ class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n class Qwen2_5_VLProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a Qwen2.5-VL processor which wraps a Qwen2.5-VL image processor and a Qwen2 tokenizer into a single processor.\n-    [`Qwen2_5_VLProcessor`] offers all the functionalities of [`Qwen2_5_VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n+    [`Qwen2_5_VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n     [`~Qwen2_5_VLProcessor.__call__`] and [`~Qwen2_5_VLProcessor.decode`] for more information.\n     Args:\n-        image_processor ([`Qwen2_5_VLImageProcessor`], *optional*):\n+        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n             The image processor is a required input.\n         tokenizer ([`Qwen2TokenizerFast`], *optional*):\n             The tokenizer is a required input.\n@@ -62,7 +62,7 @@ class Qwen2_5_VLProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     valid_kwargs = [\"chat_template\"]\n \n-    image_processor_class = \"Qwen2_5_VLImageProcessor\"\n+    image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n \n     def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n@@ -81,7 +81,7 @@ def __call__(\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n-        Qwen2_5_VLImageProcessor's [`~Qwen2_5_VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n+        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n \n         Args:\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n@@ -212,7 +212,8 @@ def post_process_image_text_to_text(self, generated_outputs):\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        return names_from_processor + [\"second_per_grid_ts\"]\n \n \n __all__ = [\"Qwen2_5_VLProcessor\"]"
        },
        {
            "sha": "64a69ef11717bbeff49cca34e6ab254114cc1d35",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33d1d715b0260efc1c2df1c16d864186b5bb9437/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=33d1d715b0260efc1c2df1c16d864186b5bb9437",
            "patch": "@@ -590,13 +590,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n-class Qwen2_5_VLImageProcessor(metaclass=DummyObject):\n-    _backends = [\"vision\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-\n-\n class Qwen2VLImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "4c991ec710ef7c1274c15875e63f93e4c36c4e6a",
            "filename": "tests/models/qwen2_5_vl/test_image_processing_qwen2_5_vl.py",
            "status": "removed",
            "additions": 0,
            "deletions": 252,
            "changes": 252,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_image_processing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_image_processing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_image_processing_qwen2_5_vl.py?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -1,252 +0,0 @@\n-# coding=utf-8\n-# Copyright 2025 The Qwen Team and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n-from transformers.models.qwen2_5_vl.image_processing_qwen2_5_vl import smart_resize\n-from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n-\n-from ...test_image_processing_common import (\n-    ImageProcessingTestMixin,\n-    prepare_image_inputs,\n-)\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import Qwen2_5_VLImageProcessor\n-\n-\n-class Qwen2_5_VLImageProcessingTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=7,\n-        num_channels=3,\n-        min_resolution=56,\n-        max_resolution=1024,\n-        min_pixels=56 * 56,\n-        max_pixels=28 * 28 * 1280,\n-        do_normalize=True,\n-        image_mean=OPENAI_CLIP_MEAN,\n-        image_std=OPENAI_CLIP_STD,\n-        do_resize=True,\n-        patch_size=14,\n-        temporal_patch_size=2,\n-        merge_size=2,\n-        do_convert_rgb=True,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.min_resolution = min_resolution\n-        self.max_resolution = max_resolution\n-        self.num_channels = num_channels\n-        self.image_mean = OPENAI_CLIP_MEAN\n-        self.image_std = OPENAI_CLIP_STD\n-        self.min_pixels = min_pixels\n-        self.max_pixels = max_pixels\n-        self.patch_size = patch_size\n-        self.temporal_patch_size = temporal_patch_size\n-        self.merge_size = merge_size\n-        self.do_resize = do_resize\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean\n-        self.image_std = image_std\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    def prepare_image_processor_dict(self):\n-        return {\n-            \"do_resize\": self.do_resize,\n-            \"image_mean\": self.image_mean,\n-            \"image_std\": self.image_std,\n-            \"min_pixels\": self.min_pixels,\n-            \"max_pixels\": self.max_pixels,\n-            \"patch_size\": self.patch_size,\n-            \"temporal_patch_size\": self.temporal_patch_size,\n-            \"merge_size\": self.merge_size,\n-        }\n-\n-    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n-        images = prepare_image_inputs(\n-            batch_size=self.batch_size,\n-            num_channels=self.num_channels,\n-            min_resolution=self.min_resolution,\n-            max_resolution=self.max_resolution,\n-            equal_resolution=equal_resolution,\n-            numpify=numpify,\n-            torchify=torchify,\n-        )\n-        return [[image] for image in images]\n-\n-\n-@require_torch\n-@require_vision\n-class Qwen2_5_VLImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n-    image_processing_class = Qwen2_5_VLImageProcessor if is_vision_available() else None\n-\n-    def setUp(self):\n-        super().setUp()\n-        self.image_processor_tester = Qwen2_5_VLImageProcessingTester(self)\n-\n-    @property\n-    def image_processor_dict(self):\n-        return self.image_processor_tester.prepare_image_processor_dict()\n-\n-    def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"min_pixels\"))\n-        self.assertTrue(hasattr(image_processing, \"max_pixels\"))\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n-        self.assertTrue(hasattr(image_processing, \"patch_size\"))\n-        self.assertTrue(hasattr(image_processing, \"temporal_patch_size\"))\n-        self.assertTrue(hasattr(image_processing, \"merge_size\"))\n-\n-    def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.min_pixels, 56 * 56)\n-        self.assertEqual(image_processor.max_pixels, 28 * 28 * 1280)\n-\n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, min_pixels=256 * 256, max_pixels=640 * 640\n-        )\n-        self.assertEqual(image_processor.min_pixels, 256 * 256)\n-        self.assertEqual(image_processor.max_pixels, 640 * 640)\n-\n-    def test_select_best_resolution(self):\n-        # Test with a final resize resolution\n-        best_resolution = smart_resize(561, 278, factor=28)\n-        self.assertEqual(best_resolution, (560, 280))\n-\n-    def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image[0], Image.Image)\n-\n-        # Test not batched input\n-        prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (4900, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Test batched\n-        prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-    def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image[0], np.ndarray)\n-\n-        # Test not batched input\n-        prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (4900, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Test batched\n-        prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-    def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n-\n-        for image in image_inputs:\n-            self.assertIsInstance(image[0], torch.Tensor)\n-\n-        # Test not batched input\n-        prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (4900, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Test batched\n-        prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-    @unittest.skip(reason=\"Qwen2_5_VLImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\")\n-    def test_call_numpy_4_channels(self):\n-        pass\n-\n-    def test_nested_input(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n-\n-        # Test batched as a list of images\n-        prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Test batched as a nested list of images, where each sublist is one batch\n-        image_inputs_nested = image_inputs[:3] + image_inputs[3:]\n-        prcocess_out = image_processing(image_inputs_nested, return_tensors=\"pt\")\n-        encoded_images_nested = prcocess_out.pixel_values\n-        image_grid_thws_nested = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Image processor should return same pixel values, independently of ipnut format\n-        self.assertTrue((encoded_images_nested == encoded_images).all())\n-        self.assertTrue((image_grid_thws_nested == expected_image_grid_thws).all())"
        },
        {
            "sha": "481e206a71869279af20d6b4b3ddb964f2ba2ca4",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/33d1d715b0260efc1c2df1c16d864186b5bb9437/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33d1d715b0260efc1c2df1c16d864186b5bb9437/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=33d1d715b0260efc1c2df1c16d864186b5bb9437",
            "patch": "@@ -27,7 +27,7 @@\n \n \n if is_vision_available():\n-    from transformers import Qwen2_5_VLImageProcessor, Qwen2_5_VLProcessor\n+    from transformers import Qwen2_5_VLProcessor, Qwen2VLImageProcessor\n \n \n @require_vision\n@@ -63,7 +63,7 @@ def test_save_load_pretrained_default(self):\n         self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor.to_json_string())\n         self.assertIsInstance(processor.tokenizer, Qwen2Tokenizer)\n-        self.assertIsInstance(processor.image_processor, Qwen2_5_VLImageProcessor)\n+        self.assertIsInstance(processor.image_processor, Qwen2VLImageProcessor)\n \n     def test_image_processor(self):\n         image_processor = self.get_image_processor()"
        }
    ],
    "stats": {
        "total": 768,
        "additions": 20,
        "deletions": 748
    }
}