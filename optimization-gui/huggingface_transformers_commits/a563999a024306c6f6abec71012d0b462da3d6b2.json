{
    "author": "zucchini-nlp",
    "message": "[processor] clean up mulitmodal tests (#37362)\n\n* clkea up mulitmodal processor tests\n\n* fixup\n\n* fix tests\n\n* fix one last test\n\n* forgot",
    "sha": "a563999a024306c6f6abec71012d0b462da3d6b2",
    "files": [
        {
            "sha": "364f8f70df1f01d1200e0455083ea128d1befa50",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 37,
            "deletions": 2,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -31,12 +31,16 @@\n     PILImageResampling,\n     get_image_size,\n     infer_channel_dimension_format,\n+    is_scaled_image,\n     make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType\n+from ...utils import TensorType, logging\n+\n+\n+logger = logging.get_logger(__name__)\n \n \n def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> List[np.array]:\n@@ -104,6 +108,12 @@ class AriaImageProcessor(BaseImageProcessor):\n             Whether to split the image.\n         do_convert_rgb (`bool`, *optional*, defaults to `True`):\n             Whether to convert the image to RGB.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image.\n         resample (PILImageResampling, *optional*, defaults to `BICUBIC`):\n@@ -121,6 +131,8 @@ def __init__(\n         split_resolutions: Optional[List[Tuple[int, int]]] = None,\n         split_image: Optional[bool] = False,\n         do_convert_rgb: Optional[bool] = True,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: Optional[bool] = True,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         **kwargs,\n@@ -141,6 +153,8 @@ def __init__(\n             split_resolutions = [(el[0] * 490, el[1] * 490) for el in split_resolutions]\n         self.split_resolutions = split_resolutions\n         self.do_convert_rgb = do_convert_rgb\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n         self.do_normalize = do_normalize\n         self.resample = resample\n \n@@ -153,6 +167,8 @@ def preprocess(\n         min_image_size: Optional[int] = None,\n         split_image: Optional[bool] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n         resample: PILImageResampling = None,\n         return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n@@ -177,6 +193,10 @@ def preprocess(\n                 Whether to split the image.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb` (True)):\n                 Whether to convert the image to RGB.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize` (True)):\n                 Whether to normalize the image.\n             resample (PILImageResampling, *optional*, defaults to `self.resample` (BICUBIC)):\n@@ -217,6 +237,8 @@ def preprocess(\n         min_image_size = min_image_size if min_image_size is not None else self.min_image_size\n         split_image = split_image if split_image is not None else self.split_image\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n         resample = resample if resample is not None else self.resample\n \n@@ -236,6 +258,8 @@ def preprocess(\n             image_mean=image_mean,\n             image_std=image_std,\n             resample=resample,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n         )\n \n         if do_convert_rgb:\n@@ -244,6 +268,12 @@ def preprocess(\n         # All transformations expect numpy arrays.\n         images = [to_numpy_array(image) for image in images]\n \n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n         if input_data_format is None:\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n@@ -297,9 +327,14 @@ def preprocess(\n                 pixel_mask[: new_size[0], : new_size[1]] = 1\n                 pixel_masks.append(pixel_mask)\n \n+                if do_rescale:\n+                    crop_image_padded = self.rescale(\n+                        image=crop_image_padded, scale=rescale_factor, input_data_format=input_data_format\n+                    )\n+\n                 if do_normalize:\n                     crop_image_padded = self.normalize(\n-                        crop_image_padded / 255.0,\n+                        crop_image_padded,\n                         self.image_mean,\n                         self.image_std,\n                         data_format=input_data_format,"
        },
        {
            "sha": "fa0858cde3927173831b035c7836e0a0e2c7f3ca",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 33,
            "deletions": 1,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -28,6 +28,7 @@\n     PILImageResampling,\n     get_image_size,\n     infer_channel_dimension_format,\n+    is_scaled_image,\n     make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n@@ -495,6 +496,12 @@ class AriaImageProcessor(BaseImageProcessor):\n             Whether to split the image.\n         do_convert_rgb (`bool`, *optional*, defaults to `True`):\n             Whether to convert the image to RGB.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image.\n         resample (PILImageResampling, *optional*, defaults to `BICUBIC`):\n@@ -512,6 +519,8 @@ def __init__(\n         split_resolutions: Optional[List[Tuple[int, int]]] = None,\n         split_image: Optional[bool] = False,\n         do_convert_rgb: Optional[bool] = True,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: Optional[bool] = True,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         **kwargs,\n@@ -532,6 +541,8 @@ def __init__(\n             split_resolutions = [(el[0] * 490, el[1] * 490) for el in split_resolutions]\n         self.split_resolutions = split_resolutions\n         self.do_convert_rgb = do_convert_rgb\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n         self.do_normalize = do_normalize\n         self.resample = resample\n \n@@ -544,6 +555,8 @@ def preprocess(\n         min_image_size: Optional[int] = None,\n         split_image: Optional[bool] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n         resample: PILImageResampling = None,\n         return_tensors: Optional[Union[str, TensorType]] = \"pt\",\n@@ -568,6 +581,10 @@ def preprocess(\n                 Whether to split the image.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb` (True)):\n                 Whether to convert the image to RGB.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_normalize (`bool`, *optional*, defaults to `self.do_normalize` (True)):\n                 Whether to normalize the image.\n             resample (PILImageResampling, *optional*, defaults to `self.resample` (BICUBIC)):\n@@ -608,6 +625,8 @@ def preprocess(\n         min_image_size = min_image_size if min_image_size is not None else self.min_image_size\n         split_image = split_image if split_image is not None else self.split_image\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n         resample = resample if resample is not None else self.resample\n \n@@ -627,6 +646,8 @@ def preprocess(\n             image_mean=image_mean,\n             image_std=image_std,\n             resample=resample,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n         )\n \n         if do_convert_rgb:\n@@ -635,6 +656,12 @@ def preprocess(\n         # All transformations expect numpy arrays.\n         images = [to_numpy_array(image) for image in images]\n \n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n         if input_data_format is None:\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n@@ -688,9 +715,14 @@ def preprocess(\n                 pixel_mask[: new_size[0], : new_size[1]] = 1\n                 pixel_masks.append(pixel_mask)\n \n+                if do_rescale:\n+                    crop_image_padded = self.rescale(\n+                        image=crop_image_padded, scale=rescale_factor, input_data_format=input_data_format\n+                    )\n+\n                 if do_normalize:\n                     crop_image_padded = self.normalize(\n-                        crop_image_padded / 255.0,\n+                        crop_image_padded,\n                         self.image_mean,\n                         self.image_std,\n                         data_format=input_data_format,"
        },
        {
            "sha": "2e6a68ca7afe3d26bcba318707efcc07df71c0cc",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -118,8 +118,10 @@ def __init__(\n             tokens_to_add = {\"additional_special_tokens\": [image_token]}\n             tokenizer.add_special_tokens(tokens_to_add)\n             self.image_token_id = tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)\n+            self.image_token = IMAGE_TOKEN\n         else:\n             self.image_token_id = tokenizer.image_token_id\n+            self.image_token = tokenizer.image_token\n \n         tokenizer.add_tokens(EXTRA_TOKENS)\n         tokenizer.add_bos_token = False"
        },
        {
            "sha": "768ef893d2da1221ddb382f3e2003e9050131548",
            "filename": "src/transformers/models/idefics/image_processing_idefics.py",
            "status": "modified",
            "additions": 23,
            "deletions": 3,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fimage_processing_idefics.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -65,6 +65,12 @@ class IdeficsImageProcessor(BaseImageProcessor):\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n         image_num_channels (`int`, *optional*, defaults to 3):\n             Number of image channels.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n@@ -75,14 +81,18 @@ def __init__(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         image_num_channels: Optional[int] = 3,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n \n         self.image_size = image_size\n         self.image_num_channels = image_num_channels\n-        self.image_mean = image_mean\n-        self.image_std = image_std\n+        self.image_mean = image_mean if image_mean is not None else IDEFICS_STANDARD_MEAN\n+        self.image_std = image_std if image_std is not None else IDEFICS_STANDARD_STD\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n \n     def preprocess(\n         self,\n@@ -92,6 +102,8 @@ def preprocess(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         transform: Callable = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n         return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n         **kwargs,\n     ) -> TensorType:\n@@ -117,6 +129,12 @@ def preprocess(\n                 A custom transform function that accepts a single image can be passed for training. For example,\n                 `torchvision.Compose` can be used to compose multiple transforms. If `None` - an inference mode is\n                 assumed - and then a preset of inference-specific transforms will be applied to the images\n+            do_rescale (`bool`, *optional*, defaults to `True`):\n+                Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+                the `preprocess` method.\n+            rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+                Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+                method.\n \n         Returns:\n             a PyTorch tensor of the processed images\n@@ -126,6 +144,8 @@ def preprocess(\n         image_num_channels = image_num_channels if image_num_channels is not None else self.image_num_channels\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         size = (image_size, image_size)\n \n         if isinstance(images, list) and len(images) == 0:\n@@ -160,7 +180,7 @@ def preprocess(\n         # further transforms expect numpy arrays\n         images = [to_numpy_array(x) for x in images]\n         images = [resize(x, size, resample=PILImageResampling.BICUBIC) for x in images]\n-        images = [self.rescale(image=image, scale=1 / 255) for image in images]\n+        images = [self.rescale(image=image, scale=rescale_factor) for image in images]\n         images = [self.normalize(x, mean=image_mean, std=image_std) for x in images]\n         images = [to_channel_dimension_format(x, ChannelDimension.FIRST) for x in images]\n         images = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)[\"pixel_values\"]"
        },
        {
            "sha": "f389487c2b4af999731a8fa3b7b91f42d3500afc",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -141,8 +141,10 @@ def __init__(\n             tokens_to_add = {\"additional_special_tokens\": [image_token]}\n             tokenizer.add_special_tokens(tokens_to_add)\n             self.image_token_id = tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)\n+            self.image_token = IMAGE_TOKEN\n         else:\n             self.image_token_id = tokenizer.image_token_id\n+            self.image_token = tokenizer.image_token\n \n         tokenizer.add_tokens(EXTRA_TOKENS)\n         tokenizer.add_bos_token = False"
        },
        {
            "sha": "17e41055c7ca8da37a728a46e1a5097584b4b8b1",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -1086,7 +1086,6 @@ def from_pretrained(\n \n         args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n         processor_dict, kwargs = cls.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n-        processor_dict.update({k: v for k, v in kwargs.items() if k in processor_dict.keys()})\n         return cls.from_args_and_dict(args, processor_dict, **kwargs)\n \n     @classmethod"
        },
        {
            "sha": "08a6c5ba7806afe5b9660c284476bf84a1a63fec",
            "filename": "tests/models/aria/test_processor_aria.py",
            "status": "modified",
            "additions": 47,
            "deletions": 164,
            "changes": 211,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processor_aria.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -16,7 +16,6 @@\n import tempfile\n import unittest\n from io import BytesIO\n-from typing import Optional\n \n import numpy as np\n import requests\n@@ -41,7 +40,7 @@ class AriaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n-        processor = AriaProcessor.from_pretrained(\"m-ric/Aria_hf_2\", image_seq_len=2)\n+        processor = AriaProcessor.from_pretrained(\"m-ric/Aria_hf_2\", size_conversion={490: 2, 980: 2})\n         processor.save_pretrained(cls.tmpdirname)\n         cls.image1 = Image.open(\n             BytesIO(\n@@ -74,7 +73,14 @@ def setUpClass(cls):\n         cls.fake_image_token_id = processor.tokenizer.convert_tokens_to_ids(cls.fake_image_token)\n         cls.global_img_tokens_id = processor.tokenizer(cls.global_img_token, add_special_tokens=False)[\"input_ids\"]\n         cls.padding_token_id = processor.tokenizer.pad_token_id\n-        cls.image_seq_len = 256\n+        cls.image_seq_len = 2\n+\n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {\n+            \"chat_template\": \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}{% elif message['content'] is iterable %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<fim_prefix><|img|><fim_suffix>{% endif %}{% endfor %}{% endif %}<|im_end|>\\n{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\",\n+            \"size_conversion\": {490: 2, 980: 2},\n+        }  # fmt: skip\n \n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n@@ -89,24 +95,6 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        processor_components = self.prepare_components()\n-        processor_components[\"image_processor\"] = self.get_component(\n-            \"image_processor\", do_rescale=True, rescale_factor=1\n-        )\n-        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(**processor_components)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n-\n     def test_process_interleaved_images_prompts_image_splitting(self):\n         processor = self.get_processor()\n         processor.image_processor.split_image = True\n@@ -236,155 +224,50 @@ def test_apply_chat_template(self):\n \"\"\"\n         self.assertEqual(rendered, expected_rendered)\n \n-    # Override as AriaProcessor needs image tokens in prompts\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n-        if batch_size is None:\n-            return \"lower newer <|img|>\"\n+    def test_image_chat_template_accepts_processing_kwargs(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n \n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n \n-        if batch_size == 1:\n-            return [\"lower newer <|img|>\"]\n-        return [\"lower newer <|img|>\", \"<|img|> upper older longer string\"] + [\"<|img|> lower newer\"] * (\n-            batch_size - 2\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            max_length=50,\n         )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 50)\n \n-    # Override tests as inputs_ids padded dimension is the second one but not the last one\n-    @require_vision\n-    @require_torch\n-    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=30)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\", max_length=30)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 30)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            common_kwargs={\"return_tensors\": \"pt\"},\n-            images_kwargs={\"max_image_size\": 980},\n-            text_kwargs={\"padding\": \"max_length\", \"max_length\": 120, \"truncation\": \"longest_first\"},\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 980)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"max_image_size\": 980},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 120, \"truncation\": \"longest_first\"},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 980)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n-\n-    @require_vision\n-    @require_torch\n-    def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=30)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 30)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs(batch_size=2)\n-        image_input = self.prepare_image_inputs(batch_size=2)\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            padding=\"longest\",\n-            max_length=76,\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n             truncation=True,\n-            max_image_size=980,\n+            max_length=5,\n         )\n+        self.assertEqual(len(formatted_prompt_tokenized[0]), 5)\n \n-        self.assertEqual(inputs[\"pixel_values\"].shape[1], 3)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 980)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n+        # Now test the ability to return dict\n+        messages[0][0][\"content\"].append(\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        )\n+        out_dict = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n             max_image_size=980,\n-            padding=\"max_length\",\n-            max_length=120,\n-            truncation=\"longest_first\",\n+            return_tensors=\"np\",\n         )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 980)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n+        self.assertListEqual(list(out_dict[self.images_input_name].shape), [1, 3, 980, 980])"
        },
        {
            "sha": "9af13eab324c37b32987475157714faedd820b61",
            "filename": "tests/models/aya_vision/test_processor_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -15,7 +15,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n from transformers import AutoProcessor, AutoTokenizer, AyaVisionProcessor\n from transformers.testing_utils import require_read_token, require_torch, require_vision\n@@ -61,6 +60,7 @@ def setUpClass(cls):\n             **processor_kwargs,\n         )\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n \n     @staticmethod\n     def prepare_processor_dict():\n@@ -79,20 +79,6 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # Override as AyaVisionProcessor needs image tokens in prompts\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n-        if batch_size is None:\n-            return \"lower newer <image>\"\n-\n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n-\n-        if batch_size == 1:\n-            return [\"lower newer <image>\"]\n-        return [\"lower newer <image>\", \"<image> upper older longer string\"] + [\"<image> lower newer\"] * (\n-            batch_size - 2\n-        )\n-\n     @require_torch\n     def test_process_interleaved_images_videos(self):\n         processor = self.get_processor()"
        },
        {
            "sha": "890b1f7f69098aa4586ed454d425b03e6c2b0c51",
            "filename": "tests/models/chameleon/test_processor_chameleon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -40,5 +40,10 @@ def setUpClass(cls):\n         tokenizer = LlamaTokenizer(vocab_file=SAMPLE_VOCAB)\n         tokenizer.pad_token_id = 0\n         tokenizer.sep_token_id = 1\n-        processor = cls.processor_class(image_processor=image_processor, tokenizer=tokenizer)\n+        processor = cls.processor_class(image_processor=image_processor, tokenizer=tokenizer, image_seq_length=2)\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n+\n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {\"image_seq_length\": 2}  # fmt: skip"
        },
        {
            "sha": "c595a91ee99ff802ba9ba4940e6e6d6875717b16",
            "filename": "tests/models/emu3/test_processor_emu3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -34,7 +34,7 @@ class Emu3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n-        image_processor = Emu3ImageProcessor()\n+        image_processor = Emu3ImageProcessor(min_pixels=28 * 28, max_pixels=56 * 56)\n         extra_special_tokens = extra_special_tokens = {\n             \"image_token\": \"<image>\",\n             \"boi_token\": \"<|image start|>\",\n@@ -51,8 +51,10 @@ def setUpClass(cls):\n             image_processor=image_processor, tokenizer=tokenizer, chat_template=\"dummy_template\"\n         )\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n \n-    def prepare_processor_dict(self):\n+    @staticmethod\n+    def prepare_processor_dict():\n         return {\n             \"chat_template\": \"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\",\n         }  # fmt: skip"
        },
        {
            "sha": "1f2c754bd597a71b65633a1e8c052b492ffbba79",
            "filename": "tests/models/fuyu/test_processor_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Ffuyu%2Ftest_processor_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Ffuyu%2Ftest_processor_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_processor_fuyu.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -332,7 +332,7 @@ def test_unstructured_kwargs_batched(self):\n             max_length=76,\n         )\n \n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 7)\n \n \n @require_torch"
        },
        {
            "sha": "a2290c9928796da94de8b71f6cf177e17a4a2b21",
            "filename": "tests/models/gemma3/test_processing_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -56,6 +56,7 @@ def setUpClass(cls):\n         processor_kwargs = cls.prepare_processor_dict()\n         processor = Gemma3Processor(image_processor=image_processor, tokenizer=tokenizer, **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.boi_token\n \n     @classmethod\n     def tearDownClass(cls):\n@@ -68,20 +69,6 @@ def prepare_processor_dict():\n             \"chat_template\": \"{{ bos_token }}\\n{%- if messages[0]['role'] == 'system' -%}\\n    {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\\n    {%- set loop_messages = messages[1:] -%}\\n{%- else -%}\\n    {%- set first_user_prefix = \\\"\\\" -%}\\n    {%- set loop_messages = messages -%}\\n{%- endif -%}\\n{%- for message in loop_messages -%}\\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\\n        {{ raise_exception(\\\"Conversation roles must alternate user/assistant/user/assistant/...\\\") }}\\n    {%- endif -%}\\n    {%- if (message['role'] == 'assistant') -%}\\n        {%- set role = \\\"model\\\" -%}\\n    {%- else -%}\\n        {%- set role = message['role'] -%}\\n    {%- endif -%}\\n    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \\\"\\\") }}\\n    {%- if message['content'] is string -%}\\n        {{ message['content'] | trim }}\\n    {%- elif message['content'] is iterable -%}\\n        {%- for item in message['content'] -%}\\n            {%- if item['type'] == 'image' -%}\\n                {{ '<start_of_image>' }}\\n            {%- elif item['type'] == 'text' -%}\\n                {{ item['text'] | trim }}\\n            {%- endif -%}\\n        {%- endfor -%}\\n    {%- else -%}\\n        {{ raise_exception(\\\"Invalid content type\\\") }}\\n    {%- endif -%}\\n    {{ '<end_of_turn>\\n' }}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{'<start_of_turn>model\\n'}}\\n{%- endif -%}\\n\",            \"image_seq_length\": 3,\n         }  # fmt: skip\n \n-    # Override as VLMs need image tokens in prompts\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n-        if batch_size is None:\n-            return \"lower newer <start_of_image>\"\n-\n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n-\n-        if batch_size == 1:\n-            return [\"lower newer <start_of_image>\"]\n-        return [\"lower newer <start_of_image>\", \"<start_of_image> upper older longer string\"] + [\n-            \"<start_of_image> lower newer\"\n-        ] * (batch_size - 2)\n-\n     # Override as Gemma3 needs images to be an explicitly nested batch\n     def prepare_image_inputs(self, batch_size: Optional[int] = None):\n         \"\"\"This function prepares a list of PIL images for testing\"\"\"\n@@ -123,7 +110,7 @@ def test_pan_and_scan(self):\n         processor_kwargs = self.prepare_processor_dict()\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"image\")\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str,"
        },
        {
            "sha": "0719d211ddad3e33b6304b75bbfa244794bd9e6f",
            "filename": "tests/models/got_ocr2/test_processor_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fgot_ocr2%2Ftest_processor_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fgot_ocr2%2Ftest_processor_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_processor_got_ocr2.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -40,6 +40,7 @@ def setUpClass(cls):\n         processor_kwargs = {}\n         processor = GotOcr2Processor(image_processor, tokenizer, **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.img_pad_token\n \n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer"
        },
        {
            "sha": "35b77c39f2ba959ff25b2da96b39b5022dfce44a",
            "filename": "tests/models/grounding_dino/test_processor_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -79,7 +79,7 @@ def setUpClass(cls):\n         cls.embed_dim = 5\n         cls.seq_length = 5\n \n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None, modality: Optional[str] = None):\n         labels = [\"a cat\", \"remote control\"]\n         labels_longer = [\"a person\", \"a car\", \"a dog\", \"a cat\"]\n "
        },
        {
            "sha": "483d1ad1e90c607ebe20fd790542f6f4e435cc6c",
            "filename": "tests/models/idefics/test_processor_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fidefics%2Ftest_processor_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fidefics%2Ftest_processor_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_processor_idefics.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -219,139 +219,3 @@ def test_model_input_names(self):\n \n         # For now the processor supports only ['pixel_values', 'input_ids', 'attention_mask']\n         self.assertSetEqual(set(inputs.keys()), set(self.input_keys))\n-\n-    # Override the following tests as Idefics image processor does not accept do_rescale and rescale_factor\n-    @require_torch\n-    @require_vision\n-    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", image_size=234)\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 234)\n-\n-    @require_torch\n-    @require_vision\n-    def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", image_size=234)\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, image_size=224)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 224)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            image_size=214,\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs(batch_size=2)\n-        image_input = self.prepare_image_inputs(batch_size=2)\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            image_size=214,\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 8)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"image_size\": 214},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"image_size\": 214},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)"
        },
        {
            "sha": "f2f06af7077425b3eecbe0b90bb43e7c6012e6d3",
            "filename": "tests/models/idefics2/test_processor_idefics2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_processor_idefics2.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -16,7 +16,6 @@\n import tempfile\n import unittest\n from io import BytesIO\n-from typing import Optional\n \n import requests\n \n@@ -84,6 +83,10 @@ def get_image_processor(self, **kwargs):\n     def get_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n \n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {\"image_seq_len\": 2}\n+\n     @classmethod\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n@@ -329,17 +332,3 @@ def test_apply_chat_template(self):\n             \"Assistant:\"\n         )\n         self.assertEqual(rendered, expected_rendered)\n-\n-    # Override as Idefics2Processor needs image tokens in prompts\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n-        if batch_size is None:\n-            return \"lower newer <image>\"\n-\n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n-\n-        if batch_size == 1:\n-            return [\"lower newer <image>\"]\n-        return [\"lower newer <image>\", \"<image> upper older longer string\"] + [\"<image> lower newer\"] * (\n-            batch_size - 2\n-        )"
        },
        {
            "sha": "ad8a24a5a11e9454ec9bb150b6ff100e1ddacbf3",
            "filename": "tests/models/idefics3/test_processor_idefics3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 154,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -16,7 +16,6 @@\n import tempfile\n import unittest\n from io import BytesIO\n-from typing import Optional\n \n import numpy as np\n import requests\n@@ -81,6 +80,10 @@ def get_image_processor(self, **kwargs):\n     def get_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n \n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {\"image_seq_len\": 2}\n+\n     def get_split_image_expected_tokens(self, processor, image_rows, image_cols):\n         text_split_images = []\n         for n_h in range(image_rows):\n@@ -352,159 +355,6 @@ def test_apply_chat_template(self):\n         )\n         self.assertEqual(rendered, expected_rendered)\n \n-    # Override as Idefics3Processor needs image tokens in prompts\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n-        if batch_size is None:\n-            return \"lower newer <image>\"\n-\n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n-\n-        if batch_size == 1:\n-            return [\"lower newer <image>\"]\n-        return [\"lower newer <image>\", \"<image> upper older longer string\"] + [\"<image> lower newer\"] * (\n-            batch_size - 2\n-        )\n-\n-    # Override tests as inputs_ids padded dimension is the second one but not the last one\n-    @require_vision\n-    @require_torch\n-    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=30)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\", max_length=30)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 30)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            common_kwargs={\"return_tensors\": \"pt\"},\n-            images_kwargs={\"max_image_size\": {\"longest_edge\": 32}},\n-            text_kwargs={\"padding\": \"max_length\", \"max_length\": 120, \"truncation\": \"longest_first\"},\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 32)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"max_image_size\": {\"longest_edge\": 32}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 120, \"truncation\": \"longest_first\"},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 32)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n-\n-    @require_vision\n-    @require_torch\n-    def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=30)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 30)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs(batch_size=2)\n-        image_input = self.prepare_image_inputs(batch_size=2)\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            padding=\"longest\",\n-            max_length=76,\n-            truncation=True,\n-            max_image_size={\"longest_edge\": 30},\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 3)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 30)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            max_image_size={\"longest_edge\": 32},\n-            padding=\"max_length\",\n-            max_length=120,\n-            truncation=\"longest_first\",\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 32)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n-\n     @require_torch\n     @require_vision\n     def test_text_only_inference(self):"
        },
        {
            "sha": "aef3539a37ea0ff3f4ea9620a926c7d5a80ef704",
            "filename": "tests/models/llama4/test_processor_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllama4%2Ftest_processor_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllama4%2Ftest_processor_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_processor_llama4.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -15,7 +15,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n from transformers import AutoProcessor, Llama4Processor, PreTrainedTokenizerFast\n from transformers.testing_utils import require_vision\n@@ -38,9 +37,10 @@ def setUpClass(cls):\n \n         image_processor = Llama4ImageProcessorFast(max_patches=1, size={\"height\": 20, \"width\": 20})\n         tokenizer = PreTrainedTokenizerFast.from_pretrained(\"unsloth/Llama-3.2-11B-Vision-Instruct-unsloth-bnb-4bit\")\n-        processor_kwargs = {}\n+        processor_kwargs = cls.prepare_processor_dict()\n         processor = Llama4Processor(image_processor, tokenizer, **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n \n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n@@ -51,21 +51,3 @@ def get_image_processor(self, **kwargs):\n     @classmethod\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname)\n-\n-    # Override as Llama4Processor needs image tokens in prompts\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n-        if batch_size is None:\n-            return \"lower newer <|image|>\"\n-\n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n-\n-        if batch_size == 1:\n-            return [\"lower newer <|image|>\"]\n-        return [\"lower newer <|image|>\", \"<|image|> upper older longer string\"] + [\"<|image|> lower newer\"] * (\n-            batch_size - 2\n-        )\n-\n-    @unittest.skip(\"This test uses return_tensors='np' which is not supported\")\n-    def test_image_chat_template_accepts_processing_kwargs(self):\n-        pass"
        },
        {
            "sha": "3a469d76f2b5beb15258dbe1ac52a3e3e1409895",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -43,6 +43,7 @@ def setUpClass(cls):\n         processor_kwargs = cls.prepare_processor_dict()\n         processor = LlavaProcessor(image_processor, tokenizer, **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n \n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n@@ -58,18 +59,10 @@ def tearDownClass(cls):\n     def prepare_processor_dict():\n         return {\n             \"chat_template\": \"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\",\n-            \"patch_size\": 3,\n+            \"patch_size\": 128,\n             \"vision_feature_select_strategy\": \"default\"\n         }  # fmt: skip\n \n-    @unittest.skip(\n-        \"Skip because the model has no processor kwargs except for chat template and\"\n-        \"chat template is saved as a separate file. Stop skipping this test when the processor\"\n-        \"has new kwargs saved in config file.\"\n-    )\n-    def test_processor_to_json_string(self):\n-        pass\n-\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n         processor_dict_loaded = json.loads(processor_loaded.to_json_string())"
        },
        {
            "sha": "47fbb241acf84a6bb76fb28ec96b9f68adef0975",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -43,6 +43,7 @@ def setUpClass(cls):\n         processor_kwargs = cls.prepare_processor_dict()\n         processor = LlavaNextProcessor(image_processor, tokenizer, **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n \n     def get_tokenizer(self, **kwargs):\n         return LlavaNextProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n@@ -54,18 +55,10 @@ def get_image_processor(self, **kwargs):\n     def prepare_processor_dict():\n         return {\n             \"chat_template\": \"{% for message in messages %}{% if message['role'] != 'system' %}{{ message['role'].upper() + ': '}}{% endif %}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] + ' '}}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] + ' '}}{% endgeneration %}{% endfor %}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT:' }}{% endif %}\",\n-            \"patch_size\": 3,\n+            \"patch_size\": 128,\n             \"vision_feature_select_strategy\": \"default\"\n         }  # fmt: skip\n \n-    @unittest.skip(\n-        \"Skip because the model has no processor kwargs except for chat template and\"\n-        \"chat template is saved as a separate file. Stop skipping this test when the processor\"\n-        \"has new kwargs saved in config file.\"\n-    )\n-    def test_processor_to_json_string(self):\n-        pass\n-\n     # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)"
        },
        {
            "sha": "207d1a63729eb89a7ffbf235da414c81e178b5e1",
            "filename": "tests/models/llava_next_video/test_processor_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -47,6 +47,8 @@ def setUpClass(cls):\n             video_processor=video_processor, image_processor=image_processor, tokenizer=tokenizer, **processor_kwargs\n         )\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n+        cls.video_token = processor.video_token\n \n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n@@ -61,20 +63,11 @@ def get_video_processor(self, **kwargs):\n     def prepare_processor_dict(cls):\n         return {\n             \"chat_template\": \"{% for message in messages %}{{'<|im_start|>' + message['role'] + ' '}}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>' }}{% endfor %}{# Render all video then #}{% for content in message['content'] | selectattr('type', 'equalto', 'video') %}{{ '<video>' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ '\\n' + content['text'] }}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ '\\n' + content['text'] }}{% endgeneration %}{% endfor %}{% endif %}{{'<|im_end|>'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\",\n-            \"num_additional_image_tokens\": 6,\n-            \"patch_size\": 4,\n+            \"num_additional_image_tokens\": 0,\n+            \"patch_size\": 128,\n             \"vision_feature_select_strategy\": \"default\",\n         }\n \n-    def test_processor_to_json_string(self):\n-        processor = self.get_processor()\n-        obj = json.loads(processor.to_json_string())\n-        for key, value in self.prepare_processor_dict().items():\n-            # chat_tempalate are tested as a separate test because they are saved in separate files\n-            if key != \"chat_template\":\n-                self.assertEqual(obj[key], value)\n-                self.assertEqual(getattr(processor, key, None), value)\n-\n     # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)"
        },
        {
            "sha": "72416f255ebd00e7ec27483eca5be18969b1753d",
            "filename": "tests/models/llava_onevision/test_processor_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -51,6 +51,8 @@ def setUpClass(cls):\n             video_processor=video_processor, image_processor=image_processor, tokenizer=tokenizer, **processor_kwargs\n         )\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n+        cls.video_token = processor.video_token\n \n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n@@ -73,15 +75,6 @@ def prepare_processor_dict():\n             \"vision_feature_select_strategy\": \"default\"\n         }  # fmt: skip\n \n-    def test_processor_to_json_string(self):\n-        processor = self.get_processor()\n-        obj = json.loads(processor.to_json_string())\n-        for key, value in self.prepare_processor_dict().items():\n-            # chat_tempalate are tested as a separate test because they are saved in separate files\n-            if key != \"chat_template\":\n-                self.assertEqual(obj[key], value)\n-                self.assertEqual(getattr(processor, key, None), value)\n-\n     # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)"
        },
        {
            "sha": "3c818107e4085c928a43f0fe4017a7ffc37182e8",
            "filename": "tests/models/mistral3/test_processor_mistral3.py",
            "status": "modified",
            "additions": 36,
            "deletions": 30,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -19,7 +19,7 @@\n import requests\n \n from transformers import PixtralProcessor\n-from transformers.testing_utils import require_read_token, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -34,7 +34,6 @@\n \n \n @require_vision\n-@require_read_token\n class Mistral3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     \"\"\"This tests Pixtral processor with the new `spatial_merge_size` argument in Mistral3.\"\"\"\n \n@@ -49,30 +48,37 @@ def setUpClass(cls):\n         cls.url_2 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n         cls.image_2 = Image.open(requests.get(cls.url_2, stream=True).raw)\n \n-    def setUp(self):\n-        self.tmpdirname = tempfile.mkdtemp()\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        cls.addClassCleanup(lambda tempdir=cls.tmpdirname: shutil.rmtree(tempdir))\n+\n+        processor_kwargs = cls.prepare_processor_dict()\n         processor = PixtralProcessor.from_pretrained(\n-            \"hf-internal-testing/Mistral-Small-3.1-24B-Instruct-2503-only-processor\"\n+            \"hf-internal-testing/Mistral-Small-3.1-24B-Instruct-2503-only-processor\", **processor_kwargs\n         )\n-        processor.save_pretrained(self.tmpdirname)\n+        processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n \n     def get_processor(self):\n         return self.processor_class.from_pretrained(self.tmpdirname)\n \n-    def tearDown(self):\n-        shutil.rmtree(self.tmpdirname)\n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {\n+            \"chat_template\": \"{%- set today = strftime_now(\\\"%Y-%m-%d\\\") %}\\n{%- set default_system_message = \\\"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\\\nYour knowledge base was last updated on 2023-10-01. The current date is \\\" + today + \\\".\\\\n\\\\nWhen you're not sure about some information, you say that you don't have the information and don't make up anything.\\\\nIf the user's question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\\\\\"What are some good restaurants around me?\\\\\\\" => \\\\\\\"Where are you?\\\\\\\" or \\\\\\\"When is the next flight to Tokyo\\\\\\\" => \\\\\\\"Where do you travel from?\\\\\\\")\\\" %}\\n\\n{{- bos_token }}\\n\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- if messages[0] is string %}\\n        {%- set system_message = messages[0]['content'] %}\\n        {%- set loop_messages = messages[1:] %}\\n    {%- else %} \\n        {%- set system_message = messages[0]['content'][0]['text'] %}\\n        {%- set loop_messages = messages[1:] %}\\n    {%- endif %}\\n{%- else %}\\n    {%- set system_message = default_system_message %}\\n    {%- set loop_messages = messages %}\\n{%- endif %}\\n{{- '[SYSTEM_PROMPT]' + system_message + '[/SYSTEM_PROMPT]' }}\\n\\n{%- for message in loop_messages %}\\n    {%- if message['role'] == 'user' %}\\n            {%- if message['content'] is string %}\\n            {{- '[INST]' + message['content'] + '[/INST]' }}\\n            {%- else %}\\n                    {{- '[INST]' }}\\n                    {%- for block in message['content'] %}\\n                            {%- if block['type'] == 'text' %}\\n                                    {{- block['text'] }}\\n                            {%- elif block['type'] == 'image' or block['type'] == 'image_url' %}\\n                                    {{- '[IMG]' }}\\n                                {%- else %}\\n                                    {{- raise_exception('Only text and image blocks are supported in message content!') }}\\n                                {%- endif %}\\n                        {%- endfor %}\\n                    {{- '[/INST]' }}\\n                {%- endif %}\\n    {%- elif message['role'] == 'system' %}\\n        {{- '[SYSTEM_PROMPT]' + message['content'] + '[/SYSTEM_PROMPT]' }}\\n    {%- elif message['role'] == 'assistant' %}\\n        {%- if message['content'] is string %}\\n            {{- message['content'] + eos_token }}\\n        {%- else %}\\n            {{- message['content'][0]['text'] + eos_token }}\\n        {%- endif %}\\n    {%- else %}\\n        {{- raise_exception('Only user, system and assistant roles are supported!') }}\\n    {%- endif %}\\n{%- endfor %}\",\n+            \"patch_size\": 128,\n+        }  # fmt: skip\n \n     def test_image_token_filling(self):\n         processor = self.processor_class.from_pretrained(self.tmpdirname)\n         # Important to check with non square image\n         image = torch.randint(0, 2, (3, 500, 316))\n-        expected_image_tokens = 198\n+        expected_image_tokens = 4\n         image_token_index = 10\n \n         messages = [\n             {\n                 \"role\": \"system\",\n-                \"content\": \"\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n             },\n             {\n                 \"role\": \"user\",\n@@ -104,14 +110,14 @@ def test_processor_with_single_image(self):\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n         self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 36]))\n \n         # fmt: off\n         input_ids = inputs_image[\"input_ids\"]\n         self.assertEqual(\n             input_ids[0].tolist(),\n             # Equivalent to \"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the content of the image? ASSISTANT:\"\n-            [1, 21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+            [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 4701, 1307, 1278, 3937, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n \n@@ -121,36 +127,36 @@ def test_processor_with_single_image(self):\n         self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n         self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 36]))\n \n         # fmt: off\n         input_ids = inputs_url[\"input_ids\"]\n         self.assertEqual(\n             input_ids[0].tolist(),\n             # Equivalent to \"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the content of the image? ASSISTANT:\"\n-            [1, 21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+            [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 4701, 1307, 1278, 3937, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n \n         # Test passing inputs as a single list\n         inputs_image = processor(text=prompt_string, images=[self.image_0], return_tensors=\"pt\")\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 36]))\n \n         # fmt: off\n         self.assertEqual(\n             inputs_image[\"input_ids\"][0].tolist(),\n-            [1, 21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+            [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 4701, 1307, 1278, 3937, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n \n         # Test as nested single list\n         inputs_image = processor(text=prompt_string, images=[[self.image_0]], return_tensors=\"pt\")\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([1, 3, 24, 36]))\n \n         # fmt: off\n         self.assertEqual(\n             inputs_image[\"input_ids\"][0].tolist(),\n-            [1, 21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+            [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 4701, 1307, 1278, 3937, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n \n@@ -168,14 +174,14 @@ def test_processor_with_multiple_images_single_list(self):\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n         self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 24, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 24, 36]))\n \n         # fmt: off\n         input_ids = inputs_image[\"input_ids\"]\n         self.assertEqual(\n             input_ids[0].tolist(),\n             # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n-            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+            [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n                     )\n         # fmt: on\n \n@@ -185,25 +191,25 @@ def test_processor_with_multiple_images_single_list(self):\n         self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n         self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n         self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 24, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 24, 36]))\n \n         # fmt: off\n         input_ids = inputs_url[\"input_ids\"]\n         self.assertEqual(\n             input_ids[0].tolist(),\n             # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n-            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+            [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n \n         # Test passing in as a nested list\n         inputs_url = processor(text=prompt_string, images=[[self.image_0, self.image_1]], return_tensors=\"pt\")\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 24, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([2, 3, 24, 36]))\n \n         # fmt: off\n         self.assertEqual(\n             inputs_url[\"input_ids\"][0].tolist(),\n-            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+            [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n \n@@ -226,14 +232,14 @@ def test_processor_with_multiple_images_multiple_lists(self):\n         self.assertTrue(len(inputs_image[\"input_ids\"]) == 2)\n         self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n         self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 30, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 36, 36]))\n \n         # fmt: off\n         input_ids = inputs_image[\"input_ids\"]\n         self.assertEqual(\n             input_ids[0].tolist(),\n             # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n-            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+            [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n \n@@ -243,27 +249,27 @@ def test_processor_with_multiple_images_multiple_lists(self):\n         self.assertTrue(len(inputs_url[\"input_ids\"]) == 2)\n         self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n         self.assertIsInstance(inputs_image[\"pixel_values\"], torch.Tensor)\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 30, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 36, 36]))\n \n         # fmt: off\n         input_ids = inputs_url[\"input_ids\"]\n         self.assertEqual(\n             input_ids[0].tolist(),\n             # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n-            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+             [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n \n         # Test passing as a single flat list\n         inputs_image = processor(\n             text=prompt_string, images=[self.image_0, self.image_1, self.image_2], return_tensors=\"pt\", padding=True\n         )\n-        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 30, 30]))\n+        self.assertTrue(inputs_image[\"pixel_values\"].shape == torch.Size([3, 3, 36, 36]))\n \n         # fmt: off\n         self.assertEqual(\n             inputs_image[\"input_ids\"][0].tolist(),\n-            [1, 21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+            [1, 21510, 1058, 1032, 10, 10, 10, 12, 10, 10, 10, 13, 10, 10, 10, 12, 10, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n         )\n         # fmt: on\n "
        },
        {
            "sha": "db86272df33b5ebb3178a87e010b7d479089bc5d",
            "filename": "tests/models/mllama/test_processor_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 16,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -16,7 +16,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n import numpy as np\n \n@@ -333,20 +332,6 @@ def test_process_interleaved_images_prompts_image_error(self):\n         with self.assertRaises(ValueError):\n             processor(text=text, images=None, padding=True)\n \n-    # Override as MllamaProcessor needs image tokens in prompts\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n-        if batch_size is None:\n-            return \"lower newer <|image|>\"\n-\n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n-\n-        if batch_size == 1:\n-            return [\"lower newer <|image|>\"]\n-        return [\"lower newer <|image|>\", \"<|image|> upper older longer string\"] + [\"<|image|> lower newer\"] * (\n-            batch_size - 2\n-        )\n-\n     def test_unstructured_kwargs_batched(self):\n         # Overriden because Mllama expects images in nested format. For 2 images it can't infer\n         # the correct nesting, so we better throw an error\n@@ -357,7 +342,7 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=2)\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n         image_input = [[image_input[0]], [image_input[1]]]\n         inputs = processor("
        },
        {
            "sha": "b22336fc40b6924ccce2ab3e287984346844cb23",
            "filename": "tests/models/paligemma/test_processor_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -37,10 +37,11 @@ class PaliGemmaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n         image_processor = SiglipImageProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n-        image_processor.image_seq_length = 0\n+        image_processor.image_seq_length = 0  # TODO: raushan fix me in #37342\n         tokenizer = GemmaTokenizer(SAMPLE_VOCAB, keep_accents=True)\n         processor = PaliGemmaProcessor(image_processor=image_processor, tokenizer=tokenizer)\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n \n     @classmethod\n     def tearDownClass(cls):"
        },
        {
            "sha": "e8aa4c68aabbaa64edb2e7cfc2797c61556067d3",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -43,17 +43,23 @@ class Qwen2_5_VLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n-        processor = Qwen2_5_VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", patch_size=4)\n+        processor = Qwen2_5_VLProcessor.from_pretrained(\n+            \"Qwen/Qwen2-VL-7B-Instruct\", patch_size=4, max_pixels=56 * 56, min_pixels=28 * 28\n+        )\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n \n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n \n     def get_image_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n-    def prepare_processor_dict(self):\n-        return {\"chat_template\": \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"}  # fmt: skip\n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {\n+            \"chat_template\": \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\",\n+        }  # fmt: skip\n \n     @classmethod\n     def tearDownClass(cls):\n@@ -206,7 +212,7 @@ def _test_apply_chat_template(\n         self.assertTrue(input_name in out_dict)\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n-        self.assertEqual(len(out_dict[input_name]), batch_size * 19200)\n+        self.assertEqual(len(out_dict[input_name]), batch_size * 192)\n \n         return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}\n         for k in out_dict:\n@@ -261,7 +267,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             num_frames=num_frames,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 115200)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 360)\n \n         # Load with `video_fps` arg\n         video_fps = 1\n@@ -273,7 +279,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             video_fps=video_fps,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 288000)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 900)\n \n         # Load with `video_fps` and `num_frames` args, should raise an error\n         with self.assertRaises(ValueError):\n@@ -294,7 +300,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             return_dict=True,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 8640000)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 27000)\n \n         # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n         # because we assume they come from one video\n@@ -312,7 +318,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             return_dict=True,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 71280)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 160)\n \n     def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         processor_components = self.prepare_components()\n@@ -328,7 +334,7 @@ def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         inputs = processor(text=input_str, images=image_input, max_pixels=56 * 56 * 4, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.images_input_name].shape[0], 612)\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(inputs[self.images_input_name].shape[0], 800)\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 100)\n \n     @require_av\n     def test_apply_chat_template_video_special_processing(self):\n@@ -395,4 +401,4 @@ def _process_messages_for_chat_template(\n         # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n         formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n         self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1756800)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 21960)"
        },
        {
            "sha": "67451144c4b6228bd3b950125f1158d169453de2",
            "filename": "tests/models/qwen2_audio/test_processor_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -14,7 +14,6 @@\n import shutil\n import tempfile\n import unittest\n-from typing import Optional\n \n from transformers import AutoProcessor, AutoTokenizer, Qwen2AudioProcessor, WhisperFeatureExtractor\n from transformers.testing_utils import require_torch, require_torchaudio\n@@ -40,6 +39,7 @@ def setUpClass(cls):\n         processor_kwargs = cls.prepare_processor_dict()\n         processor = Qwen2AudioProcessor.from_pretrained(cls.checkpoint, **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.audio_token = processor.audio_token\n \n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n@@ -57,20 +57,6 @@ def prepare_processor_dict():\n             \"chat_template\": \"{% set audio_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if 'audio' in content or 'audio_url' in content or content['type'] == 'audio' %}{% set audio_count.value = audio_count.value + 1 %}Audio {{ audio_count.value }}: <|audio_bos|><|AUDIO|><|audio_eos|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\",\n         }\n \n-    # Override as Qwen2AudioProcessor needs audio tokens in prompts\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n-        if batch_size is None:\n-            return \"lower newer <|AUDIO|>\"\n-\n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n-\n-        if batch_size == 1:\n-            return [\"lower newer <|AUDIO|>\"]\n-        return [\"lower newer <|AUDIO|>\", \"<|AUDIO|> upper older longer string\"] + [\"<|AUDIO|> lower newer\"] * (\n-            batch_size - 2\n-        )\n-\n     def test_can_load_various_tokenizers(self):\n         processor = Qwen2AudioProcessor.from_pretrained(self.checkpoint)\n         tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)"
        },
        {
            "sha": "742796b1d2a7bd0ba6353100d9b34292ffb0fa69",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -43,16 +43,20 @@ class Qwen2VLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n-        processor = Qwen2VLProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", patch_size=4)\n+        processor = Qwen2VLProcessor.from_pretrained(\n+            \"Qwen/Qwen2-VL-7B-Instruct\", patch_size=4, max_pixels=56 * 56, min_pixels=28 * 28\n+        )\n         processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n \n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n \n     def get_image_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n-    def prepare_processor_dict(self):\n+    @staticmethod\n+    def prepare_processor_dict():\n         return {\"chat_template\": \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\"}  # fmt: skip\n \n     @classmethod\n@@ -203,7 +207,7 @@ def _test_apply_chat_template(\n         self.assertTrue(input_name in out_dict)\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n-        self.assertEqual(len(out_dict[input_name]), batch_size * 19200)\n+        self.assertEqual(len(out_dict[input_name]), batch_size * 192)\n \n         return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}\n         for k in out_dict:\n@@ -258,7 +262,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             num_frames=num_frames,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 115200)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 360)\n \n         # Load with `video_fps` arg\n         video_fps = 1\n@@ -270,7 +274,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             video_fps=video_fps,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 288000)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 900)\n \n         # Load with `video_fps` and `num_frames` args, should raise an error\n         with self.assertRaises(ValueError):\n@@ -291,7 +295,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             return_dict=True,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 8640000)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 27000)\n \n         # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n         # because we assume they come from one video\n@@ -309,7 +313,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             return_dict=True,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 71280)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 160)\n \n     @require_av\n     def test_apply_chat_template_video_special_processing(self):\n@@ -376,7 +380,7 @@ def _process_messages_for_chat_template(\n         # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n         formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n         self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1756800)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 21960)\n \n     def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         processor_components = self.prepare_components()\n@@ -390,6 +394,6 @@ def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         input_str = self.prepare_text_inputs()\n         image_input = self.prepare_image_inputs()\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(inputs[self.images_input_name].shape[0], 800)\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 100)\n         inputs = processor(text=input_str, images=image_input, max_pixels=56 * 56 * 4, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.images_input_name].shape[0], 612)"
        },
        {
            "sha": "41fb4c8f56676050c74d94d1315125285ac29b6d",
            "filename": "tests/models/smolvlm/test_processor_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 131,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -16,7 +16,6 @@\n import tempfile\n import unittest\n from io import BytesIO\n-from typing import Optional\n \n import numpy as np\n import requests\n@@ -42,7 +41,8 @@ class SmolVLMProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n-        processor = SmolVLMProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\", image_seq_len=2)\n+        processor_kwargs = cls.prepare_processor_dict()\n+        processor = SmolVLMProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\", **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)\n         cls.image1 = Image.open(\n             BytesIO(\n@@ -82,9 +82,10 @@ def get_image_processor(self, **kwargs):\n     def get_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n \n-    def prepare_processor_dict(self):\n+    @staticmethod\n+    def prepare_processor_dict():\n         return {\n-            \"image_seq_len\": self.image_seq_len,\n+            \"image_seq_len\": 2,\n             \"chat_template\": \"<|im_start|>{% for message in messages %}{{message['role'] | capitalize}}{% if message['content'][0]['type'] == 'image' %}{{':'}}{% else %}{{': '}}{% endif %}{% for line in message['content'] %}{% if line['type'] == 'text' %}{{line['text']}}{% elif line['type'] == 'image' %}{{ '<image>' }}{% endif %}{% endfor %}<end_of_utterance>\\n{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}\",\n         }\n \n@@ -426,106 +427,6 @@ def test_apply_chat_template_video_frame_sampling(self):\n         # NOTE: the last assert checks are removed\n         # Loading video as a list of frames (i.e. images) is not supported in SmolVLM\n \n-    # Override as SmolVLMProcessor needs image tokens in prompts\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n-        if batch_size is None:\n-            return \"lower newer <image>\"\n-\n-        if batch_size < 1:\n-            raise ValueError(\"batch_size must be greater than 0\")\n-\n-        if batch_size == 1:\n-            return [\"lower newer <image>\"]\n-        return [\"lower newer <image>\", \"<image> upper older longer string\"] + [\"<image> lower newer\"] * (\n-            batch_size - 2\n-        )\n-\n-    # Override tests as inputs_ids padded dimension is the second one but not the last one\n-    @require_vision\n-    @require_torch\n-    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=30)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\", max_length=30)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 30)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            common_kwargs={\"return_tensors\": \"pt\"},\n-            images_kwargs={\"max_image_size\": {\"longest_edge\": 32}},\n-            text_kwargs={\"padding\": \"max_length\", \"max_length\": 120, \"truncation\": \"longest_first\"},\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 32)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"max_image_size\": {\"longest_edge\": 32}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 120, \"truncation\": \"longest_first\"},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 32)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n-\n-    @require_vision\n-    @require_torch\n-    def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=30)\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 30)\n-\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs_batched(self):\n@@ -537,7 +438,7 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=2)\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n         image_input = [[image_input[0]], [image_input[1]]]\n         inputs = processor(\n@@ -554,32 +455,6 @@ def test_unstructured_kwargs_batched(self):\n         self.assertEqual(inputs[\"pixel_values\"].shape[3], 30)\n         self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n \n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            max_image_size={\"longest_edge\": 32},\n-            padding=\"max_length\",\n-            max_length=120,\n-            truncation=\"longest_first\",\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 32)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 120)\n-\n     @require_torch\n     @require_vision\n     def test_text_only_inference(self):"
        },
        {
            "sha": "8827f67509fb4451f1aae8095329966b3d284981",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 45,
            "deletions": 32,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a563999a024306c6f6abec71012d0b462da3d6b2/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=a563999a024306c6f6abec71012d0b462da3d6b2",
            "patch": "@@ -92,7 +92,8 @@ class ProcessorTesterMixin:\n     videos_input_name = \"pixel_values_videos\"\n     audio_input_name = \"input_features\"\n \n-    def prepare_processor_dict(self):\n+    @staticmethod\n+    def prepare_processor_dict():\n         return {}\n \n     def get_component(self, attribute, **kwargs):\n@@ -123,18 +124,23 @@ def get_processor(self):\n         processor = self.processor_class(**components, **self.prepare_processor_dict())\n         return processor\n \n-    # TODO: raushan unify all these special token LLMs under the general preparation. We can get audio/image token\n-    # from tokenizer, so we can generalize instead of overriding\n-    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None, modality: Optional[str] = None):\n+        if modality is not None:\n+            special_token_to_add = getattr(self, f\"{modality}_token\", \"\")\n+        else:\n+            special_token_to_add = \"\"\n+\n         if batch_size is None:\n-            return \"lower newer\"\n+            return f\"lower newer {special_token_to_add}\"\n \n         if batch_size < 1:\n             raise ValueError(\"batch_size must be greater than 0\")\n \n         if batch_size == 1:\n-            return [\"lower newer\"]\n-        return [\"lower newer\", \"upper older longer string\"] + [\"lower newer\"] * (batch_size - 2)\n+            return [f\"lower newer {special_token_to_add}\"]\n+        return [f\"lower newer {special_token_to_add}\", f\" {special_token_to_add} upper older longer string\"] + [\n+            f\"lower newer {special_token_to_add}\"\n+        ] * (batch_size - 2)\n \n     @require_vision\n     def prepare_image_inputs(self, batch_size: Optional[int] = None):\n@@ -159,6 +165,13 @@ def test_processor_to_json_string(self):\n         for key, value in self.prepare_processor_dict().items():\n             # Chat template is saved as a separate file\n             if key not in \"chat_template\":\n+                # json converts dict keys to str, but some processors force convert back to int when init\n+                if (\n+                    isinstance(obj[key], dict)\n+                    and isinstance(list(obj[key].keys())[0], str)\n+                    and isinstance(list(value.keys())[0], int)\n+                ):\n+                    obj[key] = {int(k): v for k, v in obj[key].items()}\n                 self.assertEqual(obj[key], value)\n                 self.assertEqual(getattr(processor, key, None), value)\n \n@@ -206,7 +219,7 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"image\")\n         image_input = self.prepare_image_inputs()\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 117)\n@@ -229,7 +242,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n@@ -244,7 +257,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"image\")\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n@@ -264,7 +277,7 @@ def test_kwargs_overrides_default_image_processor_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         inputs = processor(text=input_str, images=image_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n@@ -278,7 +291,7 @@ def test_unstructured_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"image\")\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str,\n@@ -301,7 +314,7 @@ def test_unstructured_kwargs_batched(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=2)\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"image\")\n         image_input = self.prepare_image_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n@@ -327,7 +340,7 @@ def test_doubly_passed_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [self.prepare_text_inputs()]\n+        input_str = [self.prepare_text_inputs(modality=\"image\")]\n         image_input = self.prepare_image_inputs()\n         with self.assertRaises(ValueError):\n             _ = processor(\n@@ -346,7 +359,7 @@ def test_structured_kwargs_nested(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -369,7 +382,7 @@ def test_structured_kwargs_nested_from_dict(self):\n         processor_kwargs = self.prepare_processor_dict()\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n@@ -396,7 +409,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3)\n+        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n         raw_speech = floats_list((3, 1000))\n         raw_speech = [np.asarray(audio) for audio in raw_speech]\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\")\n@@ -414,7 +427,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3)\n+        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n         raw_speech = floats_list((3, 1000))\n         raw_speech = [np.asarray(audio) for audio in raw_speech]\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=300, padding=\"max_length\")\n@@ -433,7 +446,7 @@ def test_unstructured_kwargs_audio(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3)\n+        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n         raw_speech = floats_list((3, 1000))\n         raw_speech = [np.asarray(audio) for audio in raw_speech]\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=300, padding=\"max_length\")\n@@ -452,7 +465,7 @@ def test_doubly_passed_kwargs_audio(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3)\n+        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n         raw_speech = floats_list((3, 1000))\n         raw_speech = [np.asarray(audio) for audio in raw_speech]\n         with self.assertRaises(ValueError):\n@@ -476,7 +489,7 @@ def test_structured_kwargs_audio_nested(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3)\n+        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n         raw_speech = floats_list((3, 1000))\n         raw_speech = [np.asarray(audio) for audio in raw_speech]\n \n@@ -499,7 +512,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_video(self):\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n         inputs = processor(text=input_str, videos=video_input, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 117)\n@@ -522,7 +535,7 @@ def test_video_processor_defaults_preserved_by_video_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n \n         inputs = processor(text=input_str, videos=video_input, return_tensors=\"pt\")\n@@ -537,7 +550,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs_video(self):\n \n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n         inputs = processor(\n             text=input_str, videos=video_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n@@ -557,7 +570,7 @@ def test_kwargs_overrides_default_video_processor_kwargs(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n \n         inputs = processor(text=input_str, videos=video_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n@@ -571,7 +584,7 @@ def test_unstructured_kwargs_video(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n         inputs = processor(\n             text=input_str,\n@@ -594,7 +607,7 @@ def test_unstructured_kwargs_batched_video(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=2)\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"video\")\n         video_input = self.prepare_video_inputs(batch_size=2)\n         inputs = processor(\n             text=input_str,\n@@ -620,7 +633,7 @@ def test_doubly_passed_kwargs_video(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = [self.prepare_text_inputs()]\n+        input_str = [self.prepare_text_inputs(modality=\"video\")]\n         video_input = self.prepare_video_inputs()\n         with self.assertRaises(ValueError):\n             _ = processor(\n@@ -639,7 +652,7 @@ def test_structured_kwargs_nested_video(self):\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n \n         # Define the kwargs for each modality\n@@ -662,7 +675,7 @@ def test_structured_kwargs_nested_from_dict_video(self):\n         processor_kwargs = self.prepare_processor_dict()\n         processor = self.processor_class(**processor_components, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n \n         # Define the kwargs for each modality\n@@ -686,7 +699,7 @@ def test_overlapping_text_image_kwargs_handling(self):\n         processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs()\n+        input_str = self.prepare_text_inputs(modality=\"image\")\n         image_input = self.prepare_image_inputs()\n \n         with self.assertRaises(ValueError):\n@@ -713,7 +726,7 @@ def test_overlapping_text_audio_kwargs_handling(self):\n         processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor, **processor_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        input_str = self.prepare_text_inputs(batch_size=3)\n+        input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n         audio_lengths = [4000, 8000, 16000, 32000]\n         raw_speech = [np.asarray(audio)[:length] for audio, length in zip(floats_list((3, 32_000)), audio_lengths)]\n "
        }
    ],
    "stats": {
        "total": 1113,
        "additions": 300,
        "deletions": 813
    }
}