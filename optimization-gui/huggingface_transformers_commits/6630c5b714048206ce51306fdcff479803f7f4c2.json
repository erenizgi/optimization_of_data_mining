{
    "author": "Cyrilvallez",
    "message": "Add xlstm model (#39665)\n\n* Add xLSTM cleanly with optimizations.\n\n* Fix style.\n\n* Fix modeling test.\n\n* Make xLSTM package optional.\n\n* Fix: Update torch version check.\n\n* Fix: Bad variable naming in test.\n\n* Fix: Import structure cleaning with Ruff.\n\n* Fix: Update docstrings.\n\n* Fix: Mitigate unused config attr tests by explicit usage.\n\n* Fix: Skip tests, if xlstm library is not installed.\n\n* Feat: Enable longer context window for inference by chunking.\n\n* Fix: Make training test pass by lowering target accuracy.\n\n* Chore: Increase test verbosity for failing generation test.\n\n* Update docs/source/en/model_doc/xlstm.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Fix: Make xlstm available even without CUDA.\n\n* Chore: Remove unnecessary import.\n\n* Fix: Remove BOS insertion.\n\n* Chore: Improve xLSTMCache documentation.\n\n* Integrate basic xLSTM fallback code.\n\n* Chore: Remove unnecessary import.\n\n* Chore: Remove duplicate LayerNorm.\n\n* chore: update copyright, minor reformatting\n\n* fix: refactor mLSTMStateType due to missing torch import\n\n* fix: add missing import\n\n* Chore: Replace einops.\n\n* fix: apply ruff formatting\n\n* fix: run `make fix-copies` to re-generate dummy_pt_objects.py\n\n* fix: make type hints Python 3.9 compatible\n\n* fix: remove obsolete import\n\n* fix: remove obsolete method from docs\n\n* chore: remove obsolete `force_bos_token_insert` from config\n\n* Chore: Remove duplicated xLSTMCache class.\n\n* Fix: Formatting of modeling_xlstm.py\n\n* Chore: Remove xlstm package requirement from test. Re-add update_rnn_state.\n\n* Fix: Update xLSTMCache docstring.\n\n* Feat: Add proper initialization of xLSTM.\n\n* Chore: Re-format files.\n\n* Chore: Adapt format.\n\n* Fix: xLSTMCache import restructuring.\n\n* Fix: Add __all__ lists to modeling and configuration files.\n\n* Chore: Reformat.\n\n* Fix: Remove unnecessary update_rnn_state function.\n\n* Fix: Undo test accuracy quickfix.\n\n* Fix: Update copyright year, remvoe config copy.\n\n* Chore: Flatten all internal configs to xLSTMConfig.\n\n* Fix: Unused config variables check.\n\n* Chore: Remove unnecessary imports.\n\n* Fix: Unify xlstm cache argument from batch_size to max_batch_size.\n\n* Chore: Remove bad default arg value for xLSTMCache.\n\n* Chore: Rename core configuration arguments to HF default in xLSTM.\n\n* Chore: Fix formatting.\n\n* Fix: xLSTM Cache config access.\n\n* Fix: Update xlstm tests for config update.\n\n* Feat: Re-add embbeding_dim, num_blocks config options for compat with xLSTM-7B.\n\n* Fix: Configuration xLSTM python3.9 syntax.\n\n* Fix: Difference to main in test_utils.py assertion.\n\n* Fix: Bad syntax in xlstm config for python3.9.\n\n* Fix: xLSTMConfig docstring.\n\n* Fix: xLSTMConfig docstring.\n\n* Fix typing issues in xLSTM and BeiT, Paligemma.\n\n* Fix: Exclude xLSTM from test cache utils.\n\n* Chore: Fix style.\n\n* Chore: Fix format.\n\n* Chore: Remove unnecessary LayerNorm, NormLayer layer abstractions.\n\n* Chore: Remove asserts and replace with ValueErrors.\n\n* Chore: Update __init__.py structure of xLSTM.\n\n* Chore: Clean xLSTM initialization of weights.\n\n* Fix index names in modeling_xlstm.py\n\n* Update xlstm model test typing annotations.\n\n* Fix: Remove all asserts.\n\n* Revert changes to the main __init__.py\n\n* Fix: Move xLSTMCache to modeling_xlstm.py\n\n* Fix: Remove xLSTMForCausalLM mapping from modeling_auto.py\n\n* Remove xLSTMCache from dummy_pt_objects.py\n\n* Fix: Remove extended torchdynamo compilation check integrating cuda graph captures.\n\n* Revert test_cache_utils.py xLSTM change.\n\n* Fix: Move xLSTM init functions before init call.\n\n* Remove xLSTMCache from generation utils.\n\n* Fix: Clean xLSTM init functionality for recursive calls.\n\n* Fix: Move xLSTMCache before its first call.\n\n* Fix formatting.\n\n* Add partial docstring for xLSTMModel forward.\n\n* Fix xLSTMCache docstring in xLSTMModel.\n\n* Remove xLSTMCache from public documentation. Update auto_docstring.\n\n* Remove all agressive shape comments\n\n* style\n\n* Fix names\n\n* simplify\n\n* remove output_hidden_states\n\n* Update modeling_xlstm.py\n\n* Update modeling_xlstm.py\n\n* Update test_modeling_xlstm.py\n\n* Update modeling_xlstm.py\n\n* Update modeling_xlstm.py\n\n* fix\n\n* fix\n\n* style\n\n* style\n\n---------\n\nCo-authored-by: Korbinian Poeppel <korbinian.poeppel@nx-ai.com>\nCo-authored-by: Korbinian Pöppel <37810656+kpoeppel@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Sebastian Böck <sebastian.boeck@nx-ai.com>\nCo-authored-by: Korbinian Poeppel <poeppel@ml.jku.at>",
    "sha": "6630c5b714048206ce51306fdcff479803f7f4c2",
    "files": [
        {
            "sha": "f7ba50a22d83d37b7b9311fe2a61ce96e799722d",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -697,6 +697,8 @@\n         title: XLM-V\n       - local: model_doc/xlnet\n         title: XLNet\n+      - local: model_doc/xlstm\n+        title: xLSTM\n       - local: model_doc/yoso\n         title: YOSO\n       - local: model_doc/zamba"
        },
        {
            "sha": "ba47a5a97cb6d128b824f6e35d5babc5db55ad7e",
            "filename": "docs/source/en/model_doc/xlstm.md",
            "status": "added",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlstm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlstm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxlstm.md?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -0,0 +1,47 @@\n+<!--Copyright 2025 NXAI GmbH. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+# xLSTM\n+\n+## Overview\n+\n+The xLSTM model was proposed in [xLSTM: Extended Long Short-Term Memory](https://openreview.net/forum?id=ARAxPPIAhq) by Maximilian Beck*, Korbinian Pöppel*, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter and Sepp Hochreiter.\n+xLSTM updates the original LSTM architecture to be competitive with Transformer models by introducing exponential gating, matrix memory expansion, and parallelizable training and ingestion.\n+\n+The [7B model](https://hf.co/NX-AI/xLSTM-7b) variant was trained by the xLSTM team Maximilian Beck, Korbinian Pöppel, Phillip Lippe, Richard Kurle, Patrick Blies, Sebastian Böck and Sepp Hochreiter at NXAI.\n+\n+The abstract from the paper is the following:\n+\n+*In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.*\n+\n+This model was contributed by [NX-AI](https://huggingface.co/NX-AI).\n+The original code can be found [here](https://github.com/NX-AI/xlstm).\n+\n+\n+## xLSTMConfig\n+\n+[[autodoc]] xLSTMConfig\n+\n+## xLSTMModel\n+\n+[[autodoc]] xLSTMModel\n+    - forward\n+\n+## xLSTMLMHeadModel\n+\n+[[autodoc]] xLSTMForCausalLM\n+    - forward"
        },
        {
            "sha": "3670833bf6aa744024f7a7a085130a2f9fb5c332",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -353,6 +353,7 @@\n     from .xlm_roberta import *\n     from .xlm_roberta_xl import *\n     from .xlnet import *\n+    from .xlstm import *\n     from .xmod import *\n     from .yolos import *\n     from .yoso import *"
        },
        {
            "sha": "0317832fd74c5a6efbbdedc8cd328fc80a80fa47",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -410,6 +410,7 @@\n         (\"xlm-roberta\", \"XLMRobertaConfig\"),\n         (\"xlm-roberta-xl\", \"XLMRobertaXLConfig\"),\n         (\"xlnet\", \"XLNetConfig\"),\n+        (\"xlstm\", \"xLSTMConfig\"),\n         (\"xmod\", \"XmodConfig\"),\n         (\"yolos\", \"YolosConfig\"),\n         (\"yoso\", \"YosoConfig\"),\n@@ -832,6 +833,7 @@\n         (\"xlnet\", \"XLNet\"),\n         (\"xls_r\", \"XLS-R\"),\n         (\"xlsr_wav2vec2\", \"XLSR-Wav2Vec2\"),\n+        (\"xlstm\", \"xLSTM\"),\n         (\"xmod\", \"X-MOD\"),\n         (\"yolos\", \"YOLOS\"),\n         (\"yoso\", \"YOSO\"),"
        },
        {
            "sha": "cc779b79a1210c4a8b1a0802ed33d29a32db95e3",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -379,6 +379,7 @@\n         (\"xlm-roberta\", \"XLMRobertaModel\"),\n         (\"xlm-roberta-xl\", \"XLMRobertaXLModel\"),\n         (\"xlnet\", \"XLNetModel\"),\n+        (\"xlstm\", \"xLSTMModel\"),\n         (\"xmod\", \"XmodModel\"),\n         (\"yolos\", \"YolosModel\"),\n         (\"yoso\", \"YosoModel\"),\n@@ -474,6 +475,7 @@\n         (\"xlm-roberta\", \"XLMRobertaForMaskedLM\"),\n         (\"xlm-roberta-xl\", \"XLMRobertaXLForMaskedLM\"),\n         (\"xlnet\", \"XLNetLMHeadModel\"),\n+        (\"xlstm\", \"xLSTMForCausalLM\"),\n         (\"xmod\", \"XmodForMaskedLM\"),\n     ]\n )\n@@ -692,6 +694,7 @@\n         (\"xlm-roberta\", \"XLMRobertaForCausalLM\"),\n         (\"xlm-roberta-xl\", \"XLMRobertaXLForCausalLM\"),\n         (\"xlnet\", \"XLNetLMHeadModel\"),\n+        (\"xlstm\", \"xLSTMForCausalLM\"),\n         (\"xmod\", \"XmodForCausalLM\"),\n         (\"zamba\", \"ZambaForCausalLM\"),\n         (\"zamba2\", \"Zamba2ForCausalLM\"),"
        },
        {
            "sha": "3747597f3ed8964ebc420c09ce04283b3dbf25ff",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -718,6 +718,7 @@\n                 \"XLNetTokenizerFast\" if is_tokenizers_available() else None,\n             ),\n         ),\n+        (\"xlstm\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"xmod\",\n             ("
        },
        {
            "sha": "00e206973a908d82e9b8fe38a3d016fcced9ecd3",
            "filename": "src/transformers/models/xlstm/__init__.py",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fxlstm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fxlstm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2F__init__.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -0,0 +1,31 @@\n+# Copyright 2025 NXAI GmbH. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import (\n+    OptionalDependencyNotAvailable,\n+    _LazyModule,\n+    is_torch_available,\n+)\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from configuration_xlstm import *\n+    from modeling_xlstm import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "80c513adde69d3511bea2ede9c1834b45e1eb166",
            "filename": "src/transformers/models/xlstm/configuration_xlstm.py",
            "status": "added",
            "additions": 302,
            "deletions": 0,
            "changes": 302,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fxlstm%2Fconfiguration_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fxlstm%2Fconfiguration_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fconfiguration_xlstm.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -0,0 +1,302 @@\n+# Copyright 2025 NXAI GmbH. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+\"\"\"xLSTM configuration.\"\"\"\n+\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import is_xlstm_available, logging\n+\n+\n+if is_xlstm_available():\n+    from xlstm.xlstm_large.model import (\n+        BackendModeType,\n+        ChunkwiseKernelType,\n+        DtypeType,\n+        SequenceKernelType,\n+        StepKernelType,\n+        WeightModeType,\n+        round_up_to_next_multiple_of,\n+        xLSTMLargeConfig,\n+    )\n+\n+    external_xlstm = True\n+else:\n+    from typing import Literal\n+\n+    BackendModeType = Literal[\"train\", \"train_with_padding\", \"inference\"]\n+    ChunkwiseKernelType = Literal[\n+        \"chunkwise--native_autograd\",\n+        \"parallel--native_autograd\",\n+    ]\n+    DtypeType = Literal[\"float32\", \"bfloat16\", \"float16\"]\n+    SequenceKernelType = Literal[\"native_sequence__native\"]\n+    StepKernelType = Literal[\"native\"]\n+    WeightModeType = Literal[\"single\", \"fused\"]\n+\n+    def round_up_to_next_multiple_of(x: int, multiple_of: int) -> int:\n+        \"\"\"Rounds up x to the next multiple of multiple_of.\"\"\"\n+        return int(((x + multiple_of - 1) // multiple_of) * multiple_of)\n+\n+    external_xlstm = False\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class xLSTMConfig(PretrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`xLSTM`]. It is used to instantiate a xLSTM\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the xLSTM-7b [NX-AI/xLSTM-7b](https://huggingface.co/NX-AI/xLSTM-7b) model.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (int, optional, *optional*, defaults to 50304):\n+            Vocabulary size of the xLSTM model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`xLSTMModel`]. Defaults to the GPT2-NeoX tokenizer size.\n+        hidden_size (int, optional, *optional*, defaults to 4096):\n+            Dimensionality of the embeddings or hidden states.\n+        embedding_dim (int, optional, *optional*, defaults to 4096):\n+            Dimensionality of the embeddings or hidden states, use hidde_size if None.\n+        num_hidden_layers (int, optional, *optional*, defaults to 32):\n+            Number of blocks of the xLSTM model.\n+        num_blocks (int, optional, *optional*, defaults to 32):\n+            Number of blocks of the xLSTM model, use num_hidden_layers if None.\n+        num_heads (int, optional, *optional*, defaults to 8):\n+            Number of heads for the xLSTM Layer/Cell.\n+        use_bias (bool, optional, *optional*, defaults to `False`):\n+            Whether to use biases in the xLSTM model.\n+        norm_reduction_force_float32 (bool, optional, *optional*, defaults to `True`):\n+            Whether to force the float32 norm reduction op to be done in fp32 precision.\n+        tie_word_embeddings (bool, optional, *optional*, defaults to `False`):\n+            Whether to tie word embeddings to the lm head weights.\n+        add_out_norm (bool, optional, *optional*, defaults to `True`):\n+            Whether to add an output norm after the blocks before the LMHead.\n+        norm_eps (float, optional, *optional*, defaults to 1e-06):\n+            Norm eps for RMSNorm and Layer Norm.\n+        qk_dim_factor (float, optional, *optional*, defaults to 0.5):\n+            Scale factor for the query and key dimension.\n+        v_dim_factor (float, optional, *optional*, defaults to 1.0):\n+            Scale factor for the value dimension.\n+        chunkwise_kernel (ChunkwiseKernelType, optional, *optional*, defaults to `\"chunkwise--native_autograd\"`):\n+            Kernel type for chunkwise processing mode.\n+        sequence_kernel (SequenceKernelType, optional, *optional*, defaults to `\"native_sequence__native\"`):\n+            Kernel type for sequence processing mode.\n+        step_kernel (StepKernelType, optional, *optional*, defaults to `\"native\"`):\n+            Kernel type for step processing mode.\n+        mode (BackendModeType, optional, *optional*, defaults to `\"inference\"`):\n+            Operation mode (inference is needed for generation).\n+        chunk_size (int, optional, *optional*, defaults to 64):\n+            Internal chunk size.\n+        return_last_states (bool, optional, *optional*, defaults to `True`):\n+            If to return the last states / cache internally. Needed as True for generation.\n+        autocast_kernel_dtype (DtypeType, optional, *optional*, defaults to `\"bfloat16\"`):\n+            Kernel dtype for the states.\n+        eps (float, optional, *optional*, defaults to 1e-06):\n+            Epsilon for the mLSTM cell post norm.\n+        inference_state_dtype (DtypeType, optional, *optional*, defaults to `\"float32\"`):\n+            Kernel dtype for states in inference.\n+        ffn_proj_factor (float, optional, *optional*, defaults to 2.667):\n+            Size factor of the post-up projection gated Feed Forward network.\n+        ffn_round_up_to_multiple_of (int, optional, *optional*, defaults to 64):\n+            Size factor round value of the post-up projection gated Feed Forward network.\n+        gate_soft_cap (float, optional, *optional*, defaults to 15.0):\n+            Gate soft cap scale.\n+        output_logit_soft_cap (float, optional, *optional*, defaults to 30.0):\n+            Output logit soft cap scale.\n+        weight_mode (`Literal`, *optional*, defaults to `\"single\"`):\n+            Whether parallel linear layers are separated or fused (single).\n+        use_cache (bool, optional, *optional*, defaults to `True`):\n+            Whether to use the cache (xLSTMCache).\n+        pad_token_id (int, optional, *optional*, defaults to 1):\n+            Pad token id needed for generation.\n+        bos_token_id (int, optional, *optional*, defaults to 0):\n+            BOS token id needed for generation.\n+        eos_token_id (int, optional, *optional*, defaults to 2):\n+            EOS token id needed for generation.\n+        max_inference_chunksize (int, optional, *optional*, defaults to 16384):\n+            Limit the chunk size for inference to save memory.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import xLSTMConfig, xLSTMModel\n+\n+    >>> # Initializing a xLSTM configuration\n+    >>> configuration = xLSTMConfig()\n+\n+    >>> # Initializing a model (with random weights) from the configuration\n+    >>> model = xLSTMModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"xlstm\"\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 50304,\n+        hidden_size: int = 4096,\n+        embedding_dim: Optional[int] = None,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_blocks: Optional[int] = None,\n+        num_heads: int = 8,\n+        use_bias: bool = False,\n+        norm_reduction_force_float32: bool = True,\n+        tie_word_embeddings: bool = False,\n+        add_out_norm: bool = True,\n+        norm_eps: float = 1e-6,\n+        # mlstm_layer\n+        qk_dim_factor: float = 0.5,\n+        v_dim_factor: float = 1.0,\n+        # mlstm backend\n+        chunkwise_kernel: ChunkwiseKernelType = \"chunkwise--native_autograd\",\n+        sequence_kernel: SequenceKernelType = \"native_sequence__native\",\n+        step_kernel: StepKernelType = \"native\",\n+        # nedded to enable generation\n+        mode: BackendModeType = \"inference\",\n+        chunk_size: int = 64,\n+        # needed to be true for generation\n+        return_last_states: bool = True,\n+        autocast_kernel_dtype: DtypeType = \"bfloat16\",\n+        eps: float = 1e-6,\n+        inference_state_dtype: DtypeType = \"float32\",\n+        # feedforward\n+        ffn_proj_factor: float = 2.667,\n+        ffn_round_up_to_multiple_of: int = 64,\n+        # capping\n+        gate_soft_cap: float = 15.0,\n+        output_logit_soft_cap: float = 30.0,\n+        # weights\n+        weight_mode: WeightModeType = \"single\",\n+        # HF interface\n+        use_cache: bool = True,\n+        pad_token_id: int = 1,\n+        bos_token_id: int = 0,\n+        eos_token_id: int = 2,\n+        max_inference_chunksize: int = 16384,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size if hidden_size is not None else embedding_dim\n+        self.embedding_dim = embedding_dim if embedding_dim is not None else hidden_size\n+        self.num_hidden_layers = num_hidden_layers if num_hidden_layers is not None else num_blocks\n+        self.num_blocks = num_blocks if num_blocks is not None else num_hidden_layers\n+        self.num_heads = num_heads\n+        self.use_bias = use_bias\n+        self.tie_word_embeddings = tie_word_embeddings\n+        self.add_out_norm = add_out_norm\n+        self.norm_eps = norm_eps\n+        self.norm_reduction_force_float32 = norm_reduction_force_float32\n+        # mlstm_layer\n+        self.qk_dim_factor = qk_dim_factor\n+        self.v_dim_factor = v_dim_factor\n+        # mlstm backend\n+        self.chunkwise_kernel = chunkwise_kernel\n+        self.sequence_kernel = sequence_kernel\n+        self.step_kernel = step_kernel\n+        self.mode = mode\n+        self.chunk_size = chunk_size\n+        self.return_last_states = return_last_states\n+        self.autocast_kernel_dtype = autocast_kernel_dtype\n+        self.eps = eps\n+        self.inference_state_dtype = inference_state_dtype\n+        # feedforward\n+        self.ffn_proj_factor = ffn_proj_factor\n+        self.ffn_round_up_to_multiple_of = ffn_round_up_to_multiple_of\n+        # capping\n+        self.gate_soft_cap = gate_soft_cap\n+        self.output_logit_soft_cap = output_logit_soft_cap\n+        self.weight_mode = weight_mode\n+\n+        self.use_cache = use_cache\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+        self.max_inference_chunksize = max_inference_chunksize\n+\n+        super().__init__(\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            pad_token_id=pad_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+    @property\n+    def qk_dim(self):\n+        return round_up_to_next_multiple_of(\n+            self.hidden_size * self.qk_dim_factor,\n+            multiple_of=64,\n+        )\n+\n+    @property\n+    def v_dim(self):\n+        return round_up_to_next_multiple_of(\n+            self.hidden_size * self.v_dim_factor,\n+            multiple_of=64,\n+        )\n+\n+    @property\n+    def qk_head_dim(self):\n+        return self.qk_dim // self.num_heads\n+\n+    @property\n+    def v_head_dim(self):\n+        return self.v_dim // self.num_heads\n+\n+    def to_xlstm_block_config(self):\n+        if external_xlstm:\n+            return xLSTMLargeConfig(\n+                vocab_size=self.vocab_size,\n+                embedding_dim=self.hidden_size,\n+                num_blocks=self.num_hidden_layers,\n+                num_heads=self.num_heads,\n+                use_bias=self.use_bias,\n+                add_out_norm=self.add_out_norm,\n+                norm_eps=self.norm_eps,\n+                norm_reduction_force_float32=self.norm_reduction_force_float32,\n+                # mlstm_layer\n+                qk_dim_factor=self.qk_dim_factor,\n+                v_dim_factor=self.v_dim_factor,\n+                # mlstm backend\n+                chunkwise_kernel=self.chunkwise_kernel,\n+                sequence_kernel=self.sequence_kernel,\n+                step_kernel=self.step_kernel,\n+                mode=self.mode,\n+                chunk_size=self.chunk_size,\n+                return_last_states=self.return_last_states,\n+                autocast_kernel_dtype=self.autocast_kernel_dtype,\n+                eps=self.eps,\n+                inference_state_dtype=self.inference_state_dtype,\n+                # feedforward\n+                ffn_proj_factor=self.ffn_proj_factor,\n+                ffn_round_up_to_multiple_of=self.ffn_round_up_to_multiple_of,\n+                # capping\n+                gate_soft_cap=self.gate_soft_cap,\n+                output_logit_soft_cap=self.output_logit_soft_cap,\n+                weight_mode=self.weight_mode,\n+            )\n+        else:\n+            return self\n+\n+\n+__all__ = [\"xLSTMConfig\"]"
        },
        {
            "sha": "2d2d3a736ab90c288d0d84bb08a6d134225ef7c7",
            "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
            "status": "added",
            "additions": 1623,
            "deletions": 0,
            "changes": 1623,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -0,0 +1,1623 @@\n+# Copyright 2025 NXAI GmbH. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch xLSTM Model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+from torch.nn import CrossEntropyLoss\n+\n+from ...generation import GenerationMixin\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_xlstm_available\n+from .configuration_xlstm import xLSTMConfig\n+\n+\n+if is_xlstm_available():\n+    from xlstm.xlstm_large.model import mLSTMBlock as xLSTMBlock\n+    from xlstm.xlstm_large.model import mLSTMStateType, soft_cap\n+    from xlstm.xlstm_large.model import xLSTMRMSNorm as xLSTMRMSNorm\n+\n+    external_xlstm = True\n+else:\n+    from functools import partial\n+    from typing import Callable, Literal\n+\n+    from .configuration_xlstm import round_up_to_next_multiple_of\n+\n+    mLSTMLayerStateType = tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n+    mLSTMStateType = dict[int, mLSTMLayerStateType]\n+\n+    external_xlstm = False\n+\n+    def soft_cap(values: torch.Tensor, cap_value: Optional[Union[float, torch.Tensor]] = None) -> torch.Tensor:\n+        \"\"\"\n+        Soft caps a tensor to a value.\n+\n+        Performs a tanh operation on the logits and scales the result to the cap value. Common technique in attention\n+        and output language heads to prevent large logits from dominating the softmax. See for example Gemma2:\n+        https://arxiv.org/abs/2408.00118\n+\n+        Args:\n+            values: The tensor to cap.\n+            cap_value: The value to cap the values to. If None, no cap is applied.\n+\n+        Returns:\n+            The capped values.\n+        \"\"\"\n+        if cap_value is None:\n+            return values\n+        return cap_value * torch.tanh(values / cap_value)\n+\n+    def mlstm_chunkwise_recurrent_fw_C(\n+        matK: torch.Tensor,\n+        matV: torch.Tensor,\n+        vecB: torch.Tensor,\n+        vecI: torch.Tensor,\n+        matC_states: torch.Tensor = None,\n+        vecN_states: torch.Tensor = None,\n+        scaMinter_states: torch.Tensor = None,\n+        matC_initial: torch.Tensor = None,\n+        vecN_initial: torch.Tensor = None,\n+        scaMinter_initial: torch.Tensor = None,\n+        qk_scale: Optional[float] = None,\n+        chunk_size: int = 64,\n+        num_chunks: int = 1,\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        batch_size, nh, _, dhqk, dhhv = *matK.shape, matV.shape[-1]\n+        nc = num_chunks\n+        _dtype, _device = matK.dtype, matK.device\n+\n+        if qk_scale is None:\n+            qk_scale = dhqk**-0.5\n+\n+        # initialize the states tensors\n+        if matC_states is None:\n+            matC_states = torch.zeros((batch_size, nh, (nc + 1) * dhqk, dhhv), dtype=_dtype, device=_device)\n+        if vecN_states is None:\n+            vecN_states = torch.zeros((batch_size, nh, (nc + 1) * dhqk), dtype=_dtype, device=_device)\n+        if scaMinter_states is None:\n+            scaMinter_states = torch.zeros((batch_size, nh, (nc + 1)), dtype=_dtype, device=_device)\n+\n+        # assign the initial states to the running states\n+        matC_k = (\n+            torch.zeros((batch_size, nh, dhqk, dhhv), dtype=_dtype, device=_device)\n+            if matC_initial is None\n+            else matC_initial\n+        )\n+        vecN_k = (\n+            torch.zeros((batch_size, nh, dhqk), dtype=_dtype, device=_device) if vecN_initial is None else vecN_initial\n+        )\n+        scaM_inter_k = (\n+            torch.zeros((batch_size, nh, 1), dtype=_dtype, device=_device)\n+            if scaMinter_initial is None\n+            else scaMinter_initial\n+        )\n+        vecA = vecB[..., -1, None] - vecB + vecI\n+        scaG = vecB[..., -1]\n+        scaA_max = vecA.max(-1).values\n+\n+        scaM_inter_k = scaM_inter_k.squeeze(-1)\n+\n+        for key in range(0, num_chunks):\n+            # store the states from the previous iteration before updating them\n+            # in the first iteration, these are the initial states\n+            matC_states[:, :, key * dhqk : (key + 1) * dhqk, :] = matC_k\n+            vecN_states[:, :, key * dhqk : (key + 1) * dhqk] = vecN_k\n+            scaMinter_states[:, :, key] = scaM_inter_k\n+\n+            # m_k update\n+            scaA_max_k = scaA_max[:, :, key]\n+            scaG_k = scaG[:, :, key]\n+            scaM_inter_k_next = torch.max(scaG_k + scaM_inter_k, scaA_max_k)\n+            # C_k update\n+            matK_chunk = matK[:, :, key * chunk_size : (key + 1) * chunk_size, :]  # * qk_scale\n+            matV_chunk = matV[:, :, key * chunk_size : (key + 1) * chunk_size, :]\n+            vecA_k = vecA[:, :, key, :]\n+\n+            vecAbar_k = torch.exp(vecA_k - scaM_inter_k_next[..., None])[:, :, :, None]\n+\n+            matK_chunk_gated = matK_chunk * vecAbar_k\n+\n+            scaGbar_k = torch.exp(scaG_k + scaM_inter_k - scaM_inter_k_next)[:, :, None]\n+\n+            # NOTE: no update in-place (i.e. +=) as this gives error for autograd backward\n+            matC_k_next = scaGbar_k[..., None] * matC_k + matK_chunk_gated.transpose(-2, -1) @ (matV_chunk)\n+\n+            # n_k update\n+            vecN_k_next = scaGbar_k * vecN_k + matK_chunk_gated.transpose(-2, -1).sum(-1)\n+\n+            # move to the next iteration\n+            scaM_inter_k = scaM_inter_k_next\n+            matC_k = matC_k_next\n+            vecN_k = vecN_k_next\n+\n+        # store the states from the last iteration\n+        matC_states[:, :, -dhqk:, :] = matC_k\n+        vecN_states[:, :, -dhqk:] = vecN_k\n+        scaMinter_states[:, :, -1] = scaM_inter_k\n+\n+        return matC_states, vecN_states, scaMinter_states\n+\n+    def mlstm_chunkwise_parallel_fw_H(\n+        matQ: torch.Tensor,\n+        matK: torch.Tensor,\n+        matV: torch.Tensor,\n+        # these states must be all states up to the last chunk, i.e. :-1\n+        matC_states: torch.Tensor,\n+        vecN_states: torch.Tensor,\n+        scaMinter_states: torch.Tensor,\n+        vecI: torch.Tensor,\n+        vecB: torch.Tensor,\n+        qk_scale: float,\n+        chunk_size: int = 64,\n+        num_chunks: int = 1,\n+        eps: float = 1e-6,\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        _device = matQ.device\n+        nc, chunk_size = num_chunks, chunk_size\n+        batch_size, nh, dqk, dhv = matC_states.shape\n+        matC_k_states = matC_states.view(batch_size, nh, nc, dqk // nc, dhv)\n+        vecN_k_states = vecN_states.view(batch_size, nh, nc, dqk // nc)\n+        scaMinter_k_states = scaMinter_states\n+\n+        matQ = matQ.view(batch_size, nh, nc, chunk_size, dqk)\n+        matK = matK.view(batch_size, nh, nc, chunk_size, dqk)\n+        matV = matV.view(batch_size, nh, nc, chunk_size, dhv)\n+\n+        ltr = torch.tril(\n+            torch.ones(\n+                (chunk_size, chunk_size),\n+                dtype=torch.bool,\n+                device=_device,\n+            )\n+        )\n+\n+        # Compute intra chunk contribution: H_intra\n+        matF_logsig_chunk = vecB[:, :, :, :, None] - vecB[:, :, :, None, :]\n+\n+        matF_logsig_mask_chunk = torch.where(ltr, matF_logsig_chunk, -float(\"inf\"))\n+\n+        matLogD_chunk = matF_logsig_mask_chunk + vecI[:, :, :, None, :]\n+\n+        # max_state intra\n+        vecMintra_k = torch.max(matLogD_chunk, dim=-1, keepdim=False).values\n+\n+        # max_state combined\n+        vecM_b_inter = vecB + scaMinter_k_states[:, :, :, None]\n+        vecM_k_combine = torch.maximum(vecM_b_inter, vecMintra_k)\n+\n+        vecM_k_combine = vecM_k_combine[:, :, :, :, None]\n+        vecM_b_inter = vecM_b_inter[:, :, :, :, None]\n+\n+        matLogD_stabilized_chunk = matLogD_chunk - vecM_k_combine\n+        matD_chunk = torch.exp(matLogD_stabilized_chunk)\n+\n+        matS_chunk = (matQ @ matK.transpose(-2, -1)) * qk_scale\n+\n+        matM_chunk = matS_chunk * matD_chunk\n+\n+        # ? Combine H_intra with H_inter\n+        vecBbar = torch.exp(vecM_b_inter - vecM_k_combine)\n+        matQ_chunk_gated = matQ * vecBbar * qk_scale\n+\n+        matNumerator_common = matQ_chunk_gated @ matC_k_states + matM_chunk @ matV\n+\n+        vecDenom_l_common = matQ_chunk_gated @ vecN_k_states.unsqueeze(-1) + matM_chunk.sum(dim=-1, keepdim=True)\n+\n+        vecDenom_max_common = torch.maximum(torch.abs(vecDenom_l_common), torch.exp(-vecM_k_combine))\n+\n+        matH_k_chunk = matNumerator_common / (vecDenom_max_common + eps)\n+\n+        matH_out = matH_k_chunk.view(batch_size, nh, nc * chunk_size, dhv)\n+\n+        # we need the denominator and the overall max state for the backward pass\n+        vecN_out = vecDenom_max_common.reshape(batch_size, nh, nc * chunk_size)\n+        vecM_out = vecM_k_combine(batch_size, nh, nc * chunk_size)\n+        return matH_out, vecN_out, vecM_out\n+\n+    def mlstm_chunkwise_fw(\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        igate: torch.Tensor,\n+        fgate: torch.Tensor,\n+        cstate: torch.Tensor = None,\n+        nstate: torch.Tensor = None,\n+        mstate: torch.Tensor = None,\n+        qk_scale: Optional[float] = None,\n+        return_last_states: bool = False,\n+        return_all_states: bool = False,\n+        chunk_size: int = 64,\n+        eps: float = 1e-6,\n+    ) -> tuple[\n+        torch.Tensor,\n+        torch.Tensor,\n+        torch.Tensor,\n+        Optional[tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n+        Optional[tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n+    ]:\n+        batch_size, nh, sequence_length, dhqk = query.shape\n+        if sequence_length % chunk_size != 0:\n+            raise ValueError(f\"Sequence length {sequence_length} is not divisible by chunk size {chunk_size}.\")\n+        nc = sequence_length // chunk_size\n+\n+        vecI = igate.view(batch_size, nh, nc, chunk_size)\n+        vecF = fgate.view(batch_size, nh, nc, chunk_size)\n+\n+        # compute the gates, the g and the a and b vectors\n+        vecF_logsig = fgate.logsigmoid(vecF)\n+        vecB = vecF_logsig.cumsum(-1)\n+\n+        if qk_scale is None:\n+            qk_scale = dhqk**-0.5\n+\n+        #! materialize the  C_k, n_k, m_k states for each chunk\n+        matC_k_states, vecN_k_states, scaMinter_k_states = mlstm_chunkwise_recurrent_fw_C(\n+            matK=key,\n+            matV=value,\n+            vecB=vecB,\n+            vecI=vecI,\n+            matC_initial=cstate,\n+            vecN_initial=nstate,\n+            scaMinter_initial=mstate,\n+            qk_scale=qk_scale,\n+            chunk_size=chunk_size,\n+            num_chunks=nc,\n+        )\n+\n+        #! compute the outputs within each chunk\n+        matH_out, vecN_out, vecM_out = mlstm_chunkwise_parallel_fw_H(\n+            matQ=query,\n+            matK=key,\n+            matV=value,\n+            matC_states=matC_k_states[:, :, :-dhqk, :],\n+            vecN_states=vecN_k_states[:, :, :-dhqk],\n+            scaMinter_states=scaMinter_k_states[:, :, :-1],\n+            vecI=vecI,\n+            vecB=vecB,\n+            qk_scale=qk_scale,\n+            chunk_size=chunk_size,\n+            num_chunks=nc,\n+            eps=eps,\n+        )\n+\n+        ret_tuple = (matH_out, vecN_out, vecM_out)\n+        if return_last_states:\n+            ret_tuple += (\n+                (matC_k_states[:, :, -dhqk:, :], vecN_k_states[:, :, -dhqk:], scaMinter_k_states[:, :, -1:]),\n+            )\n+        else:\n+            ret_tuple += (None,)\n+\n+        if return_all_states:\n+            ret_tuple += ((matC_k_states, vecN_k_states, scaMinter_k_states),)\n+        else:\n+            ret_tuple += (None,)\n+\n+        return ret_tuple\n+\n+    def mlstm_chunkwise_native_autograd(\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        igate: torch.Tensor,\n+        fgate: torch.Tensor,\n+        c_initial: torch.Tensor = None,\n+        n_initial: torch.Tensor = None,\n+        m_initial: torch.Tensor = None,\n+        return_last_states: bool = False,\n+        eps: float = 1e-6,\n+        chunk_size: int = 64,\n+        **kwargs,\n+    ) -> Union[torch.Tensor, tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]:\n+        batch_size, nh, sequence_length, dhqk = query.shape\n+        if sequence_length % chunk_size != 0:\n+            raise ValueError(f\"Sequence length {sequence_length} is not divisible by chunk size {chunk_size}.\")\n+        nc = sequence_length // chunk_size\n+\n+        vecI = igate.view(batch_size, nh, nc, chunk_size)\n+        vecF = fgate.view(batch_size, nh, nc, chunk_size)\n+\n+        # compute the gates, the g and the a and b vectors\n+        vecF_logsig = F.logsigmoid(vecF)\n+        vecB = vecF_logsig.cumsum(-1)\n+\n+        qk_scale = dhqk**-0.5\n+\n+        #! materialize the  C_k, n_k, m_k states for each chunk\n+        matC_k_states, vecN_k_states, scaMinter_k_states = mlstm_chunkwise_recurrent_fw_C(\n+            matK=key,\n+            matV=value,\n+            vecB=vecB,\n+            vecI=vecI,\n+            matC_initial=c_initial,\n+            vecN_initial=n_initial,\n+            scaMinter_initial=m_initial,\n+            qk_scale=qk_scale,\n+            chunk_size=chunk_size,\n+            num_chunks=nc,\n+        )\n+\n+        #! compute the outputs within each chunk\n+        matH_out, vecN_out, vecM_out = mlstm_chunkwise_parallel_fw_H(\n+            matQ=query,\n+            matK=key,\n+            matV=value,\n+            matC_states=matC_k_states[:, :, :-dhqk, :],\n+            vecN_states=vecN_k_states[:, :, :-dhqk],\n+            scaMinter_states=scaMinter_k_states[:, :, :-1],\n+            vecI=vecI,\n+            vecB=vecB,\n+            qk_scale=qk_scale,\n+            chunk_size=chunk_size,\n+            num_chunks=nc,\n+            eps=eps,\n+        )\n+\n+        last_states = (matC_k_states[:, :, -dhqk:, :], vecN_k_states[:, :, -dhqk:], scaMinter_k_states[:, :, -1:])\n+\n+        if return_last_states:\n+            return matH_out, last_states\n+        else:\n+            return matH_out\n+\n+    def mlstm_recurrent_step_native(\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        igate: torch.Tensor,\n+        fgate: torch.Tensor,\n+        cstate: torch.Tensor,\n+        nstate: torch.Tensor,\n+        mstate: torch.Tensor,\n+        eps: float = 1e-6,\n+        dtype_state: torch.dtype = torch.float32,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]:\n+        \"\"\"This is a single step of the mLSTM operation in recurrent form.\"\"\"\n+        dtype_qkv = query.dtype\n+        matC_old = cstate.to(dtype=dtype_state)\n+        vecN_old = nstate.to(dtype=dtype_state)\n+        scaM_old = mstate.to(dtype=dtype_state)\n+\n+        batch_size, nh, dhqk = query.shape\n+        _, _, dhhv = value.shape\n+        if query.shape != key.shape:\n+            raise ValueError(\"query and key must have the same shape\")\n+        if matC_old.shape != (batch_size, nh, dhqk, dhhv):\n+            raise ValueError(f\"matC_old has wrong shape, got {matC_old.shape}\")\n+        if vecN_old.shape != (batch_size, nh, dhqk):\n+            raise ValueError(f\"vecN_old has wrong shape, got {vecN_old.shape}\")\n+        if scaM_old.shape != (batch_size, nh, 1):\n+            raise ValueError(f\"scaM_old has wrong shape, got {scaM_old.shape}\")\n+        if igate.shape != (batch_size, nh, 1):\n+            raise ValueError(f\"scaI has wrong shape, got {igate.shape}\")\n+        if fgate.shape != (batch_size, nh, 1):\n+            raise ValueError(f\"scaF has wrong shape, got {fgate.shape}\")\n+\n+        # gates\n+        scaF_log = torch.nn.functional.logsigmoid(fgate)\n+\n+        # update rule\n+        scaM_state_new = torch.max(scaF_log + scaM_old, igate)\n+\n+        scaF_act = torch.exp(scaF_log + scaM_old - scaM_state_new)\n+        scaI_act = torch.exp(igate - scaM_state_new)\n+\n+        vecQ_scaled = query * (dhqk ** (-0.5))\n+        matC_state_new = scaF_act[:, :, :, None] * matC_old + scaI_act[:, :, :, None] * (\n+            key[:, :, :, None] @ value[:, :, None, :]\n+        )\n+        vecN_state_new = scaF_act * vecN_old + scaI_act * key\n+        h_num = vecQ_scaled[:, :, None, :] @ matC_state_new.to(dtype=dtype_qkv)\n+        h_num = h_num.squeeze(2).to(dtype=dtype_state)\n+\n+        qn_dotproduct = vecQ_scaled[:, :, None, :] @ vecN_state_new[:, :, :, None].to(dtype=dtype_qkv)\n+        qn_dotproduct = qn_dotproduct.squeeze(2)\n+        max_val = torch.exp(-scaM_state_new)\n+        h_denom = (torch.maximum(qn_dotproduct.abs(), max_val) + eps).to(dtype=dtype_state)\n+        h = h_num / h_denom\n+\n+        h = h.to(dtype=dtype_qkv)\n+        matC_state_new = matC_state_new.to(dtype=dtype_state)\n+        vecN_state_new = vecN_state_new.to(dtype=dtype_state)\n+        scaM_state_new = scaM_state_new.to(dtype=dtype_state)\n+        return h, (matC_state_new, vecN_state_new, scaM_state_new)\n+\n+    def mlstm_recurrent_sequence_native(\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        igate: torch.Tensor,\n+        fgate: torch.Tensor,\n+        c_initial: torch.Tensor = None,\n+        n_initial: torch.Tensor = None,\n+        m_initial: torch.Tensor = None,\n+        return_last_states: bool = False,\n+        eps: float = 1e-6,\n+        dtype_state: torch.dtype = torch.float32,\n+        **kwargs,\n+    ) -> tuple[\n+        torch.Tensor,\n+        torch.Tensor,\n+        torch.Tensor,\n+        Optional[tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n+        Optional[tuple[torch.Tensor, torch.Tensor, torch.Tensor]],\n+    ]:\n+        batch_size, nh, sequence_length, dhqk = query.shape\n+        dhv = value.shape[-1]\n+        device = query.device\n+\n+        if c_initial is not None:\n+            if n_initial is None or m_initial is None:\n+                raise ValueError(\"Initial states must be provided together.\")\n+            if n_initial is None or m_initial is None:\n+                raise ValueError(\"Initial states must be provided together.\")\n+            matC_state, vecN_state, vecM_state = (\n+                c_initial.to(dtype=dtype_state),\n+                n_initial.to(dtype=dtype_state),\n+                m_initial.to(dtype=dtype_state),\n+            )\n+        else:\n+            # memory state\n+            matC_state = torch.zeros((batch_size, nh, dhqk, dhv), dtype=dtype_state, device=device)\n+            # normalizer state\n+            vecN_state = torch.zeros((batch_size, nh, dhqk), dtype=dtype_state, device=device)\n+            # max state\n+            vecM_state = torch.zeros((batch_size, nh, 1), dtype=dtype_state, device=device)\n+\n+        vecH_list = []\n+        for t in range(sequence_length):\n+            # gates\n+            vecF_t, vecI_t = fgate[:, :, t, None], igate[:, :, t, None]\n+\n+            # projections\n+            vecQ_t, vecK_t, vecV_t = query[:, :, t, :], key[:, :, t, :], value[:, :, t, :]\n+\n+            # step\n+            vecH, (matC_state, vecN_state, vecM_state) = mlstm_recurrent_step_native(\n+                cstate=matC_state,\n+                nstate=vecN_state,\n+                mstate=vecM_state,\n+                query=vecQ_t,\n+                key=vecK_t,\n+                value=vecV_t,\n+                igate=vecI_t,\n+                fgate=vecF_t,\n+                eps=eps,\n+                dtype_state=dtype_state,\n+                **kwargs,\n+            )\n+            vecH_list.append(vecH)\n+\n+        matH = torch.stack(vecH_list, dim=-2)\n+\n+        if return_last_states:\n+            return matH, (matC_state, vecN_state, vecM_state)\n+        else:\n+            return matH\n+\n+    def wrap_chunkwise_pad_zeros(\n+        mlstm_chunkwise_kernel: Callable,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        fgate: torch.Tensor,\n+        igate: torch.Tensor,\n+        c_initial: torch.Tensor = None,\n+        n_initial: torch.Tensor = None,\n+        m_initial: torch.Tensor = None,\n+        return_last_states: bool = False,\n+        eps: float = 1e-6,\n+        autocast_kernel_dtype: torch.dtype = torch.bfloat16,\n+        chunk_size: int = 64,\n+        **kwargs,\n+    ) -> Union[torch.Tensor, tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]:\n+        if return_last_states:\n+            raise ValueError(\n+                \"We are padding zeros, so we cannot return last states,\",\n+                \"as they would be not the true last states.\",\n+            )\n+\n+        batch_size, nh, sequence_length, dhqk = query.shape\n+        S_unpadded = sequence_length\n+        # padding to chunk size for kernels\n+        if sequence_length % chunk_size != 0:\n+            S_padded = ((sequence_length + chunk_size - 1) // chunk_size) * chunk_size\n+            q_pad = query.new_zeros(batch_size, nh, S_padded, query.shape[3])\n+            k_pad = key.new_zeros(batch_size, nh, S_padded, key.shape[3])\n+            v_pad = value.new_zeros(batch_size, nh, S_padded, value.shape[3])\n+            i_pad = igate.new_zeros(batch_size, nh, S_padded)\n+            f_pad = fgate.new_zeros(batch_size, nh, S_padded)\n+            q_pad[:, :, :S_unpadded, :] = query\n+            k_pad[:, :, :S_unpadded, :] = key\n+            v_pad[:, :, :S_unpadded, :] = value\n+            i_pad[:, :, :S_unpadded] = igate\n+            f_pad[:, :, :S_unpadded] = fgate\n+        else:\n+            q_pad = query\n+            k_pad = key\n+            v_pad = value\n+            i_pad = igate\n+            f_pad = fgate\n+\n+        matH = mlstm_chunkwise_kernel(\n+            query=q_pad,\n+            key=k_pad,\n+            value=v_pad,\n+            igate=i_pad,\n+            fgate=f_pad,\n+            c_initial=c_initial,\n+            n_initial=n_initial,\n+            m_initial=m_initial,\n+            return_last_states=return_last_states,\n+            eps=eps,\n+            autocast_kernel_dtype=autocast_kernel_dtype,\n+            chunk_size=chunk_size,\n+            **kwargs,\n+        )\n+        matH = matH[:, :, :S_unpadded, :]\n+        return matH\n+\n+    def wrap_chunkwise_arbitrary_sequence_length(\n+        mlstm_chunkwise_kernel: Callable,\n+        mlstm_sequence_kernel: Callable,\n+        mlstm_step_kernel: Callable,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        fgate: torch.Tensor,\n+        igate: torch.Tensor,\n+        c_initial: torch.Tensor = None,\n+        n_initial: torch.Tensor = None,\n+        m_initial: torch.Tensor = None,\n+        return_last_states: bool = True,\n+        eps: float = 1e-6,\n+        autocast_kernel_dtype: torch.dtype = torch.bfloat16,\n+        chunk_size: int = 64,\n+        enable_logging: bool = False,\n+    ) -> Union[torch.Tensor, tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]:\n+        \"\"\"This function computes the last hidden state and matH outputs of the mLSTM, independently of the sequence length.\n+\n+        For this it uses three kernels:\n+        - mlstm_chunkwise_kernel: mlstm chunkwise kernels that processes chunks of a given chunk size in parallel.\n+        - mlstm_sequence_kernel: mlstm kernel that processes the remaining sequence length in a single step recurrence.\n+        - mlstm_step_kernel: mlstm kernel that processes a sequence length of 1 in a single step.\n+\n+        It tries to maximize the chunksizes to improve performance.\n+        It will start with the given chunk size and then divides the chunksize by 2 until the chunk size is smaller than 16.\n+        At every chunksize it will process the maximal number of chunks that fit into the remaining sequence length.\n+\n+        E.g. for chunk_size = 64, this function will try the chunksizes [64, 32, 16] if necessary.\n+\n+        For the remaining sequence length, which is smaller than 16, we use a different kernel that computes the mLSTM\n+        in a single step and loop over this in pytorch.\n+\n+        Args:\n+            mlstm_chunkwise_kernel: The mLSTM chunkwise kernel that processes chunks of a given chunk size in parallel\n+            mlstm_sequence_kernel: The mLSTM kernel that processes the remaining sequence length in a single step recurrence\n+            query: The query tensor (batch_size, nh, sequence_length, dhqk)\n+            key: The key tensor (batch_size, nh, sequence_length, dhqk)\n+            value: The value tensor (batch_size, nh, sequence_length, dhhv)\n+            fgate: The forget gate tensor (batch_size, nh, sequence_length)\n+            igate: The input gate tensor (batch_size, nh, sequence_length)\n+            c_initial: The initial cell state tensor (batch_size, nh, dhqk, dhhv)\n+            n_initial: The initial hidden state tensor (batch_size, nh, dhqk)\n+            m_initial: The initial memory state tensor (batch_size, nh, 1)\n+            return_last_states: If True, the function will return the last states of the mLSTM\n+            eps: The epsilon value used for numerical stability\n+            autocast_kernel_dtype: The dtype used for the kernel computation\n+            chunk_size: The chunk size used for the chunkwise kernel\n+            enable_logging: If True, the function will log debug information. Default is False.\n+\n+        Returns:\n+            The last hidden state tensor (batch_size, nh, sequence_length, dhhv) or a tuple containing the last hidden state tensor and the last states of the mLSTM\n+            Last states are (cstate (batch_size, nh, dhqk, dhhv), nstate (batch_size, nh, dhqk), mstate (batch_size, nh, 1)).\n+        \"\"\"\n+\n+        batch_size, nh, sequence_length, dhqk = key.shape\n+        dhhv = value.shape[-1]\n+\n+        c_state = (\n+            c_initial\n+            if c_initial is not None\n+            else torch.zeros(batch_size, nh, dhqk, dhhv, device=key.device, dtype=torch.float32)\n+        )\n+        n_state = (\n+            n_initial\n+            if n_initial is not None\n+            else torch.zeros(batch_size, nh, dhqk, device=key.device, dtype=torch.float32)\n+        )\n+        m_state = (\n+            m_initial\n+            if m_initial is not None\n+            else torch.zeros(batch_size, nh, 1, device=key.device, dtype=torch.float32)\n+        )\n+\n+        if sequence_length > 1:\n+            # process the sequence length in chunks\n+            h_outs = []\n+            seq_len_start_idx = 0\n+            remaining_seq_len = sequence_length - seq_len_start_idx\n+            num_chunks = remaining_seq_len // chunk_size\n+            if num_chunks > 0:\n+                iter_seq_len = chunk_size * num_chunks\n+                seq_len_idx = seq_len_start_idx + iter_seq_len\n+                h_out, (c_state, n_state, m_state) = mlstm_chunkwise_kernel(\n+                    query=query[..., seq_len_start_idx:seq_len_idx, :].contiguous(),\n+                    key=key[..., seq_len_start_idx:seq_len_idx, :].contiguous(),\n+                    value=value[..., seq_len_start_idx:seq_len_idx, :].contiguous(),\n+                    fgate=fgate[..., seq_len_start_idx:seq_len_idx].contiguous(),\n+                    igate=igate[..., seq_len_start_idx:seq_len_idx].contiguous(),\n+                    c_initial=c_state,\n+                    n_initial=n_state,\n+                    m_initial=m_state,\n+                    chunk_size=chunk_size,\n+                    return_last_states=True,\n+                    autocast_kernel_dtype=autocast_kernel_dtype,\n+                    eps=eps,\n+                )\n+                seq_len_start_idx += iter_seq_len\n+                h_outs.append(h_out)\n+\n+            remaining_seq_len = sequence_length - seq_len_start_idx\n+\n+            if remaining_seq_len > 0:\n+                # we use here matK as query as this kernel does not need a query, since we do not care about the outputs only about the last state\n+                h_out, (c_state, n_state, m_state) = mlstm_sequence_kernel(\n+                    query=query[..., seq_len_start_idx:sequence_length, :].contiguous(),\n+                    key=key[..., seq_len_start_idx:sequence_length, :].contiguous(),\n+                    value=value[..., seq_len_start_idx:sequence_length, :].contiguous(),\n+                    igate=igate[..., seq_len_start_idx:sequence_length].contiguous(),\n+                    fgate=fgate[..., seq_len_start_idx:sequence_length].contiguous(),\n+                    c_initial=c_state,\n+                    n_initial=n_state,\n+                    m_initial=m_state,\n+                    return_last_states=True,\n+                    eps=eps,\n+                )\n+                h_outs.append(h_out)\n+            h_out = torch.concatenate(h_outs, dim=2)\n+\n+        else:\n+            if sequence_length != 1:\n+                raise ValueError(\n+                    f\"Received empty sequence (sequence_length={sequence_length}), require at least single element in the sequence.\"\n+                )\n+            # process the sequence length in a single step\n+            # while this case is also captured by the regular mode above,\n+            # it avoids the overhead of the loop and calls the step kernel directly\n+            # The step function does not want a sequence dimension\n+            # qkv shape is (batch_size, nh, dhqk/dhv)\n+            # igate, fgate shape is (batch_size, nh, 1)\n+            h_out, (c_state, n_state, m_state) = mlstm_step_kernel(\n+                query=query.squeeze(2),\n+                key=key.squeeze(2),\n+                value=value.squeeze(2),\n+                igate=igate,\n+                fgate=fgate,\n+                cstate=c_state,\n+                nstate=n_state,\n+                mstate=m_state,\n+                eps=eps,\n+            )\n+            h_out = h_out[:, :, None, :]\n+\n+        if return_last_states:\n+            return h_out, (c_state, n_state, m_state)\n+        else:\n+            return h_out\n+\n+    class xLSTMBackend(nn.Module):\n+        \"\"\"xLSTM Backend Module for PyTorch.\n+\n+        This module wraps the xLSTM kernels and provides a high-level interface for training and inference.\n+        \"\"\"\n+\n+        config_class = xLSTMConfig\n+\n+        def __init__(self, config: xLSTMConfig):\n+            super().__init__()\n+            self.config = config\n+            self.chunkwise_kernel_fn = mlstm_chunkwise_native_autograd\n+            self.sequence_kernel_fn = mlstm_recurrent_sequence_native\n+            self.step_kernel_fn = mlstm_recurrent_step_native\n+\n+            self._inference_fn = partial(\n+                wrap_chunkwise_arbitrary_sequence_length,\n+                mlstm_chunkwise_kernel=self.chunkwise_kernel_fn,\n+                mlstm_sequence_kernel=partial(\n+                    self.sequence_kernel_fn,\n+                    dtype_state=getattr(torch, config.inference_state_dtype),\n+                ),\n+                mlstm_step_kernel=partial(\n+                    self.step_kernel_fn,\n+                    dtype_state=getattr(torch, config.inference_state_dtype),\n+                ),\n+                chunk_size=config.chunk_size,\n+                eps=config.eps,\n+                autocast_kernel_dtype=getattr(torch, config.autocast_kernel_dtype),\n+                return_last_states=True,\n+            )\n+\n+            train_kernel_fn = partial(\n+                self.chunkwise_kernel_fn,\n+                autocast_kernel_dtype=getattr(torch, config.autocast_kernel_dtype),\n+                eps=config.eps,\n+                chunk_size=config.chunk_size,\n+            )\n+            if \"with_padding\" in config.mode:\n+                train_kernel_fn = partial(wrap_chunkwise_pad_zeros, mlstm_chunkwise_kernel=train_kernel_fn)\n+            self._train_fn = train_kernel_fn\n+\n+        def forward(\n+            self,\n+            query: torch.Tensor,\n+            key: torch.Tensor,\n+            value: torch.Tensor,\n+            igate: torch.Tensor,\n+            fgate: torch.Tensor,\n+            c_initial: torch.Tensor = None,\n+            n_initial: torch.Tensor = None,\n+            m_initial: torch.Tensor = None,\n+            return_last_states: bool = False,\n+            mode: Optional[Literal[\"train\", \"inference\"]] = None,\n+        ) -> Union[torch.Tensor, tuple[torch.Tensor, tuple[torch.Tensor, torch.Tensor, torch.Tensor]]]:\n+            \"\"\"Forward pass of the mLSTM backend.\n+\n+            Depending on the configured mode, this method will call the appropriate kernel function.\n+\n+            Args:\n+                query: The query tensor of shape (batch_size, nh, sequence_length, dhqk).\n+                key: The key tensor of shape (batch_size, nh, sequence_length, dhqk).\n+                value: The value tensor of shape (batch_size, nh, sequence_length, dhhv).\n+                igate: The input gate preactivation tensor of shape (batch_size, nh, sequence_length).\n+                fgate: The forget gate preactivation tensor of shape (batch_size, nh, sequence_length).\n+                c_initial: The initial cell state tensor of shape (batch_size, nh, dhqk, dhhv).\n+                                                    Defaults to None.\n+                n_initial: The initial hidden state tensor of shape (batch_size, nh, dhqk). Defaults to None.\n+                m_initial: The initial memory tensor of shape (batch_size, nh, 1). Defaults to None.\n+                return_last_states: Whether to return the last states of the sequence. Defaults to None.\n+                                                    If None, the value from the config is used.\n+\n+            Returns:\n+                hidden states of shape (batch_size, nh, sequence_length, dhhv)\n+                hidden states and last states the last states are the cell state cstate (batch_size, nh, dhqk, dhhv),\n+                the normalizer state nstate (batch_size, nh, dhqk), and the max state mstate (batch_size, nh, 1)\n+            \"\"\"\n+            if mode is None:\n+                mode = self.config.mode\n+\n+            if \"train\" in mode:\n+                if return_last_states is None:\n+                    return_last_states = self.config.return_last_states\n+\n+                if self.config.mode == \"train_with_padding\":\n+                    if return_last_states:\n+                        raise ValueError(\"return_last_states=True is not supported with train_with_padding mode.\")\n+\n+                return self._train_fn(\n+                    query=query,\n+                    key=key,\n+                    value=value,\n+                    igate=igate,\n+                    fgate=fgate,\n+                    c_initial=c_initial,\n+                    n_initial=n_initial,\n+                    m_initial=m_initial,\n+                    return_last_states=return_last_states,\n+                )\n+\n+            elif \"inference\" in mode:\n+                # inference mode always returns the last states\n+                return self._inference_fn(\n+                    query=query,\n+                    key=key,\n+                    value=value,\n+                    igate=igate,\n+                    fgate=fgate,\n+                    c_initial=c_initial,\n+                    n_initial=n_initial,\n+                    m_initial=m_initial,\n+                )\n+            else:\n+                raise ValueError(f\"Unknown mode: {self.config.mode}\")\n+\n+        def extra_repr(self) -> str:\n+            return f\"{self.config}\"\n+\n+    class xLSTMRMSNorm(nn.Module):\n+        \"\"\"Root mean square normalization layer implementation similar\n+        to https://pytorch.org/docs/stable/generated/torch.nn.RMSNorm.html.\n+\n+        It normalizes the input tensor by the root mean square of the last dimension.\n+\n+        Args:\n+            num_features: The number of features in the input tensor.\n+            eps: A small value to avoid division by zero.\n+            use_weight: Whether to use a learnable weight.\n+            use_bias: Whether to use a learnable bias.\n+            force_float32_reductions: Whether to force float32 reductions.\n+        \"\"\"\n+\n+        def __init__(\n+            self,\n+            num_features: int,\n+            eps: float = 1e-6,\n+            use_weight: bool = True,\n+            use_bias: bool = False,\n+            force_float32_reductions: bool = True,\n+        ):\n+            super().__init__()\n+            self.num_features = num_features\n+            self.eps = eps\n+            self.force_float32_reductions = force_float32_reductions\n+\n+            if use_weight:\n+                self.weight = nn.Parameter(torch.ones(num_features))\n+            else:\n+                self.weight = None\n+\n+            if use_bias:\n+                self.bias = nn.Parameter(torch.zeros(num_features))\n+            else:\n+                self.bias = None\n+\n+        def _apply_weight_bias(self, x: torch.Tensor) -> torch.Tensor:\n+            if self.weight is not None:\n+                x = x * self.weight\n+            if self.bias is not None:\n+                x = x + self.bias\n+            return x\n+\n+        def _rms_normalize(self, x: torch.Tensor) -> torch.Tensor:\n+            # apply rms norm over the last dimension, i.e. HD dimension\n+            in_dtype = x.dtype\n+            if self.force_float32_reductions:\n+                x = x.float()\n+            x = x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n+            return x.to(in_dtype)\n+\n+        def forward(self, x: torch.Tensor) -> torch.Tensor:\n+            x = self._rms_normalize(x)\n+            x = self._apply_weight_bias(x)\n+            return x\n+\n+    class xLSTMMultiHeadLayerNorm(nn.Module):\n+        \"\"\"Multi-head version of the LayerNorm layer.\n+\n+        It normalizes the last dimension of the input tensor.\n+\n+        The input is assumed to have the shape (batch_size, sequence_length, nh, DH), where:\n+        batch_size: batch size\n+        sequence_length: sequence length\n+        nh: number of heads\n+        DH: head dimension\n+\n+        The normalization is applied over the last dimension (DH) of the input tensor.\n+\n+        Args:\n+            num_heads: The number of heads.\n+            head_dim: The head dimension.\n+            eps: A small value to avoid division by zero.\n+            use_weight: Whether to use a learnable weight.\n+            use_bias: Whether to use a learnable bias.\n+            force_float32_reductions: Whether to force float32 reductions\n+\n+        Returns:\n+            The normalized tensor with the shape (batch_size, sequence_length, nh * DH).\n+        \"\"\"\n+\n+        def __init__(\n+            self,\n+            num_heads: int,\n+            head_dim: int,\n+            eps: float = 1e-6,\n+            use_weight: bool = True,\n+            use_bias: bool = False,\n+            force_float32_reductions: bool = True,\n+        ):\n+            super().__init__()\n+            self.num_features = num_heads * head_dim\n+            self.eps = eps\n+            self.force_float32_reductions = force_float32_reductions\n+\n+            if use_weight:\n+                self.weight = nn.Parameter(torch.ones(self.num_features))\n+            else:\n+                self.weight = None\n+\n+            if use_bias:\n+                self.bias = nn.Parameter(torch.zeros(self.num_features))\n+            else:\n+                self.bias = None\n+            self.num_heads = num_heads\n+            self.head_dim = head_dim\n+\n+        def _apply_weight_bias(self, x: torch.Tensor) -> torch.Tensor:\n+            if self.weight is not None:\n+                x = x * self.weight\n+            if self.bias is not None:\n+                x = x + self.bias\n+            return x\n+\n+        def _layer_normalize(self, x: torch.Tensor) -> torch.Tensor:\n+            # apply layer norm over the last dimension, i.e. HD dimension\n+            in_dtype = x.dtype\n+            if self.force_float32_reductions:\n+                x = x.float()\n+            x_centered = x - x.mean(dim=-1, keepdim=True)\n+            y = x_centered * torch.rsqrt(x.var(dim=-1, keepdim=True, unbiased=False) + self.eps)\n+            return y.to(in_dtype)\n+\n+        def forward(\n+            self,\n+            x: torch.Tensor,\n+        ) -> torch.Tensor:\n+            batch_size, sequence_length, nh, DH = x.shape\n+            if nh != self.num_heads:\n+                raise ValueError(f\"Expected {self.num_heads} heads, got {nh}, input shape: {x.shape}\")\n+            if DH != self.head_dim:\n+                raise ValueError(f\"Expected {self.head_dim} head dimension, got {DH}, input shape: {x.shape}\")\n+\n+            x = self._layer_normalize(x)\n+            x = x.reshape(batch_size, sequence_length, -1)\n+            x = self._apply_weight_bias(x)\n+            return x\n+\n+    class xLSTMFeedForward(nn.Module):\n+        def __init__(self, config: xLSTMConfig):\n+            super().__init__()\n+            self.config = config\n+\n+            self.up_proj_dim = round_up_to_next_multiple_of(\n+                config.hidden_size * config.ffn_proj_factor,\n+                config.ffn_round_up_to_multiple_of,\n+            )\n+\n+            if self.config.weight_mode == \"single\":\n+                self.proj_up_gate = nn.Linear(\n+                    in_features=config.hidden_size,\n+                    out_features=self.up_proj_dim,\n+                    bias=self.config.use_bias,\n+                )\n+                self.proj_up = nn.Linear(\n+                    in_features=config.hidden_size,\n+                    out_features=self.up_proj_dim,\n+                    bias=self.config.use_bias,\n+                )\n+            elif self.config.weight_mode == \"fused\":\n+                self.proj_up_gate_z = nn.Linear(\n+                    in_features=config.hidden_size,\n+                    out_features=2 * self.up_proj_dim,\n+                    bias=self.config.use_bias,\n+                )\n+\n+            self.proj_down = nn.Linear(\n+                in_features=self.up_proj_dim,\n+                out_features=config.hidden_size,\n+                bias=self.config.use_bias,\n+            )\n+\n+            self.act_fn = nn.SiLU()\n+\n+        def forward(self, x: torch.Tensor) -> torch.Tensor:\n+            if self.config.weight_mode == \"single\":\n+                x = self.act_fn(self.proj_up_gate(x)) * self.proj_up(x)\n+            elif self.config.weight_mode == \"fused\":\n+                x = self.proj_up_gate_z(x)\n+                gate, z = torch.tensor_split(x, (self.up_proj_dim,), dim=-1)\n+                x = self.act_fn(gate) * z\n+\n+            y = self.proj_down(x)\n+            return y\n+\n+    class xLSTMLayer(nn.Module):\n+        def __init__(self, config: xLSTMConfig):\n+            super().__init__()\n+            self.config = config\n+\n+            self.v_dim = int(config.hidden_size * config.v_dim_factor)\n+            self.qk_dim = int(config.hidden_size * config.qk_dim_factor)\n+\n+            if self.config.weight_mode == \"single\":\n+                self.query = nn.Linear(\n+                    in_features=self.config.hidden_size,\n+                    out_features=self.qk_dim,\n+                    bias=self.config.use_bias,\n+                )\n+                self.key = nn.Linear(\n+                    in_features=self.config.hidden_size,\n+                    out_features=self.qk_dim,\n+                    bias=self.config.use_bias,\n+                )\n+                self.value = nn.Linear(\n+                    in_features=self.config.hidden_size,\n+                    out_features=self.v_dim,\n+                    bias=self.config.use_bias,\n+                )\n+\n+                self.ogate_preact = nn.Linear(\n+                    in_features=self.config.hidden_size,\n+                    out_features=self.v_dim,\n+                    bias=self.config.use_bias,\n+                )\n+                self.igate_preact = nn.Linear(\n+                    in_features=self.config.hidden_size,\n+                    out_features=self.config.num_heads,\n+                    bias=True,\n+                )\n+                self.fgate_preact = nn.Linear(\n+                    in_features=self.config.hidden_size,\n+                    out_features=self.config.num_heads,\n+                    bias=True,\n+                )\n+            elif self.config.weight_mode == \"fused\":\n+                self.qkv_opreact = nn.Linear(\n+                    in_features=self.config.hidden_size,\n+                    out_features=2 * self.qk_dim + 2 * self.v_dim,\n+                    bias=self.config.use_bias,\n+                )\n+                self.ifgate_preact = nn.Linear(\n+                    in_features=self.config.hidden_size,\n+                    out_features=2 * self.config.num_heads,\n+                    bias=True,\n+                )\n+\n+            self.ogate_act_fn = nn.Sigmoid()\n+            self.mlstm_backend = xLSTMBackend(config=self.config)\n+\n+            self.multihead_norm = xLSTMMultiHeadLayerNorm(\n+                num_heads=self.config.num_heads,\n+                head_dim=self.v_dim // self.config.num_heads,\n+                eps=self.config.norm_eps,\n+                use_weight=True,\n+                use_bias=self.config.use_bias,\n+                force_float32_reductions=self.config.norm_reduction_force_float32,\n+            )\n+            self.out_proj = nn.Linear(\n+                in_features=self.v_dim,\n+                out_features=self.config.hidden_size,\n+                bias=self.config.use_bias,\n+            )\n+\n+        def forward(\n+            self, x: torch.Tensor, state: Optional[mLSTMLayerStateType] = None\n+        ) -> tuple[torch.Tensor, Optional[mLSTMLayerStateType]]:\n+            if x.ndim != 3:\n+                raise ValueError(f\"Input must have shape [batch_size, sequence_length, HD], got {x.shape}\")\n+            batch_size, sequence_length, _ = x.shape\n+            if self.config.weight_mode == \"single\":\n+                query = self.query(x)\n+                key = self.key(x)\n+                value = self.value(x)\n+                o_preact = self.ogate_preact(x)\n+                i_preact = soft_cap(self.igate_preact(x), cap_value=self.config.gate_soft_cap)\n+                f_preact = soft_cap(self.fgate_preact(x), cap_value=self.config.gate_soft_cap)\n+\n+            elif self.config.weight_mode == \"fused\":\n+                qkv_opreact = self.qkv_opreact(x)\n+                query, key, value, o_preact = torch.tensor_split(\n+                    qkv_opreact,\n+                    (\n+                        self.qk_dim,\n+                        2 * self.qk_dim,\n+                        2 * self.qk_dim + self.v_dim,\n+                    ),\n+                    dim=-1,\n+                )\n+\n+                if_preact = soft_cap(self.ifgate_preact(x), cap_value=self.config.gate_soft_cap)\n+                i_preact, f_preact = torch.tensor_split(if_preact, (self.config.num_heads,), dim=-1)\n+\n+            query = query.reshape(batch_size, sequence_length, self.config.num_heads, -1).transpose(1, 2)\n+            key = key.reshape(batch_size, sequence_length, self.config.num_heads, -1).transpose(1, 2)\n+            value = value.reshape(batch_size, sequence_length, self.config.num_heads, -1).transpose(1, 2)\n+            i_preact = i_preact.transpose(1, 2)\n+            f_preact = f_preact.transpose(1, 2)\n+            if state is None:\n+                c_initial, n_initial, m_initial = None, None, None\n+            else:\n+                c_initial, n_initial, m_initial = state\n+\n+            h, state = self.mlstm_backend(\n+                query=query,\n+                key=key,\n+                value=value,\n+                igate=i_preact,\n+                fgate=f_preact,\n+                c_initial=c_initial,\n+                n_initial=n_initial,\n+                m_initial=m_initial,\n+            )\n+            expected_h_shape = (\n+                batch_size,\n+                self.config.num_heads,\n+                sequence_length,\n+                self.v_dim // self.config.num_heads,\n+            )\n+            if h.shape != expected_h_shape:\n+                raise ValueError(f\"Got {h.shape}, expected {expected_h_shape}\")\n+\n+            h = h.transpose(1, 2)\n+            h_norm = self.multihead_norm(h)\n+            h_norm = h_norm.reshape(batch_size, sequence_length, -1)\n+\n+            h_out = self.ogate_act_fn(o_preact) * h_norm\n+\n+            y = self.out_proj(h_out)\n+            return y, state\n+\n+    class xLSTMBlock(nn.Module):\n+        def __init__(self, config: xLSTMConfig):\n+            super().__init__()\n+            self.config = config\n+            self.norm_mlstm = xLSTMRMSNorm(\n+                num_features=config.hidden_size,\n+                eps=config.norm_eps,\n+                use_weight=True,\n+                use_bias=config.use_bias,\n+                force_float32_reductions=config.norm_reduction_force_float32,\n+            )\n+            self.mlstm_layer = xLSTMLayer(config)\n+            self.norm_ffn = xLSTMRMSNorm(\n+                num_features=config.hidden_size,\n+                eps=config.norm_eps,\n+                use_weight=True,\n+                use_bias=config.use_bias,\n+                force_float32_reductions=config.norm_reduction_force_float32,\n+            )\n+            self.ffn = xLSTMFeedForward(config)\n+\n+        def forward(\n+            self, x: torch.Tensor, state: Optional[mLSTMStateType] = None\n+        ) -> tuple[torch.Tensor, mLSTMStateType]:\n+            x_mlstm = self.norm_mlstm(x)\n+            x_mlstm, state = self.mlstm_layer(x_mlstm, state)\n+            x = x + x_mlstm\n+\n+            x_ffn = self.norm_ffn(x)\n+            x_ffn = self.ffn(x_ffn)\n+            x = x + x_ffn\n+\n+            return x, state\n+\n+\n+def small_init_method(dim):\n+    \"\"\"\n+    Adapted from: https://github.com/EleutherAI/gpt-neox/blob/main/megatron/model/init_functions.py\n+    Fills the input Tensor with values according to the method described in Transformers without Tears: Improving\n+    the Normalization of Self-Attention - Nguyen, T. & Salazar, J. (2019), using a normal distribution.\"\"\"\n+    std = (2 / (5 * dim)) ** (1 / 2)\n+\n+    def init_(tensor):\n+        return torch.nn.init.normal_(tensor, mean=0.0, std=std)\n+\n+    return init_\n+\n+\n+def wang_init_method(n_layers, dim):\n+    \"\"\"\n+    Adapted from https://github.com/EleutherAI/gpt-neox/blob/main/megatron/model/init_functions.py\n+    \"\"\"\n+    std = 2 / n_layers / dim ** (1 / 2)\n+\n+    def init_(tensor):\n+        return torch.nn.init.normal_(tensor, mean=0.0, std=std)\n+\n+    return init_\n+\n+\n+class xLSTMPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class for an interface to loading a pre-trained xLSTM model.\n+    \"\"\"\n+\n+    config_class = xLSTMConfig\n+    base_model_prefix = \"backbone\"\n+    _no_split_modules = [\"xLSTMBlock\"]\n+    supports_gradient_checkpointing = True\n+    _is_stateful = True\n+\n+    def _module_name_map(self, module):\n+        for name, mod in self.named_modules():\n+            if mod is module:\n+                return name\n+        return \"\"\n+\n+    def _init_weights(self, module):\n+        if isinstance(module, nn.Embedding):\n+            small_init_method(self.config.hidden_size)(self.embeddings.weight)\n+        elif isinstance(module, nn.Linear):\n+            if module.bias is not None:\n+                torch.nn.init.zeros_(module.bias)\n+            if self.config.weight_mode == \"single\" and \"gate\" in self._module_name_map(module):\n+                torch.nn.init.zeros_(module.weight)\n+                with torch.no_grad():\n+                    if \"igate\" in self._module_name_map(module):\n+                        module.bias.copy_(-10.0 * torch.ones_like(module.bias))\n+                    elif \"fgate\" in self._module_name_map(module):\n+                        module.bias.copy_(\n+                            torch.linspace(\n+                                3.0,\n+                                6.0,\n+                                module.bias.shape[-1],\n+                            ).to(\n+                                device=module.bias.device,\n+                                dtype=module.bias.dtype,\n+                            )\n+                        )\n+            elif self.config.weight_mode == \"fused\" and \"gate\" in self._module_name_map(module):\n+                torch.nn.init.zeros_(module.weight)\n+                with torch.no_grad():\n+                    module.bias[: self.config.num_heads] += -module.bias[\n+                        : self.config.num_heads\n+                    ] - 10.0 * torch.ones_like(module.bias)\n+                    module.bias[: self.config.num_heads] += -module.bias[self.config.num_heads :] + torch.linspace(\n+                        3.0,\n+                        6.0,\n+                        module.bias.shape[-1],\n+                    ).to(\n+                        device=module.bias.device,\n+                        dtype=module.bias.dtype,\n+                    )\n+            elif \"proj_down\" in self._module_name_map(module):\n+                wang_init_method(dim=module.weight.shape[1], n_layers=self.config.num_hidden_layers)(module.weight)\n+            elif \"out_proj\" in self._module_name_map(module):\n+                wang_init_method(dim=self.config.hidden_size, n_layers=self.config.num_hidden_layers)(module.weight)\n+            elif module.weight is not None:\n+                small_init_method(self.config.hidden_size)(module.weight)\n+        elif isinstance(module, xLSTMRMSNorm) or hasattr(module, \"_layer_normalize\"):\n+            torch.nn.init.ones_(module.weight)\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                torch.nn.init.zeros_(module.bias)\n+\n+\n+class xLSTMCache:\n+    \"\"\"\n+    Cache for xLSTM model which does not have attention mechanism and key value states.\n+\n+    Arguments:\n+        config (`PretrainedConfig):\n+            The configuration file defining the shape-related attributes required to initialize the static cache.\n+        max_batch_size (`int`):\n+            The batch size with which the model will be used.\n+        dtype (`torch.dtype`, *optional*, defaults to `torch.bfloat16`):\n+            The default `dtype` to use when initializing the layer.\n+        device (`torch.device` or `str`, *optional*):\n+            The device on which the cache should be initialized. Should be the same as the layer.\n+\n+    Attributes:\n+        seqlen_offset: int\n+        dtype: torch.dtype\n+\n+    Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, xLSTMForCausalLM, xLSTMCache\n+\n+        >>> model = xLSTMForCausalLM.from_pretrained(\"NX-AI/xLSTM-7b\")\n+        >>> tokenizer = xLSTMTokenizer.from_pretrained(\"NX-AI/xLSTM-7b\")\n+\n+        >>> inputs = tokenizer(text=\"I am an xLSTM\", return_tensors=\"pt\")\n+\n+        >>> # Prepare a cache class and pass it to model's forward\n+        >>> cache_params = xLSTMCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n+        >>> outputs = model(**inputs, cache_params=cache_params, use_cache=True)\n+        >>> outputs.cache_params\n+        xLSTMCache()\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: xLSTMConfig,\n+        max_batch_size: int,\n+        dtype: torch.dtype = torch.bfloat16,\n+        device: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        self.seqlen_offset = 0\n+        self.dtype = dtype\n+        self.config = config\n+        self.rnn_state = {\n+            layer: (\n+                torch.zeros(\n+                    [max_batch_size, config.num_heads, config.qk_head_dim, config.v_head_dim],\n+                    dtype=dtype,\n+                    device=device,\n+                ),\n+                torch.zeros([max_batch_size, config.num_heads, config.qk_head_dim], dtype=dtype, device=device),\n+                torch.zeros([max_batch_size, config.num_heads, 1], dtype=dtype, device=device),\n+            )\n+            for layer in range(config.num_hidden_layers)\n+        }\n+\n+    def reset(self):\n+        self.rnn_state = {\n+            layer: (\n+                torch.zeros_like(self.rnn_state[layer][0]),\n+                torch.zeros_like(self.rnn_state[layer][1]),\n+                torch.zeros_like(self.rnn_state[layer][2]),\n+            )\n+            for layer in self.rnn_state\n+        }\n+\n+\n+@dataclass\n+@auto_docstring\n+class xLSTMOutput(ModelOutput):\n+    r\"\"\"\n+    cache_params (`xLSTMCache`):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor]\n+    cache_params: Optional[xLSTMCache] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+@auto_docstring\n+class xLSTMModel(xLSTMPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        # use embbeding_dim and num_blocks once here to make use of them\n+        self.embeddings = nn.Embedding(config.vocab_size, config.embedding_dim)\n+        self.blocks = nn.ModuleList([xLSTMBlock(config) for _ in range(config.num_blocks)])\n+        self.out_norm = xLSTMRMSNorm(config.hidden_size, eps=config.norm_eps)\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings\n+\n+    def set_input_embeddings(self, new_embedding):\n+        self.embeddings = new_embedding\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.LongTensor] = None,\n+        cache_params: Optional[xLSTMCache] = None,\n+        use_cache: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[tuple, xLSTMOutput]:\n+        r\"\"\"\n+        cache_params (`xLSTMCache`, *optional*):\n+            The xLSTMCache that carries the RNN states.\n+        \"\"\"\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else (self.config.use_cache if not self.training else False)\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            use_cache = False\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embeddings(input_ids)\n+\n+        if use_cache and cache_params is None:\n+            cache_params = xLSTMCache(\n+                self.config, inputs_embeds.size(0), device=inputs_embeds.device, dtype=inputs_embeds.dtype\n+            )\n+\n+        hidden_states = inputs_embeds\n+\n+        if (\n+            not self.training\n+            and self.config.max_inference_chunksize < hidden_states.shape[1]\n+            and not output_hidden_states\n+        ):\n+            offset = 0\n+            with torch.no_grad():\n+                if cache_params is None:\n+                    cache_params = xLSTMCache(config=self.config, batch_size=hidden_states.shape[0])\n+                final_state = torch.zeros_like(hidden_states)\n+                while offset < hidden_states.shape[1]:\n+                    hidden_states_chunk = hidden_states[\n+                        :, offset : min(offset + self.config.max_inference_chunksize, hidden_states.shape[1])\n+                    ]\n+                    for layer_idx, xlstm_block in enumerate(self.blocks):\n+                        hidden_states_chunk, rnn_state = xlstm_block(\n+                            hidden_states_chunk,\n+                            state=cache_params.rnn_state[layer_idx],\n+                        )\n+                        for state_idx in range(len(cache_params.rnn_state[layer_idx])):\n+                            local_rnn_state = rnn_state[state_idx]\n+                            cache_params.rnn_state[layer_idx][state_idx].copy_(local_rnn_state)\n+                        cache_params.rnn_state_initial = False\n+                    final_state[\n+                        :, offset : min(offset + self.config.max_inference_chunksize, hidden_states.shape[1])\n+                    ] = hidden_states_chunk\n+                    offset += self.config.max_inference_chunksize\n+                hidden_states = final_state\n+        else:\n+            all_hidden_states = () if output_hidden_states else None\n+            for layer_idx, xlstm_block in enumerate(self.blocks):\n+                if self.gradient_checkpointing and self.training:\n+                    hidden_states, rnn_state = self._gradient_checkpointing_func(\n+                        xlstm_block.__call__,\n+                        hidden_states,\n+                        cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n+                    )\n+                else:\n+                    hidden_states, rnn_state = xlstm_block(\n+                        hidden_states,\n+                        state=cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n+                    )\n+                if cache_params:\n+                    for state_idx in range(len(cache_params.rnn_state[layer_idx])):\n+                        local_rnn_state = rnn_state[state_idx]\n+                        cache_params.rnn_state[layer_idx][state_idx].copy_(local_rnn_state)\n+                    cache_params.rnn_state_initial = False\n+\n+                if output_hidden_states:\n+                    all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if use_cache:\n+            cache_params.seqlen_offset += inputs_embeds.shape[1]\n+\n+        hidden_states = self.out_norm(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        return xLSTMOutput(\n+            last_hidden_state=hidden_states,\n+            cache_params=cache_params,\n+            hidden_states=all_hidden_states,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring\n+class xLSTMCausalLMOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    cache_params (`xLSTMCache`, *optional*, carrying the RNN states):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    cache_params: Optional[xLSTMCache] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+@auto_docstring\n+class xLSTMForCausalLM(xLSTMPreTrainedModel, GenerationMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.backbone = xLSTMModel(config)\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def get_input_embeddings(self):\n+        return self.backbone.get_input_embeddings()\n+\n+    def set_input_embeddings(self, new_embeddings):\n+        return self.backbone.set_input_embeddings(new_embeddings)\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        inputs_embeds=None,\n+        use_cache=None,\n+        cache_params: Optional[xLSTMCache] = None,\n+        **kwargs,\n+    ):\n+        if use_cache and cache_params is not None:\n+            # If the first cache position is non-zero, we assume we are in generation mode.\n+            # Thus, the cache_params state is assumed to be the state before the last token\n+            # (lastly generated token), and all previous tokens are already ingested.\n+            # This should as well support generation from scratch with the [BOS] token inserted first.\n+            input_ids = input_ids[:, -1:]\n+            if inputs_embeds is not None:\n+                inputs_embeds = inputs_embeds[:, -1:]\n+\n+        if inputs_embeds is not None and cache_params is None:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds}\n+        else:\n+            model_inputs = {\"input_ids\": input_ids}\n+\n+        model_inputs.update({\"cache_params\": cache_params, \"use_cache\": use_cache})\n+        return model_inputs\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_params: Optional[xLSTMCache] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[tuple, xLSTMCausalLMOutput]:\n+        r\"\"\"\n+        cache_params (`xLSTMCache`, *optional*):\n+            The xLSTMCache that carries the RNN states.\n+        \"\"\"\n+        xlstm_outputs = self.backbone(\n+            input_ids,\n+            cache_params=cache_params,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_hidden_states=output_hidden_states,\n+            **kwargs,\n+        )\n+        hidden_states = xlstm_outputs[0]\n+\n+        logits = self.lm_head(hidden_states.to(self.lm_head.weight.dtype)).float()\n+\n+        if not self.training and self.config.max_inference_chunksize < logits.shape[1]:\n+            offset = 0\n+            with torch.no_grad():\n+                while offset < logits.shape[1]:\n+                    logits[:, offset : min(offset + self.config.max_inference_chunksize, logits.shape[1])] = soft_cap(\n+                        logits[:, offset : min(offset + self.config.max_inference_chunksize, logits.shape[1])],\n+                        self.config.output_logit_soft_cap,\n+                    )\n+                    offset += self.config.max_inference_chunksize\n+        else:\n+            logits = soft_cap(logits, self.config.output_logit_soft_cap)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            # Shift so that tokens < nstate predict nstate\n+            shift_logits = logits[..., :-1, :].contiguous()\n+            shift_labels = labels[..., 1:].contiguous()\n+            # Flatten the tokens\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n+\n+        return xLSTMCausalLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            cache_params=xlstm_outputs.cache_params,\n+            hidden_states=xlstm_outputs.hidden_states,\n+        )\n+\n+\n+__all__ = [\n+    \"xLSTMForCausalLM\",\n+    \"xLSTMModel\",\n+    \"xLSTMPreTrainedModel\",\n+]"
        },
        {
            "sha": "c45a93e406d6b36fff48dce8a4b3115edff75b44",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -270,6 +270,7 @@\n     is_uroman_available,\n     is_vision_available,\n     is_vptq_available,\n+    is_xlstm_available,\n     is_yt_dlp_available,\n     requires_backends,\n     torch_only_method,"
        },
        {
            "sha": "fb9d56e160faeff5fe7124fffb43a1ffe562ffed",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -588,6 +588,12 @@ def is_causal_conv1d_available():\n     return False\n \n \n+def is_xlstm_available():\n+    if is_torch_available():\n+        return _is_package_available(\"xlstm\")\n+    return False\n+\n+\n def is_mambapy_available():\n     if is_torch_available():\n         return _is_package_available(\"mambapy\")"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/xlstm/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/tests%2Fmodels%2Fxlstm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/tests%2Fmodels%2Fxlstm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlstm%2F__init__.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2"
        },
        {
            "sha": "3ad5f67100a94db4bfbeef766a839878cf93425a",
            "filename": "tests/models/xlstm/test_modeling_xlstm.py",
            "status": "added",
            "additions": 371,
            "deletions": 0,
            "changes": 371,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -0,0 +1,371 @@\n+# Copyright 2025 NXAI GmbH. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+from parameterized import parameterized\n+\n+from transformers import AutoTokenizer, is_torch_available, xLSTMConfig\n+from transformers.testing_utils import require_read_token, require_torch, require_torch_gpu, slow, torch_device\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        xLSTMForCausalLM,\n+        xLSTMModel,\n+    )\n+    from transformers.models.xlstm.modeling_xlstm import xLSTMBlock, xLSTMCache\n+    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_2\n+else:\n+    is_torch_greater_or_equal_than_2_2 = False\n+\n+\n+class xLSTMModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        num_heads=2,\n+        seq_length=7,\n+        is_training=True,\n+        use_labels=True,\n+        vocab_size=99,\n+        hidden_size=128,\n+        qk_dim_factor=0.5,\n+        v_dim_factor=1.0,\n+        num_hidden_layers=2,\n+        max_position_embeddings=512,\n+        type_vocab_size=16,\n+        type_sequence_label_size=2,\n+        num_labels=3,\n+        num_choices=4,\n+        scope=None,\n+        chunkwise_kernel=\"chunkwise--native_autograd\",\n+        sequence_kernel=\"native_sequence__native\",\n+        step_kernel=\"native\",\n+        tie_word_embeddings=False,\n+    ):\n+        self.parent = parent\n+        self.num_heads = num_heads\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.hidden_size = hidden_size\n+        self.qk_dim_factor = qk_dim_factor\n+        self.v_dim_factor = v_dim_factor\n+        self.max_position_embeddings = max_position_embeddings\n+        self.type_vocab_size = type_vocab_size\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.num_labels = num_labels\n+        self.num_choices = num_choices\n+        self.scope = scope\n+        self.bos_token_id = vocab_size - 1\n+        self.eos_token_id = vocab_size - 1\n+        self.pad_token_id = vocab_size - 1\n+        self.chunkwise_kernel = chunkwise_kernel\n+        self.sequence_kernel = sequence_kernel\n+        self.step_kernel = step_kernel\n+        self.tie_word_embeddings = tie_word_embeddings\n+\n+    def get_large_model_config(self):\n+        cfg = xLSTMConfig.from_pretrained(\"NX-AI/xLSTM-7b\")\n+        return cfg\n+\n+    def prepare_config_and_inputs(self, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        sequence_labels = None\n+        token_labels = None\n+        choice_labels = None\n+        if self.use_labels:\n+            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n+            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n+\n+        config = self.get_config()\n+\n+        return (\n+            config,\n+            input_ids,\n+            None,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+        )\n+\n+    def get_config(self):\n+        cfg = xLSTMConfig(\n+            num_heads=self.num_heads,\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            qk_dim_factor=self.qk_dim_factor,\n+            v_dim_factor=self.v_dim_factor,\n+            n_positions=self.max_position_embeddings,\n+            type_vocab_size=self.type_vocab_size,\n+            use_cache=True,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+            chunkwise_kernel=self.chunkwise_kernel,\n+            sequence_kernel=self.sequence_kernel,\n+            step_kernel=self.step_kernel,\n+            tie_word_embeddings=self.tie_word_embeddings,\n+        )\n+        # this is needed for compatibility with generic tests\n+        # cfg.hidden_size = cfg.embedding_dim\n+        # cfg.num_hidden_layers = cfg.num_blocks\n+        return cfg\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        (\n+            config,\n+            input_ids,\n+            _,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+        ) = self.prepare_config_and_inputs()\n+        inputs_dict = {\"input_ids\": input_ids}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class xLSTMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (xLSTMModel, xLSTMForCausalLM) if is_torch_available() else ()\n+    all_generative_model_classes = (xLSTMForCausalLM,) if is_torch_available() else ()\n+    has_attentions = False  # xLSTM does not support attentions\n+    fx_compatible = False\n+    test_torchscript = False\n+    test_model_parallel = False\n+    test_pruning = False\n+    test_head_masking = False  # xLSTM does not have attention heads\n+\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": xLSTMModel, \"text-generation\": xLSTMForCausalLM} if is_torch_available() else {}\n+    )\n+\n+    def setUp(self):\n+        self.model_tester = xLSTMModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self, config_class=xLSTMConfig, n_embd=37, common_properties=[\"hidden_size\", \"num_hidden_layers\"]\n+        )\n+\n+    def test_initialization(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=config)\n+            for name, param in model.named_parameters():\n+                if \"D\" in name:\n+                    if param.requires_grad:\n+                        # check if it's a ones like\n+                        self.assertTrue(torch.allclose(param.data, torch.ones_like(param.data), atol=1e-5, rtol=1e-5))\n+\n+    @unittest.skip(reason=\"xLSTM has no tied weights\")\n+    def test_tied_weights_keys(self):\n+        pass\n+\n+    @unittest.skip(reason=\"xLSTM cache slicing test case is an edge case\")\n+    def test_generate_without_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"xLSTM cache slicing test case is an edge case\")\n+    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n+    def test_generate_from_inputs_embeds(self, _, num_beams):\n+        pass\n+\n+    @unittest.skip(reason=\"xLSTM cache slicing test case is an edge case\")\n+    def test_greedy_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(reason=\"xLSTM cache slicing is interacting with beam search\")\n+    def test_beam_search_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    def test_model_outputs_equivalence(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        def check_equivalence(model, tuple_inputs, dict_inputs, additional_kwargs={}):\n+            with torch.no_grad():\n+                tuple_output = model(**tuple_inputs, return_dict=False, **additional_kwargs)\n+                dict_output = model(**dict_inputs, return_dict=True, **additional_kwargs).to_tuple()\n+\n+                def recursive_check(tuple_object, dict_object):\n+                    if isinstance(tuple_object, xLSTMCache):\n+                        recursive_check(tuple_object.rnn_state, dict_object.rnn_state)\n+                    elif isinstance(tuple_object, (list, tuple)):\n+                        for tuple_iterable_value, dict_iterable_value in zip(tuple_object, dict_object):\n+                            recursive_check(tuple_iterable_value, dict_iterable_value)\n+                    elif isinstance(tuple_object, dict):\n+                        for tuple_iterable_value, dict_iterable_value in zip(\n+                            tuple_object.values(), dict_object.values()\n+                        ):\n+                            recursive_check(tuple_iterable_value, dict_iterable_value)\n+                    elif tuple_object is None:\n+                        return\n+                    else:\n+                        self.assertTrue(\n+                            torch.allclose(tuple_object, dict_object, atol=1e-5),\n+                            msg=(\n+                                \"Tuple and dict output are not equal. Difference:\"\n+                                f\" {torch.max(torch.abs(tuple_object - dict_object))}. Tuple has `nan`:\"\n+                                f\" {torch.isnan(tuple_object).any()} and `inf`: {torch.isinf(tuple_object)}. Dict has\"\n+                                f\" `nan`: {torch.isnan(dict_object).any()} and `inf`: {torch.isinf(dict_object)}.\"\n+                            ),\n+                        )\n+\n+                recursive_check(tuple_output, dict_output)\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            check_equivalence(model, tuple_inputs, dict_inputs)\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            check_equivalence(model, tuple_inputs, dict_inputs)\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n+            check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n+\n+            tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n+\n+\n+@require_torch\n+@slow\n+@require_read_token\n+class xLSTMIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.model_id = \"NX-AI/xLSTM-7b\"\n+        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, from_slow=True, legacy=False)\n+        self.prompt = (\"[INST]Write a hello world program in C++.\",)\n+\n+    def test_simple_generate(self, device):\n+        \"\"\"\n+        Simple generate test to avoid regressions.\n+        Note: state-spaces (cuda) implementation and pure torch implementation\n+        have irreconciliable differences as of now, which will cause this test to fail\n+        in an environment with state-spaces installed.\n+        \"\"\"\n+        tokenizer = self.tokenizer\n+        tokenizer.pad_token_id = tokenizer.eos_token_id\n+\n+        model = xLSTMForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16)\n+        model.to(device)\n+        input_ids = tokenizer(\"[INST]Write a hello world program in C++.[/INST]\", return_tensors=\"pt\")[\"input_ids\"].to(\n+            device\n+        )\n+\n+        out = model.generate(input_ids, do_sample=False, use_cache=True, max_new_tokens=30)\n+        output_sentence = tokenizer.decode(out[0])\n+        ground_truth_sentence = \"\"\"<s>[INST]Write a hello world program in C++.[/INST] Sure, here is a simple \"Hello, World!\" program in C++:\\n\\n```cpp\\n#include <iostream>\\n\\n\"\"\"\n+        self.assertEqual(output_sentence, ground_truth_sentence)\n+\n+    def test_batched_equivalence_with_cache(self):\n+        \"\"\"\n+        Verifies that batched generation matches individual generation.\n+        Important because of the specific caching mechanism + statefulness of the xLSTM model.\n+        Depending on precision and devices, differences can be observed from generation to generation.\n+        \"\"\"\n+        tokenizer = self.tokenizer\n+        prompt = [\n+            \"[INST]Write C#.[/INST]\",\n+            \"[INST]Write a hello world in C++.[/INST]\",\n+            \"[INST] Write a simple Fibonacci number computation function in Rust that does memoization, with comments, in safe Rust.[/INST]\",\n+        ]\n+\n+        model = xLSTMForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16).to(torch_device)\n+        tokenizer.pad_token_id = tokenizer.eos_token_id\n+        # batched generation\n+        tokenized_prompts = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").to(torch_device)\n+        batched_gen = model.generate(**tokenized_prompts, max_new_tokens=30, use_cache=True)\n+        batched_output = tokenizer.batch_decode(batched_gen, skip_special_tokens=True)\n+\n+        # individual generation\n+\n+        for index_gen, individual_prompt in enumerate(prompt):\n+            inputs = tokenizer(individual_prompt, return_tensors=\"pt\", padding=\"longest\").to(torch_device)\n+            individual_gen = model.generate(**inputs, max_new_tokens=30, use_cache=True)\n+            individual_output = tokenizer.batch_decode(individual_gen, skip_special_tokens=True)[0]\n+            self.assertEqual(individual_output[:100], batched_output[index_gen][:100])\n+\n+    def test_batched_equivalence_without_cache(self):\n+        \"\"\"\n+        Verifies that batched generation matches individual generation without cache.\n+        Important because of the specific caching mechanism + statefulness of the xLSTM model.\n+        Depending on precision and devices, differences can be observed from generation to generation.\n+        \"\"\"\n+        tokenizer = self.tokenizer\n+        prompt = [\n+            \"[INST]Write C#.[/INST]\",\n+            \"[INST]Write a hello world in C++.[/INST]\",\n+            \"[INST] Write a simple Fibonacci number computation function in Rust that does memoization, with comments, in safe Rust.[/INST]\",\n+        ]\n+\n+        model = xLSTMForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16).to(torch_device)\n+        tokenizer.pad_token_id = tokenizer.eos_token_id\n+        # batched generation\n+        tokenized_prompts = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").to(torch_device)\n+        batched_gen = model.generate(**tokenized_prompts, max_new_tokens=30, use_cache=True)\n+        batched_output = tokenizer.batch_decode(batched_gen, skip_special_tokens=True)\n+\n+        # individual generation\n+\n+        for index_gen, individual_prompt in enumerate(prompt):\n+            inputs = tokenizer(individual_prompt, return_tensors=\"pt\", padding=\"longest\").to(torch_device)\n+            individual_gen = model.generate(**inputs, max_new_tokens=30, use_cache=True)\n+            individual_output = tokenizer.batch_decode(individual_gen, skip_special_tokens=True)[0]\n+            self.assertEqual(individual_output[:100], batched_output[index_gen][:100])\n+\n+    @require_torch_gpu\n+    def test_xlstm_block_train_vs_eval_equivalence(self):\n+        # Based on https://github.com/sustcsonglin/flash-linear-attention/issues/63\n+        # Credit to zhixuan-lin\n+\n+        B, T, D = 4, 512, 768\n+        dtype = torch.bfloat16\n+        config = xLSTMConfig(num_heads=24, head_dim=64, hidden_size=768, expand=2, n_groups=1)\n+\n+        torch.manual_seed(42)\n+        with torch.amp.autocast(device_type=\"cuda\", dtype=dtype):\n+            with torch.no_grad():\n+                block = xLSTMBlock(config.to_xlstm_block_config(), layer_idx=0).to(\"cuda\")\n+                hidden_states = torch.rand(size=(B, T, D), dtype=dtype, device=\"cuda\")\n+\n+                block.train()\n+                out_train = block(hidden_states)\n+\n+                block.eval()\n+                out_eval = block(hidden_states)\n+\n+                self.assertTrue(torch.allclose(out_train, out_eval, atol=1e-3))"
        },
        {
            "sha": "c358e6a3935a527e2499f503d252b804277d12c7",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6630c5b714048206ce51306fdcff479803f7f4c2/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6630c5b714048206ce51306fdcff479803f7f4c2/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=6630c5b714048206ce51306fdcff479803f7f4c2",
            "patch": "@@ -32,6 +32,7 @@\n CONFIG_MAPPING = transformers.models.auto.configuration_auto.CONFIG_MAPPING\n \n SPECIAL_CASES_TO_ALLOW = {\n+    \"xLSTMConfig\": [\"add_out_norm\", \"chunkwise_kernel\", \"sequence_kernel\", \"step_kernel\"],\n     \"Ernie4_5Config\": [\"tie_word_embeddings\"],\n     \"Ernie4_5_MoeConfig\": [\"tie_word_embeddings\"],\n     \"Lfm2Config\": [\"full_attn_idxs\", \"tie_word_embeddings\"],"
        }
    ],
    "stats": {
        "total": 2391,
        "additions": 2391,
        "deletions": 0
    }
}