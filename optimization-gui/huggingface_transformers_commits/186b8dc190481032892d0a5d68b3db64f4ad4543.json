{
    "author": "gante",
    "message": "Tests: upgrade `test_eager_matches_sdpa_generate` (#34386)",
    "sha": "186b8dc190481032892d0a5d68b3db64f4ad4543",
    "files": [
        {
            "sha": "6f2eaf734df14fa570d0f04b934f278f1ce5b5bd",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -15,6 +15,7 @@\n \n \n import copy\n+import gc\n import inspect\n import tempfile\n import unittest\n@@ -33,6 +34,7 @@\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n+    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -2046,6 +2048,86 @@ def test_inherits_generation_mixin(self):\n         for model_class in self.all_generative_model_classes:\n             self.assertTrue(\"GenerationMixin\" in str(model_class.__bases__))\n \n+    @require_torch_sdpa\n+    @slow\n+    def test_eager_matches_sdpa_generate(self):\n+        max_new_tokens = 30\n+\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_sdpa:\n+                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n+\n+            config, original_inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            inputs_dict = {}\n+            for input_name, input_data in original_inputs_dict.items():\n+                if isinstance(input_data, torch.Tensor) and input_data.dtype in [torch.float32, torch.bfloat16]:\n+                    inputs_dict[input_name] = input_data.to(torch.float16)\n+                else:\n+                    inputs_dict[input_name] = input_data\n+            main_input = inputs_dict[model_class.main_input_name]\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + main_input.shape[1] + 1\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                del model\n+                gc.collect()\n+\n+                generate_kwargs = {\n+                    \"max_new_tokens\": max_new_tokens,\n+                    \"do_sample\": False,\n+                    \"return_dict_in_generate\": True,\n+                    \"output_scores\": True,\n+                }\n+\n+                model_sdpa = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    low_cpu_mem_usage=True,\n+                ).to(torch_device)\n+                res_sdpa = model_sdpa.generate(**inputs_dict, **generate_kwargs)\n+                del model_sdpa\n+                gc.collect()\n+\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    low_cpu_mem_usage=True,\n+                    attn_implementation=\"eager\",\n+                ).to(torch_device)\n+                res_eager = model_eager.generate(**inputs_dict, **generate_kwargs)\n+                del model_eager\n+                gc.collect()\n+\n+                # Eager and SDPA are very similar, but not exactly the same. Because we are using random models, this\n+                # test would be flaky if we only checked the sequences. Two situations in which this test passes:\n+                # 1. The sequences are the same\n+                # 2. The sequences are different, but the scores up until the first mismatch are nearly identical\n+                output_matches = res_eager.sequences == res_sdpa.sequences\n+                has_matching_outputs = output_matches.all()\n+                has_matching_scores = None\n+                if not has_matching_outputs:\n+                    input_length = main_input.shape[1]\n+                    for batch_idx in range(res_eager.sequences.shape[0]):\n+                        batch_matches = output_matches[batch_idx]\n+                        if batch_matches.all():\n+                            continue\n+                        first_mismatch_idx = batch_matches.int().argmin()  # gets the index of the first False\n+                        first_mismatch_idx -= input_length  # scores doesn't include data regarding input tokens\n+                        sdpa_first_mismatch_scores = res_sdpa.scores[first_mismatch_idx][batch_idx]\n+                        eager_first_mismatch_scores = res_eager.scores[first_mismatch_idx][batch_idx]\n+                        has_matching_scores = torch.allclose(\n+                            sdpa_first_mismatch_scores, eager_first_mismatch_scores, rtol=1e-3, atol=1e-3\n+                        )\n+                        if not has_matching_scores:\n+                            break\n+\n+                self.assertTrue(has_matching_outputs or has_matching_scores)\n+\n     def _check_outputs(self, output, main_input, config, use_cache=False, num_return_sequences=1):\n         # we can be sure what is batch size from main input but seq length depends on model type and whether input is text/audio/image\n         # so we infer actual text seq length from model_tester, same was as it is done in `test_modeling_common.py` tests`"
        },
        {
            "sha": "8ac1c3d2b409d0aa37e610b00104941c1eda6d3e",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -22,7 +22,6 @@\n     CaptureLogger,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -672,79 +671,6 @@ def test_torchscript_device_change(self):\n                 loaded = torch.jit.load(os.path.join(tmp, \"bert.pt\"), map_location=torch_device)\n                 loaded(inputs_dict[\"input_ids\"].to(torch_device), inputs_dict[\"attention_mask\"].to(torch_device))\n \n-    # This test was copied from the common test_eager_matches_sdpa_generate(), but without low_cpu_mem_usage=True.\n-    # TODO: Remove this and use the parent method (in common tests) once BERT supports low_cpu_mem_usage=True.\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        max_new_tokens = 30\n-\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(f\"{self.__class__.__name__} tests a model that does support generate: skipping this test\")\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    # low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    # low_cpu_mem_usage=True,\n-                    attn_implementation=\"eager\",\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa:\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # Just test that a large cache works as expected\n-                res_eager = model_eager.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                res_sdpa = model_sdpa.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n-\n \n @require_torch\n class BertModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "cd3b2f978e7ab772a352aca66198a172e31f2567",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 58,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -307,64 +307,6 @@ def test_model_various_embeddings(self):\n     def test_torch_fx_output_loss(self):\n         super().test_torch_fx_output_loss()\n \n-    @require_bitsandbytes\n-    @require_torch_sdpa\n-    @require_torch_multi_gpu\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        max_new_tokens = 30\n-\n-        model_id = \"CohereForAI/c4ai-command-r-v01-4bit\"\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-\n-        model_sdpa = CohereForCausalLM.from_pretrained(\n-            model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\"\n-        )\n-        self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-        model_eager = CohereForCausalLM.from_pretrained(\n-            model_id, torch_dtype=torch.float16, attn_implementation=\"eager\", device_map=\"auto\"\n-        )\n-\n-        self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-        for name, submodule in model_eager.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-        has_sdpa = False\n-        for name, submodule in model_sdpa.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                has_sdpa = True\n-                break\n-        if not has_sdpa:\n-            raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-        texts = [\n-            \"hi here's a longer context, getting longer and\",\n-            \"Hello this is a very long sentence my friend, very long for real\",\n-            \"Today I am in Paris and\",\n-        ]\n-\n-        for padding_side in [\"left\", \"right\"]:\n-            tokenizer.padding_side = padding_side\n-            tokenizer.pad_token = tokenizer.eos_token\n-\n-            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(torch_device)\n-\n-            res_eager = model_eager.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-            res_sdpa = model_sdpa.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-\n-            with self.subTest(f\"{padding_side}\"):\n-                torch.testing.assert_close(\n-                    res_eager,\n-                    res_sdpa,\n-                    msg=f\"\\n{tokenizer.batch_decode(res_eager)} \\nvs\\n{tokenizer.batch_decode(res_sdpa)}\",\n-                )\n-\n \n @require_torch\n @slow"
        },
        {
            "sha": "ce04fae94ea9042ca590a3d78de290b9e57d37ec",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Falcon model.\"\"\"\n \n-import tempfile\n import unittest\n \n from parameterized import parameterized\n@@ -27,7 +26,6 @@\n     set_seed,\n )\n from transformers.testing_utils import (\n-    is_flaky,\n     require_bitsandbytes,\n     require_torch,\n     require_torch_sdpa,\n@@ -520,78 +518,6 @@ def test_model_rope_scaling(self):\n             torch.testing.assert_close(ntk_sin_long, original_sin_long)\n         self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n \n-    # TODO: @Fxmarty\n-    @is_flaky(max_attempts=3, description=\"flaky on some models.\")\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        max_new_tokens = 30\n-\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(f\"{self.__class__.__name__} tests a model that does support generate: skipping this test\")\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                    attn_implementation=\"eager\",\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                # NOTE: This check is disabled for Falcon as the non-SDPA/SDPA implementation is in the same class (legacy reason).\n-                # for name, submodule in model_eager.named_modules():\n-                #     if \"SdpaAttention\" in submodule.__class__.__name__:\n-                #         raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                # has_sdpa = False\n-                # for name, submodule in model_sdpa.named_modules():\n-                #     if \"SdpaAttention\" in submodule.__class__.__name__:\n-                #         has_sdpa = True\n-                #         break\n-                # if not has_sdpa:\n-                #     raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # Just test that a large cache works as expected\n-                res_eager = model_eager.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                res_sdpa = model_sdpa.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n-\n \n @require_torch\n class FalconLanguageGenerationTest(unittest.TestCase):"
        },
        {
            "sha": "32bce7cbfa615e0c53d8b7c3a3b0a70f2be9b0b5",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -758,77 +758,6 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n \n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_generate(self):\n-        \"\"\"Overwrite to add flakyness: outputs sometimes start to diverge after some tokens\"\"\"\n-\n-        max_new_tokens = 30\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                    attn_implementation=\"eager\",\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa:\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # Just test that a large cache works as expected\n-                res_eager = model_eager.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                res_sdpa = model_sdpa.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n-\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "2c3319f02475cc2bfa0deb5d8a8fa4792a269b93",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 63,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -19,7 +19,7 @@\n from parameterized import parameterized\n \n from transformers import AutoTokenizer, GPTNeoXConfig, is_torch_available, set_seed\n-from transformers.testing_utils import require_torch, require_torch_sdpa, slow, torch_device\n+from transformers.testing_utils import require_torch, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -434,68 +434,6 @@ def test_model_rope_scaling(self):\n             torch.testing.assert_close(ntk_sin_long, original_sin_long)\n         self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n \n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        \"\"\"\n-        Based on tests.models.llama.test_modeling_llama.LlamaModelTest.test_eager_matches_sdpa_generate\n-        which also overwrites the common test as the test is flaky on tiny models.\n-        \"\"\"\n-        max_new_tokens = 30\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-1b\")\n-\n-        model_sdpa = GPTNeoXForCausalLM.from_pretrained(\n-            \"EleutherAI/pythia-1b\",\n-            torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n-        ).to(torch_device)\n-\n-        self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-        model_eager = GPTNeoXForCausalLM.from_pretrained(\n-            \"EleutherAI/pythia-1b\",\n-            torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n-            attn_implementation=\"eager\",\n-        ).to(torch_device)\n-\n-        self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-        for name, submodule in model_eager.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-        has_sdpa = False\n-        for name, submodule in model_sdpa.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                has_sdpa = True\n-                break\n-        if not has_sdpa:\n-            raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-        texts = [\n-            \"hi here's a longer context, getting longer and\",\n-            \"Hello this is a very long sentence my friend, very long for real\",\n-            \"Today I am in Paris and\",\n-        ]\n-\n-        for padding_side in [\"left\", \"right\"]:\n-            tokenizer.padding_side = padding_side\n-            tokenizer.pad_token = tokenizer.eos_token\n-\n-            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(torch_device)\n-\n-            res_eager = model_eager.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-            res_sdpa = model_sdpa.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-\n-            with self.subTest(f\"{padding_side}\"):\n-                torch.testing.assert_close(\n-                    res_eager,\n-                    res_sdpa,\n-                    msg=f\"\\n{tokenizer.batch_decode(res_eager)} \\nvs\\n{tokenizer.batch_decode(res_sdpa)}\",\n-                )\n-\n \n @require_torch\n class GPTNeoXLanguageGenerationTest(unittest.TestCase):"
        },
        {
            "sha": "a04d8bba741a23db01d0e7dd6c5bbd12bea290b2",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -24,11 +24,9 @@\n from transformers import AutoTokenizer, JetMoeConfig, is_torch_available\n from transformers.testing_utils import (\n     backend_empty_cache,\n-    is_flaky,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -302,13 +300,6 @@ class JetMoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     test_disk_offload_bin = False\n     test_disk_offload_safetensors = False\n \n-    # TODO: @Fxmarty\n-    @is_flaky(max_attempts=3, description=\"flaky on some models.\")\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        super().test_eager_matches_sdpa_generate()\n-\n     @parameterized.expand([(1, False), (1, True), (4, False)])\n     def test_new_cache_format(self, num_beams, do_sample):\n         pass"
        },
        {
            "sha": "824337d8bdda010a9ecd3d443c9d1526cf8056f1",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 62,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -32,7 +32,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -651,67 +650,6 @@ def test_use_flash_attention_2_true(self):\n                 if not has_flash:\n                     raise ValueError(\"The flash model should have flash attention layers\")\n \n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        max_new_tokens = 30\n-\n-        tokenizer = LlamaTokenizer.from_pretrained(\"saibo/llama-1B\")\n-\n-        model_sdpa = LlamaForCausalLM.from_pretrained(\n-            \"saibo/llama-1B\",\n-            torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n-        ).to(torch_device)\n-\n-        self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-        model_eager = LlamaForCausalLM.from_pretrained(\n-            \"saibo/llama-1B\",\n-            torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n-            attn_implementation=\"eager\",\n-        ).to(torch_device)\n-\n-        self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-        for name, submodule in model_eager.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-        has_sdpa = False\n-        for name, submodule in model_sdpa.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                has_sdpa = True\n-                break\n-        if not has_sdpa:\n-            raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-        texts = [\n-            \"hi here's a longer context, getting longer and\",\n-            \"Hello this is a very long sentence my friend, very long for real\",\n-            \"Today I am in Paris and\",\n-        ]\n-\n-        for padding_side in [\"left\", \"right\"]:\n-            tokenizer.padding_side = padding_side\n-            tokenizer.pad_token = tokenizer.eos_token\n-\n-            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(torch_device)\n-\n-            res_eager = model_eager.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-            res_sdpa = model_sdpa.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-\n-            with self.subTest(f\"{padding_side}\"):\n-                torch.testing.assert_close(\n-                    res_eager,\n-                    res_sdpa,\n-                    msg=f\"\\n{tokenizer.batch_decode(res_eager)} \\nvs\\n{tokenizer.batch_decode(res_sdpa)}\",\n-                )\n-\n     @unittest.skip(\"Broken by the loss update will fix soon @ArthurZucker\")\n     def test_torch_fx_output_loss(self, *args, **kwargs):\n         pass"
        },
        {
            "sha": "f2ee714bcdbafc0fea92298fe4bdf19b2317c508",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -24,7 +24,6 @@\n from transformers import AutoTokenizer, MistralConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n     backend_empty_cache,\n-    is_flaky,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n@@ -332,13 +331,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    # TODO: @Fxmarty\n-    @is_flaky(max_attempts=3, description=\"flaky on some models.\")\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        super().test_eager_matches_sdpa_generate()\n-\n     def setUp(self):\n         self.model_tester = MistralModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=MistralConfig, hidden_size=37)"
        },
        {
            "sha": "b9b5faed851fe424f9a5593317b7911f88f8002b",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -21,11 +21,9 @@\n \n from transformers import MixtralConfig, is_torch_available\n from transformers.testing_utils import (\n-    is_flaky,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -332,13 +330,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    # TODO: @Fxmarty\n-    @is_flaky(max_attempts=3, description=\"flaky on some models.\")\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        super().test_eager_matches_sdpa_generate()\n-\n     def setUp(self):\n         self.model_tester = MixtralModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=MixtralConfig, hidden_size=37)"
        },
        {
            "sha": "3efa7b778fb75cc2522f8ac0ecdb3b3c6e1ea651",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -132,12 +132,6 @@ def setUp(self):\n         self.model_tester = MllamaText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=MllamaTextConfig, has_text_modality=True)\n \n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_generate(self):\n-        super().test_eager_matches_sdpa_generate()\n-\n \n class MllamaVisionText2TextModelTester:\n     def __init__(\n@@ -360,12 +354,6 @@ def _check_attentions_for_generate(\n \n             self.assertListEqual([layer_attention.shape for layer_attention in iter_attentions], expected_shapes)\n \n-    @require_torch_sdpa\n-    @slow\n-    @is_flaky()\n-    def test_eager_matches_sdpa_generate(self):\n-        super().test_eager_matches_sdpa_generate()\n-\n     @require_torch_sdpa\n     @slow\n     @is_flaky()"
        },
        {
            "sha": "dd9302ee2c55baba49e473c8da253a1ed33a6750",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -788,14 +788,10 @@ def test_left_padding_compatibility(self):\n     @slow\n     @is_flaky(max_attempts=5, description=\"flaky on some models.\")\n     def test_eager_matches_sdpa_generate(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n+        \"\"\"Overwritten -- mochi has custom inputs and custom output checks\"\"\"\n \n         max_new_tokens = 5\n \n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(f\"{self.__class__.__name__} tests a model that does support generate: skipping this test\")\n-\n         for model_class in self.all_generative_model_classes:\n             if not model_class._supports_sdpa:\n                 self.skipTest(f\"{model_class.__name__} does not support SDPA\")"
        },
        {
            "sha": "346ad60debe23f3d71120ec6743bcd926589d22e",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -819,74 +819,6 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n \n-    @require_torch_sdpa\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_generate\n-    def test_eager_matches_sdpa_generate(self):\n-        max_new_tokens = 30\n-\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                    attn_implementation=\"eager\",\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa:\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # Just test that a large cache works as expected\n-                res_eager = model_eager.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                res_sdpa = model_sdpa.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n-\n \n def prepare_musicgen_inputs_dict(\n     config,\n@@ -2085,74 +2017,6 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n \n-    @require_torch_sdpa\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_generate\n-    def test_eager_matches_sdpa_generate(self):\n-        max_new_tokens = 30\n-\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                    attn_implementation=\"eager\",\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa:\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # Just test that a large cache works as expected\n-                res_eager = model_eager.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                res_sdpa = model_sdpa.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n-\n     def test_requires_grad_with_frozen_encoders(self):\n         config = self.model_tester.get_config()\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "f3b6be0ac652eb5c968f781c65811fee19fcc367",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 68,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -1866,74 +1866,6 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n \n-    @require_torch_sdpa\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_generate\n-    def test_eager_matches_sdpa_generate(self):\n-        max_new_tokens = 30\n-\n-        # Ignore copy\n-        for model_class in self.greedy_sample_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                    attn_implementation=\"eager\",\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    if \"SdpaAttention\" in submodule.__class__.__name__:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa:\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # Just test that a large cache works as expected\n-                res_eager = model_eager.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                res_sdpa = model_sdpa.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n-\n     def test_requires_grad_with_frozen_encoders(self):\n         config = self.model_tester.get_config()\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "a85e9db34586f9aac2eb2a7143f1e0aad40d6805",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -24,10 +24,8 @@\n from transformers.models.auto.tokenization_auto import AutoTokenizer\n from transformers.models.gpt_neox.tokenization_gpt_neox_fast import GPTNeoXTokenizerFast\n from transformers.testing_utils import (\n-    is_flaky,\n     require_tokenizers,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -317,13 +315,6 @@ def test_model_various_embeddings(self):\n     def test_save_load_fast_init_from_base(self):\n         pass\n \n-    # TODO: @Fxmarty\n-    @is_flaky(max_attempts=3, description=\"flaky on some models.\")\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        super().test_eager_matches_sdpa_generate()\n-\n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n     def test_model_rope_scaling(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "9efadb06eb416ba5b0e7f73c400b75254a3d0820",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -22,10 +22,8 @@\n from transformers.models.auto.tokenization_auto import AutoTokenizer\n from transformers.models.gpt_neox.tokenization_gpt_neox_fast import GPTNeoXTokenizerFast\n from transformers.testing_utils import (\n-    is_flaky,\n     require_tokenizers,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -330,13 +328,6 @@ def test_model_various_embeddings(self):\n     def test_save_load_fast_init_from_base(self):\n         pass\n \n-    # TODO: @Fxmarty\n-    @is_flaky(max_attempts=3, description=\"flaky on some models.\")\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        super().test_eager_matches_sdpa_generate()\n-\n     @parameterized.expand([(\"linear\",), (\"dynamic\",)])\n     def test_model_rope_scaling(self, scaling_type):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "8bae2af804500bb1900eddfffc74478dac0659a9",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 63,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -25,7 +25,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -339,68 +338,6 @@ def test_opt_sequence_classification_model_for_multi_label(self):\n         result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n         self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n \n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        max_new_tokens = 30\n-\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350M\")\n-\n-        texts = [\n-            \"hi here's a longer context, getting longer and\",\n-            \"Hello this is a very long sentence my friend, very long for real\",\n-            \"Today I am in Paris and\",\n-        ]\n-\n-        model_sdpa = OPTForCausalLM.from_pretrained(\n-            \"facebook/opt-350M\",\n-            torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n-            attn_implementation=\"sdpa\",\n-        ).to(torch_device)\n-\n-        self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-        model_eager = OPTForCausalLM.from_pretrained(\n-            \"facebook/opt-350M\",\n-            torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n-            attn_implementation=\"eager\",\n-        ).to(torch_device)\n-\n-        self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-        for _, submodule in model_eager.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-        has_sdpa = False\n-        for _, submodule in model_sdpa.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                has_sdpa = True\n-                break\n-        if not has_sdpa:\n-            raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-        for padding_side in [\"left\", \"right\"]:\n-            tokenizer.padding_side = padding_side\n-            tokenizer.pad_token = tokenizer.eos_token\n-\n-            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(torch_device)\n-\n-            res_eager = model_eager.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-            res_sdpa = model_sdpa.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-\n-            with self.subTest(f\"{padding_side}\"):\n-                torch.testing.assert_close(\n-                    res_eager,\n-                    res_sdpa,\n-                    msg=f\"\\n{tokenizer.batch_decode(res_eager)} \\nvs\\n{tokenizer.batch_decode(res_sdpa)}\",\n-                )\n-\n     @unittest.skip(reason=\"Does not work on the tiny model as we keep hitting edge cases.\")\n     def test_model_parallelism(self):\n         super().test_model_parallelism()"
        },
        {
            "sha": "4e57f8e0f002fbff98c676fde2c4e2edf44576f9",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -343,14 +343,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    # Ignore copy\n-    # TODO: @Fxmarty\n-    @require_torch_sdpa\n-    @slow\n-    @unittest.skip(reason=\"Currently failing.\")\n-    def test_eager_matches_sdpa_generate(self):\n-        super().test_eager_matches_sdpa_generate()\n-\n     def setUp(self):\n         self.model_tester = Qwen2ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Qwen2Config, hidden_size=37)"
        },
        {
            "sha": "c545e882faeeb3bd918181fd199ec03cef46facb",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -368,12 +368,6 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n-    # Ignore copy\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        super().test_eager_matches_sdpa_generate()\n-\n     def setUp(self):\n         self.model_tester = Qwen2MoeModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Qwen2MoeConfig, hidden_size=37)"
        },
        {
            "sha": "91044a4eb750d14c5aefc286e0ed2ce1ac893431",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -21,11 +21,9 @@\n \n from transformers import StableLmConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n-    is_flaky,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n@@ -558,67 +556,3 @@ def test_model_3b_long_prompt(self):\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-3:].tolist())\n-\n-    # Copied from transformers.tests.models.llama.test_modeling_llama.LlamaModelTest.test_eager_matches_sdpa_generate with Llama->StableLm,saibo/llama-1B->stabilityai/stablelm-3b-4e1t\n-    # TODO: @Fxmarty\n-    @is_flaky(max_attempts=3, description=\"flaky on some models.\")\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n-        \"\"\"\n-        max_new_tokens = 30\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\"stabilityai/stablelm-3b-4e1t\")\n-\n-        model_sdpa = StableLmForCausalLM.from_pretrained(\n-            \"stabilityai/stablelm-3b-4e1t\",\n-            torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n-        ).to(torch_device)\n-\n-        self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-        model_eager = StableLmForCausalLM.from_pretrained(\n-            \"stabilityai/stablelm-3b-4e1t\",\n-            torch_dtype=torch.float16,\n-            low_cpu_mem_usage=True,\n-            attn_implementation=\"eager\",\n-        ).to(torch_device)\n-\n-        self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-        for name, submodule in model_eager.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-        has_sdpa = False\n-        for name, submodule in model_sdpa.named_modules():\n-            if \"SdpaAttention\" in submodule.__class__.__name__:\n-                has_sdpa = True\n-                break\n-        if not has_sdpa:\n-            raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-        texts = [\n-            \"hi here's a longer context, getting longer and\",\n-            \"Hello this is a very long sentence my friend, very long for real\",\n-            \"Today I am in Paris and\",\n-        ]\n-\n-        for padding_side in [\"left\", \"right\"]:\n-            tokenizer.padding_side = padding_side\n-            tokenizer.pad_token = tokenizer.eos_token\n-\n-            inputs = tokenizer(texts, return_tensors=\"pt\", padding=True).to(torch_device)\n-\n-            res_eager = model_eager.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-            res_sdpa = model_sdpa.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n-\n-            with self.subTest(f\"{padding_side}\"):\n-                torch.testing.assert_close(\n-                    res_eager,\n-                    res_sdpa,\n-                    msg=f\"\\n{tokenizer.batch_decode(res_eager)} \\nvs\\n{tokenizer.batch_decode(res_sdpa)}\",\n-                )"
        },
        {
            "sha": "5d9abb238e793d4e027fcab2586344bbff3de39c",
            "filename": "tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 80,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -14,11 +14,10 @@\n # limitations under the License.\n \n \n-import tempfile\n import unittest\n \n from transformers import XLMRobertaXLConfig, is_torch_available\n-from transformers.testing_utils import require_torch, require_torch_sdpa, slow, torch_device\n+from transformers.testing_utils import require_torch, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -523,84 +522,6 @@ def test_create_position_ids_from_inputs_embeds(self):\n         self.assertEqual(position_ids.shape, expected_positions.shape)\n         self.assertTrue(torch.all(torch.eq(position_ids, expected_positions)))\n \n-    # TODO: Remove this and use the parent method (in common tests) once XLM RoBERTa XL supports low_cpu_mem_usage=True.\n-    @require_torch_sdpa\n-    @slow\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_generate\n-    def test_eager_matches_sdpa_generate(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        max_new_tokens = 30\n-\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(f\"{self.__class__.__name__} tests a model that does support generate: skipping this test\")\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                # Ignore copy\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=False,\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                # Ignore copy\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=False,\n-                    attn_implementation=\"eager\",\n-                ).to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa:\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # Just test that a large cache works as expected\n-                res_eager = model_eager.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                res_sdpa = model_sdpa.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n-\n \n @require_torch\n class XLMRobertaModelXLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "51d51dfcc2825c8a745c8217a5b52c3ee22e5e97",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 56,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/186b8dc190481032892d0a5d68b3db64f4ad4543/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=186b8dc190481032892d0a5d68b3db64f4ad4543",
            "patch": "@@ -4469,62 +4469,6 @@ def test_sdpa_can_compile_dynamic(self):\n                 with torch.no_grad():\n                     _ = model(**inputs_dict)\n \n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_generate(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        max_new_tokens = 30\n-\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(f\"{self.__class__.__name__} tests a model that does support generate: skipping this test\")\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_sdpa:\n-                self.skipTest(f\"{model_class.__name__} does not support SDPA\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n-                dummy_input = dummy_input.to(torch.float16)\n-\n-            # make sure that all models have enough positions for generation\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                dummy_attention_mask = inputs_dict.get(\"attention_mask\", torch.ones_like(dummy_input))\n-\n-                model_sdpa = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                ).to(torch_device)\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch.float16,\n-                    low_cpu_mem_usage=True,\n-                    attn_implementation=\"eager\",\n-                ).to(torch_device)\n-\n-                # Just test that a large cache works as expected\n-                res_eager = model_eager.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                res_sdpa = model_sdpa.generate(\n-                    dummy_input, attention_mask=dummy_attention_mask, max_new_tokens=max_new_tokens, do_sample=False\n-                )\n-\n-                self.assertTrue(torch.allclose(res_eager, res_sdpa))\n-\n     @require_torch_sdpa\n     def test_sdpa_matches_eager_sliding_window(self):\n         if not self.has_attentions:"
        }
    ],
    "stats": {
        "total": 1031,
        "additions": 85,
        "deletions": 946
    }
}