{
    "author": "baptxste",
    "message": "Fix import torchao.prototype.low_bit_optim since torchao  v0.11 (#38174)\n\n* Fix ModuleNotFoundError torchao.prototype.low_bit_optim since torchao v 0.11.0\n\n* Fix space on blank line\n\n* update torchao's AdamW4bit and AdamW8bit import for v0.11.0\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "a4389494c74891b7d496a5aafd4ae795f909b21d",
    "files": [
        {
            "sha": "7df1fa6a930b3d25747e37f8ba162e82eee56458",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4389494c74891b7d496a5aafd4ae795f909b21d/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4389494c74891b7d496a5aafd4ae795f909b21d/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=a4389494c74891b7d496a5aafd4ae795f909b21d",
            "patch": "@@ -1679,8 +1679,11 @@ def optimizer_hook(param):\n                     \"You need to have `torch>2.4` in order to use torch 4-bit optimizers. \"\n                     \"Install it with `pip install --upgrade torch` it is available on pipy. Otherwise, you need to install torch nightly.\"\n                 )\n-            from torchao.prototype.low_bit_optim import AdamW4bit, AdamW8bit\n-\n+            if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.11.0\"):\n+                # https://github.com/pytorch/ao/pull/2159\n+                from torchao.optim import AdamW4bit, AdamW8bit\n+            else:\n+                from torchao.prototype.low_bit_optim import AdamW4bit, AdamW8bit\n             if args.optim == OptimizerNames.ADAMW_TORCH_4BIT:\n                 optimizer_cls = AdamW4bit\n             elif args.optim == OptimizerNames.ADAMW_TORCH_8BIT:"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 5,
        "deletions": 2
    }
}