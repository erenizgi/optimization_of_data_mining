{
    "author": "zucchini-nlp",
    "message": "Update Glm4V processor and add tests (#39988)\n\n* update GLm4V and add tests\n\n* Update tests/models/glm4v/test_processor_glm4v.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* remove min/max pixels for BC\n\n* fix video tests\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a",
    "files": [
        {
            "sha": "678eb10406a974e53ae6e0ecd36e6be85ea354df",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a",
            "patch": "@@ -70,8 +70,8 @@ def smart_resize(\n \n     if t_bar * h_bar * w_bar > max_pixels:\n         beta = math.sqrt((num_frames * height * width) / max_pixels)\n-        h_bar = math.floor(height / beta / factor) * factor\n-        w_bar = math.floor(width / beta / factor) * factor\n+        h_bar = max(factor, math.floor(height / beta / factor) * factor)\n+        w_bar = max(factor, math.floor(width / beta / factor) * factor)\n     elif t_bar * h_bar * w_bar < min_pixels:\n         beta = math.sqrt(min_pixels / (num_frames * height * width))\n         h_bar = math.ceil(height * beta / factor) * factor\n@@ -247,6 +247,8 @@ def _preprocess(\n                     width=width,\n                     temporal_factor=temporal_patch_size,\n                     factor=patch_size * merge_size,\n+                    min_pixels=size[\"shortest_edge\"],\n+                    max_pixels=size[\"longest_edge\"],\n                 )\n                 image = resize(\n                     image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n@@ -371,7 +373,7 @@ def preprocess(\n \n         if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n             raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n-        else:\n+        elif size is None:\n             size = {\"shortest_edge\": 112 * 112, \"longest_edge\": 28 * 28 * 15000}\n \n         do_resize = do_resize if do_resize is not None else self.do_resize\n@@ -451,6 +453,7 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         \"\"\"\n         patch_size = images_kwargs.get(\"patch_size\", self.patch_size)\n         merge_size = images_kwargs.get(\"merge_size\", self.merge_size)\n+        size = images_kwargs.get(\"size\", self.size)\n \n         factor = patch_size * merge_size\n         resized_height, resized_width = smart_resize(\n@@ -459,6 +462,8 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n             width=width,\n             factor=factor,\n             temporal_factor=self.temporal_patch_size,\n+            min_pixels=size[\"shortest_edge\"],\n+            max_pixels=size[\"longest_edge\"],\n         )\n         grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n         return grid_h * grid_w"
        },
        {
            "sha": "d93bc5370219d318b58fc51f1a815f072c0b4a69",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v_fast.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py?ref=1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a",
            "patch": "@@ -87,11 +87,30 @@ class Glm4vImageProcessorFast(BaseImageProcessorFast):\n \n     def __init__(self, **kwargs: Unpack[Glm4vFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n+        if self.size is not None and (\n+            self.size.get(\"shortest_edge\", None) is None or self.size.get(\"longest_edge\", None) is None\n+        ):\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+\n+        return super()._further_process_kwargs(size=size, **kwargs)\n \n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n         do_resize: bool,\n+        size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_rescale: bool,\n         rescale_factor: float,\n@@ -121,6 +140,8 @@ def _preprocess(\n                 width=width,\n                 temporal_factor=temporal_patch_size,\n                 factor=patch_size * merge_size,\n+                min_pixels=size.shortest_edge,\n+                max_pixels=size.longest_edge,\n             )\n             all_target_sizes.append((resized_height, resized_width))\n "
        },
        {
            "sha": "ddee09a8e876a816ec91a81b3c97c22b8cedfcaa",
            "filename": "src/transformers/models/glm4v/video_processing_glm4v.py",
            "status": "modified",
            "additions": 21,
            "deletions": 1,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py?ref=1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a",
            "patch": "@@ -99,6 +99,24 @@ class Glm4vVideoProcessor(BaseVideoProcessor):\n \n     def __init__(self, **kwargs: Unpack[Glm4vVideoProcessorInitKwargs]):\n         super().__init__(**kwargs)\n+        if self.size is not None and (\n+            self.size.get(\"shortest_edge\", None) is None or self.size.get(\"longest_edge\", None) is None\n+        ):\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+\n+        return super()._further_process_kwargs(size=size, **kwargs)\n \n     def sample_frames(\n         self,\n@@ -144,6 +162,7 @@ def _preprocess(\n         videos: list[torch.Tensor],\n         video_metadata: Optional[Union[list[VideoMetadata], list[dict]]] = None,\n         do_resize: bool = True,\n+        size: bool = SizeDict,\n         interpolation: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255.0,\n@@ -188,7 +207,8 @@ def _preprocess(\n                     width=width,\n                     temporal_factor=temporal_patch_size,\n                     factor=patch_size * merge_size,\n-                    max_pixels=self.max_image_size[\"longest_edge\"],\n+                    min_pixels=size.shortest_edge,\n+                    max_pixels=size.longest_edge,\n                 )\n                 stacked_videos = stacked_videos.view(B * T, C, H, W)\n                 stacked_videos = self.resize("
        },
        {
            "sha": "eef9b20d91f0631ae9d85b2805a1e38a447f45c6",
            "filename": "tests/models/glm4v/test_processor_glm4v.py",
            "status": "added",
            "additions": 257,
            "deletions": 0,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py?ref=1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a",
            "patch": "@@ -0,0 +1,257 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+\n+from transformers import AutoProcessor\n+from transformers.testing_utils import require_av, require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import Glm4vProcessor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@require_vision\n+@require_torch\n+class Glm4vProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Glm4vProcessor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        processor = Glm4vProcessor.from_pretrained(\n+            \"THUDM/GLM-4.1V-9B-Thinking\", patch_size=4, size={\"shortest_edge\": 12 * 12, \"longest_edge\": 18 * 18}\n+        )\n+        processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_video_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n+\n+    def get_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    @require_torch\n+    @require_av\n+    def _test_apply_chat_template(\n+        self,\n+        modality: str,\n+        batch_size: int,\n+        return_tensors: str,\n+        input_name: str,\n+        processor_name: str,\n+        input_data: list[str],\n+    ):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        if processor_name not in self.processor_class.attributes:\n+            self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n+\n+        batch_messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [{\"type\": \"text\", \"text\": \"Describe this.\"}],\n+                },\n+            ]\n+        ] * batch_size\n+\n+        # Test that jinja can be applied\n+        formatted_prompt = processor.apply_chat_template(batch_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), batch_size)\n+\n+        # Test that tokenizing with template and directly with `self.tokenizer` gives same output\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            batch_messages, add_generation_prompt=True, tokenize=True, return_tensors=return_tensors\n+        )\n+        add_special_tokens = True\n+        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n+            add_special_tokens = False\n+        tok_output = processor.tokenizer(\n+            formatted_prompt, return_tensors=return_tensors, add_special_tokens=add_special_tokens\n+        )\n+        expected_output = tok_output.input_ids\n+        self.assertListEqual(expected_output.tolist(), formatted_prompt_tokenized.tolist())\n+\n+        # Test that kwargs passed to processor's `__call__` are actually used\n+        tokenized_prompt_100 = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            truncation=True,\n+            return_tensors=return_tensors,\n+            max_length=100,\n+        )\n+        self.assertEqual(len(tokenized_prompt_100[0]), 100)\n+\n+        # Test that `return_dict=True` returns text related inputs in the dict\n+        out_dict_text = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+        )\n+        self.assertTrue(all(key in out_dict_text for key in [\"input_ids\", \"attention_mask\"]))\n+        self.assertEqual(len(out_dict_text[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict_text[\"attention_mask\"]), batch_size)\n+\n+        # Test that with modality URLs and `return_dict=True`, we get modality inputs in the dict\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": modality, \"url\": url}]\n+\n+        out_dict = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+            fps=2,  # by default no more than 2 frames per second, otherwise too slow\n+        )\n+        input_name = getattr(self, input_name)\n+        self.assertTrue(input_name in out_dict)\n+        self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n+\n+        if modality == \"video\":\n+            # qwen pixels don't scale with bs same way as other models, calculate expected video token count based on video_grid_thw\n+            expected_video_token_count = 0\n+            for thw in out_dict[\"video_grid_thw\"]:\n+                expected_video_token_count += thw[0] * thw[1] * thw[2]\n+            mm_len = expected_video_token_count\n+        else:\n+            mm_len = batch_size * 4\n+        self.assertEqual(len(out_dict[input_name]), mm_len)\n+\n+        return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}\n+        for k in out_dict:\n+            self.assertIsInstance(out_dict[k], return_tensor_to_type[return_tensors])\n+\n+    @require_av\n+    @unittest.skip(\"GLM4V can't sample frames from image frames\")\n+    def test_apply_chat_template_video_1(self):\n+        pass\n+\n+    @require_av\n+    @unittest.skip(\"GLM4V can't sample frames from image frames\")\n+    def test_apply_chat_template_video_2(self):\n+        pass\n+\n+    @require_av\n+    def test_apply_chat_template_video_frame_sampling(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\"},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Add video URL for return dict and load with `num_frames` arg\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+        }\n+\n+        # Load with `video_fps` arg\n+        video_fps = 1\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            video_fps=video_fps,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 40)\n+\n+        # Load without any arg should load the whole video\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            do_sample_frames=False,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 600)\n+\n+        # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n+        # because we assume they come from one video\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": [\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+            ],\n+        }\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            do_sample_frames=False,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 4)"
        },
        {
            "sha": "5f597aae79cae543a715dfdd71d3e932e647ddb9",
            "filename": "tests/models/glm4v/test_video_processing_glm4v.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_video_processing_glm4v.py?ref=1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a",
            "patch": "@@ -52,7 +52,7 @@ def __init__(\n         image_std=IMAGENET_STANDARD_STD,\n         do_convert_rgb=True,\n     ):\n-        size = size if size is not None else {\"longest_edge\": 20}\n+        size = size if size is not None else {\"longest_edge\": 20, \"shortest_edge\": 10}\n         self.parent = parent\n         self.batch_size = batch_size\n         self.num_frames = num_frames\n@@ -129,6 +129,8 @@ def expected_output_video_shape(self, videos):\n                 height,\n                 width,\n                 factor=self.patch_size * self.merge_size,\n+                min_pixels=self.size[\"shortest_edge\"],\n+                max_pixels=self.size[\"longest_edge\"],\n             )\n             grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n             seq_len += grid_t * grid_h * grid_w\n@@ -163,10 +165,12 @@ def video_processor_dict(self):\n \n     def test_video_processor_from_dict_with_kwargs(self):\n         video_processor = self.fast_video_processing_class.from_dict(self.video_processor_dict)\n-        self.assertEqual(video_processor.size, {\"longest_edge\": 20})\n+        self.assertEqual(video_processor.size, {\"longest_edge\": 20, \"shortest_edge\": 10})\n \n-        video_processor = self.fast_video_processing_class.from_dict(self.video_processor_dict, size=42)\n-        self.assertEqual(video_processor.size, {\"height\": 42, \"width\": 42})\n+        video_processor = self.fast_video_processing_class.from_dict(\n+            self.video_processor_dict, size={\"longest_edge\": 42, \"shortest_edge\": 42}\n+        )\n+        self.assertEqual(video_processor.size, {\"longest_edge\": 42, \"shortest_edge\": 42})\n \n     def test_call_pil(self):\n         for video_processing_class in self.video_processor_list:"
        },
        {
            "sha": "f757a6199908c1d83732bec08fe4f7d5a0d3203b",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 21,
            "deletions": 6,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=1c5e17c025c4f1f5f5bc70e7d7e2b7b886d14a3a",
            "patch": "@@ -536,7 +536,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_video(self):\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n-        inputs = processor(text=input_str, videos=video_input, return_tensors=\"pt\")\n+        inputs = processor(text=input_str, videos=video_input, do_sample_frames=False, return_tensors=\"pt\")\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 167)\n \n     def test_video_processor_defaults_preserved_by_video_kwargs(self):\n@@ -560,7 +560,7 @@ def test_video_processor_defaults_preserved_by_video_kwargs(self):\n         input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n \n-        inputs = processor(text=input_str, videos=video_input, return_tensors=\"pt\")\n+        inputs = processor(text=input_str, videos=video_input, do_sample_frames=False, return_tensors=\"pt\")\n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n \n     def test_kwargs_overrides_default_tokenizer_kwargs_video(self):\n@@ -575,7 +575,12 @@ def test_kwargs_overrides_default_tokenizer_kwargs_video(self):\n         input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n         inputs = processor(\n-            text=input_str, videos=video_input, return_tensors=\"pt\", max_length=162, padding=\"max_length\"\n+            text=input_str,\n+            videos=video_input,\n+            do_sample_frames=False,\n+            return_tensors=\"pt\",\n+            max_length=162,\n+            padding=\"max_length\",\n         )\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 162)\n \n@@ -595,7 +600,14 @@ def test_kwargs_overrides_default_video_processor_kwargs(self):\n         input_str = self.prepare_text_inputs(modality=\"video\")\n         video_input = self.prepare_video_inputs()\n \n-        inputs = processor(text=input_str, videos=video_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n+        inputs = processor(\n+            text=input_str,\n+            videos=video_input,\n+            do_sample_frames=False,\n+            do_rescale=True,\n+            rescale_factor=-1,\n+            return_tensors=\"pt\",\n+        )\n         self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n \n     def test_unstructured_kwargs_video(self):\n@@ -611,6 +623,7 @@ def test_unstructured_kwargs_video(self):\n         inputs = processor(\n             text=input_str,\n             videos=video_input,\n+            do_sample_frames=False,\n             return_tensors=\"pt\",\n             do_rescale=True,\n             rescale_factor=-1,\n@@ -634,6 +647,7 @@ def test_unstructured_kwargs_batched_video(self):\n         inputs = processor(\n             text=input_str,\n             videos=video_input,\n+            do_sample_frames=False,\n             return_tensors=\"pt\",\n             do_rescale=True,\n             rescale_factor=-1,\n@@ -661,6 +675,7 @@ def test_doubly_passed_kwargs_video(self):\n             _ = processor(\n                 text=input_str,\n                 videos=video_input,\n+                do_sample_frames=False,\n                 videos_kwargs={\"do_rescale\": True, \"rescale_factor\": -1},\n                 do_rescale=True,\n                 return_tensors=\"pt\",\n@@ -680,7 +695,7 @@ def test_structured_kwargs_nested_video(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1, \"do_sample_frames\": False},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 176},\n         }\n \n@@ -703,7 +718,7 @@ def test_structured_kwargs_nested_from_dict_video(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n+            \"videos_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1, \"do_sample_frames\": False},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 176},\n         }\n "
        }
    ],
    "stats": {
        "total": 350,
        "additions": 336,
        "deletions": 14
    }
}