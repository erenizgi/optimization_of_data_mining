{
    "author": "yao-matrix",
    "message": "enable more test cases on xpu (#38572)\n\n* enable glm4 integration cases on XPU, set xpu expectation for blip2\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n* more\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* refine wording\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* refine test case names\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* run\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* add gemma2 and chameleon\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix review comments\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>",
    "sha": "89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
    "files": [
        {
            "sha": "fbc956986f4c6b1341936efe64b63ab5a2bac1ef",
            "filename": "tests/generation/test_paged_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fgeneration%2Ftest_paged_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fgeneration%2Ftest_paged_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_paged_attention.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -25,8 +25,8 @@\n \n \n @slow\n-@require_torch_gpu\n @require_flash_attn\n+@require_torch_gpu\n class TestBatchGeneration(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):"
        },
        {
            "sha": "701cd7938c8c7e787dc8c100f6807fef8c6e9562",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -34,6 +34,7 @@\n     BarkSemanticGenerationConfig,\n )\n from transformers.testing_utils import (\n+    backend_torch_accelerator_module,\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n@@ -1306,7 +1307,7 @@ def test_generate_end_to_end_with_offload(self):\n             # standard generation\n             output_with_no_offload = self.model.generate(**input_ids, do_sample=False, temperature=1.0)\n \n-            torch_accelerator_module = getattr(torch, torch_device, torch.cuda)\n+            torch_accelerator_module = backend_torch_accelerator_module(torch_device)\n \n             torch_accelerator_module.empty_cache()\n "
        },
        {
            "sha": "06c87300e7325c679e0f0a241a828a7485c263be",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 3,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -1708,10 +1708,14 @@ def test_inference_t5(self):\n \n         expectations = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n+                    \"a woman is playing with her dog on the beach\",\n+                ],\n                 (\"cuda\", 7): [\n                     [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n                     \"a woman is playing with her dog on the beach\",\n-                ]\n+                ],\n             }\n         )\n         expected_outputs = expectations.get_expectation()\n@@ -1729,10 +1733,14 @@ def test_inference_t5(self):\n \n         expectations = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    [0, 3, 7, 152, 2515, 11389, 3523, 1],\n+                    \"san francisco\",\n+                ],\n                 (\"cuda\", 7): [\n                     [0, 3, 7, 152, 2515, 11389, 3523, 1],\n                     \"san francisco\",\n-                ]\n+                ],\n             }\n         )\n         expected_outputs = expectations.get_expectation()\n@@ -1755,10 +1763,14 @@ def test_inference_t5_batched_beam_search(self):\n \n         expectations = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n+                    [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n+                ],\n                 (\"cuda\", 7): [\n                     [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n                     [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n-                ]\n+                ],\n             }\n         )\n         expected_predictions = expectations.get_expectation()"
        },
        {
            "sha": "bca344e2f7305a1b8da8a0f5e1a5cfd49c9fa102",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -420,6 +420,7 @@ def test_model_7b(self):\n         # greedy generation outputs\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n+                (\"xpu\", 3): ['Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue dot in the center representing the star Altair. The star map is set against a black background, with the constellations visible in the night'],\n                 (\"cuda\", 7): ['Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue dot in the center representing the star Alpha Centauri. The star map is a representation of the night sky, showing the positions of stars in'],\n                 (\"cuda\", 8): ['Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue dot representing the position of the star Alpha Centauri. Alpha Centauri is the brightest star in the constellation Centaurus and is located'],\n             }\n@@ -457,6 +458,10 @@ def test_model_7b_batched(self):\n         # greedy generation outputs\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    'Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue dot in the center representing the star Altair. The star map is set against a black background, with the constellations visible in the night',\n+                    'What constellation is this image showing?The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.',\n+                ],\n                 (\"cuda\", 7): [\n                     'Describe what do you see here and tell me about the history behind it?The image depicts a star map, with a bright blue line extending across the center of the image. The line is labeled \"390 light years\" and is accompanied by a small black and',\n                     'What constellation is this image showing?The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.The image shows the constellation of Orion.',"
        },
        {
            "sha": "c2add22ee54c1e890264a1de2a73872b9e354342",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -19,7 +19,7 @@\n from transformers.testing_utils import (\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n     require_torch_sdpa,\n     slow,\n     torch_device,\n@@ -203,7 +203,7 @@ def test_torch_fx_output_loss(self):\n @require_torch\n @slow\n class CohereIntegrationTest(unittest.TestCase):\n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     @require_bitsandbytes\n     def test_batched_4bit(self):\n         model_id = \"CohereForAI/c4ai-command-r-v01-4bit\""
        },
        {
            "sha": "80aae3e9cfd52ac73050aa6785b55f7127da7c00",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch ColQwen2 model.\"\"\"\n \n-import gc\n import unittest\n from typing import ClassVar\n \n@@ -27,7 +26,7 @@\n from transformers.models.colqwen2.configuration_colqwen2 import ColQwen2Config\n from transformers.models.colqwen2.modeling_colqwen2 import ColQwen2ForRetrieval, ColQwen2ForRetrievalOutput\n from transformers.models.colqwen2.processing_colqwen2 import ColQwen2Processor\n-from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.testing_utils import cleanup, require_torch, require_vision, slow, torch_device\n \n \n if is_torch_available():\n@@ -282,8 +281,7 @@ def setUp(self):\n         self.processor = ColQwen2Processor.from_pretrained(self.model_name)\n \n     def tearDown(self):\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     def test_model_integration_test(self):"
        },
        {
            "sha": "f6bf929eadf9c918f19b8a2fd15e2a735f8221fb",
            "filename": "tests/models/deformable_detr/test_image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -19,7 +19,13 @@\n \n import numpy as np\n \n-from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import AnnotationFormatTestMixin, ImageProcessingTestMixin, prepare_image_inputs\n@@ -607,9 +613,9 @@ def test_longest_edge_shortest_edge_resizing_strategy(self):\n             self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 50, 50]))\n \n     @slow\n-    @require_torch_gpu\n-    # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations\n-    def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n+    @require_torch_accelerator\n+    # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_fast_processor_equivalence_cpu_accelerator_coco_detection_annotations\n+    def test_fast_processor_equivalence_cpu_accelerator_coco_detection_annotations(self):\n         # prepare image and target\n         image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n         with open(\"./tests/fixtures/tests_samples/COCO/coco_annotations.txt\") as f:\n@@ -622,8 +628,8 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n \n         # 1. run processor on CPU\n         encoding_cpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cpu\")\n-        # 2. run processor on GPU\n-        encoding_gpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cuda\")\n+        # 2. run processor on accelerator\n+        encoding_gpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=torch_device)\n \n         # verify pixel values\n         self.assertEqual(encoding_cpu[\"pixel_values\"].shape, encoding_gpu[\"pixel_values\"].shape)\n@@ -665,9 +671,9 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n         torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\"))\n \n     @slow\n-    @require_torch_gpu\n-    # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations\n-    def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n+    @require_torch_accelerator\n+    # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_fast_processor_equivalence_cpu_accelerator_coco_panoptic_annotations\n+    def test_fast_processor_equivalence_cpu_accelerator_coco_panoptic_annotations(self):\n         # prepare image, target and masks_path\n         image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n         with open(\"./tests/fixtures/tests_samples/COCO/coco_panoptic_annotations.txt\") as f:\n@@ -684,9 +690,9 @@ def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n         encoding_cpu = processor(\n             images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=\"cpu\"\n         )\n-        # 2. run processor on GPU\n+        # 2. run processor on accelerator\n         encoding_gpu = processor(\n-            images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=\"cuda\"\n+            images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=torch_device\n         )\n \n         # verify pixel values"
        },
        {
            "sha": "718673e617f3a541d8a0a1a9cfce1781ef588484",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -746,7 +746,7 @@ def test_inference_object_detection_head_with_box_refine_two_stage(self):\n         torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=1e-4, atol=1e-4)\n \n     @require_torch_accelerator\n-    def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n+    def test_inference_object_detection_head_equivalence_cpu_accelerator(self):\n         image_processor = self.default_image_processor\n         image = prepare_img()\n         encoding = image_processor(images=image, return_tensors=\"pt\")\n@@ -759,7 +759,7 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n         with torch.no_grad():\n             cpu_outputs = model(pixel_values, pixel_mask)\n \n-        # 2. run model on GPU\n+        # 2. run model on accelerator\n         model.to(torch_device)\n \n         with torch.no_grad():"
        },
        {
            "sha": "d9cff61a10619c0061adc6b8c9d37ec2fad6ce28",
            "filename": "tests/models/detr/test_image_processing_detr.py",
            "status": "modified",
            "additions": 16,
            "deletions": 9,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -18,7 +18,14 @@\n \n import numpy as np\n \n-from transformers.testing_utils import require_torch, require_torch_gpu, require_torchvision, require_vision, slow\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torchvision,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import AnnotationFormatTestMixin, ImageProcessingTestMixin, prepare_image_inputs\n@@ -666,9 +673,9 @@ def test_longest_edge_shortest_edge_resizing_strategy(self):\n             self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 50, 50]))\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_torchvision\n-    def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n+    def test_fast_processor_equivalence_cpu_accelerator_coco_detection_annotations(self):\n         # prepare image and target\n         image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n         with open(\"./tests/fixtures/tests_samples/COCO/coco_annotations.txt\") as f:\n@@ -679,8 +686,8 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n         processor = self.image_processor_list[1]()\n         # 1. run processor on CPU\n         encoding_cpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cpu\")\n-        # 2. run processor on GPU\n-        encoding_gpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cuda\")\n+        # 2. run processor on accelerator\n+        encoding_gpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=torch_device)\n \n         # verify pixel values\n         self.assertEqual(encoding_cpu[\"pixel_values\"].shape, encoding_gpu[\"pixel_values\"].shape)\n@@ -722,9 +729,9 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n         torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\"))\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_torchvision\n-    def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n+    def test_fast_processor_equivalence_cpu_accelerator_coco_panoptic_annotations(self):\n         # prepare image, target and masks_path\n         image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n         with open(\"./tests/fixtures/tests_samples/COCO/coco_panoptic_annotations.txt\") as f:\n@@ -739,9 +746,9 @@ def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n         encoding_cpu = processor(\n             images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=\"cpu\"\n         )\n-        # 2. run processor on GPU\n+        # 2. run processor on accelerator\n         encoding_gpu = processor(\n-            images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=\"cuda\"\n+            images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=torch_device\n         )\n \n         # verify pixel values"
        },
        {
            "sha": "825bf16506540a98551447523893c1c00b8a3d50",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -258,10 +258,14 @@ def test_model_2b_pipeline_bf16_flex_attention(self):\n         # EXPECTED_TEXTS should match the same non-pipeline test, minus the special tokens\n         EXPECTED_BATCH_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    \"Hello I am doing a project on the 1960s and I am trying to find out what the average\",\n+                    \"Hi today I'm going to be talking about the 10 most powerful characters in the Naruto series.\",\n+                ],\n                 (\"cuda\", 8): [\n                     \"Hello I am doing a project on the 1960s and I am trying to find out what the average\",\n                     \"Hi today I'm going to be talking about the 10 most powerful characters in the Naruto series.\",\n-                ]\n+                ],\n             }\n         )\n         EXPECTED_BATCH_TEXT = EXPECTED_BATCH_TEXTS.get_expectation()\n@@ -315,6 +319,9 @@ def test_export_static_cache(self):\n         tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\", pad_token=\"</s>\", padding_side=\"right\")\n         EXPECTED_TEXT_COMPLETIONS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\"\n+                ],\n                 (\"cuda\", 7): [\n                     \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\"\n                 ],"
        },
        {
            "sha": "40d6166ab062c744aadad66bd7538093be0bdd39",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 23,
            "deletions": 4,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -31,6 +31,7 @@\n     Expectations,\n     cleanup,\n     is_flash_attn_2_available,\n+    require_deterministic_for_xpu,\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n@@ -386,6 +387,7 @@ def setUp(self):\n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n+    @require_deterministic_for_xpu\n     def test_model_4b_bf16(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n@@ -406,6 +408,7 @@ def test_model_4b_bf16(self):\n \n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water in the background. It looks like a lovely,'],\n                 (\"cuda\", 7): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water in the background. It looks like a lovely,'],\n                 (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and'],\n             }\n@@ -414,6 +417,7 @@ def test_model_4b_bf16(self):\n         self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @require_torch_large_accelerator\n+    @require_deterministic_for_xpu\n     def test_model_4b_batch(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n@@ -450,12 +454,17 @@ def test_model_4b_batch(self):\n \n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3):\n+                    [\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and',\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. They depict very different scenes:\\n\\n*   **Image 1** shows a cow standing on a beach.',\n+                    ],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8):\n                     [\n                         'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and',\n                         'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. They depict very different scenes:\\n\\n*   **Image 1** shows a cow standing on a beach.',\n-                    ]\n+                    ],\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n@@ -493,15 +502,17 @@ def test_model_4b_crops(self):\n         EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.'],\n                 (\"cuda\", 7): [],\n-                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.']\n+                (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.'],\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n         self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n         self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @require_torch_large_accelerator\n+    @require_deterministic_for_xpu\n     def test_model_4b_batch_crops(self):\n         model_id = \"google/gemma-3-4b-it\"\n \n@@ -546,11 +557,15 @@ def test_model_4b_batch_crops(self):\n         EXPECTED_NUM_IMAGES = 9  # 3 * (one for the origin image and two crops of images) = 9\n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.',\n+                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nThe first image shows a cow on a beach, while the second image shows a street scene with a',\n+                ],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8): [\n                     'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.',\n                     'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nThe first image shows a cow on a beach, while the second image shows a street scene with a',\n-                ]\n+                ],\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n@@ -589,13 +604,15 @@ def test_model_4b_multiimage(self):\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image!\\n\\nHere's a description of the scene:\\n\\n*   **Chinese Arch\"],\n                 (\"cuda\", 7): [],\n-                (\"cuda\", 8): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Main Features:**\\n\\n*   **Chinese Archway:** The most prominent\"]\n+                (\"cuda\", 8): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Main Features:**\\n\\n*   **Chinese Archway:** The most prominent\"],\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n         self.assertEqual(output_text, EXPECTED_TEXT)\n \n+    @require_deterministic_for_xpu\n     def test_model_1b_text_only(self):\n         model_id = \"google/gemma-3-1b-it\"\n \n@@ -610,6 +627,7 @@ def test_model_1b_text_only(self):\n \n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): ['Write a poem about Machine Learning.\\n\\n---\\n\\nThe data flows, a river deep,\\nWith patterns hidden, secrets sleep.\\nA neural net, a watchful eye,\\nLearning'],\n                 (\"cuda\", 7): ['Write a poem about Machine Learning.\\n\\n---\\n\\nThe data flows, a silent stream,\\nInto the neural net, a waking dream.\\nAlgorithms hum, a coded grace,\\n'],\n                 (\"cuda\", 8): ['Write a poem about Machine Learning.\\n\\n---\\n\\nThe data flows, a silent stream,\\nInto the neural net, a waking dream.\\nAlgorithms hum, a coded grace,\\n'],\n             }\n@@ -641,6 +659,7 @@ def test_model_4b_flash_attn(self):\n \n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant island in the background. It looks like a sunny day'],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant island in the background. It looks like a sunny day'],\n             }"
        },
        {
            "sha": "2775b401d5ee7afbb6ba5ffefe05172f4904c732",
            "filename": "tests/models/glm4/test_modeling_glm4.py",
            "status": "modified",
            "additions": 19,
            "deletions": 1,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4%2Ftest_modeling_glm4.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -24,6 +24,7 @@\n     cleanup,\n     require_flash_attn,\n     require_torch,\n+    require_torch_large_accelerator,\n     require_torch_large_gpu,\n     require_torch_sdpa,\n     slow,\n@@ -79,7 +80,7 @@ class Glm4ModelTest(CausalLMModelTest, unittest.TestCase):\n \n \n @slow\n-@require_torch_large_gpu\n+@require_torch_large_accelerator\n class Glm4IntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n     model_id = \"THUDM/GLM-4-9B-0414\"\n@@ -90,6 +91,10 @@ def tearDown(self):\n     def test_model_9b_fp16(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8): [\n                     \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n@@ -114,6 +119,10 @@ def test_model_9b_fp16(self):\n     def test_model_9b_bf16(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8): [\n                     \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n@@ -138,6 +147,10 @@ def test_model_9b_bf16(self):\n     def test_model_9b_eager(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and who\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8): [\n                     \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n@@ -167,6 +180,10 @@ def test_model_9b_eager(self):\n     def test_model_9b_sdpa(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n+                (\"xpu\", 3): [\n+                    \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n+                    \"Hi today I am going to tell you about the most common disease in the world. This disease is called diabetes\",\n+                ],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", 8): [\n                     \"Hello I am doing a project on the history of the internet and I need to know what the first website was and what\",\n@@ -193,6 +210,7 @@ def test_model_9b_sdpa(self):\n         self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @require_flash_attn\n+    @require_torch_large_gpu\n     @pytest.mark.flash_attn_test\n     def test_model_9b_flash_attn(self):\n         EXPECTED_TEXTS = Expectations("
        },
        {
            "sha": "84636954a989b2d2ef6ca16a55592a32397120c6",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -718,7 +718,7 @@ def test_inference_object_detection_head(self):\n \n     @require_torch_accelerator\n     @is_flaky()\n-    def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n+    def test_inference_object_detection_head_equivalence_cpu_accelerator(self):\n         processor = self.default_processor\n         image = prepare_img()\n         text = prepare_text()\n@@ -730,7 +730,7 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n         with torch.no_grad():\n             cpu_outputs = model(**encoding)\n \n-        # 2. run model on GPU\n+        # 2. run model on accelerator\n         model.to(torch_device)\n         encoding = encoding.to(torch_device)\n         with torch.no_grad():"
        },
        {
            "sha": "dc8a3dc1e6ed9dfbd40e862bf96087965b8bd458",
            "filename": "tests/models/llama4/test_modeling_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -18,7 +18,7 @@\n from transformers import is_torch_available\n from transformers.testing_utils import (\n     require_read_token,\n-    require_torch_large_gpu,\n+    require_torch_large_accelerator,\n     slow,\n     torch_device,\n )\n@@ -34,7 +34,7 @@\n \n \n @slow\n-@require_torch_large_gpu\n+@require_torch_large_accelerator\n @require_read_token\n class Llama4IntegrationTest(unittest.TestCase):\n     model_id = \"meta-llama/Llama-4-Scout-17B-16E\""
        },
        {
            "sha": "11568f66f4dc73440eee4f20ef046917a1052b2d",
            "filename": "tests/models/omdet_turbo/test_modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -870,7 +870,7 @@ def test_inference_object_detection_head_batched(self):\n         self.assertListEqual([result[\"text_labels\"] for result in results], expected_text_labels)\n \n     @require_torch_accelerator\n-    def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n+    def test_inference_object_detection_head_equivalence_cpu_accelerator(self):\n         processor = self.default_processor\n         image = prepare_img()\n         text_labels, task = prepare_text()\n@@ -881,7 +881,7 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n         with torch.no_grad():\n             cpu_outputs = model(**encoding)\n \n-        # 2. run model on GPU\n+        # 2. run model on accelerator\n         model.to(torch_device)\n         encoding = encoding.to(torch_device)\n         with torch.no_grad():"
        },
        {
            "sha": "67cf5cc7a7160d8cb67d6912f08c5139efc3c758",
            "filename": "tests/models/rt_detr/test_image_processing_rt_detr.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -19,10 +19,11 @@\n from transformers.testing_utils import (\n     is_flaky,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_torchvision,\n     require_vision,\n     slow,\n+    torch_device,\n )\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n@@ -379,10 +380,10 @@ def test_batched_coco_detection_annotations(self):\n             torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_torchvision\n-    # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations\n-    def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n+    # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_fast_processor_equivalence_cpu_accelerator_coco_detection_annotations\n+    def test_fast_processor_equivalence_cpu_accelerator_coco_detection_annotations(self):\n         # prepare image and target\n         image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n         with open(\"./tests/fixtures/tests_samples/COCO/coco_annotations.txt\") as f:\n@@ -393,8 +394,8 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n         processor = self.image_processor_list[1]()\n         # 1. run processor on CPU\n         encoding_cpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cpu\")\n-        # 2. run processor on GPU\n-        encoding_gpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cuda\")\n+        # 2. run processor on accelerator\n+        encoding_gpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=torch_device)\n \n         # verify pixel values\n         self.assertEqual(encoding_cpu[\"pixel_values\"].shape, encoding_gpu[\"pixel_values\"].shape)"
        },
        {
            "sha": "fbc0b727a90974bdb853adc1e93eb2e0c62639cd",
            "filename": "tests/models/shieldgemma2/test_modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_modeling_shieldgemma2.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -22,7 +22,7 @@\n from transformers import is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -35,7 +35,7 @@\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n # @require_read_token\n class ShieldGemma2IntegrationTest(unittest.TestCase):\n     def tearDown(self):"
        },
        {
            "sha": "ec8748b32e234c0345619c1ddc291f9e8f28d2f9",
            "filename": "tests/models/whisper/test_feature_extraction_whisper.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -23,7 +23,11 @@\n from datasets import load_dataset\n \n from transformers import WhisperFeatureExtractor\n-from transformers.testing_utils import check_json_file_has_correct_format, require_torch, require_torch_gpu\n+from transformers.testing_utils import (\n+    check_json_file_has_correct_format,\n+    require_torch,\n+    require_torch_accelerator,\n+)\n from transformers.utils.import_utils import is_torch_available\n \n from ...test_sequence_feature_extraction_common import SequenceFeatureExtractionTestMixin\n@@ -254,7 +258,7 @@ def _load_datasamples(self, num_samples):\n \n         return [x[\"array\"] for x in speech_samples]\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_torch\n     def test_torch_integration(self):\n         # fmt: off\n@@ -303,7 +307,7 @@ def test_zero_mean_unit_variance_normalization_trunc_np_longest(self):\n         self.assertTrue(np.all(np.mean(audio) < 1e-3))\n         self.assertTrue(np.all(np.abs(np.var(audio) - 1) < 1e-3))\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_torch\n     def test_torch_integration_batch(self):\n         # fmt: off"
        },
        {
            "sha": "01755d8feee339bed6e2e73b4a5ff2aa365efb5b",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -730,7 +730,7 @@ def check_inference_correctness(self, model):\n         output_text = self.tokenizer.decode(output_parallel[0], skip_special_tokens=True)\n         self.assertIn(output_text, self.EXPECTED_OUTPUTS)\n \n-    def test_cpu_gpu_loading_random_device_map(self):\n+    def test_cpu_accelerator_loading_random_device_map(self):\n         r\"\"\"\n         A test to check is dispatching a model on cpu & gpu works correctly using a random `device_map`.\n         \"\"\"\n@@ -778,7 +778,7 @@ def test_cpu_gpu_loading_random_device_map(self):\n \n         self.check_inference_correctness(model_8bit)\n \n-    def test_cpu_gpu_loading_custom_device_map(self):\n+    def test_cpu_accelerator_loading_custom_device_map(self):\n         r\"\"\"\n         A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.\n         This time the device map is more organized than the test above and uses the abstraction\n@@ -805,7 +805,7 @@ def test_cpu_gpu_loading_custom_device_map(self):\n \n         self.check_inference_correctness(model_8bit)\n \n-    def test_cpu_gpu_disk_loading_custom_device_map(self):\n+    def test_cpu_accelerator_disk_loading_custom_device_map(self):\n         r\"\"\"\n         A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.\n         This time we also add `disk` on the device_map.\n@@ -832,7 +832,7 @@ def test_cpu_gpu_disk_loading_custom_device_map(self):\n \n             self.check_inference_correctness(model_8bit)\n \n-    def test_cpu_gpu_disk_loading_custom_device_map_kwargs(self):\n+    def test_cpu_accelerator_disk_loading_custom_device_map_kwargs(self):\n         r\"\"\"\n         A test to check is dispatching a model on cpu & gpu works correctly using a custom `device_map`.\n         This time we also add `disk` on the device_map - using the kwargs directly instead of the quantization config"
        },
        {
            "sha": "6a0321f06551eaea070f322b7009c5ac572e8ee5",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -20,7 +20,7 @@\n from transformers.testing_utils import (\n     require_gguf,\n     require_read_token,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -35,7 +35,7 @@\n \n \n @require_gguf\n-@require_torch_gpu\n+@require_torch_accelerator\n @slow\n class GgufQuantizationTests(unittest.TestCase):\n     \"\"\"\n@@ -107,7 +107,7 @@ def test_imatrix_quants(self, quant_type: str, expected_text: str):\n \n \n @require_gguf\n-@require_torch_gpu\n+@require_torch_accelerator\n @slow\n class GgufIntegrationTests(unittest.TestCase):\n     \"\"\"\n@@ -263,7 +263,7 @@ def test_gguf_errors_disk_offload(self):\n \n \n @require_gguf\n-@require_torch_gpu\n+@require_torch_accelerator\n @slow\n class GgufModelTests(unittest.TestCase):\n     mistral_model_id = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\""
        },
        {
            "sha": "5a3f7d77f53c0702d373e68b8f018702eef61bfa",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -11,17 +11,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import gc\n import unittest\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, QuarkConfig\n from transformers.testing_utils import (\n+    cleanup,\n     is_torch_available,\n     require_accelerate,\n     require_quark,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n+    torch_device,\n )\n from transformers.utils.import_utils import is_quark_available\n \n@@ -79,11 +80,10 @@ def setUpClass(cls):\n \n     def tearDown(self):\n         r\"\"\"\n-        TearDown function needs to be called at the end of each test to free the GPU memory and cache, also to\n+        TearDown function needs to be called at the end of each test to free the accelerator memory and cache, also to\n         avoid unexpected behaviors. Please see: https://discuss.pytorch.org/t/how-can-we-release-gpu-memory-cache/14530/27\n         \"\"\"\n-        gc.collect()\n-        torch.cuda.empty_cache()\n+        cleanup(torch_device, gc_collect=True)\n \n     def test_memory_footprint(self):\n         mem_quantized = self.quantized_model.get_memory_footprint()"
        },
        {
            "sha": "6c0fedf26a7ceba61afe07cb523e0945863f4f33",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -30,7 +30,7 @@\n     check_json_file_has_correct_format,\n     is_flaky,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_vision,\n     slow,\n     torch_device,\n@@ -562,7 +562,7 @@ def test_image_processor_preprocess_arguments(self):\n             self.skipTest(reason=\"No validation found for `preprocess` method\")\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_vision\n     def test_can_compile_fast_image_processor(self):\n         if self.fast_image_processing_class is None:"
        },
        {
            "sha": "be6a86d5ac61824de14ffc65d760ec56cb84f3db",
            "filename": "tests/test_video_processing_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Ftest_video_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa/tests%2Ftest_video_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_video_processing_common.py?ref=89542fb81cb2306e9fb3bcfa024d2c3ec75f34fa",
            "patch": "@@ -26,7 +26,7 @@\n from transformers.testing_utils import (\n     check_json_file_has_correct_format,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     require_vision,\n     slow,\n     torch_device,\n@@ -165,7 +165,7 @@ def test_init_without_params(self):\n             self.assertIsNotNone(video_processor)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_vision\n     def test_can_compile_fast_video_processor(self):\n         if self.fast_video_processing_class is None:"
        }
    ],
    "stats": {
        "total": 222,
        "additions": 150,
        "deletions": 72
    }
}