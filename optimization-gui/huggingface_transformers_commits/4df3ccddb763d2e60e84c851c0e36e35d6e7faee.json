{
    "author": "glegendre01",
    "message": "Migrate the CI runners to the new clusters (#33849)\n\n* try fixing push-ci\r\n\r\n* move to new runners\r\n\r\n* move benchmark.yml to new runners\r\n\r\n* move doctest_job.yml to new runners\r\n\r\n* move doctests.yml to new runners\r\n\r\n* move push-important-models.yml to new runners\r\n\r\n* move self-pr-slow-ci.yml to new runners\r\n\r\n* fix typo\r\n\r\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\r\n\r\n* fix working directory\r\n\r\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\r\n\r\n* fix working directory\r\n\r\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\r\n\r\n* improve code\r\n\r\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "4df3ccddb763d2e60e84c851c0e36e35d6e7faee",
    "files": [
        {
            "sha": "75a837d693e7c68868fa2a47a9fae34161a45223",
            "filename": ".github/workflows/benchmark.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fbenchmark.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fbenchmark.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbenchmark.yml?ref=4df3ccddb763d2e60e84c851c0e36e35d6e7faee",
            "patch": "@@ -13,7 +13,8 @@ env:\n jobs:\r\n   benchmark:\r\n     name: Benchmark\r\n-    runs-on: [single-gpu, nvidia-gpu, a10, ci]\r\n+    runs-on: \r\n+      group: aws-g5-4xlarge-cache\r\n     container:\r\n       image: huggingface/transformers-all-latest-gpu\r\n       options: --gpus all --privileged --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\r"
        },
        {
            "sha": "eb62b797b8eb5575a33a6c0452563d809ce7fce7",
            "filename": ".github/workflows/doctest_job.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fdoctest_job.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fdoctest_job.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fdoctest_job.yml?ref=4df3ccddb763d2e60e84c851c0e36e35d6e7faee",
            "patch": "@@ -27,7 +27,8 @@ jobs:\n       fail-fast: false\n       matrix:\n         split_keys: ${{ fromJson(inputs.split_keys) }}\n-    runs-on: [single-gpu, nvidia-gpu, t4, ci]\n+    runs-on: \n+      group: aws-g4dn-2xlarge-cache\n     container:\n       image: huggingface/transformers-all-latest-gpu\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/"
        },
        {
            "sha": "472b07684ed12ada716cc5f561665e5947c2f835",
            "filename": ".github/workflows/doctests.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fdoctests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fdoctests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fdoctests.yml?ref=4df3ccddb763d2e60e84c851c0e36e35d6e7faee",
            "patch": "@@ -14,7 +14,8 @@ env:\n jobs:\n   setup:\n     name: Setup\n-    runs-on: [single-gpu, nvidia-gpu, t4, ci]\n+    runs-on: \n+      group: aws-g4dn-2xlarge-cache\n     container:\n       image: huggingface/transformers-all-latest-gpu\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -85,4 +86,4 @@ jobs:\n         uses: actions/upload-artifact@v4\n         with:\n           name: doc_test_results\n-          path: doc_test_results\n\\ No newline at end of file\n+          path: doc_test_results"
        },
        {
            "sha": "1887af0f4c5bac3be916510180dbd483bdde7c14",
            "filename": ".github/workflows/push-important-models.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fpush-important-models.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fpush-important-models.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fpush-important-models.yml?ref=4df3ccddb763d2e60e84c851c0e36e35d6e7faee",
            "patch": "@@ -52,7 +52,8 @@ jobs:\n   test_modified_files:\n     needs: get_modified_models\n     name: Slow & FA2 tests\n-    runs-on: [single-gpu, nvidia-gpu, a10, ci]\n+    runs-on:\n+      group: aws-g5-4xlarge-cache\n     container:\n       image: huggingface/transformers-all-latest-gpu\n       options: --gpus all --privileged --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/"
        },
        {
            "sha": "bcf4d4d689ea6c0fc1db44cbea4a04d03f5dbce6",
            "filename": ".github/workflows/self-pr-slow-ci.yml",
            "status": "modified",
            "additions": 26,
            "deletions": 10,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fself-pr-slow-ci.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fself-pr-slow-ci.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-pr-slow-ci.yml?ref=4df3ccddb763d2e60e84c851c0e36e35d6e7faee",
            "patch": "@@ -65,8 +65,9 @@ jobs:\n         fail-fast: false\n         matrix:\n           folders: ${{ fromJson(needs.find_models_to_run.outputs.models) }}\n-          machine_type: [single-gpu, multi-gpu]\n-      runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, ci]\n+          machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+      runs-on:\n+        group: '${{ matrix.machine_type }}'\n       container:\n         image: huggingface/transformers-all-latest-gpu\n         options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -99,6 +100,21 @@ jobs:\n         run: |\n           nvidia-smi\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV    \n+\n       - name: Environment\n         working-directory: /transformers\n         run: |\n@@ -113,23 +129,23 @@ jobs:\n         run: |\n           export CUDA_VISIBLE_DEVICES=\"$(python3 utils/set_cuda_devices_for_ci.py --test_folder ${{ matrix.folders }})\"\n           echo $CUDA_VISIBLE_DEVICES\n-          python3 -m pytest -v -rsfE --make-reports=${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n+          python3 -m pytest -v -rsfE --make-reports=${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n+        run: cat /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n \n       - name: Make sure report directory exists\n         shell: bash\n         run: |\n-          mkdir -p /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n+          mkdir -p /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n+          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n+          echo \"${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n+          name: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports"
        },
        {
            "sha": "940495c2875327d636cc949d157806453e4b22e7",
            "filename": ".github/workflows/self-push.yml",
            "status": "modified",
            "additions": 103,
            "deletions": 30,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fself-push.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/.github%2Fworkflows%2Fself-push.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-push.yml?ref=4df3ccddb763d2e60e84c851c0e36e35d6e7faee",
            "patch": "@@ -32,8 +32,9 @@ jobs:\n     name: Setup\n     strategy:\n       matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, push-ci]\n+        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-all-latest-gpu-push-ci\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -131,8 +132,9 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [single-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, push-ci]\n+        machine_type: [aws-g4dn-2xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-all-latest-gpu-push-ci\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -162,6 +164,23 @@ jobs:\n           echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n           echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n       - name: Update clone using environment variables\n         working-directory: /transformers\n         run: |\n@@ -203,19 +222,19 @@ jobs:\n       - name: Run all non-slow selected tests on GPU\n         working-directory: /transformers\n         run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ matrix.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n+          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n+        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_tests_gpu_${{ matrix.folders }}\n+          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}\n \n   run_tests_multi_gpu:\n     name: Model tests\n@@ -226,8 +245,9 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.setup.outputs.matrix) }}\n-        machine_type: [multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, push-ci]\n+        machine_type: [aws-g4dn-12xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-all-latest-gpu-push-ci\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -257,6 +277,23 @@ jobs:\n           echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n           echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+          \n       - name: Update clone using environment variables\n         working-directory: /transformers\n         run: |\n@@ -300,19 +337,19 @@ jobs:\n           MKL_SERVICE_FORCE_INTEL: 1\n         working-directory: /transformers\n         run: |\n-          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ matrix.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n+          python3 -m pytest -n 2 --dist=loadfile -v --make-reports=${{ env.machine_type }}_tests_gpu_${{ matrix.folders }} ${{ fromJson(needs.setup.outputs.test_map)[matrix.folders] }}\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n+        run: cat /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_tests_gpu_${{ matrix.folders }}\n+          name: ${{ env.machine_type }}_run_all_tests_gpu_${{ env.matrix_folders }}_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_tests_gpu_${{ matrix.folders }}\n \n   run_tests_torch_cuda_extensions_single_gpu:\n     name: Torch CUDA extension tests\n@@ -321,8 +358,9 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [single-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, push-ci]\n+        machine_type: [aws-g4dn-2xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -352,6 +390,23 @@ jobs:\n           echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n           echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /workspace/transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+          \n       - name: Update clone using environment variables\n         working-directory: /workspace/transformers\n         run: |\n@@ -392,19 +447,19 @@ jobs:\n         working-directory: /workspace/transformers\n         # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.\n         run: |\n-          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n+          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /workspace/transformers/reports/${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n+        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: /workspace/transformers/reports/${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n+          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n+          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n \n   run_tests_torch_cuda_extensions_multi_gpu:\n     name: Torch CUDA extension tests\n@@ -413,8 +468,9 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, push-ci]\n+        machine_type: [aws-g4dn-12xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -444,6 +500,23 @@ jobs:\n           echo \"env.CI_BRANCH = ${{ env.CI_BRANCH }}\"\n           echo \"env.CI_SHA = ${{ env.CI_SHA }}\"\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /workspace/transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+          \n       - name: Update clone using environment variables\n         working-directory: /workspace/transformers\n         run: |\n@@ -484,19 +557,19 @@ jobs:\n         working-directory: /workspace/transformers\n         # TODO: Here we pass all tests in the 2 folders for simplicity. It's better to pass only the identified tests.\n         run: |\n-          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n+          python -m pytest -n 1 --dist=loadfile -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /workspace/transformers/reports/${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n+        run: cat /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: /workspace/transformers/reports/${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n+          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n+          path: /workspace/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n \n   send_results:\n     name: Send results to webhook"
        },
        {
            "sha": "9e15f2e115ec611298607d35d104b6eb20d748a3",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4df3ccddb763d2e60e84c851c0e36e35d6e7faee/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=4df3ccddb763d2e60e84c851c0e36e35d6e7faee",
            "patch": "@@ -1153,6 +1153,7 @@ def parse_commit_message(commit_message: str) -> Dict[str, bool]:\n \n \n def create_test_list_from_filter(full_test_list, out_path):\n+    os.makedirs(out_path, exist_ok=True)\n     all_test_files = \"\\n\".join(full_test_list)\n     for job_name, _filter in JOB_TO_TEST_FILE.items():\n         file_name = os.path.join(out_path, f\"{job_name}_test_list.txt\")"
        }
    ],
    "stats": {
        "total": 184,
        "additions": 139,
        "deletions": 45
    }
}