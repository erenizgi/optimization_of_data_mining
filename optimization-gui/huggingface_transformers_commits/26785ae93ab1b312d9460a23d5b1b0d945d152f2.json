{
    "author": "Cyrilvallez",
    "message": "Fix the init_weights for the MoE models (#42306)\n\n* fix the modulars\n\n* apply modulars\n\n* forgot jamba\n\n* fix doc",
    "sha": "26785ae93ab1b312d9460a23d5b1b0d945d152f2",
    "files": [
        {
            "sha": "e67c3222f3415f9487bc1ca8e4646a65ce933263",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -2140,12 +2140,6 @@ def _init_weights(self, module):\n                 init.ones_(module.weight)\n             if hasattr(module, \"bias\") and module.bias is not None:\n                 init.zeros_(module.bias)\n-        if isinstance(getattr(module, \"gate_up_proj\", None), nn.Parameter):\n-            init.normal_(module.gate_up_proj, mean=0.0, std=std)\n-        if isinstance(getattr(module, \"down_proj\", None), nn.Parameter):\n-            init.normal_(module.down_proj, mean=0.0, std=std)\n-        if isinstance(getattr(module, \"gate\", None), nn.Parameter):\n-            init.normal_(module.gate, mean=0.0, std=std)\n \n     def _initialize_weights(self, module):\n         \"\"\"\n@@ -2166,10 +2160,6 @@ def initialize_weights(self):\n         module graph along the recursion. It can handle an arbitrary number of sub-models. Without it, every composite\n         model would have to recurse a second time on all sub-models explicitly in the outer-most `_init_weights`, which\n         is extremely error prone and inefficient.\n-\n-        Note that the `torch.no_grad()` decorator is very important as well, as most of our `_init_weights` do not use\n-        `torch.nn.init` functions (which are all no_grad by default), but simply do in-place ops such as\n-        `module.weight.zero_()`.\n         \"\"\"\n         if not hasattr(torch.nn.Module, \"smart_apply\"):\n             # This function is equivalent to `torch.nn.Module.apply`, except that it dynamically adjust the function"
        },
        {
            "sha": "89230d7a80b2c261e204a646f8e1276091e78b3d",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -465,8 +465,9 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         super()._init_weights(module)\n-        if isinstance(module, DeepseekV2Moe):\n-            init.normal_(module.gate.weight, mean=0.0, std=self.config.initializer_range)\n+        if isinstance(module, DeepseekV2Experts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "46b05073a2dd6ce61ecc40b0e80be773c1280276",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -437,8 +437,9 @@ class DeepseekV2PreTrainedModel(LlamaPreTrainedModel):\n     @torch.no_grad()\n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n-        if isinstance(module, DeepseekV2Moe):\n-            init.normal_(module.gate.weight, mean=0.0, std=self.config.initializer_range)\n+        if isinstance(module, DeepseekV2Experts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n class DeepseekV2Model(LlamaModel):"
        },
        {
            "sha": "cfd8d91dfb9a288aa0ee7115094830f92b0fe2a1",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -554,6 +554,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, DeepseekV3TopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, DeepseekV3NaiveMoe):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "97c3f0cd425bfad863d8e709785736dbf6128ce5",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -310,6 +310,9 @@ def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV3TopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, DeepseekV3NaiveMoe):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n class DeepseekV3Model(LlamaModel):"
        },
        {
            "sha": "b8ae00b6ab60a8f2e8a0ecac8ae2effc98cb40bf",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -472,6 +472,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Dots1TopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Dots1NaiveMoe):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "ccd05fe26347d4a548eefd86327b34924a19487b",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -501,6 +501,12 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Ernie4_5_MoeStatics):\n             init.zeros_(module.e_score_correction_bias)\n+        elif isinstance(module, Ernie4_5_MoeExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+            if module.gate_up_proj_bias is not None:\n+                init.zeros_(module.gate_up_proj_bias)\n+                init.zeros_(module.down_proj_bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "48c43140ba4849d1b9b212eb807a92dd2f69dc36",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -242,6 +242,12 @@ def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Ernie4_5_MoeStatics):\n             init.zeros_(module.e_score_correction_bias)\n+        elif isinstance(module, Ernie4_5_MoeExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+            if module.gate_up_proj_bias is not None:\n+                init.zeros_(module.gate_up_proj_bias)\n+                init.zeros_(module.down_proj_bias)\n \n \n @auto_docstring"
        },
        {
            "sha": "84e6dd3bd77d0a25424c9a4542cdf79d417a323f",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -498,6 +498,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Glm4MoeTopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Glm4MoeNaiveMoe):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "373d49bc942cfb01caf8e610f479243d1766027a",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -559,6 +559,9 @@ def _init_weights(self, module):\n         super()._init_weights(module)\n         if isinstance(module, Glm4vMoeTextTopkRouter):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, Glm4vMoeTextNaiveMoe):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n @dataclass"
        },
        {
            "sha": "281a50a9e2ccdfe42bb31183c10dd4fb32d51c28",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -26,10 +26,9 @@\n import torch.nn.functional as F\n from torch import nn\n \n-from transformers.cache_utils import Cache\n-\n+from ... import initialization as init\n from ...activations import ACT2FN\n-from ...cache_utils import DynamicCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n@@ -378,6 +377,13 @@ class HunYuanMoEV1PreTrainedModel(PreTrainedModel):\n         \"attentions\": HunYuanMoEV1Attention,\n     }\n \n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, HunYuanMoEV1Experts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+\n \n class HunYuanMoEV1RotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`"
        },
        {
            "sha": "f94c4ed4c2c786d52551cfaeb97966ca62a2513d",
            "filename": "src/transformers/models/hunyuan_v1_moe/modular_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -21,14 +21,11 @@\n import torch.nn.functional as F\n from torch import nn\n \n-from transformers.cache_utils import Cache\n-from transformers.utils import (\n-    logging,\n-)\n-\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ... import initialization as init\n+from ...cache_utils import Cache\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs\n+from ...utils import TransformersKwargs, logging\n from ..hunyuan_v1_dense.modeling_hunyuan_v1_dense import HunYuanDenseV1RotaryEmbedding\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -182,6 +179,13 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n class HunYuanMoEV1PreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False\n \n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, HunYuanMoEV1Experts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+\n \n class HunYuanMoEV1RotaryEmbedding(HunYuanDenseV1RotaryEmbedding):\n     pass"
        },
        {
            "sha": "fbda366fc319c9558f170eedf7cf0e47e5d7ae03",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -731,6 +731,9 @@ def _init_weights(self, module):\n             A = A.expand(module.intermediate_size, -1).contiguous()\n             init.copy_(module.A_log, torch.log(A))\n             init.ones_(module.D)\n+        elif isinstance(module, JambaExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n ALL_DECODER_LAYER_TYPES = {\"attention\": JambaAttentionDecoderLayer, \"mamba\": JambaMambaDecoderLayer}"
        },
        {
            "sha": "f853e76adc331d152f90dacc413049e2306fa601",
            "filename": "src/transformers/models/jamba/modular_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -616,6 +616,9 @@ def _init_weights(self, module):\n             A = A.expand(module.intermediate_size, -1).contiguous()\n             init.copy_(module.A_log, torch.log(A))\n             init.ones_(module.D)\n+        elif isinstance(module, JambaExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "73b9c4a8fde07b9dc598e641e0805017430e3aea",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -24,6 +24,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n@@ -679,6 +680,13 @@ class Lfm2MoePreTrainedModel(PreTrainedModel):\n         \"attentions\": Lfm2MoeAttention,\n     }\n \n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, Lfm2MoeExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+\n \n @auto_docstring\n class Lfm2MoeModel(Lfm2MoePreTrainedModel):"
        },
        {
            "sha": "1ada897f00db0a78e58c75e653035d49c388cfdd",
            "filename": "src/transformers/models/lfm2_moe/modular_lfm2_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodular_lfm2_moe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -16,8 +16,10 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import MoeModelOutputWithPast\n+from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n from ...utils.import_utils import is_causal_conv1d_available\n@@ -131,6 +133,13 @@ def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n class Lfm2MoePreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False\n \n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, Lfm2MoeExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+\n \n class Lfm2MoeModel(MixtralModel):\n     def __init__(self, config: Lfm2MoeConfig):"
        },
        {
            "sha": "4135bce33d83ee5b27c3692426490485d1266a6a",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -564,8 +564,9 @@ def _init_weights(self, module):\n         if isinstance(module, LongcatFlashTopkRouter):\n             init.normal_(module.classifier.weight, mean=0.0, std=self.config.initializer_range)\n         if isinstance(module, LongcatFlashExperts):\n-            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n-            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+            if module.gate_up_proj is not None:\n+                init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+                init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "f85d17a3e9a93ec5d7223581abd65fd87aa5f2a0",
            "filename": "src/transformers/models/longcat_flash/modular_longcat_flash.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -348,8 +348,9 @@ def _init_weights(self, module):\n         if isinstance(module, LongcatFlashTopkRouter):\n             init.normal_(module.classifier.weight, mean=0.0, std=self.config.initializer_range)\n         if isinstance(module, LongcatFlashExperts):\n-            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n-            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+            if module.gate_up_proj is not None:\n+                init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+                init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n \n \n class LongcatFlashModel(DeepseekV3Model):"
        },
        {
            "sha": "f078518e0c1f4eca993f1c6c08bed8de6280ae18",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -23,6 +23,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -432,6 +433,15 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, OlmoeExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, OlmoeTopKRouter):\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+\n \n @auto_docstring\n class OlmoeModel(OlmoePreTrainedModel):"
        },
        {
            "sha": "eef444e6f24a0d2f323058d3d95c24de17c351f5",
            "filename": "src/transformers/models/olmoe/modular_olmoe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/26785ae93ab1b312d9460a23d5b1b0d945d152f2/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py?ref=26785ae93ab1b312d9460a23d5b1b0d945d152f2",
            "patch": "@@ -17,6 +17,7 @@\n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n@@ -167,6 +168,15 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n \n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, OlmoeExperts):\n+            init.normal_(module.gate_up_proj, mean=0.0, std=self.config.initializer_range)\n+            init.normal_(module.down_proj, mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, OlmoeTopKRouter):\n+            init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+\n \n @auto_docstring\n class OlmoeModel(MixtralModel):"
        }
    ],
    "stats": {
        "total": 130,
        "additions": 102,
        "deletions": 28
    }
}