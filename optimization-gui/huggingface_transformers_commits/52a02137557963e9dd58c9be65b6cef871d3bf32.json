{
    "author": "Rocketknight1",
    "message": "Add assistant prefill for chat templates and TextGenerationPipeline (#33198)\n\n* Add assistant prefill to chat templates\r\n\r\n* Add assistant prefill to pipeline\r\n\r\n* Add assistant prefill to pipeline\r\n\r\n* Tweak another test that ended in assistant message\r\n\r\n* Update tests that ended in assistant messages\r\n\r\n* Update tests that ended in assistant messages\r\n\r\n* Replace assistant_prefill with continue_final_message\r\n\r\n* Allow passing continue_final_message to pipeline\r\n\r\n* Small fixup\r\n\r\n* Add continue_final_message as a pipeline kwarg\r\n\r\n* Update docstrings\r\n\r\n* Move repos to hf-internal-testing!\r\n\r\n* Update src/transformers/tokenization_utils_base.py\r\n\r\nCo-authored-by: Lysandre Debut <hi@lysand.re>\r\n\r\n* Add explanatory comment\r\n\r\n* make fixup\r\n\r\n* Update chat templating docs to explain continue_last_message\r\n\r\n---------\r\n\r\nCo-authored-by: Lysandre Debut <hi@lysand.re>",
    "sha": "52a02137557963e9dd58c9be65b6cef871d3bf32",
    "files": [
        {
            "sha": "10b094e08f9b450221e8019de8c12693ebe8c319",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/52a02137557963e9dd58c9be65b6cef871d3bf32/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/52a02137557963e9dd58c9be65b6cef871d3bf32/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=52a02137557963e9dd58c9be65b6cef871d3bf32",
            "patch": "@@ -197,6 +197,43 @@ Not all models require generation prompts. Some models, like BlenderBot and LLaM\n special tokens before bot responses. In these cases, the `add_generation_prompt` argument will have no effect. The exact\n effect that `add_generation_prompt` has will depend on the template being used.\n \n+## What does \"continue_last_message\" do?\n+\n+When passing a list of messages to `apply_chat_template` or `TextGenerationPipeline`, you can choose\n+to format the chat so the model will continue the final message in the chat instead of starting a new one. This is done\n+by removing any end-of-sequence tokens that indicate the end of the final message, so that the model will simply\n+extend the final message when it begins to generate text. This is useful for \"prefilling\" the model's response. \n+\n+Here's an example:\n+\n+```python\n+chat = [\n+    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n+    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n+]\n+\n+formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_last_message=True)\n+model.generate(**formatted_chat)\n+```\n+\n+The model will generate text that continues the JSON string, rather than starting a new message. This approach\n+can be very useful for improving the accuracy of the model's instruction-following when you know how you want\n+it to start its replies.\n+\n+Because `add_generation_prompt` adds the tokens that start a new message, and `continue_last_message` removes any\n+end-of-message tokens from the final message, it does not make sense to use them together. As a result, you'll\n+get an error if you try!\n+\n+<Tip>\n+\n+The default behaviour of `TextGenerationPipeline` is to set `add_generation_prompt=True` so that it starts a new\n+message. However, if the final message in the input chat has the \"assistant\" role, it will assume that this message is \n+a prefill and switch to `continue_final_message=True` instead, because most models do not support multiple \n+consecutive assistant messages. You can override this behaviour by explicitly passing the `continue_last_message` \n+argument when calling the pipeline.\n+\n+</Tip>\n+\n ## Can I use chat templates in training?\n \n Yes! This is a good way to ensure that the chat template matches the tokens the model sees during training."
        },
        {
            "sha": "8bd1017ffc66967007b2955e9abc6a28936d2c8f",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 39,
            "deletions": 5,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/52a02137557963e9dd58c9be65b6cef871d3bf32/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52a02137557963e9dd58c9be65b6cef871d3bf32/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=52a02137557963e9dd58c9be65b6cef871d3bf32",
            "patch": "@@ -131,6 +131,7 @@ def _sanitize_parameters(\n         stop_sequence=None,\n         truncation=None,\n         max_length=None,\n+        continue_final_message=None,\n         **generate_kwargs,\n     ):\n         preprocess_params = {}\n@@ -165,6 +166,9 @@ def _sanitize_parameters(\n                 )\n             preprocess_params[\"handle_long_generation\"] = handle_long_generation\n \n+        if continue_final_message is not None:\n+            preprocess_params[\"continue_final_message\"] = continue_final_message\n+\n         preprocess_params.update(generate_kwargs)\n         forward_params = generate_kwargs\n \n@@ -183,6 +187,8 @@ def _sanitize_parameters(\n             postprocess_params[\"return_type\"] = return_type\n         if clean_up_tokenization_spaces is not None:\n             postprocess_params[\"clean_up_tokenization_spaces\"] = clean_up_tokenization_spaces\n+        if continue_final_message is not None:\n+            postprocess_params[\"continue_final_message\"] = continue_final_message\n \n         if stop_sequence is not None:\n             stop_sequence_ids = self.tokenizer.encode(stop_sequence, add_special_tokens=False)\n@@ -226,6 +232,10 @@ def __call__(self, text_inputs, **kwargs):\n                 *return_text* is set to True.\n             clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n                 Whether or not to clean up the potential extra spaces in the text output.\n+            continue_final_message( `bool`, *optional*): This indicates that you want the model to continue the\n+                last message in the input chat rather than starting a new one, allowing you to \"prefill\" its response.\n+                By default this is `True` when the final message in the input chat has the `assistant` role and\n+                `False` otherwise, but you can manually override that behaviour by setting this flag.\n             prefix (`str`, *optional*):\n                 Prefix added to prompt.\n             handle_long_generation (`str`, *optional*):\n@@ -270,6 +280,7 @@ def preprocess(\n         truncation=None,\n         padding=None,\n         max_length=None,\n+        continue_final_message=None,\n         **generate_kwargs,\n     ):\n         # Only set non-None tokenizer kwargs, so as to rely on the tokenizer's defaults\n@@ -283,9 +294,14 @@ def preprocess(\n \n         if isinstance(prompt_text, Chat):\n             tokenizer_kwargs.pop(\"add_special_tokens\", None)  # ignore add_special_tokens on chats\n+            # If the user passes a chat that ends in an assistant message, we treat it as a prefill by default\n+            # because very few models support multiple separate, consecutive assistant messages\n+            if continue_final_message is None:\n+                continue_final_message = prompt_text.messages[-1][\"role\"] == \"assistant\"\n             inputs = self.tokenizer.apply_chat_template(\n                 prompt_text.messages,\n-                add_generation_prompt=True,\n+                add_generation_prompt=not continue_final_message,\n+                continue_final_message=continue_final_message,\n                 return_dict=True,\n                 return_tensors=self.framework,\n                 **tokenizer_kwargs,\n@@ -356,7 +372,13 @@ def _forward(self, model_inputs, **generate_kwargs):\n             generated_sequence = tf.reshape(generated_sequence, (in_b, out_b // in_b, *generated_sequence.shape[1:]))\n         return {\"generated_sequence\": generated_sequence, \"input_ids\": input_ids, \"prompt_text\": prompt_text}\n \n-    def postprocess(self, model_outputs, return_type=ReturnType.FULL_TEXT, clean_up_tokenization_spaces=True):\n+    def postprocess(\n+        self,\n+        model_outputs,\n+        return_type=ReturnType.FULL_TEXT,\n+        clean_up_tokenization_spaces=True,\n+        continue_final_message=None,\n+    ):\n         generated_sequence = model_outputs[\"generated_sequence\"][0]\n         input_ids = model_outputs[\"input_ids\"]\n         prompt_text = model_outputs[\"prompt_text\"]\n@@ -390,9 +412,21 @@ def postprocess(self, model_outputs, return_type=ReturnType.FULL_TEXT, clean_up_\n                     if isinstance(prompt_text, str):\n                         all_text = prompt_text + all_text\n                     elif isinstance(prompt_text, Chat):\n-                        # Explicit list parsing is necessary for parsing chat datasets\n-                        all_text = list(prompt_text.messages) + [{\"role\": \"assistant\", \"content\": all_text}]\n-\n+                        if continue_final_message is None:\n+                            # If the user passes a chat ending in an assistant message, we treat it as a prefill by\n+                            # default because very few models support multiple separate, consecutive assistant messages\n+                            continue_final_message = prompt_text.messages[-1][\"role\"] == \"assistant\"\n+                        if continue_final_message:\n+                            # With assistant prefill, concat onto the end of the last message\n+                            all_text = list(prompt_text.messages)[:-1] + [\n+                                {\n+                                    \"role\": prompt_text.messages[-1][\"role\"],\n+                                    \"content\": prompt_text.messages[-1][\"content\"] + all_text,\n+                                }\n+                            ]\n+                        else:\n+                            # When we're not starting from a prefill, the output is a new assistant message\n+                            all_text = list(prompt_text.messages) + [{\"role\": \"assistant\", \"content\": all_text}]\n                 record = {\"generated_text\": all_text}\n             records.append(record)\n "
        },
        {
            "sha": "608c6516668b29fb6ec56b863082d81e1b46108d",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 20,
            "deletions": 2,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/52a02137557963e9dd58c9be65b6cef871d3bf32/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52a02137557963e9dd58c9be65b6cef871d3bf32/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=52a02137557963e9dd58c9be65b6cef871d3bf32",
            "patch": "@@ -1704,6 +1704,7 @@ def apply_chat_template(\n         documents: Optional[List[Dict[str, str]]] = None,\n         chat_template: Optional[str] = None,\n         add_generation_prompt: bool = False,\n+        continue_final_message: bool = False,\n         tokenize: bool = True,\n         padding: bool = False,\n         truncation: bool = False,\n@@ -1737,10 +1738,16 @@ def apply_chat_template(\n             chat_template (`str`, *optional*):\n                 A Jinja template to use for this conversion. It is usually not necessary to pass anything to this\n                 argument, as the model's template will be used by default.\n-            add_generation_prompt (bool, *optional*): Whether to end the prompt with the token(s) that indicate\n-                the start of an assistant message. This is useful when you want to generate a response from the model.\n+            add_generation_prompt (bool, *optional*):\n+                If this is set, a prompt with the token(s) that indicate\n+                the start of an assistant message will be appended to the formatted output. This is useful when you want to generate a response from the model.\n                 Note that this argument will be passed to the chat template, and so it must be supported in the\n                 template for this argument to have any effect.\n+            continue_final_message (bool, *optional*):\n+                If this is set, the chat will be formatted so that the final\n+                message in the chat is open-ended, without any EOS tokens. The model will continue this message\n+                rather than starting a new one. This allows you to \"prefill\" part of\n+                the model's response for it. Cannot be used at the same time as `add_generation_prompt`.\n             tokenize (`bool`, defaults to `True`):\n                 Whether to tokenize the output. If `False`, the output will be a string.\n             padding (`bool`, defaults to `False`):\n@@ -1803,6 +1810,14 @@ def apply_chat_template(\n             conversations = [conversation]\n             is_batched = False\n \n+        if continue_final_message:\n+            if add_generation_prompt:\n+                raise ValueError(\n+                    \"continue_final_message and add_generation_prompt are not compatible. Use continue_final_message when you want the model to continue the final message, and add_generation_prompt when you want to add a header that will prompt it to start a new assistant message instead.\"\n+                )\n+            if return_assistant_tokens_mask:\n+                raise ValueError(\"continue_final_message is not compatible with return_assistant_tokens_mask.\")\n+\n         # We accept either JSON schemas or functions for tools. If we get functions, we convert them to schemas\n         if tools is not None:\n             tool_schemas = []\n@@ -1849,6 +1864,9 @@ def apply_chat_template(\n                     add_generation_prompt=add_generation_prompt,\n                     **template_kwargs,\n                 )\n+            if continue_final_message:\n+                final_message = chat[-1][\"content\"]\n+                rendered_chat = rendered_chat[: rendered_chat.rindex(final_message) + len(final_message)].rstrip()\n             rendered.append(rendered_chat)\n \n         if not is_batched:"
        },
        {
            "sha": "ea36ae5728d161d676824e09fcd269db3b2a8042",
            "filename": "tests/pipelines/test_pipelines_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/52a02137557963e9dd58c9be65b6cef871d3bf32/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52a02137557963e9dd58c9be65b6cef871d3bf32/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_common.py?ref=52a02137557963e9dd58c9be65b6cef871d3bf32",
            "patch": "@@ -877,8 +877,8 @@ def test_custom_code_with_string_tokenizer(self):\n         # See https://github.com/huggingface/transformers/issues/31669\n         text_generator = pipeline(\n             \"text-generation\",\n-            model=\"Rocketknight1/fake-custom-model-test\",\n-            tokenizer=\"Rocketknight1/fake-custom-model-test\",\n+            model=\"hf-internal-testing/tiny-random-custom-architecture\",\n+            tokenizer=\"hf-internal-testing/tiny-random-custom-architecture\",\n             trust_remote_code=True,\n         )\n \n@@ -888,8 +888,8 @@ def test_custom_code_with_string_tokenizer(self):\n     def test_custom_code_with_string_feature_extractor(self):\n         speech_recognizer = pipeline(\n             \"automatic-speech-recognition\",\n-            model=\"Rocketknight1/fake-custom-wav2vec2\",\n-            feature_extractor=\"Rocketknight1/fake-custom-wav2vec2\",\n+            model=\"hf-internal-testing/fake-custom-wav2vec2\",\n+            feature_extractor=\"hf-internal-testing/fake-custom-wav2vec2\",\n             trust_remote_code=True,\n         )\n \n@@ -899,8 +899,8 @@ def test_custom_code_with_string_feature_extractor(self):\n     def test_custom_code_with_string_preprocessor(self):\n         mask_generator = pipeline(\n             \"mask-generation\",\n-            model=\"Rocketknight1/fake-custom-sam\",\n-            processor=\"Rocketknight1/fake-custom-sam\",\n+            model=\"hf-internal-testing/fake-custom-sam\",\n+            processor=\"hf-internal-testing/fake-custom-sam\",\n             trust_remote_code=True,\n         )\n "
        },
        {
            "sha": "930eb34bfb7411c4e649a38838f30ce0b5c3f973",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 67,
            "deletions": 10,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/52a02137557963e9dd58c9be65b6cef871d3bf32/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52a02137557963e9dd58c9be65b6cef871d3bf32/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=52a02137557963e9dd58c9be65b6cef871d3bf32",
            "patch": "@@ -148,18 +148,16 @@ def test_small_model_pt(self):\n     @require_torch\n     def test_small_chat_model_pt(self):\n         text_generator = pipeline(\n-            task=\"text-generation\", model=\"rocketknight1/tiny-gpt2-with-chatml-template\", framework=\"pt\"\n+            task=\"text-generation\", model=\"hf-internal-testing/tiny-gpt2-with-chatml-template\", framework=\"pt\"\n         )\n         # Using `do_sample=False` to force deterministic output\n         chat1 = [\n             {\"role\": \"system\", \"content\": \"This is a system message.\"},\n             {\"role\": \"user\", \"content\": \"This is a test\"},\n-            {\"role\": \"assistant\", \"content\": \"This is a reply\"},\n         ]\n         chat2 = [\n             {\"role\": \"system\", \"content\": \"This is a system message.\"},\n             {\"role\": \"user\", \"content\": \"This is a second test\"},\n-            {\"role\": \"assistant\", \"content\": \"This is a reply\"},\n         ]\n         outputs = text_generator(chat1, do_sample=False, max_new_tokens=10)\n         expected_chat1 = chat1 + [\n@@ -179,7 +177,7 @@ def test_small_chat_model_pt(self):\n         expected_chat2 = chat2 + [\n             {\n                 \"role\": \"assistant\",\n-                \"content\": \" factors factors factors factors factors factors factors factors factors factors\",\n+                \"content\": \" stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs\",\n             }\n         ]\n \n@@ -191,6 +189,68 @@ def test_small_chat_model_pt(self):\n             ],\n         )\n \n+    @require_torch\n+    def test_small_chat_model_continue_final_message(self):\n+        # Here we check that passing a chat that ends in an assistant message is handled correctly\n+        # by continuing the final message rather than starting a new one\n+        text_generator = pipeline(\n+            task=\"text-generation\", model=\"hf-internal-testing/tiny-gpt2-with-chatml-template\", framework=\"pt\"\n+        )\n+        # Using `do_sample=False` to force deterministic output\n+        chat1 = [\n+            {\"role\": \"system\", \"content\": \"This is a system message.\"},\n+            {\"role\": \"user\", \"content\": \"This is a test\"},\n+            {\"role\": \"assistant\", \"content\": \"This is\"},\n+        ]\n+        outputs = text_generator(chat1, do_sample=False, max_new_tokens=10)\n+\n+        # Assert that we continued the last message and there isn't a sneaky <|im_end|>\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"generated_text\": [\n+                        {\"role\": \"system\", \"content\": \"This is a system message.\"},\n+                        {\"role\": \"user\", \"content\": \"This is a test\"},\n+                        {\n+                            \"role\": \"assistant\",\n+                            \"content\": \"This is stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs\",\n+                        },\n+                    ]\n+                }\n+            ],\n+        )\n+\n+    @require_torch\n+    def test_small_chat_model_continue_final_message_override(self):\n+        # Here we check that passing a chat that ends in an assistant message is handled correctly\n+        # by continuing the final message rather than starting a new one\n+        text_generator = pipeline(\n+            task=\"text-generation\", model=\"hf-internal-testing/tiny-gpt2-with-chatml-template\", framework=\"pt\"\n+        )\n+        # Using `do_sample=False` to force deterministic output\n+        chat1 = [\n+            {\"role\": \"system\", \"content\": \"This is a system message.\"},\n+            {\"role\": \"user\", \"content\": \"This is a test\"},\n+        ]\n+        outputs = text_generator(chat1, do_sample=False, max_new_tokens=10, continue_final_message=True)\n+\n+        # Assert that we continued the last message and there isn't a sneaky <|im_end|>\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"generated_text\": [\n+                        {\"role\": \"system\", \"content\": \"This is a system message.\"},\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": \"This is a test stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs\",\n+                        },\n+                    ]\n+                }\n+            ],\n+        )\n+\n     @require_torch\n     def test_small_chat_model_with_dataset_pt(self):\n         from torch.utils.data import Dataset\n@@ -202,7 +262,6 @@ class MyDataset(Dataset):\n                 [\n                     {\"role\": \"system\", \"content\": \"This is a system message.\"},\n                     {\"role\": \"user\", \"content\": \"This is a test\"},\n-                    {\"role\": \"assistant\", \"content\": \"This is a reply\"},\n                 ],\n             ]\n \n@@ -213,7 +272,7 @@ def __getitem__(self, i):\n                 return {\"text\": self.data[i]}\n \n         text_generator = pipeline(\n-            task=\"text-generation\", model=\"rocketknight1/tiny-gpt2-with-chatml-template\", framework=\"pt\"\n+            task=\"text-generation\", model=\"hf-internal-testing/tiny-gpt2-with-chatml-template\", framework=\"pt\"\n         )\n \n         dataset = MyDataset()\n@@ -277,18 +336,16 @@ def test_small_model_tf(self):\n     @require_tf\n     def test_small_chat_model_tf(self):\n         text_generator = pipeline(\n-            task=\"text-generation\", model=\"rocketknight1/tiny-gpt2-with-chatml-template\", framework=\"tf\"\n+            task=\"text-generation\", model=\"hf-internal-testing/tiny-gpt2-with-chatml-template\", framework=\"tf\"\n         )\n         # Using `do_sample=False` to force deterministic output\n         chat1 = [\n             {\"role\": \"system\", \"content\": \"This is a system message.\"},\n             {\"role\": \"user\", \"content\": \"This is a test\"},\n-            {\"role\": \"assistant\", \"content\": \"This is a reply\"},\n         ]\n         chat2 = [\n             {\"role\": \"system\", \"content\": \"This is a system message.\"},\n             {\"role\": \"user\", \"content\": \"This is a second test\"},\n-            {\"role\": \"assistant\", \"content\": \"This is a reply\"},\n         ]\n         outputs = text_generator(chat1, do_sample=False, max_new_tokens=10)\n         expected_chat1 = chat1 + [\n@@ -308,7 +365,7 @@ def test_small_chat_model_tf(self):\n         expected_chat2 = chat2 + [\n             {\n                 \"role\": \"assistant\",\n-                \"content\": \" factors factors factors factors factors factors factors factors factors factors\",\n+                \"content\": \" stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs\",\n             }\n         ]\n "
        },
        {
            "sha": "64c860e3fc177d75c17890fa97e5b241ede4a01d",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/52a02137557963e9dd58c9be65b6cef871d3bf32/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52a02137557963e9dd58c9be65b6cef871d3bf32/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=52a02137557963e9dd58c9be65b6cef871d3bf32",
            "patch": "@@ -1327,6 +1327,36 @@ def test_chat_template_return_assistant_tokens_mask(self):\n                     [0] * (assistant_start2 - assistant_end - 1),\n                 )\n \n+    @require_jinja\n+    def test_continue_final_message(self):\n+        dummy_template = \"\"\"\n+        {%- for message in messages %}\n+            {{- \"<|im_start|>\" + message['role'] + \"\\n\" + message['content'] + \"<|im_end|>\" + \"\\n\"}}\n+        {%- endfor %}\"\"\"\n+        dummy_conversation = [\n+            {\"role\": \"system\", \"content\": \"system message\"},\n+            {\"role\": \"user\", \"content\": \"user message\"},\n+            {\"role\": \"assistant\", \"content\": \"assistant message\"},\n+        ]\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n+                output = tokenizer.apply_chat_template(\n+                    dummy_conversation, chat_template=dummy_template, tokenize=False, continue_final_message=False\n+                )\n+                self.assertEqual(\n+                    output,\n+                    \"<|im_start|>system\\nsystem message<|im_end|>\\n<|im_start|>user\\nuser message<|im_end|>\\n<|im_start|>assistant\\nassistant message<|im_end|>\\n\",\n+                )\n+                prefill_output = tokenizer.apply_chat_template(\n+                    dummy_conversation, chat_template=dummy_template, tokenize=False, continue_final_message=True\n+                )\n+                # Assert that the final message is unterminated\n+                self.assertEqual(\n+                    prefill_output,\n+                    \"<|im_start|>system\\nsystem message<|im_end|>\\n<|im_start|>user\\nuser message<|im_end|>\\n<|im_start|>assistant\\nassistant message\",\n+                )\n+\n     @require_jinja\n     def test_chat_template_dict(self):\n         dummy_template_1 = \"{{'a'}}\""
        }
    ],
    "stats": {
        "total": 222,
        "additions": 199,
        "deletions": 23
    }
}