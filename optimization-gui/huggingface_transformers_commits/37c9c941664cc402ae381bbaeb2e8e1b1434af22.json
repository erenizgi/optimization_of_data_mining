{
    "author": "stefgina",
    "message": "Fix Qwen3-VL typos and clarify image/video/vision token descriptions (#43033)\n\n* qwen3-vl-processor-videos-arg-correction\n\n* minor sequence typo in processor __call__()\n\n* minor kwarg typo in processor __call__()\n\n* attention bias duplicate default hinting\n\n* image, video, vision token ids - clarification\n\n* qwen3vl configuration file update\n\n* qwen3vl processing file update",
    "sha": "37c9c941664cc402ae381bbaeb2e8e1b1434af22",
    "files": [
        {
            "sha": "47b2a8245e01ab1447d613d33c28247353c0b517",
            "filename": "src/transformers/models/qwen3_vl/configuration_qwen3_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/37c9c941664cc402ae381bbaeb2e8e1b1434af22/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37c9c941664cc402ae381bbaeb2e8e1b1434af22/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fconfiguration_qwen3_vl.py?ref=37c9c941664cc402ae381bbaeb2e8e1b1434af22",
            "patch": "@@ -110,7 +110,7 @@ class Qwen3VLTextConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n@@ -197,13 +197,13 @@ class Qwen3VLConfig(PreTrainedConfig):\n         vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen3VLVisionConfig`):\n             The config object or dictionary of the vision backbone.\n         image_token_id (`int`, *optional*, defaults to 151655):\n-            The image token index to encode the image prompt.\n+            The token id used as the placeholder for image inputs.\n         video_token_id (`int`, *optional*, defaults to 151656):\n-            The video token index to encode the image prompt.\n+            The token id used as the placeholder for video inputs.\n         vision_start_token_id (`int`, *optional*, defaults to 151652):\n-            The start token index to encode the image prompt.\n+            The token id that marks the start of a vision segment (image or video).\n         vision_end_token_id (`int`, *optional*, defaults to 151653):\n-            The end token index to encode the image prompt.\n+            The token id that marks the end of a vision segment (image or video).\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie the word embeddings.\n "
        },
        {
            "sha": "31bbf99ef29a27c622233b2afc3569053f1d6d5f",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/37c9c941664cc402ae381bbaeb2e8e1b1434af22/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37c9c941664cc402ae381bbaeb2e8e1b1434af22/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=37c9c941664cc402ae381bbaeb2e8e1b1434af22",
            "patch": "@@ -152,7 +152,7 @@ class Qwen3VLTextConfig(PreTrainedConfig):\n             Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n             a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n             with longer `max_position_embeddings`.\n-        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n@@ -239,13 +239,13 @@ class Qwen3VLConfig(PreTrainedConfig):\n         vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `Qwen3VLVisionConfig`):\n             The config object or dictionary of the vision backbone.\n         image_token_id (`int`, *optional*, defaults to 151655):\n-            The image token index to encode the image prompt.\n+            The token id used as the placeholder for image inputs.\n         video_token_id (`int`, *optional*, defaults to 151656):\n-            The video token index to encode the image prompt.\n+            The token id used as the placeholder for video inputs.\n         vision_start_token_id (`int`, *optional*, defaults to 151652):\n-            The start token index to encode the image prompt.\n+            The token id that marks the start of a vision segment (image or video).\n         vision_end_token_id (`int`, *optional*, defaults to 151653):\n-            The end token index to encode the image prompt.\n+            The token id that marks the end of a vision segment (image or video).\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether to tie the word embeddings.\n \n@@ -1404,9 +1404,9 @@ def __call__(\n         **kwargs: Unpack[Qwen3VLProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        Main method to prepare for the model one or several sequence(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n+        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n         Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n \n         Args:\n@@ -1418,7 +1418,7 @@ def __call__(\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n+                The video or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n                 tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:"
        },
        {
            "sha": "14fdab3e8008a23706fd93fb16a2ccc8a4cf5a7e",
            "filename": "src/transformers/models/qwen3_vl/processing_qwen3_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/37c9c941664cc402ae381bbaeb2e8e1b1434af22/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37c9c941664cc402ae381bbaeb2e8e1b1434af22/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py?ref=37c9c941664cc402ae381bbaeb2e8e1b1434af22",
            "patch": "@@ -99,9 +99,9 @@ def __call__(\n         **kwargs: Unpack[Qwen3VLProcessorKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        Main method to prepare for the model one or several sequence(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwrags` arguments to\n+        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n         Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n \n         Args:\n@@ -113,7 +113,7 @@ def __call__(\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n+                The video or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n                 tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 16,
        "deletions": 16
    }
}