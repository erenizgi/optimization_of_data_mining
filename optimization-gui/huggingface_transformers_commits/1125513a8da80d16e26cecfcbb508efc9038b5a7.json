{
    "author": "remi-or",
    "message": "Blip2 fixes (#39080)\n\n* Fixed some devices errors\n\n* Fixed other device issues and more expectations\n\n* Reverted support flags\n\n* style\n\n* More granular support\n\n* Fixed some rebase stuff\n\n* add a not None check before .to",
    "sha": "1125513a8da80d16e26cecfcbb508efc9038b5a7",
    "files": [
        {
            "sha": "48636496f99bd422198da4f6682ed3a40f036d48",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 6,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/1125513a8da80d16e26cecfcbb508efc9038b5a7/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1125513a8da80d16e26cecfcbb508efc9038b5a7/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=1125513a8da80d16e26cecfcbb508efc9038b5a7",
            "patch": "@@ -415,6 +415,7 @@ class Blip2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\n         \"Blip2Attention\",\n         \"Blip2QFormerMultiHeadAttention\",\n+        \"Blip2EncoderLayer\",\n         \"Blip2TextEmbeddings\",\n         \"T5Block\",\n         \"OPTDecoderLayer\",\n@@ -1262,6 +1263,7 @@ class Blip2Model(Blip2PreTrainedModel):\n     config_class = Blip2Config\n     main_input_name = \"pixel_values\"\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n+    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -1646,6 +1648,7 @@ def forward(\n class Blip2TextModelWithProjection(Blip2PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n+    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -1738,6 +1741,7 @@ def forward(\n class Blip2VisionModelWithProjection(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n+    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -1857,6 +1861,7 @@ class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n+    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -2086,9 +2091,13 @@ def forward(\n             else:\n                 special_image_mask = input_ids == self.config.image_token_id\n \n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n+            special_image_mask = (\n+                special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(language_model_inputs.device)\n+            )\n+            language_model_inputs = language_model_inputs.to(inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.to(language_model_inputs.device).masked_scatter(\n+                special_image_mask, language_model_inputs\n+            )\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in BLIP-2 should be done in processing. \"\n@@ -2234,9 +2243,15 @@ def generate(\n             else:\n                 special_image_mask = input_ids == self.config.image_token_id\n \n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n+            special_image_mask = (\n+                special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(language_model_inputs.device)\n+            )\n+            language_model_inputs = language_model_inputs.to(inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.to(language_model_inputs.device).masked_scatter(\n+                special_image_mask, language_model_inputs\n+            )\n+\n+            attention_mask = attention_mask.to(language_attention_mask.device)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in BLIP-2 should be done in processing. \"\n@@ -2259,6 +2274,8 @@ def generate(\n \n         inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}\n         if not self.language_model.config.is_encoder_decoder:\n+            if input_ids is not None:\n+                input_ids = input_ids.to(language_model_inputs.device)\n             inputs[\"input_ids\"] = input_ids\n \n         outputs = self.language_model.generate(**inputs, **generate_kwargs)\n@@ -2275,6 +2292,7 @@ def generate(\n class Blip2ForImageTextRetrieval(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n+    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)"
        },
        {
            "sha": "4cac2f38136c21166d1afed91e258019babdaa13",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 12,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/1125513a8da80d16e26cecfcbb508efc9038b5a7/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1125513a8da80d16e26cecfcbb508efc9038b5a7/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=1125513a8da80d16e26cecfcbb508efc9038b5a7",
            "patch": "@@ -1786,7 +1786,8 @@ def test_inference_opt_multi_accelerator(self):\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n         # Test output\n-        self.assertEqual(predictions[0].tolist(), [2, 102, 693, 2828, 15, 5, 4105, 19, 10, 2335, 50118])\n+        expected_ids = [2, 102, 693, 2828, 15, 5, 4105, 19, 10, 2335, 50118]\n+        self.assertEqual(predictions[0].tolist(), [50265] * 32 + expected_ids)  # 50265 is the img token id\n         self.assertEqual(\"a woman sitting on the beach with a dog\", generated_text)\n \n         # image and context\n@@ -1797,10 +1798,8 @@ def test_inference_opt_multi_accelerator(self):\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n         # Test output\n-        self.assertEqual(\n-            predictions[0].tolist(),\n-            [2, 45641, 35, 61, 343, 16, 42, 116, 31652, 35, 24, 18, 45, 10, 343, 6, 24, 18, 10, 4105, 50118],\n-        )\n+        expected_ids = [2, 45641, 35, 61, 343, 16, 42, 116, 31652, 35, 24, 18, 45, 10, 343, 6, 24, 18, 10, 4105, 50118]\n+        self.assertEqual(predictions[0].tolist(), [50265] * 32 + expected_ids)  # 50265 is the img token id\n         self.assertEqual(generated_text, \"Question: which city is this? Answer: it's not a city, it's a beach\")\n \n     @require_torch_multi_accelerator\n@@ -1826,8 +1825,17 @@ def test_inference_t5_multi_accelerator(self):\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n         # Test output\n-        self.assertEqual(predictions[0].tolist(), [0, 2335, 1556, 28, 1782, 30, 8, 2608, 1])\n-        self.assertEqual(\"woman playing with dog on the beach\", generated_text)\n+        expected_ids_and_text = Expectations(\n+            {\n+                (\"cuda\", None): ([0, 2335, 1556, 28, 1782, 30, 8, 2608, 1], \"woman playing with dog on the beach\"),\n+                (\"rocm\", (9, 5)): (\n+                    [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n+                    \"a woman is playing with her dog on the beach\",\n+                ),\n+            }\n+        ).get_expectation()\n+        self.assertEqual(predictions[0].tolist(), expected_ids_and_text[0])\n+        self.assertEqual(generated_text, expected_ids_and_text[1])\n \n         # image and context\n         prompt = \"Question: which city is this? Answer:\"\n@@ -1837,11 +1845,17 @@ def test_inference_t5_multi_accelerator(self):\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n         # Test output\n-        self.assertEqual(\n-            predictions[0].tolist(),\n-            [0, 3, 7, 152, 67, 839, 1],\n-        )\n-        self.assertEqual(generated_text, \"san diego\")\n+        expected_ids_and_text = Expectations(\n+            {\n+                (\"cuda\", None): ([0, 3, 7, 152, 67, 839, 1], \"san diego\"),\n+                (\"rocm\", (9, 5)): (\n+                    [0, 3, 7, 152, 2515, 11389, 3523, 1],\n+                    \"san francisco\",  # TODO: check if this is ok\n+                ),\n+            }\n+        ).get_expectation()\n+        self.assertEqual(predictions[0].tolist(), expected_ids_and_text[0])\n+        self.assertEqual(generated_text, expected_ids_and_text[1])\n \n     def test_expansion_in_processing(self):\n         processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 50,
        "deletions": 18
    }
}