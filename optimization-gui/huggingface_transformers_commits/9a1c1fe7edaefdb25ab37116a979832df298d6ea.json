{
    "author": "gante",
    "message": "[CI] green llama tests (#37244)\n\n* green llama tests\n\n* use cleanup instead\n\n* better test comment; cleanup upgrade\n\n* better test comment; cleanup upgrade",
    "sha": "9a1c1fe7edaefdb25ab37116a979832df298d6ea",
    "files": [
        {
            "sha": "1857dee3d66b1ac0497e50e45dfe2f5fdead76aa",
            "filename": "benchmark/llama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/benchmark%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/benchmark%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fllama.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -118,7 +118,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n         with torch.no_grad():\n             past_key_values = StaticCache(\n                 model.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 device=device,\n                 dtype=torch.float16,\n                 max_cache_len=seq_length + num_tokens_to_generate,\n@@ -144,7 +144,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n \n             past_key_values = StaticCache(\n                 model.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 device=device,\n                 dtype=torch.float16,\n                 max_cache_len=seq_length + num_tokens_to_generate,\n@@ -187,7 +187,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n             # TODO use  decode_one_token(model, input_id.clone(), cache_position) for verification\n             past_key_values = StaticCache(\n                 model.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 device=device,\n                 dtype=torch.float16,\n                 max_cache_len=seq_length + num_tokens_to_generate + 10,\n@@ -254,7 +254,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n \n             past_key_values = StaticCache(\n                 model.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 device=device,\n                 dtype=torch.float16,\n                 max_cache_len=seq_length + 128,\n@@ -271,7 +271,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n \n             past_key_values = StaticCache(\n                 model.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 device=device,\n                 dtype=torch.float16,\n                 max_cache_len=seq_length + 128,\n@@ -287,7 +287,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n \n             past_key_values = StaticCache(\n                 model.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 device=device,\n                 dtype=torch.float16,\n                 max_cache_len=seq_length + 128,\n@@ -303,7 +303,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n \n             past_key_values = StaticCache(\n                 model.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 device=device,\n                 dtype=torch.float16,\n                 max_cache_len=seq_length + 128,"
        },
        {
            "sha": "e8e20dab5db60d7158b393020ed216ace870618a",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -93,7 +93,7 @@ model.generation_config.max_new_tokens = 16\n \n past_key_values = StaticCache(\n     config=model.config,\n-    batch_size=1,\n+    max_batch_size=1,\n     # If you plan to reuse the cache, make sure the cache length is large enough for all cases\n     max_cache_len=prompt_length+(model.generation_config.max_new_tokens*2),\n     device=model.device,\n@@ -159,7 +159,7 @@ from torch.nn.attention import SDPBackend, sdpa_kernel\n batch_size, seq_length = inputs[\"input_ids\"].shape\n with torch.no_grad():\n     past_key_values = StaticCache(\n-        config=model.config, batch_size=2, max_cache_len=4096, device=torch_device, dtype=model.dtype\n+        config=model.config, max_batch_size=2, max_cache_len=4096, device=torch_device, dtype=model.dtype\n     )\n     cache_position = torch.arange(seq_length, device=torch_device)\n     generated_ids = torch.zeros("
        },
        {
            "sha": "f6eaa58c0004d835418c397cbce0dd47640afd80",
            "filename": "docs/source/ko/llm_optims.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/docs%2Fsource%2Fko%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/docs%2Fsource%2Fko%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_optims.md?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -99,7 +99,7 @@ model.generation_config.max_new_tokens = 16\n \n past_key_values = StaticCache(\n     config=model.config,\n-    batch_size=1,\n+    max_batch_size=1,\n     # 캐시를 재사용할 계획이 있는 경우, 모든 경우에 충분한 캐시 길이를 설정해야 합니다\n     max_cache_len=prompt_length+(model.generation_config.max_new_tokens*2),\n     device=model.device,\n@@ -109,7 +109,7 @@ outputs = model.generate(**input_ids, past_key_values=past_key_values)\n print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n ['The theory of special relativity states 1. The speed of light is constant in all inertial reference frames. 2']\n \n-# 생성된 텍스트와 동일한 캐시 객체를 전달하여, 중단한 곳에서 생성을 계속합니다. \n+# 생성된 텍스트와 동일한 캐시 객체를 전달하여, 중단한 곳에서 생성을 계속합니다.\n # 다중 턴 대화의 경우, 생성된 텍스트에 새로운 사용자 입력을 추가할 수 있습니다.\n new_input_ids = outputs\n outputs = model.generate(new_input_ids, past_key_values=past_key_values)"
        },
        {
            "sha": "00fc950424268984be2fbb68ae320eca5be8a588",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -3075,6 +3075,7 @@ def cleanup(device: str, gc_collect=False):\n     if gc_collect:\n         gc.collect()\n     backend_empty_cache(device)\n+    torch._dynamo.reset()\n \n \n # Type definition of key used in `Expectations` class."
        },
        {
            "sha": "e726ecbd5070152efc63567971d8a199c5b6ed03",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -2285,6 +2285,14 @@ def _test_attention_implementation(self, attn_implementation):\n                     inputs_dict[input_name] = input_data\n             main_input = inputs_dict[model_class.main_input_name]\n \n+            # FA2 doesn't accept masking in the middle of the sequence for now. We usually generate right-padded\n+            # attention masks at test time and, with generate, the mask will be appended with 1s on the right,\n+            # resulting in a mask with holes (not supported properly by FA2).\n+            if attn_implementation == \"flash_attention_2\":\n+                for input_name in (\"attention_mask\", \"decoder_attention_mask\", \"encoder_attention_mask\"):\n+                    if input_name in inputs_dict:\n+                        inputs_dict[input_name] = torch.ones_like(inputs_dict[input_name])\n+\n             # make sure that all models have enough positions for generation\n             if hasattr(config, \"max_position_embeddings\"):\n                 config.max_position_embeddings = max_new_tokens + main_input.shape[1] + 1\n@@ -2339,8 +2347,6 @@ def test_eager_matches_sdpa_generate(self):\n     @slow\n     def test_eager_matches_fa2_generate(self):\n         \"\"\"Tests that generate has equivalent outputs with FA2 and eager attention implementations.\"\"\"\n-        # TODO (@joao @raushan) -- this test is failing the output checks on most models, investigate. After fixing,\n-        # check whether we still need the overwrites\n         self._test_attention_implementation(\"flash_attention_2\")\n \n     def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n@@ -3974,15 +3980,15 @@ def test_init_static_cache_multi_gpu(self):\n         # TODO: We need to raise a warning in case the cache is not set correctly\n         # with self.assertRaisesRegex(ValueError, \"If you are manually initializing the cache\"):\n         #     past_key_values = StaticCache(\n-        #         config=model.config, batch_size=1, max_cache_len=30, device=torch_device, dtype=model.dtype\n+        #         config=model.config, max_batch_size=1, max_cache_len=30, device=torch_device, dtype=model.dtype\n         #     )\n         #     results = model.generate(input_ids, past_key_values=past_key_values, **generation_kwargs)\n \n         # deduced from the device_map : layer 0 on device 0 and layer 1 on device 1\n         layer_device_map = {0: 0, 1: 1}\n         past_key_values = StaticCache(\n             config=model.config,\n-            batch_size=1,\n+            max_batch_size=1,\n             max_cache_len=30,\n             device=torch_device,\n             dtype=model.dtype,\n@@ -4183,7 +4189,11 @@ def test_prepare_inputs_for_generation_decoder_llm(self):\n         batch_size = 2\n         query_length = input_ids.shape[-1] - init_input_ids.shape[-1]\n         static_cache = StaticCache(\n-            config=config, batch_size=batch_size, max_cache_len=max_cache_len, device=torch_device, dtype=torch.float32\n+            config=config,\n+            max_batch_size=batch_size,\n+            max_cache_len=max_cache_len,\n+            device=torch_device,\n+            dtype=torch.float32,\n         )\n         static_cache = model(init_input_ids, past_key_values=static_cache).past_key_values\n         model_inputs = model.prepare_inputs_for_generation("
        },
        {
            "sha": "be37d3cd1759c42fd8ef1ad214cda18e42e8cb53",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers import AutoTokenizer, DeepseekV3Config, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    cleanup,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n@@ -605,6 +606,10 @@ def setUpClass(cls):\n             # 8 is for A100 / A10 and 7 for T4\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n+    def tearDown(self):\n+        # See LlamaIntegrationTest.tearDown(). Can be removed once LlamaIntegrationTest.tearDown() is removed.\n+        cleanup(torch_device, gc_collect=False)\n+\n     @slow\n     @require_torch_accelerator\n     @require_read_token"
        },
        {
            "sha": "08bd0295539d7014167a59f36b621be769018d20",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -25,6 +25,7 @@\n from transformers import AutoTokenizer, DiffLlamaConfig, StaticCache, is_torch_available, set_seed\n from transformers.testing_utils import (\n     backend_empty_cache,\n+    cleanup,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n@@ -685,6 +686,10 @@ def setUpClass(cls):\n             # 8 is for A100 / A10 and 7 for T4\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n+    def tearDown(self):\n+        # See LlamaIntegrationTest.tearDown(). Can be removed once LlamaIntegrationTest.tearDown() is removed.\n+        cleanup(torch_device, gc_collect=False)\n+\n     @slow\n     @require_torch_accelerator\n     @require_read_token\n@@ -884,7 +889,7 @@ def test_stacked_causal_mask_static_cache(self):\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n         past_key_values = StaticCache(\n             config=self.model.config,\n-            batch_size=1,\n+            max_batch_size=1,\n             max_cache_len=max_cache_len,\n             device=torch_device,\n             dtype=self.model.dtype,\n@@ -932,7 +937,7 @@ def test_partial_stacked_causal_mask_static_cache(self):\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n         past_key_values = StaticCache(\n             config=self.model.config,\n-            batch_size=1,\n+            max_batch_size=1,\n             max_cache_len=max_cache_len,\n             device=torch_device,\n             dtype=self.model.dtype,"
        },
        {
            "sha": "84802395b5c22655c8789f4dae62b724e0d19227",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -23,6 +23,7 @@\n from transformers import AutoModelForCausalLM, AutoTokenizer, GemmaConfig, is_torch_available\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n+    cleanup,\n     is_flaky,\n     require_bitsandbytes,\n     require_flash_attn,\n@@ -498,6 +499,10 @@ def setUpClass(cls):\n             # 8 is for A100 / A10 and 7 for T4\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n+    def tearDown(self):\n+        # See LlamaIntegrationTest.tearDown(). Can be removed once LlamaIntegrationTest.tearDown() is removed.\n+        cleanup(torch_device, gc_collect=False)\n+\n     @require_read_token\n     def test_model_2b_fp16(self):\n         model_id = \"google/gemma-2b\""
        },
        {
            "sha": "7f7930fd8afad3dc549617d4f4ddd282feee7164",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -549,6 +549,13 @@ def setUpClass(cls):\n             # 8 is for A100 / A10 and 7 for T4\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n+    def tearDown(self):\n+        # TODO (joao): automatic compilation, i.e. compilation when `cache_implementation=\"static\"` is used, leaves\n+        # some memory allocated in the cache, which means some object is not being released properly. This causes some\n+        # unoptimal memory usage, e.g. after certain tests a 7B model in FP16 no longer fits in a 24GB GPU.\n+        # Investigate the root cause.\n+        cleanup(torch_device, gc_collect=False)\n+\n     @slow\n     @require_read_token\n     def test_llama_3_1_hard(self):\n@@ -748,14 +755,6 @@ def test_export_static_cache(self):\n                 \"Simply put, the theory of relativity states that 1) the speed of light is the same for all \"\n                 \"observers, regardless of their location, and 2) the laws of physics are the same for all observers\"\n             ],\n-            \"meta-llama/Llama-3.2-3B\": [\n-                \"Simply put, the theory of relativity states that 1. the speed of light is constant, and 2. \"\n-                \"the speed of light is the fastest speed possible\"\n-            ],\n-            \"meta-llama/Llama-2-7b-hf\": [\n-                \"Simply put, the theory of relativity states that 1) the speed of light is a constant, and 2) \"\n-                \"the laws of physics are the same for all\",\n-            ],\n         }\n \n         for llama_model_ckp, EXPECTED_TEXT_COMPLETION in llama_models.items():\n@@ -946,7 +945,7 @@ def test_stacked_causal_mask_static_cache(self):\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n         past_key_values = StaticCache(\n             config=self.model.config,\n-            batch_size=1,\n+            max_batch_size=1,\n             max_cache_len=max_cache_len,\n             device=torch_device,\n             dtype=self.model.dtype,\n@@ -994,7 +993,7 @@ def test_partial_stacked_causal_mask_static_cache(self):\n         max_cache_len = 16  # note that max_cache_len is greater than the attention_mask.shape[-1]\n         past_key_values = StaticCache(\n             config=self.model.config,\n-            batch_size=1,\n+            max_batch_size=1,\n             max_cache_len=max_cache_len,\n             device=torch_device,\n             dtype=self.model.dtype,"
        },
        {
            "sha": "666da7d2b6b66639eef98b98ee30f0fac7442c9c",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -53,7 +53,7 @@ def __init__(self, model: Phi3ForCausalLM, batch_size: int, max_seq_len: int):\n             self.model = model\n             self.cache = StaticCache(\n                 config=model.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 max_cache_len=max_seq_len,\n                 device=self.model.device,\n                 dtype=self.model.dtype,"
        },
        {
            "sha": "b150116818c5e2a369fca0480cca616f90e840fc",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -227,10 +227,6 @@ def test_initialization(self):\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         pass\n \n-    @unittest.skip(reason=\"This one tries to use right padding as well\")\n-    def test_eager_matches_fa2_generate(self):\n-        pass\n-\n     @unittest.skip(reason=\"Depending on input modalities, some params may not have gradients\")\n     def test_training_gradient_checkpointing(self):\n         pass"
        },
        {
            "sha": "0277335c8301e436cc5a6cf85873dccdffb67d0f",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -52,7 +52,7 @@ def __init__(self, model: PhimoeForCausalLM, batch_size: int, max_seq_len: int):\n             self.model = model\n             self.cache = StaticCache(\n                 config=model.config,\n-                batch_size=batch_size,\n+                max_batch_size=batch_size,\n                 max_cache_len=max_seq_len,\n                 device=self.model.device,\n                 dtype=self.model.dtype,"
        },
        {
            "sha": "ce543e606618c386a4d32701e61318acc390ab45",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -24,6 +24,7 @@\n from transformers.models.auto.modeling_auto import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n from transformers.testing_utils import (\n+    cleanup,\n     require_accelerate,\n     require_sentencepiece,\n     require_tokenizers,\n@@ -1170,6 +1171,10 @@ def import_accelerate_mock(name, *args, **kwargs):\n @require_sentencepiece\n @require_tokenizers\n class T5ModelIntegrationTests(unittest.TestCase):\n+    def tearDown(self):\n+        # See LlamaIntegrationTest.tearDown(). Can be removed once LlamaIntegrationTest.tearDown() is removed.\n+        cleanup(torch_device, gc_collect=False)\n+\n     @cached_property\n     def model(self):\n         return T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-base\").to(torch_device)"
        },
        {
            "sha": "d5d45f43cc319a8d452cdb3fb3a9ac6521498cb3",
            "filename": "tests/quantization/aqlm_integration/test_aqlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -226,7 +226,7 @@ def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_valu\n         # Setup static KV cache for generation\n         past_key_values = StaticCache(\n             config=self.quantized_model.config,\n-            batch_size=1,\n+            max_batch_size=1,\n             max_cache_len=seq_length + self.max_new_tokens + 1,\n             device=torch_device,\n             dtype=self.quantized_model.config._pre_quantization_dtype,"
        },
        {
            "sha": "425cce664c02c6ea873c2559374890e52f4f4f54",
            "filename": "tests/quantization/spqr_integration/test_spqr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a1c1fe7edaefdb25ab37116a979832df298d6ea/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py?ref=9a1c1fe7edaefdb25ab37116a979832df298d6ea",
            "patch": "@@ -207,7 +207,7 @@ def decode_one_tokens(model, cur_token, input_pos, cache_position, past_key_valu\n         # Setup static KV cache for generation\n         past_key_values = StaticCache(\n             config=self.quantized_model.config,\n-            batch_size=1,\n+            max_batch_size=1,\n             max_cache_len=seq_length + self.max_new_tokens + 1,\n             device=torch_device,\n             dtype=self.quantized_model.config._pre_quantization_dtype,"
        }
    ],
    "stats": {
        "total": 98,
        "additions": 62,
        "deletions": 36
    }
}