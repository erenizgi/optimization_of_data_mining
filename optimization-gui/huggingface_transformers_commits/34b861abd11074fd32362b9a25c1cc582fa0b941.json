{
    "author": "vasqu",
    "message": ":rotating_light: [`Attention Masks`] Bidirectional masks for encoder and encoder-decoder models (#41265)\n\n* new masks\n\n* fixes\n\n* adjust comments\n\n* fix unnecessary mask creation on sdpa\n\n* simplify masks more\n\n* propogate to other models\n\n* style + repo consistency\n\n* copies\n\n* no comment\n\n* fix attempt\n\n* finally fix grounding dinos\n\n* fix distilbert\n\n* fix executorch\n\n* move to own module\n\n* address first few comments WIP\n\n* revert device comments, simplify executorch further\n\n* fix typo\n\n* add a test for cuda graphs\n\n* move cleanup...\n\n* fix conflict with new main\n\n* fix esm and evolla",
    "sha": "34b861abd11074fd32362b9a25c1cc582fa0b941",
    "files": [
        {
            "sha": "44156dad2e30a6be33ff1c8192f371f89c8e9ee5",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 144,
            "deletions": 0,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -26,6 +26,7 @@\n from ..generation.configuration_utils import GenerationConfig\n from ..masking_utils import (\n     ALL_MASK_ATTENTION_FUNCTIONS,\n+    _ignore_bidirectional_mask_sdpa,\n     _ignore_causal_mask_sdpa,\n     _is_torch_greater_or_equal_than_2_5,\n     prepare_padding_mask,\n@@ -192,6 +193,101 @@ def generate(\n         pass\n \n \n+class TorchExportableModuleForEncoderOnlyLM(torch.nn.Module):\n+    \"\"\"\n+    A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n+    specifically for encoder-only LM. This module ensures that the exported model is compatible\n+    with further lowering and execution in `ExecuTorch`.\n+    \"\"\"\n+\n+    def __init__(self, model: PreTrainedModel) -> None:\n+        \"\"\"\n+        Initializes the exportable module.\n+\n+        Args:\n+            model (`PreTrainedModel`): The pretrained model to wrap.\n+        \"\"\"\n+        super().__init__()\n+\n+        self.model = model\n+        # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n+        ALL_MASK_ATTENTION_FUNCTIONS.register(\n+            \"sdpa_bidirectional_mask_without_vmap\", sdpa_bidirectional_mask_without_vmap\n+        )\n+        ALL_ATTENTION_FUNCTIONS.register(\"sdpa_bidirectional_mask_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n+        self.model.config._attn_implementation = \"sdpa_bidirectional_mask_without_vmap\"\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n+\n+        Args:\n+            input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n+            inputs_embeds (`torch.Tensor`): Tensor representing current input embeddings to the module.\n+            cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n+\n+        Returns:\n+            torch.Tensor: Logits output from the model.\n+        \"\"\"\n+        return self.model.forward(\n+            input_ids=input_ids,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+        )\n+\n+    def export(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        strict: Optional[bool] = None,\n+    ) -> torch.export.ExportedProgram:\n+        \"\"\"\n+        Export the wrapped module using `torch.export`.\n+\n+        Args:\n+            input_ids (`Optional[torch.Tensor]`):\n+                Tensor representing current input token id to the module. Must specify either this or inputs_embeds.\n+            inputs_embeds (`Optional[torch.Tensor]`):\n+                Tensor representing current input embeddings to the module. Must specify either this or input_ids.\n+            strict(`Optional[bool]`):\n+                Flag to instruct `torch.export` to use `torchdynamo`.\n+\n+        Returns:\n+            torch.export.ExportedProgram: The exported program that can be used for inference.\n+\n+        \"\"\"\n+        if not (input_ids is None) ^ (inputs_embeds is None):\n+            raise ValueError(\"Need to specify either input_ids or inputs_embeds.\")\n+\n+        if input_ids is not None:\n+            input_kwargs = {\n+                \"input_ids\": input_ids,\n+                \"attention_mask\": attention_mask if attention_mask is not None else torch.ones_like(input_ids),\n+            }\n+        else:\n+            input_kwargs = {\n+                \"inputs_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask\n+                if attention_mask is not None\n+                else torch.ones_like(inputs_embeds)[..., 0],\n+            }\n+\n+        exported_program = torch.export.export(\n+            self.model,\n+            args=(),\n+            kwargs=input_kwargs,\n+            strict=strict if strict is not None else True,\n+        )\n+\n+        return exported_program\n+\n+\n class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n     \"\"\"\n     A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n@@ -1200,3 +1296,51 @@ def sdpa_mask_without_vmap(\n     if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:\n         causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n     return causal_mask\n+\n+\n+def sdpa_bidirectional_mask_without_vmap(\n+    kv_length: int,\n+    kv_offset: int = 0,\n+    attention_mask: Optional[torch.Tensor] = None,\n+    allow_torch_fix: bool = True,\n+    allow_is_bidirectional_skip: bool = True,\n+    **kwargs,\n+) -> Optional[torch.Tensor]:\n+    \"\"\"\n+    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n+    the element should take part in the attention computation, and False that it should not.\n+\n+    This is similar to `masking_utils.sdpa_mask` but does not use `vmap` which is incompatible with export.\n+    Additionally, surrounding logic for causal masks is omitted for simplicity.\n+\n+    Args:\n+        kv_length (`int`):\n+            The size that the key and value states will have during the attention computation.\n+        kv_offset (`int`, optional):\n+            An optional offset to indicate at which first position the key and values states will refer to.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n+        allow_torch_fix (`bool`, optional):\n+            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n+            versions. We need an arg to skip it when using eager. By default `True`.\n+        allow_is_bidirectional_skip (`bool`, optional):\n+            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n+            i.e. full attention without any padding. Default to `True`.\n+    \"\"\"\n+    # Potentially pad the 2D mask, and slice it correctly\n+    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n+\n+    # Under specific conditions, we can avoid materializing the mask\n+    if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n+        return None\n+\n+    bidirectional_mask = None\n+    if padding_mask is not None:\n+        bidirectional_mask = padding_mask[:, None, None, :]\n+\n+    # Due to a bug in some older torch version, we need to update the mask in case a query is not attending to any\n+    # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n+    if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix and bidirectional_mask is not None:\n+        bidirectional_mask |= torch.all(~bidirectional_mask, dim=-1, keepdim=True)\n+\n+    return bidirectional_mask"
        },
        {
            "sha": "395eb11e1df2b3230dac5aec1af3638831f9589d",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 123,
            "deletions": 2,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -79,6 +79,13 @@ def causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int)\n     return kv_idx <= q_idx\n \n \n+def bidirectional_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+    \"\"\"\n+    This creates a full bidirectional mask.\n+    \"\"\"\n+    return q_idx.new_ones((), dtype=torch.bool)\n+\n+\n def sliding_window_overlay(sliding_window: int) -> Callable:\n     \"\"\"\n     This is an overlay depicting a sliding window pattern. Add it on top of a causal mask for a proper sliding\n@@ -263,6 +270,21 @@ def _ignore_causal_mask_sdpa(\n     return False\n \n \n+def _ignore_bidirectional_mask_sdpa(padding_mask: Optional[torch.Tensor]) -> bool:\n+    \"\"\"\n+    Detects whether the bidirectional mask can be ignored in case PyTorch's SDPA is used, i.e. when there is full\n+    attention with no padding.\n+    \"\"\"\n+    is_tracing = torch.jit.is_tracing() or isinstance(padding_mask, torch.fx.Proxy) or is_torchdynamo_compiling()\n+\n+    # When using `torch.export` or `torch.onnx.dynamo_export`, we need to avoid to check the contents of the mask;\n+    # otherwise, we will encounter dynamic control flows\n+    if not is_tracing and (padding_mask is None or padding_mask.all()):\n+        return True\n+\n+    return False\n+\n+\n def sdpa_mask_recent_torch(\n     batch_size: int,\n     cache_position: torch.Tensor,\n@@ -272,6 +294,7 @@ def sdpa_mask_recent_torch(\n     attention_mask: Optional[torch.Tensor] = None,\n     local_size: Optional[int] = None,\n     allow_is_causal_skip: bool = True,\n+    allow_is_bidirectional_skip: bool = False,\n     **kwargs,\n ) -> Optional[torch.Tensor]:\n     \"\"\"\n@@ -301,6 +324,9 @@ def sdpa_mask_recent_torch(\n         allow_torch_fix (`bool`, optional):\n             Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n             versions. We need an arg to skip it when using eager. By default `True`.\n+        allow_is_bidirectional_skip (`bool`, optional):\n+            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n+            i.e. full attention without any padding. Default to `False`.\n \n \n     ## Creating a simple causal mask:\n@@ -371,9 +397,13 @@ def sdpa_mask_recent_torch(\n     # Potentially pad the 2D mask, and slice it correctly\n     padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n \n-    # Under specific conditions, we can avoid materializing the mask, instead relying on the `is_causal` argument\n+    # Under specific conditions, we can avoid materializing the mask\n+    #   1. Causal masks can rely on the `is_causal` argument\n+    #   2. Bidirectional do not need any further processing (no bias)\n     if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, kv_offset, local_size):\n         return None\n+    if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n+        return None\n \n     # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n     # but without data-dependent slicing (i.e. torch.compile friendly)\n@@ -405,6 +435,7 @@ def sdpa_mask_older_torch(\n     local_size: Optional[int] = None,\n     allow_is_causal_skip: bool = True,\n     allow_torch_fix: bool = True,\n+    allow_is_bidirectional_skip: bool = False,\n     **kwargs,\n ) -> Optional[torch.Tensor]:\n     \"\"\"\n@@ -438,14 +469,21 @@ def sdpa_mask_older_torch(\n         allow_torch_fix (`bool`, optional):\n             Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n             versions. We need an arg to skip it when using eager. By default `True`.\n+        allow_is_bidirectional_skip (`bool`, optional):\n+            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n+            i.e. full attention without any padding. Default to `False`.\n     \"\"\"\n     q_length = cache_position.shape[0]\n     # Potentially pad the 2D mask, and slice it correctly\n     padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset)\n \n-    # Under specific conditions, we can avoid materializing the mask, instead relying on the `is_causal` argument\n+    # Under specific conditions, we can avoid materializing the mask\n+    #   1. Causal masks can rely on the `is_causal` argument\n+    #   2. Bidirectional do not need any further processing (no bias)\n     if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, kv_offset, local_size):\n         return None\n+    if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n+        return None\n \n     # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n     # but without data-dependent slicing (i.e. torch.compile friendly)\n@@ -506,6 +544,7 @@ def eager_mask(\n     \"\"\"\n     # The masks for eager attention are simply boolean mask from sdpa, casted to 0 and -inf\n     _ = kwargs.pop(\"allow_is_causal_skip\", None)\n+    _ = kwargs.pop(\"allow_is_bidirectional_skip\", None)\n     mask = sdpa_mask(\n         batch_size=batch_size,\n         cache_position=cache_position,\n@@ -514,6 +553,7 @@ def eager_mask(\n         mask_function=mask_function,\n         attention_mask=attention_mask,\n         allow_is_causal_skip=False,\n+        allow_is_bidirectional_skip=False,\n         allow_torch_fix=False,\n         **kwargs,\n     )\n@@ -837,6 +877,87 @@ def create_causal_mask(\n     return causal_mask\n \n \n+def create_bidirectional_mask(\n+    config: PreTrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    encoder_hidden_states: Optional[torch.Tensor] = None,\n+    or_mask_function: Optional[Callable] = None,\n+    and_mask_function: Optional[Callable] = None,\n+) -> Optional[Union[torch.Tensor, BlockMask]]:\n+    \"\"\"\n+    Create a standard bidirectional mask based on the attention implementation used (stored in the config).\n+\n+    Args:\n+        config (`PreTrainedConfig`):\n+            The model config.\n+        input_embeds (`torch.Tensor`):\n+            The input embeddings of shape (batch_size, query_length, hidden_dim). This is only used to infer metadata\n+            such as the batch size, query length, dtype, and device.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, kv_length).\n+            It can also be an already prepared 4D mask of shape (batch_size, 1, query_length, kv_length),\n+            in which case it is returned as-is.\n+        encoder_hidden_states (`torch.Tensor`, optional):\n+            The input embeddings of shape (batch_size, kv_length, hidden_dim). If provided, it is used instead of\n+            `input_embeds` to infer the batch size, kv length and dtype.\n+        or_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the base mask function (by doing the union of both). This is\n+            useful to easily overlay another mask on top, for example for image tokens handling.\n+        and_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the base mask function (by doing the intersection of both). This is\n+            useful to easily overlay another mask on top, for example for image tokens handling.\n+    \"\"\"\n+    # Due to the logic surrounding `cache_position` in inferring query-related information, we\n+    # construct a dummy tensor imitating initial positions\n+    cache_position = torch.arange(input_embeds.shape[1], device=input_embeds.device, dtype=torch.long)\n+\n+    embeds = encoder_hidden_states if encoder_hidden_states is not None else input_embeds\n+    # We ignore a few irrelevant arguments at the end as we do not have a (growing) cache here\n+    early_exit, attention_mask, _, kv_length, kv_offset = _preprocess_mask_arguments(\n+        config, embeds, attention_mask, cache_position, None, None, 0\n+    )\n+    if early_exit:\n+        return attention_mask\n+\n+    batch_size, dtype = embeds.shape[0], embeds.dtype\n+    mask_factory_function = bidirectional_mask_function\n+    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n+\n+    # Allow skipping the mask creation except we have additional masking operators (and/or masks)\n+    allow_is_bidirectional_skip = True\n+\n+    # Allow slight deviations from the base mask\n+    # Note that it is very important to apply this before any other deviations of the mask (such as packed sequence mask,\n+    # padding mask, etc) as the resulting mask may otherwise not be correct!\n+    if or_mask_function is not None:\n+        if not _is_torch_greater_or_equal_than_2_6:\n+            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n+        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n+        allow_is_bidirectional_skip = False\n+    if and_mask_function is not None:\n+        if not _is_torch_greater_or_equal_than_2_6:\n+            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.6\")\n+        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n+        allow_is_bidirectional_skip = False\n+\n+    # We now create the mask\n+    attention_mask = mask_interface(\n+        batch_size=batch_size,\n+        cache_position=cache_position,\n+        kv_length=kv_length,\n+        kv_offset=kv_offset,\n+        mask_function=mask_factory_function,\n+        attention_mask=attention_mask,\n+        # Additional kwargs for sdpa\n+        allow_is_causal_skip=False,\n+        allow_is_bidirectional_skip=allow_is_bidirectional_skip,\n+        dtype=dtype,  # Additional kwarg for eager\n+        config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n+    )\n+    return attention_mask\n+\n+\n def create_sliding_window_causal_mask(\n     config: PreTrainedConfig,\n     input_embeds: torch.Tensor,"
        },
        {
            "sha": "e8d6500431697ff6c79d499c00313d299f043cee",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 28,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -38,15 +38,11 @@\n from ...pytorch_utils import (\n     apply_chunking_to_forward,\n )\n-from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_albert import AlbertConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -399,7 +395,11 @@ def forward(\n             input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n         )\n \n-        attention_mask = self._update_full_mask(attention_mask, embedding_output)\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=embedding_output,\n+            attention_mask=attention_mask,\n+        )\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n@@ -417,27 +417,6 @@ def forward(\n             pooler_output=pooled_output,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "a2baa20532a6abe822ed40dfa1194fcc078bd487",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -841,7 +841,7 @@ def _init_weights(self, module: nn.Module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    # copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n     def _update_full_mask(\n         self,\n         attention_mask: Union[torch.Tensor, None],\n@@ -863,7 +863,7 @@ def _update_full_mask(\n         return attention_mask\n \n \n-# Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesTransformerEncoder with TimeSeriesTransformer->Autoformer,TimeSeries->Autoformer\n+# copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesTransformerEncoder with TimeSeriesTransformer->Autoformer,TimeSeries->Autoformer\n class AutoformerEncoder(AutoformerPreTrainedModel):\n     \"\"\"\n     Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a"
        },
        {
            "sha": "02a621fd60de596c146f8db67ccb1ade583cdd8d",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 16,
            "deletions": 208,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -26,11 +26,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -47,17 +43,12 @@\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_bart import BartConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -509,191 +500,6 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class PretrainedBartModel(BartPreTrainedModel):\n     def __init_subclass__(self):\n@@ -816,9 +622,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -1017,17 +824,18 @@ def forward(\n             else past_key_values\n         )\n \n-        attention_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions"
        },
        {
            "sha": "ebf78025c9c9fe44574c377b1ff94c12a20c4a2a",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 24,
            "deletions": 102,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -27,8 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -44,15 +43,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_bert import BertConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -683,12 +678,11 @@ def forward(\n \n         if input_ids is not None:\n             device = input_ids.device\n-            input_shape = input_ids.shape\n+            seq_length = input_ids.shape[1]\n         else:\n             device = inputs_embeds.device\n-            input_shape = inputs_embeds.shape[:-1]\n+            seq_length = inputs_embeds.shape[1]\n \n-        seq_length = input_shape[1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:\n             cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n@@ -702,7 +696,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -733,109 +726,38 @@ def forward(\n \n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "4fd9a7e9476e8856e6b3674bf83566566f0d534a",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 21,
            "deletions": 99,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -23,8 +23,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -33,17 +32,12 @@\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     logging,\n )\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_bert_generation import BertGenerationConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -564,7 +558,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -594,109 +587,38 @@ def forward(\n     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class BertGenerationOnlyLMHead(nn.Module):\n     def __init__(self, config):"
        },
        {
            "sha": "88d6a8cf9aaee11b92585a7c58676084b548036d",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 14,
            "deletions": 193,
            "changes": 207,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -26,11 +26,8 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -44,20 +41,10 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TransformersKwargs,\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling, logging\n from .configuration_bigbird_pegasus import BigBirdPegasusConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n _EXPECTED_OUTPUT_SHAPE = [1, 7, 1024]\n@@ -1576,173 +1563,6 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class BigBirdPegasusEncoder(BigBirdPegasusPreTrainedModel):\n     \"\"\"\n@@ -2178,17 +1998,18 @@ def forward(\n             else past_key_values\n         )\n \n-        attention_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions"
        },
        {
            "sha": "05870dd85359e4eebf493a6f41d601cfde0df881",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 144,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -41,14 +41,10 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from .configuration_biogpt import BioGptConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -344,139 +340,6 @@ class BioGptPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring\n class BioGptModel(BioGptPreTrainedModel):\n@@ -564,11 +427,12 @@ def forward(\n \n         self_attn_cache = past_key_values\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n \n         # embed positions"
        },
        {
            "sha": "73c29a4d0b6d460cac9839a29679e88d1247883d",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 146,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -24,9 +24,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-)\n+from ...masking_utils import create_causal_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -38,7 +36,6 @@\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     logger,\n )\n from ..bart.modeling_bart import (\n@@ -50,10 +47,6 @@\n from .configuration_biogpt import BioGptConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n class BioGptLearnedPositionalEmbedding(OPTLearnedPositionalEmbedding):\n     def forward(\n         self,\n@@ -169,139 +162,6 @@ class BioGptPreTrainedModel(PreTrainedModel):\n \n     _can_compile_fullgraph = True\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring\n class BioGptModel(BioGptPreTrainedModel):\n@@ -389,11 +249,12 @@ def forward(\n \n         self_attn_cache = past_key_values\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n \n         # embed positions"
        },
        {
            "sha": "4b828610656fa5321fc6633a5b68a16b4560fbf5",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 16,
            "deletions": 211,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -27,11 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -46,18 +42,13 @@\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n )\n from ..blenderbot_small import BlenderbotSmallForConditionalGeneration, BlenderbotSmallModel\n from .configuration_blenderbot import BlenderbotConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -472,194 +463,6 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class BlenderbotEncoder(BlenderbotPreTrainedModel):\n     \"\"\"\n@@ -765,9 +568,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -970,17 +774,18 @@ def forward(\n             else past_key_values\n         )\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions"
        },
        {
            "sha": "552412c3da23121b7b2bdc1288af2a156ac662a9",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 16,
            "deletions": 211,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -25,11 +25,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -44,17 +40,12 @@\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_blenderbot_small import BlenderbotSmallConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -465,194 +456,6 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class BlenderbotSmallEncoder(BlenderbotSmallPreTrainedModel):\n     \"\"\"\n@@ -757,9 +560,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -957,17 +761,18 @@ def forward(\n             else past_key_values\n         )\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions"
        },
        {
            "sha": "98f922ab624089eeeb9412b1779c1a25139d450e",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 22,
            "deletions": 99,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -25,8 +25,7 @@\n \n from ...activations import ACT2FN, QuickGELUActivation\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -38,15 +37,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging, torch_int\n+from ...utils import TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.generic import can_return_tuple\n from .configuration_bridgetower import BridgeTowerConfig, BridgeTowerTextConfig, BridgeTowerVisionConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n _TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n@@ -1070,7 +1065,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -1107,109 +1101,38 @@ def forward(\n     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "79dc20a8d2bc6a24bdfb119a3dcb6d915b362392",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 24,
            "deletions": 100,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -30,8 +30,7 @@\n from ...activations import ACT2FN, gelu\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -46,15 +45,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_camembert import CamembertConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -663,12 +658,11 @@ def forward(\n \n         if input_ids is not None:\n             device = input_ids.device\n-            input_shape = input_ids.shape\n+            seq_length = input_ids.shape[1]\n         else:\n             device = inputs_embeds.device\n-            input_shape = inputs_embeds.shape[:-1]\n+            seq_length = inputs_embeds.shape[1]\n \n-        seq_length = input_shape[1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:\n             cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n@@ -682,7 +676,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -713,107 +706,38 @@ def forward(\n \n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring\n class CamembertForMaskedLM(CamembertPreTrainedModel):"
        },
        {
            "sha": "e051bc3863fddfcd343bcd6b5184c0dda145be71",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 6,
            "deletions": 29,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -32,7 +32,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -45,14 +45,10 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, is_peft_available, is_torch_flex_attn_available\n+from ...utils import TransformersKwargs, auto_docstring, is_peft_available\n from .configuration_data2vec_audio import Data2VecAudioConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n class Data2VecAudioConvLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -378,9 +374,10 @@ def forward(\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n@@ -422,26 +419,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class Data2VecAudioAdapterLayer(nn.Module):\n     def __init__(self, config):"
        },
        {
            "sha": "f0012cf298787ec2d71232f289580fb9fd68dd84",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 24,
            "deletions": 100,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -29,8 +29,7 @@\n from ...activations import ACT2FN, gelu\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -45,15 +44,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_data2vec_text import Data2VecTextConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -623,12 +618,11 @@ def forward(\n \n         if input_ids is not None:\n             device = input_ids.device\n-            input_shape = input_ids.shape\n+            seq_length = input_ids.shape[1]\n         else:\n             device = inputs_embeds.device\n-            input_shape = inputs_embeds.shape[:-1]\n+            seq_length = inputs_embeds.shape[1]\n \n-        seq_length = input_shape[1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:\n             cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n@@ -642,7 +636,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -673,107 +666,38 @@ def forward(\n \n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class Data2VecTextLMHead(nn.Module):\n     \"\"\"Data2VecText Head for masked language modeling.\"\"\""
        },
        {
            "sha": "d10867e68175d87f1466d2aa00c6835793fd30c9",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 11,
            "deletions": 78,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -28,8 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...integrations import use_kernel_forward_from_hub\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -41,22 +40,11 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TransformersKwargs,\n-    auto_docstring,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from .configuration_dia import DiaConfig, DiaDecoderConfig, DiaEncoderConfig\n from .generation_dia import DiaGenerationMixin\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -450,9 +438,10 @@ def forward(\n         position_ids = torch.arange(input_ids.shape[-1], device=input_ids.device)[None, :]\n         position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -482,27 +471,6 @@ def forward(\n             last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class DiaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DiaDecoderConfig, layer_idx: int):\n@@ -623,13 +591,12 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            position_ids=position_ids,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            hidden_states.shape[:2],\n-            hidden_states,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         all_hidden_states = () if output_hidden_states else None\n@@ -671,40 +638,6 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "8f20ce3dbd8ecf03c623705491a9feed8e26c730",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 11,
            "deletions": 74,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -21,11 +21,7 @@\n from torch import nn\n \n from ...cache_utils import DynamicCache, EncoderDecoderCache\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -36,7 +32,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaRMSNorm,\n@@ -48,10 +44,6 @@\n from .generation_dia import DiaGenerationMixin\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -267,9 +259,10 @@ def forward(\n         position_ids = torch.arange(input_ids.shape[-1], device=input_ids.device)[None, :]\n         position_embeddings = self.rotary_embeddings(hidden_states, position_ids)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -299,27 +292,6 @@ def forward(\n             last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class DiaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: DiaDecoderConfig, layer_idx: int):\n@@ -440,13 +412,12 @@ def forward(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n-            position_ids=position_ids,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            hidden_states.shape[:2],\n-            hidden_states,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         all_hidden_states = () if output_hidden_states else None\n@@ -488,40 +459,6 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "6f2fb86fb88510797282239a32230b5eb5ab8ab8",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 30,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import get_activation\n from ...configuration_utils import PreTrainedConfig\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -47,17 +47,12 @@\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     logging,\n )\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_distilbert import DistilBertConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -416,9 +411,10 @@ def forward(\n \n         embeddings = self.embeddings(input_ids, inputs_embeds, position_ids)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            embeddings,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=embeddings,\n+            attention_mask=attention_mask,\n         )\n \n         return self.transformer(\n@@ -427,27 +423,6 @@ def forward(\n             **kwargs,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "08a681ecc0c1669cf1a59860e0ec3c1126061dc0",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 21,
            "deletions": 99,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -25,8 +25,7 @@\n from ...activations import ACT2FN, get_activation\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n@@ -45,17 +44,12 @@\n     ModelOutput,\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     logging,\n )\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_electra import ElectraConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -644,7 +638,6 @@ def forward(\n             embedding_output = self.embeddings_project(embedding_output)\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -672,109 +665,38 @@ def forward(\n     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class ElectraClassificationHead(nn.Module):\n     \"\"\"Head for sentence-level classification tasks.\"\"\""
        },
        {
            "sha": "52dd9a6d64f1e58cfa2b98fda112e72b3ab1085c",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 30,
            "deletions": 133,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -31,8 +31,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -48,15 +47,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_ernie import ErnieConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -681,43 +676,14 @@ def forward(\n             past_key_values_length=past_key_values_length,\n         )\n \n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n@@ -741,107 +707,38 @@ def forward(\n \n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "e4cbb56154e106cb58d2d06d9373888fdf769b3e",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "modified",
            "additions": 40,
            "deletions": 94,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -22,8 +22,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPoolingAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -36,7 +35,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from ..bert.modeling_bert import (\n     BertCrossAttention,\n@@ -60,10 +59,6 @@\n from .configuration_ernie import ErnieConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -268,43 +263,14 @@ def forward(\n             past_key_values_length=past_key_values_length,\n         )\n \n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n@@ -326,60 +292,40 @@ def forward(\n             past_key_values=encoder_outputs.past_key_values,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n+    # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n+    def _create_attention_masks(\n         self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n     ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n+        if encoder_attention_mask is not None:\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n-        return encoder_attention_mask\n+        return attention_mask, encoder_attention_mask\n \n \n class ErnieForPreTrainingOutput(BertForPreTrainingOutput):"
        },
        {
            "sha": "358370d0f9f026ac958f75b1c9c0f4c5ba9c7849",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 40,
            "deletions": 66,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -23,7 +23,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n@@ -34,15 +34,11 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_esm import EsmConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -657,16 +653,14 @@ def forward(\n                 position_ids=position_ids,\n             )\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n-        )\n-\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            inputs_embeds.shape[:2],\n-            inputs_embeds,\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=inputs_embeds,\n+            encoder_hidden_states=encoder_hidden_states,\n+            # There is no real logic for decoder generation, creating values on the fly\n+            cache_position=torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device),\n+            past_key_values=None,\n         )\n \n         encoder_outputs = self.encoder(\n@@ -684,60 +678,40 @@ def forward(\n             pooler_output=pooled_output,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n+    # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n+    def _create_attention_masks(\n         self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n     ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n+        if encoder_attention_mask is not None:\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n-        return encoder_attention_mask\n+        return attention_mask, encoder_attention_mask\n \n     def predict_contacts(self, tokens, attention_mask):\n         attns = self(tokens, attention_mask=attention_mask, return_dict=True, output_attentions=True).attentions"
        },
        {
            "sha": "56e536ddfb51fd3b1fc3db6162c6e90fecdad798",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 6,
            "deletions": 31,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -31,8 +31,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n@@ -44,15 +43,11 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_evolla import EvollaConfig, SaProtConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n def create_position_ids_from_input_ids(input_ids, padding_idx):\n     \"\"\"\n     Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n@@ -565,9 +560,10 @@ def forward(\n             attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n         inputs_embeds = self.embeddings(input_ids=input_ids, attention_mask=attention_mask)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_outputs = self.encoder(inputs_embeds, attention_mask=attention_mask, **kwargs)\n@@ -580,27 +576,6 @@ def forward(\n             cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class EvollaSequenceCompressorAttention(nn.Module):\n     def __init__(self, dim, dim_head=64, heads=8):"
        },
        {
            "sha": "46542b541c2a4f5c8ef4d5ece0e66f6ba63de402",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 5,
            "deletions": 31,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -21,8 +21,7 @@\n \n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -33,7 +32,6 @@\n from ...utils import (\n     auto_docstring,\n     can_return_tuple,\n-    is_torch_flex_attn_available,\n     logging,\n )\n from ...utils.generic import OutputRecorder, check_model_inputs\n@@ -59,10 +57,6 @@\n from .configuration_evolla import EvollaConfig, SaProtConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -251,9 +245,10 @@ def forward(\n             attention_mask = torch.ones(((batch_size, seq_length)), device=device)\n         inputs_embeds = self.embeddings(input_ids=input_ids, attention_mask=attention_mask)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_outputs = self.encoder(inputs_embeds, attention_mask=attention_mask, **kwargs)\n@@ -266,27 +261,6 @@ def forward(\n             cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class EvollaSequenceCompressorAttention(nn.Module):\n     def __init__(self, dim, dim_head=64, heads=8):"
        },
        {
            "sha": "c09f6da200c8c495c92b091c3378bb8d9176f16c",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -32,13 +32,7 @@\n from ...modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TransformersKwargs,\n-    auto_docstring,\n-    can_return_tuple,\n-    is_torch_available,\n-    logging,\n-)\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_available, logging\n from ..auto import AutoModel\n from .configuration_florence2 import Florence2Config, Florence2VisionConfig\n "
        },
        {
            "sha": "2bb5bb842a9f509ffae71b81e628447dd64f6fe9",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -2110,6 +2110,11 @@ def forward(\n             token_type_ids = token_type_ids[:, :max_text_len]\n             text_token_mask = text_token_mask[:, :max_text_len]\n \n+        # 3D -> 4D correction (add head dim)\n+        # NOTE: we squeeze this later again as there is custom 3D logic in this model\n+        if text_self_attention_masks.ndim == 3:\n+            text_self_attention_masks = text_self_attention_masks[:, None, :, :]\n+\n         # Extract text features from text backbone\n         text_outputs = self.text_backbone(\n             input_ids, text_self_attention_masks, token_type_ids, position_ids, return_dict=return_dict\n@@ -2192,7 +2197,7 @@ def forward(\n                 text_features=text_features,\n                 text_attention_mask=~text_token_mask,\n                 text_position_embedding=None,\n-                text_self_attention_masks=~text_self_attention_masks,\n+                text_self_attention_masks=~text_self_attention_masks.squeeze(1),\n                 text_position_ids=position_ids,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "6594702faab3ce94d683a989454de174b3c85b15",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 52,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -31,20 +31,16 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from .configuration_hubert import HubertConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -438,9 +434,10 @@ def forward(\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n@@ -482,26 +479,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class HubertAttnAdapterLayer(nn.Module):\n     def __init__(self, config):\n@@ -602,9 +579,10 @@ def forward(\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n@@ -648,26 +626,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n @auto_docstring\n class HubertPreTrainedModel(PreTrainedModel):"
        },
        {
            "sha": "e342d4f4af6cca9924fc0d27d73ab600949f9e6c",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 14,
            "deletions": 113,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -28,12 +28,8 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -46,14 +42,10 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from .configuration_informer import InformerConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -262,98 +254,6 @@ def _init_weights(self, module: nn.Module):\n         if isinstance(module, InformerSinusoidalPositionalEmbedding):\n             module._init_weight()\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-        past_key_values_length: int,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            # 2d mask is passed through the layers\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self.config._attn_implementation == \"sdpa\":\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n-            )\n-        elif self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_shape),\n-                        device=inputs_embeds.device,\n-                    )\n-                )\n-        else:\n-            # 4d mask is passed through the layers\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n def eager_attention_forward(\n     module: nn.Module,\n@@ -1159,17 +1059,18 @@ def forward(\n                 past_key_values_length, past_key_values_length + input_shape[1], device=inputs_embeds.device\n             )\n \n-        attention_mask = self._update_causal_mask(\n-            attention_mask,\n-            input_shape,\n-            inputs_embeds,\n-            past_key_values_length,\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         hidden_states = self.value_embedding(inputs_embeds)"
        },
        {
            "sha": "acdefe299b30fe7c380c3a5b854fc2c2173f2380",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 106,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -21,22 +21,14 @@\n from torch import nn\n \n from ...cache_utils import Cache, EncoderDecoderCache\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n-)\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import (\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-)\n+from ...utils import auto_docstring\n from ..bart.modeling_bart import BartAttention\n from ..time_series_transformer.modeling_time_series_transformer import (\n     TimeSeriesFeatureEmbedder,\n@@ -55,10 +47,6 @@\n from .configuration_informer import InformerConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n def nll(input: torch.distributions.Distribution, target: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n     Computes the negative log likelihood loss from input distribution with respect to target.\n@@ -102,98 +90,6 @@ def _init_weights(self, module: nn.Module):\n         if isinstance(module, InformerSinusoidalPositionalEmbedding):\n             module._init_weight()\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-        past_key_values_length: int,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            # 2d mask is passed through the layers\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self.config._attn_implementation == \"sdpa\":\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n-            )\n-        elif self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_shape),\n-                        device=inputs_embeds.device,\n-                    )\n-                )\n-        else:\n-            # 4d mask is passed through the layers\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class InformerAttention(BartAttention):\n     pass"
        },
        {
            "sha": "3272b7302efbaaaba69ee349fb294f10c96ebda2",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 18,
            "deletions": 217,
            "changes": 235,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -27,11 +27,7 @@\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import (\n     FlashAttentionKwargs,\n )\n@@ -44,20 +40,10 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TransformersKwargs,\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling, logging\n from .configuration_m2m_100 import M2M100Config\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -544,194 +530,6 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class M2M100Encoder(M2M100PreTrainedModel):\n     \"\"\"\n@@ -839,9 +637,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -1039,18 +838,20 @@ def forward(\n             else past_key_values\n         )\n \n-        attention_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n+\n         # embed positions\n         positions = self.embed_positions(input_ids, inputs_embeds, past_key_values_length)\n         positions = positions.to(inputs_embeds.device)"
        },
        {
            "sha": "a401c214a7a4b14c7a847c299a336c567b93c432",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 16,
            "deletions": 213,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -27,11 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -46,19 +42,12 @@\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_marian import MarianConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -484,194 +473,6 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class MarianEncoder(MarianPreTrainedModel):\n     \"\"\"\n@@ -773,9 +574,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -970,17 +772,18 @@ def forward(\n             else past_key_values\n         )\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions"
        },
        {
            "sha": "95697ab5f08898c77c2199eeb8c836d93a450e78",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 17,
            "deletions": 207,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -25,11 +25,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import (\n     FlashAttentionKwargs,\n )\n@@ -56,7 +52,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n+    pass\n \n \n logger = logging.get_logger(__name__)\n@@ -507,194 +503,6 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class MBartEncoder(MBartPreTrainedModel):\n     \"\"\"\n@@ -808,9 +616,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -1013,17 +822,18 @@ def forward(\n             else past_key_values\n         )\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions"
        },
        {
            "sha": "9104b24916325b53bb0f10034553ede4b89c6189",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -2005,6 +2005,11 @@ def forward(\n             token_type_ids = token_type_ids[:, :max_text_len]\n             text_token_mask = text_token_mask[:, :max_text_len]\n \n+        # 3D -> 4D correction (add head dim)\n+        # NOTE: we squeeze this later again as there is custom 3D logic in this model\n+        if text_self_attention_masks.ndim == 3:\n+            text_self_attention_masks = text_self_attention_masks[:, None, :, :]\n+\n         # Extract text features from text backbone\n         text_outputs = self.text_backbone(\n             input_ids, text_self_attention_masks, token_type_ids, position_ids, return_dict=return_dict\n@@ -2087,7 +2092,7 @@ def forward(\n                 text_features=text_features,\n                 text_attention_mask=~text_token_mask,\n                 text_position_embedding=None,\n-                text_self_attention_masks=~text_self_attention_masks,\n+                text_self_attention_masks=~text_self_attention_masks.squeeze(1),\n                 text_position_ids=position_ids,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,"
        },
        {
            "sha": "d08b70399da206ce36c2b0e866138594689ffc3c",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 30,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -30,7 +30,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -44,15 +44,11 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_mobilebert import MobileBertConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -647,9 +643,10 @@ def forward(\n             inputs_embeds=inputs_embeds,\n         )\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            embedding_output,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=embedding_output,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_outputs = self.encoder(\n@@ -665,27 +662,6 @@ def forward(\n             pooler_output=pooled_output,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "b0d60e10a9f2e883c2f4d70090ab26a90caac731",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 22,
            "deletions": 112,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -26,12 +26,7 @@\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -43,14 +38,11 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import OutputRecorder, can_return_tuple, check_model_inputs\n from .configuration_nllb_moe import NllbMoeConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -747,9 +739,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         for encoder_layer in self.layers:\n@@ -763,26 +756,6 @@ def forward(\n         last_hidden_state = self.layer_norm(hidden_states)\n         return MoEModelOutput(last_hidden_state=last_hidden_state)\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class NllbMoeDecoder(NllbMoePreTrainedModel):\n     \"\"\"\n@@ -859,17 +832,23 @@ def forward(\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        attention_mask = self._update_causal_mask(\n-            attention_mask,\n-            input_shape,\n-            inputs_embeds,\n-            past_key_values_length,\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + input_shape[1], device=inputs_embeds.device\n+            )\n+\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions\n@@ -906,75 +885,6 @@ def forward(\n             last_hidden_state=last_hidden_states, past_key_values=past_key_values\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-        past_key_values_length: int,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            # 2d mask is passed through the layers\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self.config._attn_implementation == \"sdpa\":\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n-            )\n-        elif self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_shape),\n-                        device=inputs_embeds.device,\n-                    )\n-                )\n-        else:\n-            # 4d mask is passed through the layers\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n-\n-        return attention_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring\n class NllbMoeModel(NllbMoePreTrainedModel):"
        },
        {
            "sha": "d17ca6acddf33ec67b59ab260059054c9262a949",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 16,
            "deletions": 211,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -27,11 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -46,17 +42,12 @@\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n )\n from .configuration_pegasus import PegasusConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -463,194 +454,6 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class PegasusEncoder(PegasusPreTrainedModel):\n     \"\"\"\n@@ -786,9 +589,10 @@ def forward(\n \n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -1020,17 +824,18 @@ def forward(\n             else past_key_values\n         )\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions"
        },
        {
            "sha": "d76759e9104cdb84201c7b9033e0d7223d888271",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 13,
            "deletions": 214,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -27,11 +27,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -42,20 +38,10 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TransformersKwargs,\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling, logging\n from .configuration_pegasus_x import PegasusXConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -773,194 +759,6 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class PegasusXEncoder(PegasusXPreTrainedModel):\n     \"\"\"\n@@ -1318,17 +1116,18 @@ def forward(\n             else past_key_values\n         )\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions"
        },
        {
            "sha": "042d5c0307b1fdd95d90e342392c26c6e562806d",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 17,
            "deletions": 217,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -30,11 +30,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -47,20 +43,10 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TransformersKwargs,\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling, logging\n from .configuration_plbart import PLBartConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -87,194 +73,6 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class PLBartLearnedPositionalEmbedding(nn.Embedding):\n     \"\"\"\n@@ -629,9 +427,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -938,17 +737,18 @@ def forward(\n             else past_key_values\n         )\n \n-        attention_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            self_attn_cache,\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=self_attn_cache,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions"
        },
        {
            "sha": "af8a9b44709ff441045ed5373360867e7b73be03",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 198,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -23,18 +23,13 @@\n \n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     Seq2SeqLMOutput,\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, is_torch_flex_attn_available\n+from ...utils import auto_docstring\n from ..bart.modeling_bart import (\n     BartClassificationHead,\n     BartDecoder,\n@@ -47,10 +42,6 @@\n from .configuration_plbart import PLBartConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import BlockMask, make_flex_block_causal_mask\n-\n-\n class PLBartScaledWordEmbedding(BartScaledWordEmbedding):\n     pass\n \n@@ -65,194 +56,6 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Optional[Union[torch.Tensor, \"BlockMask\"]],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-    ):\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_tensor.shape[0], input_tensor.shape[1]),\n-                        device=attention_mask.device,\n-                    )\n-                )\n-            return attention_mask\n-\n-        if \"flash\" in self.config._attn_implementation:\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class PLBartEncoder(BartEncoder):\n     pass"
        },
        {
            "sha": "c041e4732168b5e525c2fb829cf8179eee474e99",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 24,
            "deletions": 100,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -30,8 +30,7 @@\n from ...activations import ACT2FN, gelu\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -46,15 +45,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_roberta import RobertaConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -633,12 +628,11 @@ def forward(\n \n         if input_ids is not None:\n             device = input_ids.device\n-            input_shape = input_ids.shape\n+            seq_length = input_ids.shape[1]\n         else:\n             device = inputs_embeds.device\n-            input_shape = inputs_embeds.shape[:-1]\n+            seq_length = inputs_embeds.shape[1]\n \n-        seq_length = input_shape[1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:\n             cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n@@ -652,7 +646,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -683,107 +676,38 @@ def forward(\n \n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "29d3aa21c94e087e53a6fd115ecc0a21608a0e61",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 22,
            "deletions": 99,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -25,8 +25,7 @@\n from ...activations import ACT2FN, gelu\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -41,15 +40,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_roberta_prelayernorm import RobertaPreLayerNormConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -676,7 +671,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -709,109 +703,38 @@ def forward(\n     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "507de4ff101071a4bbd5be7e521eeab5c728685b",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 22,
            "deletions": 99,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -24,8 +24,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -40,15 +39,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_roc_bert import RoCBertConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -762,7 +757,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -794,109 +788,38 @@ def forward(\n     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "e2146ab444d5758830f624ede8f23d9f2a7272d1",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 21,
            "deletions": 116,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -25,12 +25,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -44,16 +39,11 @@\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n-    is_torch_flex_attn_available,\n     logging,\n )\n from .configuration_speech_to_text import Speech2TextConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -634,9 +624,10 @@ def forward(\n         hidden_states = inputs_embeds + embed_pos\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -676,27 +667,6 @@ def forward(\n             last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class Speech2TextDecoder(Speech2TextPreTrainedModel):\n     \"\"\"\n@@ -829,17 +799,23 @@ def forward(\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        attention_mask = self._update_causal_mask(\n-            attention_mask,\n-            input_shape,\n-            inputs_embeds,\n-            past_key_values_length,\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length, past_key_values_length + input_shape[1], device=inputs_embeds.device\n+            )\n+\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         # embed positions\n@@ -899,77 +875,6 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-        past_key_values_length: int,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            # 2d mask is passed through the layers\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self.config._attn_implementation == \"sdpa\":\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n-            )\n-        elif self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_shape),\n-                        device=inputs_embeds.device,\n-                    )\n-                )\n-        else:\n-            # 4d mask is passed through the layers\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring\n class Speech2TextModel(Speech2TextPreTrainedModel):"
        },
        {
            "sha": "f6b31e9642598d8ad19587c2bce181d9c039f487",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 17,
            "deletions": 116,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -24,12 +24,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -42,14 +37,10 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from .configuration_time_series_transformer import TimeSeriesTransformerConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -636,98 +627,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-        past_key_values_length: int,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            # 2d mask is passed through the layers\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif self.config._attn_implementation == \"sdpa\":\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask,\n-                input_shape,\n-                inputs_embeds,\n-                past_key_values_length,\n-            )\n-        elif self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            # Other attention flavors support in-built causal (when `mask is None`)\n-            # while we need to create our specific block mask regardless\n-            elif attention_mask is None:\n-                attention_mask = make_flex_block_causal_mask(\n-                    torch.ones(\n-                        size=(input_shape),\n-                        device=inputs_embeds.device,\n-                    )\n-                )\n-        else:\n-            # 4d mask is passed through the layers\n-            attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class TimeSeriesTransformerEncoder(TimeSeriesTransformerPreTrainedModel):\n     \"\"\"\n@@ -799,9 +698,10 @@ def forward(\n         hidden_states = self.layernorm_embedding(hidden_states + embed_pos)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            inputs_embeds,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n         )\n \n         encoder_states = () if output_hidden_states else None\n@@ -946,17 +846,18 @@ def forward(\n                 past_key_values_length, past_key_values_length + input_shape[1], device=inputs_embeds.device\n             )\n \n-        attention_mask = self._update_causal_mask(\n-            attention_mask,\n-            input_shape,\n-            inputs_embeds,\n-            past_key_values_length,\n+        attention_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n-        encoder_attention_mask = self._update_cross_attn_mask(\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-            input_shape,\n-            inputs_embeds,\n+        encoder_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=encoder_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n         )\n \n         hidden_states = self.value_embedding(inputs_embeds)"
        },
        {
            "sha": "cdb935fb57aad47f8780e02e23a823ac7fffc4c7",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 10,
            "deletions": 52,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -33,7 +33,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -45,14 +45,10 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from .configuration_unispeech import UniSpeechConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -470,9 +466,10 @@ def forward(\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n@@ -514,26 +511,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class UniSpeechAttnAdapterLayer(nn.Module):\n     def __init__(self, config):\n@@ -634,9 +611,10 @@ def forward(\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n@@ -680,26 +658,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class UniSpeechGumbelVectorQuantizer(nn.Module):\n     \"\"\""
        },
        {
            "sha": "87825ae405bb6c295fe040cf5b069489dc9da83d",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 10,
            "deletions": 52,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -33,7 +33,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -47,14 +47,10 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, is_peft_available, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_peft_available, logging\n from .configuration_unispeech_sat import UniSpeechSatConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -475,9 +471,10 @@ def forward(\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n@@ -519,26 +516,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class UniSpeechSatAttnAdapterLayer(nn.Module):\n     def __init__(self, config):\n@@ -639,9 +616,10 @@ def forward(\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n@@ -685,26 +663,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class UniSpeechSatGumbelVectorQuantizer(nn.Module):\n     \"\"\""
        },
        {
            "sha": "da25e36663d0afdc49574b1f2bd5866afdfa1ae4",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 56,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -29,10 +29,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -53,7 +50,6 @@\n     cached_file,\n     check_torch_load_is_safe,\n     is_peft_available,\n-    is_torch_flex_attn_available,\n     logging,\n )\n from .configuration_wav2vec2 import Wav2Vec2Config\n@@ -62,9 +58,6 @@\n WAV2VEC2_ADAPTER_PT_FILE = \"adapter.{}.bin\"\n WAV2VEC2_ADAPTER_SAFE_FILE = \"adapter.{}.safetensors\"\n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n \n logger = logging.get_logger(__name__)\n \n@@ -704,9 +697,10 @@ def forward(\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n@@ -748,27 +742,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class Wav2Vec2EncoderStableLayerNorm(nn.Module):\n     def __init__(self, config):\n@@ -798,9 +771,10 @@ def forward(\n             expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n             hidden_states[~expand_attention_mask] = 0\n \n-        attention_mask = self._update_full_mask(\n-            attention_mask,\n-            hidden_states,\n+        attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n         )\n \n         position_embeddings = self.pos_conv_embed(hidden_states)\n@@ -844,27 +818,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n \n class Wav2Vec2GumbelVectorQuantizer(nn.Module):\n     \"\"\""
        },
        {
            "sha": "752d52fd0d47a49ed5e8128ed0a325be5310e31f",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 24,
            "deletions": 100,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -30,8 +30,7 @@\n from ...activations import ACT2FN, gelu\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -46,15 +45,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_xlm_roberta import XLMRobertaConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -652,12 +647,11 @@ def forward(\n \n         if input_ids is not None:\n             device = input_ids.device\n-            input_shape = input_ids.shape\n+            seq_length = input_ids.shape[1]\n         else:\n             device = inputs_embeds.device\n-            input_shape = inputs_embeds.shape[:-1]\n+            seq_length = inputs_embeds.shape[1]\n \n-        seq_length = input_shape[1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:\n             cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n@@ -671,7 +665,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -702,107 +695,38 @@ def forward(\n \n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "e4d226c8a1475a1db47e409bbdb6cd2aeb50197f",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 24,
            "deletions": 100,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -35,8 +35,7 @@\n from ...activations import ACT2FN, gelu\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -51,15 +50,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_xlm_roberta_xl import XLMRobertaXLConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -639,12 +634,11 @@ def forward(\n \n         if input_ids is not None:\n             device = input_ids.device\n-            input_shape = input_ids.shape\n+            seq_length = input_ids.shape[1]\n         else:\n             device = inputs_embeds.device\n-            input_shape = inputs_embeds.shape[:-1]\n+            seq_length = inputs_embeds.shape[1]\n \n-        seq_length = input_shape[1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:\n             cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n@@ -658,7 +652,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -689,107 +682,38 @@ def forward(\n \n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n class XLMRobertaXLLMHead(nn.Module):\n     \"\"\"XLM-RoBERTa-XL Head for masked language modeling.\"\"\""
        },
        {
            "sha": "6711695e1eab409b15ed1ee90ccc086909e7b333",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 22,
            "deletions": 99,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -24,8 +24,7 @@\n from ...activations import ACT2FN, gelu\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...masking_utils import create_causal_mask\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_bidirectional_mask, create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -40,15 +39,11 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_xmod import XmodConfig\n \n \n-if is_torch_flex_attn_available():\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -782,7 +777,6 @@ def forward(\n         )\n \n         attention_mask, encoder_attention_mask = self._create_attention_masks(\n-            input_shape=input_shape,\n             attention_mask=attention_mask,\n             encoder_attention_mask=encoder_attention_mask,\n             embedding_output=embedding_output,\n@@ -815,109 +809,38 @@ def forward(\n     # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n     def _create_attention_masks(\n         self,\n-        input_shape,\n         attention_mask,\n         encoder_attention_mask,\n         embedding_output,\n         encoder_hidden_states,\n         cache_position,\n         past_key_values,\n     ):\n-        if attention_mask is not None and attention_mask.dim() == 2:\n-            if self.config.is_decoder:\n-                attention_mask = create_causal_mask(\n-                    config=self.config,\n-                    input_embeds=embedding_output,\n-                    attention_mask=attention_mask,\n-                    cache_position=cache_position,\n-                    past_key_values=past_key_values,\n-                )\n-            else:\n-                attention_mask = self._update_full_mask(\n-                    attention_mask,\n-                    embedding_output,\n-                )\n-        elif attention_mask is not None and attention_mask.dim() == 3:\n-            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                raise ValueError(\n-                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                )\n-            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config.is_decoder:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=past_key_values,\n+            )\n+        else:\n+            attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=attention_mask,\n+            )\n \n         if encoder_attention_mask is not None:\n-            if encoder_attention_mask.dim() == 2:\n-                encoder_attention_mask = self._update_cross_attn_mask(\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    embedding_output.shape[:2],\n-                    embedding_output,\n-                )\n-            else:\n-                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n-                    raise ValueError(\n-                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n-                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n-                    )\n-                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+            encoder_attention_mask = create_bidirectional_mask(\n+                config=self.config,\n+                input_embeds=embedding_output,\n+                attention_mask=encoder_attention_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n \n         return attention_mask, encoder_attention_mask\n \n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n-    def _update_full_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, None],\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        if attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                attention_mask = attention_mask if 0 in attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(attention_mask, torch.Tensor):\n-                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n-\n-        return attention_mask\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n-    def _update_cross_attn_mask(\n-        self,\n-        encoder_hidden_states: Union[torch.Tensor, None],\n-        encoder_attention_mask: Union[torch.Tensor, None],\n-        input_shape: torch.Size,\n-        inputs_embeds: torch.Tensor,\n-    ):\n-        # expand encoder attention mask\n-        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if \"flash\" in self.config._attn_implementation:\n-                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n-            elif self.config._attn_implementation == \"sdpa\":\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask,\n-                    inputs_embeds.dtype,\n-                    tgt_len=input_shape[-1],\n-                )\n-            elif self.config._attn_implementation == \"flex_attention\":\n-                if isinstance(encoder_attention_mask, torch.Tensor):\n-                    encoder_attention_mask = make_flex_block_causal_mask(\n-                        encoder_attention_mask,\n-                        query_length=input_shape[-1],\n-                        is_causal=False,\n-                    )\n-            else:\n-                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-                encoder_attention_mask = _prepare_4d_attention_mask(\n-                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n-                )\n-\n-        return encoder_attention_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "b83f9df137db087c7e5714c4819d64ebcc050f6a",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -337,6 +337,8 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n+        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n+\n         distilbert_model = \"albert/albert-base-v2\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -363,13 +365,15 @@ def test_export(self):\n             [\"capital\", \"capitol\", \"comune\", \"arrondissement\", \"bastille\"],\n         )\n \n-        exported_program = torch.export.export(\n-            model,\n-            args=(inputs[\"input_ids\"],),\n-            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n+        exported_program = exportable_module.export(\n+            input_ids=inputs[\"input_ids\"],\n+            attention_mask=inputs[\"attention_mask\"],\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        result = exported_program.module().forward(\n+            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n+        )\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
        },
        {
            "sha": "fe336cb89275a1c48e35e310cedec9208b35d0ac",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 43,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -496,14 +496,6 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_model_3d_mask_shapes(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        # manipulate input_mask\n-        config_and_inputs = list(config_and_inputs)\n-        batch_size, seq_length = config_and_inputs[3].shape\n-        config_and_inputs[3] = random_attention_mask([batch_size, seq_length, seq_length])\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n     def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n@@ -535,36 +527,6 @@ def test_model_as_decoder_with_default_input_mask(self):\n             encoder_attention_mask,\n         )\n \n-    def test_model_as_decoder_with_3d_input_mask(self):\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        ) = self.model_tester.prepare_config_and_inputs_for_decoder()\n-\n-        batch_size, seq_length = input_mask.shape\n-        input_mask = random_attention_mask([batch_size, seq_length, seq_length])\n-        batch_size, seq_length = encoder_attention_mask.shape\n-        encoder_attention_mask = random_attention_mask([batch_size, seq_length, seq_length])\n-\n-        self.model_tester.create_and_check_model_as_decoder(\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n     def test_for_causal_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_for_causal_lm(*config_and_inputs)\n@@ -747,6 +709,8 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n+        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n+\n         bert_model = \"google-bert/bert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -771,13 +735,15 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"barber\", \"mechanic\", \"salesman\"])\n \n-        exported_program = torch.export.export(\n-            model,\n-            args=(inputs[\"input_ids\"],),\n-            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n+        exported_program = exportable_module.export(\n+            input_ids=inputs[\"input_ids\"],\n+            attention_mask=inputs[\"attention_mask\"],\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        result = exported_program.module().forward(\n+            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n+        )\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
        },
        {
            "sha": "39dd2ee81617bfb5fac1ab1575d2cf4ec5cacd83",
            "filename": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -224,8 +224,7 @@ class BlenderbotModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = True\n-\n+    fx_compatible = False\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "ad18e8714aa44a0f9f3ee5791ae5b81c43c1a372",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -216,8 +216,7 @@ class BlenderbotSmallModelTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = True\n-\n+    fx_compatible = False\n     test_missing_keys = False\n \n     # TODO: Fix the failed tests when this model gets more usage"
        },
        {
            "sha": "4026836dbdf6602ab6d9d842ad778ab9b3cc887b",
            "filename": "tests/models/distilbert/test_modeling_distilbert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -404,6 +404,8 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n+        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n+\n         distilbert_model = \"distilbert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -430,13 +432,15 @@ def test_export(self):\n             [\"capital\", \"birthplace\", \"northernmost\", \"centre\", \"southernmost\"],\n         )\n \n-        exported_program = torch.export.export(\n-            model,\n-            args=(inputs[\"input_ids\"],),\n-            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n+        exported_program = exportable_module.export(\n+            input_ids=inputs[\"input_ids\"],\n+            attention_mask=inputs[\"attention_mask\"],\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        result = exported_program.module().forward(\n+            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n+        )\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
        },
        {
            "sha": "e4c257d721440d9a80064f283bc7313b44fc1607",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -237,8 +237,7 @@ class M2M100ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = True\n-\n+    fx_compatible = False\n     test_missing_keys = False\n \n     # TODO: Fix the failed tests"
        },
        {
            "sha": "5d4b92a9805e089e5334832351ac3c2c1cbdc605",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -229,8 +229,7 @@ class MarianModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = True\n-\n+    fx_compatible = False\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "3bd1b520fd18922938796f5e3efc6ebb6f404980",
            "filename": "tests/models/mobilebert/test_modeling_mobilebert.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -395,6 +395,8 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n+        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n+\n         mobilebert_model = \"google/mobilebert-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"eager\"\n@@ -418,13 +420,15 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"mechanic\", \"teacher\", \"clerk\"])\n \n-        exported_program = torch.export.export(\n-            model,\n-            args=(inputs[\"input_ids\"],),\n-            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n+        exported_program = exportable_module.export(\n+            input_ids=inputs[\"input_ids\"],\n+            attention_mask=inputs[\"attention_mask\"],\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        result = exported_program.module().forward(\n+            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n+        )\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
        },
        {
            "sha": "09e76aa7dd34e35744fcfdebff7522a6f449f0c6",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -234,7 +234,7 @@ class PegasusModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = True\n+    fx_compatible = False\n     test_resize_position_embeddings = True\n \n     test_missing_keys = False"
        },
        {
            "sha": "8ba0af2c4dfe6e289cff1881877a34bca22aa236",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -691,6 +691,8 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n+        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n+\n         roberta_model = \"FacebookAI/roberta-base\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -715,13 +717,15 @@ def test_export(self):\n         eager_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask.split(), [\"happiness\", \"love\", \"peace\", \"freedom\", \"simplicity\"])\n \n-        exported_program = torch.export.export(\n-            model,\n-            args=(inputs[\"input_ids\"],),\n-            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n+        exported_program = exportable_module.export(\n+            input_ids=inputs[\"input_ids\"],\n+            attention_mask=inputs[\"attention_mask\"],\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        result = exported_program.module().forward(\n+            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n+        )\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
        },
        {
            "sha": "6640f5ae6ec53b2a36f05c24214af4bb511a3bda",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -265,8 +265,7 @@ class Speech2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTest\n         else {}\n     )\n     is_encoder_decoder = True\n-    fx_compatible = True\n-\n+    fx_compatible = False\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "1c4548d6578789e027c6d6a9b415c7850db3e525",
            "filename": "tests/utils/test_masking_utils.py",
            "status": "modified",
            "additions": 88,
            "deletions": 2,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Futils%2Ftest_masking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34b861abd11074fd32362b9a25c1cc582fa0b941/tests%2Futils%2Ftest_masking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_masking_utils.py?ref=34b861abd11074fd32362b9a25c1cc582fa0b941",
            "patch": "@@ -14,7 +14,13 @@\n \n import unittest\n \n-from transformers.testing_utils import is_torch_available, require_torch\n+from transformers.testing_utils import (\n+    cleanup,\n+    is_torch_available,\n+    require_torch,\n+    require_torch_accelerator,\n+    torch_device,\n+)\n \n \n if is_torch_available():\n@@ -23,7 +29,12 @@\n \n     from transformers import DynamicCache, LlamaConfig\n     from transformers.cache_utils import DynamicSlidingWindowLayer\n-    from transformers.masking_utils import create_causal_mask, create_chunked_causal_mask, find_packed_sequence_indices\n+    from transformers.masking_utils import (\n+        create_bidirectional_mask,\n+        create_causal_mask,\n+        create_chunked_causal_mask,\n+        find_packed_sequence_indices,\n+    )\n \n \n # fmt: off\n@@ -56,6 +67,12 @@\n \n @require_torch\n class MaskTest(unittest.TestCase):\n+    def setup(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def test_packed_sequence_mask_sdpa(self):\n         config = LlamaConfig()\n         config._attn_implementation = \"sdpa\"\n@@ -244,3 +261,72 @@ def test_chunked_mask_with_left_padding_decoding(self):\n         # fmt: on\n \n         self.assertTrue((chunked_attention_mask == EXPECTED_CHUNKED_MASK).all())\n+\n+    @require_torch_accelerator\n+    def test_bidirectional_mask_cudagraphs(self):\n+        \"\"\"\n+        Checks whether the bidirectional mask creation is compatible with cuda graphs, i.e. we do not into any error\n+        during this test.\n+        \"\"\"\n+\n+        def run_mask_creation(mask_fn, config, input_embeds, encoder_mask, cross_mask, encoder_hidden_states):\n+            _ = mask_fn(\n+                config=config,\n+                input_embeds=input_embeds,\n+                attention_mask=encoder_mask,\n+            )\n+\n+            _ = mask_fn(\n+                config=config,\n+                input_embeds=input_embeds,\n+                attention_mask=cross_mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+            )\n+\n+        # We use llama but could be also bert/bart --> we only need the `_attn_implementation` here\n+        config = LlamaConfig()\n+        config._attn_implementation = \"sdpa\"\n+\n+        # Meta data\n+        batch_size = 2\n+        q_length = 10\n+        kv_length = 5\n+\n+        input_embeds = torch.ones((batch_size, q_length, 1), device=torch_device, dtype=torch.float16)\n+        encoder_hidden_states = torch.ones((batch_size, kv_length, 1), device=torch_device, dtype=torch.float16)\n+\n+        encoder_mask = torch.ones_like(input_embeds)[..., 0]\n+        cross_mask = torch.ones_like(encoder_hidden_states)[..., 0]\n+\n+        mask_creation_function = torch.compile(create_bidirectional_mask, mode=\"reduce-overhead\")\n+\n+        # Case 1: Full mask\n+        run_mask_creation(\n+            mask_fn=mask_creation_function,\n+            config=config,\n+            input_embeds=input_embeds,\n+            encoder_mask=encoder_mask,\n+            cross_mask=cross_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+        )\n+        run_mask_creation(\n+            mask_fn=mask_creation_function,\n+            config=config,\n+            input_embeds=input_embeds,\n+            encoder_mask=None,\n+            cross_mask=None,\n+            encoder_hidden_states=encoder_hidden_states,\n+        )\n+\n+        # Case 2: Padding involved\n+        cross_mask[:, -1] = 0\n+        encoder_mask[:, -1] = 0\n+\n+        run_mask_creation(\n+            mask_fn=mask_creation_function,\n+            config=config,\n+            input_embeds=input_embeds,\n+            encoder_mask=encoder_mask,\n+            cross_mask=cross_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+        )"
        }
    ],
    "stats": {
        "total": 6426,
        "additions": 1152,
        "deletions": 5274
    }
}