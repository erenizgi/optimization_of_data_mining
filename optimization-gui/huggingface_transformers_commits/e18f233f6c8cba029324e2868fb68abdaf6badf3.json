{
    "author": "cyan-channel-io",
    "message": "Fix default attention mask of generate in MoshiForConditionalGeneration (#36171)",
    "sha": "e18f233f6c8cba029324e2868fb68abdaf6badf3",
    "files": [
        {
            "sha": "d1f4f8a9cab356b9d92a57b6b66f118e00b0c3bd",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 33,
            "deletions": 2,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/e18f233f6c8cba029324e2868fb68abdaf6badf3/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e18f233f6c8cba029324e2868fb68abdaf6badf3/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=e18f233f6c8cba029324e2868fb68abdaf6badf3",
            "patch": "@@ -2099,6 +2099,31 @@ def forward(\n             depth_attentions=None if decoder_outputs is None else decoder_outputs.attentions,\n         )\n \n+    def _prepare_attention_mask_for_generation(\n+        self,\n+        input_ids: torch.LongTensor,\n+        generation_config: GenerationConfig,\n+        kwargs: Dict[str, Any],\n+    ) -> torch.LongTensor:\n+        pad_token_id = generation_config.pad_token_id\n+        eos_token_id = generation_config.eos_token_id\n+\n+        default_attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n+        if pad_token_id is None:\n+            return default_attention_mask\n+\n+        is_pad_token_in_inputs = (pad_token_id is not None) and torch.isin(input_ids, pad_token_id).any()\n+        is_pad_token_not_equal_to_eos_token_id = (eos_token_id is None) or ~torch.isin(\n+            eos_token_id, pad_token_id\n+        ).any()\n+        can_infer_attention_mask = is_pad_token_in_inputs * is_pad_token_not_equal_to_eos_token_id\n+        attention_mask_from_padding = input_ids.ne(pad_token_id).long()\n+\n+        attention_mask = (\n+            attention_mask_from_padding * can_infer_attention_mask + default_attention_mask * ~can_infer_attention_mask\n+        )\n+        return attention_mask\n+\n     def _prepare_inputs_embeds_for_generation(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2315,6 +2340,12 @@ def generate(\n         kwargs_depth_decoder = depth_decoder_generation_config\n \n         attention_mask = kwargs.pop(\"attention_mask\", None)\n+        if attention_mask is None:\n+            attention_mask = self._prepare_attention_mask_for_generation(\n+                input_ids=input_ids,\n+                generation_config=generation_config,\n+                kwargs=kwargs,\n+            )\n         (\n             inputs_embeds,\n             input_ids,\n@@ -2497,11 +2528,11 @@ def prepare_inputs_for_generation(\n                 batch_size, sequence_length = input_ids.shape\n                 device = input_ids.device\n \n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask = self.decoder.model._prepare_4d_causal_attention_mask_with_cache_position(\n                 attention_mask,\n                 sequence_length=sequence_length,\n                 target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n+                dtype=self.decoder.lm_head.weight.dtype,\n                 device=device,\n                 cache_position=cache_position,\n                 batch_size=batch_size,"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 33,
        "deletions": 2
    }
}