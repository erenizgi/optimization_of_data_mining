{
    "author": "sarathc-cerebras",
    "message": "adds jais2 model support (#42684)\n\n* adds jais2 model support\n\n* updates tests\n\n* addresses review comment\n\n* review comments addressed\n\n* addresses test review comments\n\n* fixes date\n\n* format issue fix\n\n* Update src/transformers/models/jais2/__init__.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update src/transformers/models/jais2/modular_jais2.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update tests/models/jais2/test_modeling_jais2.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update src/transformers/models/jais2/modular_jais2.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update src/transformers/models/jais2/modular_jais2.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update src/transformers/models/jais2/modular_jais2.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update src/transformers/models/jais2/modular_jais2.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update src/transformers/models/jais2/modular_jais2.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* fixes tests as per review comment\n\n* updates layernorm setup\n\n* Apply suggestions from code review\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* addressed review comments and updated tests as recomended\n\n* fixup tests\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\nCo-authored-by: vasqu <antonprogamer@gmail.com>",
    "sha": "0dbf808573cb3433992a9d4177222fdccd67b4ee",
    "files": [
        {
            "sha": "bb9305dfec9d4aea2f20f1b6d02941373937b8a8",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -551,6 +551,8 @@\n         title: HunYuanMoEV1\n       - local: model_doc/ibert\n         title: I-BERT\n+      - local: model_doc/jais2\n+        title: Jais2\n       - local: model_doc/jamba\n         title: Jamba\n       - local: model_doc/jetmoe"
        },
        {
            "sha": "23f9504ec9cf7ce87765ddc68d5fb148736b579e",
            "filename": "docs/source/en/model_doc/jais2.md",
            "status": "added",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/docs%2Fsource%2Fen%2Fmodel_doc%2Fjais2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/docs%2Fsource%2Fen%2Fmodel_doc%2Fjais2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjais2.md?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -0,0 +1,40 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on 2025-12-09 and added to Hugging Face Transformers on 2025-12-16.*\n+\n+# Jais2\n+\n+## Overview\n+\n+Jais2 a next-generation Arabic open-weight LLM trained on the richest Arabic-first dataset to date. Built from the ground up with 8B and 70B parameters, Jais 2 understands Arabic the way it's truly spoken across dialects, cuulutre, and modern expression. It is developed by MBZUAI, Inception and Cerebras Systems and based on the transformer architecture with modifications including:\n+\n+- LayerNorm instead of RMSNorm\n+- ReLUÂ² activation function\n+- Rotary Position Embeddings (RoPE)\n+\n+## Jais2Config\n+\n+[[autodoc]] Jais2Config\n+\n+## Jais2Model\n+\n+[[autodoc]] Jais2Model\n+    - forward\n+\n+## Jais2ForCausalLM\n+\n+[[autodoc]] Jais2ForCausalLM\n+    - forward"
        },
        {
            "sha": "4f8e61b3bcebab35bd536b7722b74b82ce07111e",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -215,6 +215,7 @@\n         (\"instructblipvideo\", \"InstructBlipVideoConfig\"),\n         (\"internvl\", \"InternVLConfig\"),\n         (\"internvl_vision\", \"InternVLVisionConfig\"),\n+        (\"jais2\", \"Jais2Config\"),\n         (\"jamba\", \"JambaConfig\"),\n         (\"janus\", \"JanusConfig\"),\n         (\"jetmoe\", \"JetMoeConfig\"),\n@@ -665,6 +666,7 @@\n         (\"instructblipvideo\", \"InstructBlipVideo\"),\n         (\"internvl\", \"InternVL\"),\n         (\"internvl_vision\", \"InternVLVision\"),\n+        (\"jais2\", \"Jais2\"),\n         (\"jamba\", \"Jamba\"),\n         (\"janus\", \"Janus\"),\n         (\"jetmoe\", \"JetMoe\"),"
        },
        {
            "sha": "ce30248e5698dcdb229519c80f65dcdc2186b25c",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -216,6 +216,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"instructblipvideo\", \"InstructBlipVideoModel\"),\n         (\"internvl\", \"InternVLModel\"),\n         (\"internvl_vision\", \"InternVLVisionModel\"),\n+        (\"jais2\", \"Jais2Model\"),\n         (\"jamba\", \"JambaModel\"),\n         (\"janus\", \"JanusModel\"),\n         (\"jetmoe\", \"JetMoeModel\"),\n@@ -695,6 +696,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"helium\", \"HeliumForCausalLM\"),\n         (\"hunyuan_v1_dense\", \"HunYuanDenseV1ForCausalLM\"),\n         (\"hunyuan_v1_moe\", \"HunYuanMoEV1ForCausalLM\"),\n+        (\"jais2\", \"Jais2ForCausalLM\"),\n         (\"jamba\", \"JambaForCausalLM\"),\n         (\"jetmoe\", \"JetMoeForCausalLM\"),\n         (\"lfm2\", \"Lfm2ForCausalLM\"),"
        },
        {
            "sha": "a2f9ebd67a3944ecd8384f1f7df0349d7ab9e636",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -178,6 +178,7 @@\n         (\"instructblip\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"instructblipvideo\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"internvl\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None),\n+        (\"jais2\", \"GPT2Tokenizer\" if is_tokenizers_available() else None),\n         (\"jamba\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"janus\", \"LlamaTokenizer\" if is_tokenizers_available() else None),\n         (\"jetmoe\", \"LlamaTokenizer\" if is_tokenizers_available() else None),"
        },
        {
            "sha": "137ee4e32e1425730fe8d6346776ebbf64719a0f",
            "filename": "src/transformers/models/jais2/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fjais2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fjais2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjais2%2F__init__.py?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_jais2 import *\n+    from .modeling_jais2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "257520a4d0d25927f7bfd0b9db6b0b8c4588674b",
            "filename": "src/transformers/models/jais2/configuration_jais2.py",
            "status": "added",
            "additions": 152,
            "deletions": 0,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fjais2%2Fconfiguration_jais2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fjais2%2Fconfiguration_jais2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjais2%2Fconfiguration_jais2.py?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -0,0 +1,152 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/jais2/modular_jais2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_jais2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ...modeling_rope_utils import RopeParameters\n+\n+\n+class Jais2Config(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Jais2Model`]. It is used to instantiate a Jais2\n+    model according to the specified arguments, defining the model architecture.\n+    [inceptionai/Jais-2-8B-Chat](https://huggingface.co/inceptionai/Jais-2-8B-Chat).\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 150272):\n+            Vocabulary size of the Jais2 model.\n+        hidden_size (`int`, *optional*, defaults to 3328):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 26624):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 26):\n+            Number of attention heads for each attention layer.\n+        num_key_value_heads (`int`, *optional*):\n+            Number of key_value heads for Grouped Query Attention.\n+        hidden_act (`str`, *optional*, defaults to `\"relu2\"`):\n+            The non-linear activation function in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether to return last key/values attentions.\n+        pad_token_id (`int`, *optional*):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 0):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 150024):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings.\n+        attention_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use a bias in the query, key, value and output projection layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers.\n+        head_dim (`int`, *optional*):\n+            The attention head dimension.\n+        rope_parameters (`dict`, *optional*):\n+            The RoPE parameters.\n+    \"\"\"\n+\n+    model_type = \"jais2\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size: Optional[int] = 150272,\n+        hidden_size: Optional[int] = 3328,\n+        intermediate_size: Optional[int] = 26624,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 26,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"relu2\",\n+        max_position_embeddings: Optional[int] = 8192,\n+        initializer_range: Optional[float] = 0.02,\n+        layer_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 150024,\n+        tie_word_embeddings: Optional[bool] = False,\n+        attention_bias: Optional[bool] = True,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = True,\n+        head_dim: Optional[int] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.use_cache = use_cache\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        self.rope_parameters = rope_parameters\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.layer_norm_eps = layer_norm_eps\n+\n+\n+__all__ = [\"Jais2Config\"]"
        },
        {
            "sha": "6ccfbfb3c97e81cd56f74d979d6dd609a78f1741",
            "filename": "src/transformers/models/jais2/modeling_jais2.py",
            "status": "added",
            "additions": 486,
            "deletions": 0,
            "changes": 486,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fjais2%2Fmodeling_jais2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fjais2%2Fmodeling_jais2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjais2%2Fmodeling_jais2.py?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -0,0 +1,486 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/jais2/modular_jais2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_jais2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub, use_kernelized_func\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs, maybe_autocast\n+from .configuration_jais2 import Jais2Config\n+\n+\n+class Jais2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        return self.down_proj(self.act_fn(self.up_proj(x)))\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+@use_kernelized_func(apply_rotary_pos_emb)\n+class Jais2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Jais2Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Jais2DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Jais2Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = Jais2Attention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = Jais2MLP(config)\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Jais2PreTrainedModel(PreTrainedModel):\n+    config: Jais2Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Jais2DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Jais2DecoderLayer,\n+        \"attentions\": Jais2Attention,\n+    }\n+\n+\n+class Jais2RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Jais2Config, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[Jais2Config] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with maybe_autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@auto_docstring\n+class Jais2Model(Jais2PreTrainedModel):\n+    def __init__(self, config: Jais2Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Jais2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.rotary_emb = Jais2RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = (\n+                torch.arange(inputs_embeds.shape[1], device=inputs_embeds.device) + past_seen_tokens\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class Jais2ForCausalLM(Jais2PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Jais2Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Jais2ForCausalLM\n+\n+        >>> model = Jais2ForCausalLM.from_pretrained(\"inceptionai/Jais-2-8B-Chat\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/Jais-2-8B-Chat\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"Jais2Model\", \"Jais2ForCausalLM\", \"Jais2PreTrainedModel\"]"
        },
        {
            "sha": "7cd4004c077290c7c1a039325efd9dc3207e36d9",
            "filename": "src/transformers/models/jais2/modular_jais2.py",
            "status": "added",
            "additions": 196,
            "deletions": 0,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fjais2%2Fmodular_jais2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/src%2Ftransformers%2Fmodels%2Fjais2%2Fmodular_jais2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjais2%2Fmodular_jais2.py?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -0,0 +1,196 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional\n+\n+import torch.nn as nn\n+\n+from ...modeling_rope_utils import RopeParameters\n+from ...utils import auto_docstring, can_return_tuple\n+from ..llama.configuration_llama import LlamaConfig\n+from ..llama.modeling_llama import (\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaModel,\n+    LlamaPreTrainedModel,\n+)\n+from ..nemotron.modeling_nemotron import NemotronMLP\n+\n+\n+class Jais2Config(LlamaConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Jais2Model`]. It is used to instantiate a Jais2\n+    model according to the specified arguments, defining the model architecture.\n+    [inceptionai/Jais-2-8B-Chat](https://huggingface.co/inceptionai/Jais-2-8B-Chat).\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 150272):\n+            Vocabulary size of the Jais2 model.\n+        hidden_size (`int`, *optional*, defaults to 3328):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 26624):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 26):\n+            Number of attention heads for each attention layer.\n+        num_key_value_heads (`int`, *optional*):\n+            Number of key_value heads for Grouped Query Attention.\n+        hidden_act (`str`, *optional*, defaults to `\"relu2\"`):\n+            The non-linear activation function in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 8192):\n+            The maximum sequence length.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether to return last key/values attentions.\n+        pad_token_id (`int`, *optional*):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 0):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 150024):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings.\n+        attention_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use a bias in the query, key, value and output projection layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers.\n+        head_dim (`int`, *optional*):\n+            The attention head dimension.\n+        rope_parameters (`dict`, *optional*):\n+            The RoPE parameters.\n+    \"\"\"\n+\n+    model_type = \"jais2\"\n+\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size: Optional[int] = 150272,\n+        hidden_size: Optional[int] = 3328,\n+        intermediate_size: Optional[int] = 26624,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_attention_heads: Optional[int] = 26,\n+        num_key_value_heads: Optional[int] = None,\n+        hidden_act: Optional[str] = \"relu2\",\n+        max_position_embeddings: Optional[int] = 8192,\n+        initializer_range: Optional[float] = 0.02,\n+        layer_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = 0,\n+        eos_token_id: Optional[int] = 150024,\n+        tie_word_embeddings: Optional[bool] = False,\n+        attention_bias: Optional[bool] = True,\n+        attention_dropout: Optional[float] = 0.0,\n+        mlp_bias: Optional[bool] = True,\n+        head_dim: Optional[int] = None,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            hidden_act=hidden_act,\n+            max_position_embeddings=max_position_embeddings,\n+            initializer_range=initializer_range,\n+            use_cache=use_cache,\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            attention_bias=attention_bias,\n+            attention_dropout=attention_dropout,\n+            mlp_bias=mlp_bias,\n+            head_dim=head_dim,\n+            rope_parameters=rope_parameters,\n+            **kwargs,\n+        )\n+        self.layer_norm_eps = layer_norm_eps\n+        del self.rms_norm_eps\n+        del self.pretraining_tp\n+\n+\n+class Jais2MLP(NemotronMLP):\n+    pass\n+\n+\n+class Jais2DecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: Jais2Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+\n+class Jais2PreTrainedModel(LlamaPreTrainedModel):\n+    pass\n+\n+\n+class Jais2Model(LlamaModel):\n+    def __init__(self, config: Jais2Config):\n+        super().__init__(config)\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+\n+class Jais2ForCausalLM(LlamaForCausalLM):\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(self, **super_kwargs):\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Jais2ForCausalLM\n+\n+        >>> model = Jais2ForCausalLM.from_pretrained(\"inceptionai/Jais-2-8B-Chat\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/Jais-2-8B-Chat\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        return super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"Jais2Config\",\n+    \"Jais2Model\",\n+    \"Jais2ForCausalLM\",\n+    \"Jais2PreTrainedModel\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/jais2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/tests%2Fmodels%2Fjais2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/tests%2Fmodels%2Fjais2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjais2%2F__init__.py?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee"
        },
        {
            "sha": "447272badf457d8f37bce253e6a4ef401fbf72cb",
            "filename": "tests/models/jais2/test_modeling_jais2.py",
            "status": "added",
            "additions": 127,
            "deletions": 0,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/0dbf808573cb3433992a9d4177222fdccd67b4ee/tests%2Fmodels%2Fjais2%2Ftest_modeling_jais2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0dbf808573cb3433992a9d4177222fdccd67b4ee/tests%2Fmodels%2Fjais2%2Ftest_modeling_jais2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjais2%2Ftest_modeling_jais2.py?ref=0dbf808573cb3433992a9d4177222fdccd67b4ee",
            "patch": "@@ -0,0 +1,127 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Jais2 model.\"\"\"\n+\n+import unittest\n+\n+from transformers import AutoTokenizer, is_torch_available\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Jais2Config,\n+        Jais2ForCausalLM,\n+        Jais2Model,\n+    )\n+\n+\n+class Jais2ModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = Jais2Config\n+        base_model_class = Jais2Model\n+        causal_lm_class = Jais2ForCausalLM\n+\n+    config_overrides = {\n+        \"hidden_act\": \"relu2\",\n+    }\n+\n+\n+@require_torch\n+class Jais2ModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = Jais2ModelTester\n+    all_model_classes = (\n+        (\n+            Jais2Model,\n+            Jais2ForCausalLM,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+\n+    all_generative_model_classes = (Jais2ForCausalLM,) if is_torch_available() else ()\n+\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Jais2Model,\n+            \"text-generation\": Jais2ForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+\n+@slow\n+@require_torch_accelerator\n+class Jais2IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @require_read_token\n+    def test_model_logits(self):\n+        model_id = \"inceptionai/Jais-2-8B-Chat\"\n+        dummy_input = torch.LongTensor([[0, 0, 0, 0, 0, 0, 1, 2, 3], [1, 1, 2, 3, 4, 5, 6, 7, 8]]).to(torch_device)\n+        attention_mask = dummy_input.ne(0).to(torch.long)\n+\n+        model = Jais2ForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n+\n+        with torch.no_grad():\n+            logits = model(dummy_input, attention_mask=attention_mask).logits\n+        logits = logits.float()\n+\n+        EXPECTED_LOGITS_BATCH0 = [-0.9795, -1.0957, -0.9644, -0.9570, -0.9648, -0.9595, -0.9668, -0.9688, -0.9688, -0.9644, -0.9609, -0.9707, -0.9629, -0.9736, -0.9712]  # fmt: skip\n+        EXPECTED_LOGITS_BATCH1 = [-1.5332, -1.6289, -1.5264, -1.5195, -1.5264, -1.5215, -1.5303, -1.5303, -1.5312, -1.5264, -1.5234, -1.5322, -1.5254, -1.5352, -1.5332]  # fmt: skip\n+\n+        torch.testing.assert_close(\n+            logits[0, -1, :15],\n+            torch.tensor(EXPECTED_LOGITS_BATCH0, device=torch_device),\n+            rtol=1e-3,\n+            atol=1e-3,\n+        )\n+        torch.testing.assert_close(\n+            logits[1, -1, :15],\n+            torch.tensor(EXPECTED_LOGITS_BATCH1, device=torch_device),\n+            rtol=1e-3,\n+            atol=1e-3,\n+        )\n+\n+    @require_read_token\n+    def test_model_generation(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"inceptionai/Jais-2-8B-Chat\")\n+        model = Jais2ForCausalLM.from_pretrained(\n+            \"inceptionai/Jais-2-8B-Chat\", torch_dtype=torch.float16, device_map=\"auto\"\n+        )\n+        input_text = \"Simply put, the theory of relativity states that\"\n+        model_inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+        model_inputs.pop(\"token_type_ids\", None)\n+\n+        generated_ids = model.generate(**model_inputs, max_new_tokens=32, do_sample=False)\n+        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+\n+        EXPECTED_TEXT = \"Simply put, the theory of relativity states that the laws of physics are the same for all non-accelerating observers, and that the speed of light in a vacuum is the same for all observers,\"  # fmt: skip\n+        self.assertEqual(generated_text, EXPECTED_TEXT)"
        }
    ],
    "stats": {
        "total": 1035,
        "additions": 1035,
        "deletions": 0
    }
}