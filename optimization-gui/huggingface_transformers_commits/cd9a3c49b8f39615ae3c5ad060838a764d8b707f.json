{
    "author": "fabxoe",
    "message": "ğŸŒ [i18n-KO] Translated `model_doc/dbrx.md` to Korean  (#33951)\n\n* docs: ko: model_doc/dbrx.md\r\n\r\n* feat: nmt draft\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: SeongWooChoi <46990061+nuatmochoi@users.noreply.github.com>\r\n\r\n* fix: resolve suggestions\r\n\r\n* fix: resolve suggestions\r\n\r\n---------\r\n\r\nCo-authored-by: SeongWooChoi <46990061+nuatmochoi@users.noreply.github.com>",
    "sha": "cd9a3c49b8f39615ae3c5ad060838a764d8b707f",
    "files": [
        {
            "sha": "be595fef2686972a9b9d3ec1e6fba302845eb731",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd9a3c49b8f39615ae3c5ad060838a764d8b707f/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd9a3c49b8f39615ae3c5ad060838a764d8b707f/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=cd9a3c49b8f39615ae3c5ad060838a764d8b707f",
            "patch": "@@ -362,6 +362,8 @@\n         title: (ë²ˆì—­ì¤‘) CPMANT\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) CTRL\n+      - local: model_doc/dbrx\n+        title: DBRX\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) DeBERTa\n       - local: in_translation"
        },
        {
            "sha": "4b33e95700691edd07f82c989c9c07e36aa0b419",
            "filename": "docs/source/ko/model_doc/dbrx.md",
            "status": "added",
            "additions": 125,
            "deletions": 0,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd9a3c49b8f39615ae3c5ad060838a764d8b707f/docs%2Fsource%2Fko%2Fmodel_doc%2Fdbrx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd9a3c49b8f39615ae3c5ad060838a764d8b707f/docs%2Fsource%2Fko%2Fmodel_doc%2Fdbrx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fdbrx.md?ref=cd9a3c49b8f39615ae3c5ad060838a764d8b707f",
            "patch": "@@ -0,0 +1,125 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# DBRX[[dbrx]]\n+\n+## ê°œìš”[[overview]]\n+\n+DBRXëŠ” [íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜](https://www.isattentionallyouneed.com/) ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë””ì½”ë” ì „ìš© LLM ëª¨ë¸ì…ë‹ˆë‹¤.\n+ì´ 132B ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì§„ *ì„¸ë°€í•œ* ì „ë¬¸ê°€ í˜¼í•©(MoE) ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ë©°, ì´ ì¤‘ 36B ë§¤ê°œë³€ìˆ˜ê°€ ì…ë ¥ë§ˆë‹¤ í™œì„±í™”ë©ë‹ˆë‹¤.\n+12T í† í°ì˜ í…ìŠ¤íŠ¸ì™€ ì½”ë“œ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+Mixtral-8x7Bì™€ Grok-1ê³¼ ê°™ì€ ë‹¤ë¥¸ ê³µê°œ MoE ëª¨ë¸ë“¤ê³¼ ë¹„êµí–ˆì„ ë•Œ, DBRXëŠ” ë” ë§ì€ ìˆ˜ì˜ ì‘ì€ ì „ë¬¸ê°€ë“¤ì„ ì‚¬ìš©í•˜ëŠ” ì„¸ë°€í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. DBRXëŠ” 16ê°œì˜ ì „ë¬¸ê°€ ì¤‘ 4ê°œë¥¼ ì„ íƒí•˜ëŠ” ë°˜ë©´, Mixtral-8x7Bì™€ Grok-1ì€ 8ê°œì˜ ì „ë¬¸ê°€ ì¤‘ 2ê°œë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n+\n+ì´ëŠ” 65ë°° ë” ë§ì€ ì „ë¬¸ê°€ ì¡°í•©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©°, ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ í’ˆì§ˆì´ í–¥ìƒë˜ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n+DBRXëŠ” íšŒì „ ìœ„ì¹˜ ì¸ì½”ë”©(RoPE), ê²Œì´íŠ¸ ì„ í˜• ìœ ë‹›(GLU), ê·¸ë£¹ ì¿¼ë¦¬ ì–´í…ì…˜(GQA)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+BPE ê¸°ë°˜ ëª¨ë¸ì´ë©° [tiktoken](https://github.com/openai/tiktoken) ì €ì¥ì†Œì— ì„¤ëª…ëœ GPT-4 í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+ì´ëŸ¬í•œ ì„ íƒë“¤ì€ ì² ì €í•œ í‰ê°€ì™€ ìŠ¤ì¼€ì¼ë§ ì‹¤í—˜ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë£¨ì–´ì¡ŒìŠµë‹ˆë‹¤.\n+\n+DBRXëŠ” ì‹ ì¤‘í•˜ê²Œ ì„ ë³„ëœ 12T í† í°ì˜ ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµë˜ì—ˆìœ¼ë©°, ìµœëŒ€ ë¬¸ë§¥ ê¸¸ì´ëŠ” 32K í† í°ì…ë‹ˆë‹¤.\n+ì´ ë°ì´í„°ëŠ” í† í° ëŒ€ë¹„ MPT ê³„ì—´ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ë³´ë‹¤ ìµœì†Œ 2ë°° ì´ìƒ ë” ì¢‹ì€ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.\n+ì´ ìƒˆë¡œìš´ ë°ì´í„°ì…‹ì€ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ Apache Sparkâ„¢ì™€ Databricks ë…¸íŠ¸ë¶, ê·¸ë¦¬ê³  ë°ì´í„° ê´€ë¦¬ì™€ ê±°ë²„ë„ŒìŠ¤ë¥¼ ìœ„í•œ Unity Catalogë¥¼ í¬í•¨í•œ Databricks ë„êµ¬ ì „ì²´ë¥¼ í™œìš©í•˜ì—¬ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.\n+ìš°ë¦¬ëŠ” ì‚¬ì „ í•™ìŠµì„ ìœ„í•´ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì„ ì‚¬ìš©í–ˆìœ¼ë©°, í•™ìŠµ ì¤‘ ë°ì´í„° ë¯¹ìŠ¤ë¥¼ ë³€ê²½í•˜ëŠ” ë°©ì‹ì´ ëª¨ë¸ í’ˆì§ˆì„ ìƒë‹¹íˆ ê°œì„ í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n+\n+\n+DBRX Instructì™€ DBRX Baseì— ëŒ€í•œ ë” ìì„¸í•œ ì •ë³´ëŠ” ì´ [ê¸°ìˆ  ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì´ ëª¨ë¸ì€ [eitan-turok](https://huggingface.co/eitanturok)ì™€ [abhi-db](https://huggingface.co/abhi-db)ê°€ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œëŠ” [ì´ê³³](https://github.com/databricks/dbrx-instruct)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆì§€ë§Œ, ìµœì‹  ë²„ì „ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© ì˜ˆ[[usage-examples]]\n+\n+`generate()` ë©”ì†Œë“œëŠ” DBRXë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í‘œì¤€ ì–´í…ì…˜ êµ¬í˜„, í”Œë˜ì‹œ ì–´í…ì…˜, PyTorchì˜ ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜(Scaled Dot-Product Attention)ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í›„ìì˜ ë‘ ì–´í…ì…˜ êµ¬í˜„ ë°©ì‹ì€ ì²˜ë¦¬ ì†ë„ë¥¼ í¬ê²Œ ë†’ì—¬ì¤ë‹ˆë‹¤.\n+\n+```python\n+from transformers import DbrxForCausalLM, AutoTokenizer\n+import torch\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOUR_HF_TOKEN\")\n+model = DbrxForCausalLM.from_pretrained(\n+    \"databricks/dbrx-instruct\",\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+    token=\"YOUR_HF_TOKEN\",\n+    )\n+\n+input_text = \"What does it take to build a great LLM?\"\n+messages = [{\"role\": \"user\", \"content\": input_text}]\n+input_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+\n+outputs = model.generate(**input_ids, max_new_tokens=200)\n+print(tokenizer.decode(outputs[0]))\n+```\n+\n+`pip install flash-attn`ë¥¼ í†µí•´ í”Œë˜ì‹œ ì–´í…ì…˜ì„ ì„¤ì¹˜í•˜ë©´, ë” ë¹ ë¥¸ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (í”Œë˜ì‹œ ì–´í…ì…˜ì— ëŒ€í•œ HuggingFace ë¬¸ì„œëŠ” [ì´ê³³](https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n+\n+\n+```python\n+from transformers import DbrxForCausalLM, AutoTokenizer\n+import torch\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOUR_HF_TOKEN\")\n+model = DbrxForCausalLM.from_pretrained(\n+    \"databricks/dbrx-instruct\",\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+    token=\"YOUR_HF_TOKEN\",\n+    attn_implementation=\"flash_attention_2\",\n+    )\n+\n+input_text = \"What does it take to build a great LLM?\"\n+messages = [{\"role\": \"user\", \"content\": input_text}]\n+input_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+\n+outputs = model.generate(**input_ids, max_new_tokens=200)\n+print(tokenizer.decode(outputs[0]))\n+```\n+\n+PyTorchì˜ ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜ì„ ì‚¬ìš©í•˜ì—¬ë„ ë” ë¹ ë¥¸ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (ìŠ¤ì¼€ì¼ëœ ë‚´ì  ì–´í…ì…˜ì— ëŒ€í•œ HuggingFace ë¬¸ì„œëŠ” [ì´ê³³](https://huggingface.co/docs/transformers/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)\n+\n+\n+```python\n+from transformers import DbrxForCausalLM, AutoTokenizer\n+import torch\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\", token=\"YOUR_HF_TOKEN\")\n+model = DbrxForCausalLM.from_pretrained(\n+    \"databricks/dbrx-instruct\",\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+    token=\"YOUR_HF_TOKEN\",\n+    attn_implementation=\"sdpa\",\n+    )\n+\n+input_text = \"What does it take to build a great LLM?\"\n+messages = [{\"role\": \"user\", \"content\": input_text}]\n+input_ids = tokenizer.apply_chat_template(messages, return_dict=True, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+\n+outputs = model.generate(**input_ids, max_new_tokens=200)\n+print(tokenizer.decode(outputs[0]))\n+```\n+\n+## DbrxConfig[[transformers.DbrxConfig]]\n+\n+[[autodoc]] DbrxConfig\n+\n+\n+## DbrxModel[[transformers.DbrxModel]]\n+\n+[[autodoc]] DbrxModel\n+    - forward\n+\n+\n+## DbrxForCausalLM[[transformers.DbrxForCausalLM]]\n+\n+[[autodoc]] DbrxForCausalLM\n+    - forward\n+"
        }
    ],
    "stats": {
        "total": 127,
        "additions": 127,
        "deletions": 0
    }
}