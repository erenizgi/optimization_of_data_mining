{
    "author": "konstantinos-p",
    "message": "Change RT-Detr docs to reflect fixed 640x640 input size (#41364)\n\n* Update rt_detr docs to mention 640x640 input size\n\nThe authors of RT-Detr mention that the model was trained on 640x640 images and was meant to be used for inference on 640x640 images.\nAlso, the current implementation has certain quirks that make training/inferring on images of different sizes problematic. For example,\nthe pixel masks used for batches of varying image sizes are discarded. I've added a few lines in the docs to notify the user about these issues.\n\n* Batching not possible with variable image sizes\n\n* Remove reference to batching\n\n---------\n\nCo-authored-by: Konstantinos Pitas <kostasp210@gmail.com>",
    "sha": "bf38b2d11dd17c61a27ea91fac53f8ad92e3171e",
    "files": [
        {
            "sha": "b1ada9f6f11b49f2413fff09fde6df0553790c2b",
            "filename": "docs/source/en/model_doc/rt_detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf38b2d11dd17c61a27ea91fac53f8ad92e3171e/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf38b2d11dd17c61a27ea91fac53f8ad92e3171e/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md?ref=bf38b2d11dd17c61a27ea91fac53f8ad92e3171e",
            "patch": "@@ -40,8 +40,7 @@ The model version was contributed by [rafaelpadilla](https://huggingface.co/rafa\n \n ## Usage tips\n \n-Initially, an image is processed using a pre-trained convolutional neural network, specifically a Resnet-D variant as referenced in the original code. This network extracts features from the final three layers of the architecture. Following this, a hybrid encoder is employed to convert the multi-scale features into a sequential array of image features. Then, a decoder, equipped with auxiliary prediction heads is used to refine the object queries. This process facilitates the direct generation of bounding boxes, eliminating the need for any additional post-processing to acquire the logits and coordinates for the bounding boxes.\n-\n+Initially, an image is processed using a pre-trained convolutional neural network, specifically a Resnet-D variant as referenced in the original code. This network extracts features from the final three layers of the architecture. Following this, a hybrid encoder is employed to convert the multi-scale features into a sequential array of image features. Then, a decoder, equipped with auxiliary prediction heads is used to refine the object queries. This process facilitates the direct generation of bounding boxes, eliminating the need for any additional post-processing to acquire the logits and coordinates for the bounding boxes. The model is meant to be used on images resized to a size 640x640 with the corresponding ImageProcessor. Reshaping to other sizes will generally degrade performance. \n ```py\n >>> import torch\n >>> import requests"
        },
        {
            "sha": "fcd4ba945ee528b077344ffc7ff1f1580d50f928",
            "filename": "docs/source/en/model_doc/rt_detr_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf38b2d11dd17c61a27ea91fac53f8ad92e3171e/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf38b2d11dd17c61a27ea91fac53f8ad92e3171e/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md?ref=bf38b2d11dd17c61a27ea91fac53f8ad92e3171e",
            "patch": "@@ -42,6 +42,7 @@ This second version of RT-DETR improves how the decoder finds objects in an imag\n - **flexible attention** – can use smooth (bilinear) or fixed (discrete) sampling\n - **optimized processing** – improves how attention weights mix information\n \n+The model is meant to be used on images resized to a size 640x640 with the corresponding ImageProcessor. Reshaping to other sizes will generally degrade performance.\n ```py\n >>> import torch\n >>> import requests"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}