{
    "author": "HuangBugWei",
    "message": "Fix: remove the redundant snippet of _whole_word_mask (#36759)\n\nremove the redundant snippet of _whole_word_mask",
    "sha": "a63e92e2f02966bc625494b83c4954da05ba798d",
    "files": [
        {
            "sha": "61f4e88f22fbdcc4617c33951e786b77ce8a2325",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a63e92e2f02966bc625494b83c4954da05ba798d/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a63e92e2f02966bc625494b83c4954da05ba798d/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=a63e92e2f02966bc625494b83c4954da05ba798d",
            "patch": "@@ -1216,13 +1216,6 @@ def _whole_word_mask(self, input_tokens: List[str], max_predictions=512):\n             # predictions, then just skip this candidate.\n             if len(masked_lms) + len(index_set) > num_to_predict:\n                 continue\n-            is_any_index_covered = False\n-            for index in index_set:\n-                if index in covered_indexes:\n-                    is_any_index_covered = True\n-                    break\n-            if is_any_index_covered:\n-                continue\n             for index in index_set:\n                 covered_indexes.add(index)\n                 masked_lms.append(index)"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 0,
        "deletions": 7
    }
}