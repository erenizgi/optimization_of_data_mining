{
    "author": "zucchini-nlp",
    "message": "Rename `_supports_flash_attn_2` in examples and tests (#39471)\n\n* delete `_supports_flash_attn_2` from examples and tests\n\n* simplify docs",
    "sha": "8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
    "files": [
        {
            "sha": "01aca3a4d5af099667249df7dcb486ed2f6c4700",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -497,8 +497,7 @@ class Multimodal2VisionPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"multimodal2_vision\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "981d40bb6d2679731539d8e20808ff60b8d72ef8",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -289,8 +289,7 @@ class MyNewModel2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MyNewModel2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "21aabae4d36b34f2729175995f967e508f91cd44",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -95,8 +95,7 @@ class NewTaskModelPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "c44f12f02fac671fbac72acc09c5ceef417d2774",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -288,8 +288,7 @@ class SuperPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SuperDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "c3d90771e11f83b68c73eb4b82774bc15ab6c79c",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -221,7 +221,7 @@ class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n     base_model_prefix = \"model\"\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _no_split_modules = [\"ModernBertDecoderLayer\"]\n-    _supports_flash_attn_2 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = False\n     _supports_gradient_checkpointing = True\n     _supports_static_cache = False"
        },
        {
            "sha": "d215ccbf0bc7df65b8234bc09472194653283f38",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -398,7 +398,7 @@ class ModernBertDecoderPreTrainedModel(ModernBertPreTrainedModel):\n     base_model_prefix = \"model\"\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _no_split_modules = [\"ModernBertDecoderLayer\"]\n-    _supports_flash_attn_2 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = False\n     _supports_gradient_checkpointing = True\n     _supports_static_cache = False"
        },
        {
            "sha": "b13f824bf7f46da4bd8f25ef68dc4627d3882f7b",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -422,7 +422,7 @@ def test_model_rope_scaling(self):\n     @slow\n     def test_flash_attn_2_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(reason=\"Model does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "fab1672b5c868acd59263543d66fe3cb4cdf9583",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -2297,8 +2297,8 @@ def _test_attention_implementation(self, attn_implementation):\n         max_new_tokens = 3\n         support_flag = {\n             \"sdpa\": \"_supports_sdpa\",\n-            \"flash_attention_2\": \"_supports_flash_attn_2\",\n-            \"flash_attention_3\": \"_supports_flash_attn_3\",\n+            \"flash_attention_2\": \"_supports_flash_attn\",\n+            \"flash_attention_3\": \"_supports_flash_attn\",\n         }\n \n         set_model_tester_for_less_flaky_test(self)"
        },
        {
            "sha": "86ac5087dc01c567391143fda5d4cc2f6d858e05",
            "filename": "tests/models/aimv2/test_modeling_aimv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -478,7 +478,7 @@ def test_load_vision_text_config(self):\n     @slow\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -516,7 +516,7 @@ def test_flash_attn_2_inference_equivalence(self):\n     @mark.flash_attn_test\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "e1d8128a2c2242e97f10d6a33cee7931e9f51b40",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -524,7 +524,7 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids_seq_id\n         max_new_tokens = 30\n \n         for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "f8dabd2fa37aaa9e6f202f66c4db6cf5c86158f7",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -934,7 +934,7 @@ def test_resize_embeddings_untied(self):\n     @slow\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(reason=\"Model does not support flash_attention_2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -991,7 +991,7 @@ def test_flash_attn_2_inference_equivalence(self):\n     @slow\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(reason=\"Model does not support flash_attention_2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "90506e26db2c8bc40846a0454e4385ec72783d78",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -715,7 +715,7 @@ def test_sdpa_can_compile_dynamic(self):\n     @slow\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -753,7 +753,7 @@ def test_flash_attn_2_inference_equivalence(self):\n     @mark.flash_attn_test\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "79dd701efdbb57caa3c1554973723857f10a90bd",
            "filename": "tests/models/esm/test_modeling_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -318,7 +318,7 @@ def test_resize_tokens_embeddings(self):\n     @slow\n     def test_flash_attn_2_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(reason=\"Model does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "c9aae794380a3efefefb915b105a6c1ee2adef3a",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -548,7 +548,7 @@ def _test_attention_implementation(self, attn_implementation):\n         max_new_tokens = 30\n         support_flag = {\n             \"sdpa\": \"_supports_sdpa\",\n-            \"flash_attention_2\": \"_supports_flash_attn_2\",\n+            \"flash_attention_2\": \"_supports_flash_attn\",\n         }\n \n         for model_class in self.all_generative_model_classes:"
        },
        {
            "sha": "e7eee02ce8d90b1befde7316d2a63d991322f587",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -292,7 +292,7 @@ def test_greedy_generate_stereo_outputs(self):\n     # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -372,7 +372,7 @@ def test_flash_attn_2_inference_equivalence(self):\n     # Copied from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -948,7 +948,7 @@ def test_greedy_generate_stereo_outputs(self):\n     # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -1096,7 +1096,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n     # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "3d7b45b643a59198fef12798c4cc9389102a7a15",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -300,7 +300,7 @@ def test_greedy_generate_stereo_outputs(self):\n     # Copied from tests.models.musicgen.test_modeling_musicgen.MusicgenDecoderTest.test_flash_attn_2_inference_equivalence\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -382,7 +382,7 @@ def test_flash_attn_2_inference_equivalence(self):\n     # Copied from tests.models.musicgen.test_modeling_musicgen.MusicgenDecoderTest.test_flash_attn_2_inference_equivalence_right_padding\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -948,7 +948,7 @@ def test_greedy_generate_stereo_outputs(self):\n     # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -1096,7 +1096,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n     # Adapted from tests.test_modeling_common.ModelTesterMixin.test_flash_attn_2_inference_equivalence_right_padding\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "4bff15040ec8f68449e2042559a0d0358f895f03",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -610,7 +610,7 @@ def test_model_from_pretrained(self):\n     @slow\n     def test_flash_attn_2_inference_equivalence(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "f6825308ff120bb98eba54dd90099d4b8826a2f2",
            "filename": "tests/models/siglip2/test_modeling_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -99,7 +99,7 @@ def test_flash_attn_2_inference_equivalence(self):\n         dtype = torch.float16\n \n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             # Prepare inputs"
        },
        {
            "sha": "f8d6f0efae0a9f92bec8db235d8e8521f15d7a29",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -359,7 +359,7 @@ def test_flash_attn_2_inference_equivalence(self):\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "f0494cc059bfc0fb434c15fccbd781201cabe500",
            "filename": "tests/models/vit_mae/test_modeling_vit_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -273,7 +273,7 @@ def test_flash_attn_2_inference_equivalence(self):\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "e6526852b99d00c92ed2129f5c71e6acc3a18f4a",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -867,7 +867,7 @@ def test_flash_attn_2_inference_equivalence(self):\n         import torch\n \n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(reason=\"Model does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -913,7 +913,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n         import torch\n \n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(reason=\"Model does not support flash_attention_2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "62641ad3560be8d8e5014a8fcf8117b45fc049ea",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 22,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=8c102e2eb1c3d6c590eb3ace22fc6c249f1b69a5",
            "patch": "@@ -3471,9 +3471,7 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         for model_class in self.all_model_classes:\n-            if (attn_implementation == \"flash_attention_2\" and not model_class._supports_flash_attn_2) or (\n-                attn_implementation == \"flash_attention_3\" and not model_class._supports_flash_attn_3\n-            ):\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -3969,22 +3967,12 @@ def flash_attn_can_dispatch_composite_models(self, attn_implementation: str):\n                 model = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n \n                 sub_models_supporting_fa = [\n-                    (\n-                        module._supports_flash_attn_3\n-                        if attn_implementation == \"flash_attention_3\"\n-                        else module._supports_flash_attn_2\n-                    )\n+                    module._supports_flash_attn\n                     for name, module in model.named_modules()\n                     if isinstance(module, PreTrainedModel) and name != \"\"\n                 ]\n                 supports_fa_all_modules = (\n-                    all(sub_models_supporting_fa)\n-                    if len(sub_models_supporting_fa) > 0\n-                    else (\n-                        model._supports_flash_attn_3\n-                        if attn_implementation == \"flash_attention_3\"\n-                        else model._supports_flash_attn_2\n-                    )\n+                    all(sub_models_supporting_fa) if len(sub_models_supporting_fa) > 0 else model._supports_flash_attn\n                 )\n                 if not supports_fa_all_modules:\n                     with self.assertRaises(ValueError):\n@@ -4037,7 +4025,7 @@ def test_flash_attn_2_fp32_ln(self):\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn_2:\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n@@ -4104,9 +4092,8 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n         torch_dtype = torch.float16\n \n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        config._attn_implementation = \"flash_attention_2\"\n-        cls = self._torch_compile_train_cls\n-        model = cls(config).to(device=torch_device, dtype=torch_dtype)\n+        cls = self._torch_compile_train_cls  # e.g. LlamaFroCausalLM\n+        model = cls(config, attn_implementation=\"flash_attention_2\").to(device=torch_device, dtype=torch_dtype)\n \n         inputs = {\n             \"input_ids\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),\n@@ -4268,9 +4255,7 @@ def flash_attn_from_config(self, attn_implementation: str):\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         for model_class in self.all_generative_model_classes:\n-            if (attn_implementation == \"flash_attention_2\" and not model_class._supports_flash_attn_2) or (\n-                attn_implementation == \"flash_attention_3\" and not model_class._supports_flash_attn_3\n-            ):\n+            if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        }
    ],
    "stats": {
        "total": 97,
        "additions": 39,
        "deletions": 58
    }
}