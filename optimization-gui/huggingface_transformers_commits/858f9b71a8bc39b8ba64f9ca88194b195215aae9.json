{
    "author": "lhoestq",
    "message": "Remove script datasets in tests (#38940)\n\n* remove trust_remote_code\n\n* again\n\n* Revert \"Skip some tests for now (#38931)\"\n\nThis reverts commit 31d30b72245aacfdf70249165964b53790d9c4d8.\n\n* again\n\n* style\n\n* again\n\n* again\n\n* style\n\n* fix integration test\n\n* fix tests\n\n* style\n\n* fix\n\n* fix\n\n* fix the last ones\n\n* style\n\n* last one\n\n* fix last\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "858f9b71a8bc39b8ba64f9ca88194b195215aae9",
    "files": [
        {
            "sha": "d523408f78fa2a20e83d8320eca71cf67d6b057d",
            "filename": "docs/source/en/model_doc/seamless_m4t.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t.md?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -56,7 +56,7 @@ Here is how to use the processor to process text and audio:\n ```python\n >>> # let's load an audio sample from an Arabic speech corpus\n >>> from datasets import load_dataset\n->>> dataset = load_dataset(\"arabic_speech_corpus\", split=\"test\", streaming=True, trust_remote_code=True)\n+>>> dataset = load_dataset(\"halabi2016/arabic_speech_corpus\", split=\"test\", streaming=True)\n >>> audio_sample = next(iter(dataset))[\"audio\"]\n \n >>> # now, process it"
        },
        {
            "sha": "c98b7b4dd8db3b22f4d7501c60020b670931c4f0",
            "filename": "docs/source/en/model_doc/seamless_m4t_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -56,7 +56,7 @@ Here is how to use the processor to process text and audio:\n ```python\n >>> # let's load an audio sample from an Arabic speech corpus\n >>> from datasets import load_dataset\n->>> dataset = load_dataset(\"arabic_speech_corpus\", split=\"test\", streaming=True, trust_remote_code=True)\n+>>> dataset = load_dataset(\"halabi2016/arabic_speech_corpus\", split=\"test\", streaming=True)\n >>> audio_sample = next(iter(dataset))[\"audio\"]\n \n >>> # now, process it"
        },
        {
            "sha": "ab1930e001c2f905bd988529855810ab366607f3",
            "filename": "examples/flax/test_flax_examples.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/examples%2Fflax%2Ftest_flax_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/examples%2Fflax%2Ftest_flax_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftest_flax_examples.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -264,7 +264,6 @@ def test_run_flax_speech_recognition_seq2seq(self):\n             --dataset_config clean\n             --train_split_name validation\n             --eval_split_name validation\n-            --trust_remote_code\n             --output_dir {tmp_dir}\n             --overwrite_output_dir\n             --num_train_epochs=2"
        },
        {
            "sha": "14ee36b293f2890a5a7e68e6b1fc26809156eba9",
            "filename": "examples/pytorch/test_accelerate_examples.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/examples%2Fpytorch%2Ftest_accelerate_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/examples%2Fpytorch%2Ftest_accelerate_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftest_accelerate_examples.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -312,7 +312,6 @@ def test_run_image_classification_no_trainer(self):\n             {self.examples_dir}/pytorch/image-classification/run_image_classification_no_trainer.py\n             --model_name_or_path google/vit-base-patch16-224-in21k\n             --dataset_name hf-internal-testing/cats_vs_dogs_sample\n-            --trust_remote_code\n             --learning_rate 1e-4\n             --per_device_train_batch_size 2\n             --per_device_eval_batch_size 1"
        },
        {
            "sha": "d27cc305d6acae070ded87067602f4a80dca0554",
            "filename": "examples/pytorch/test_pytorch_examples.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/examples%2Fpytorch%2Ftest_pytorch_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/examples%2Fpytorch%2Ftest_pytorch_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftest_pytorch_examples.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -17,7 +17,6 @@\n import logging\n import os\n import sys\n-import unittest\n from unittest.mock import patch\n \n from transformers import ViTMAEForPreTraining, Wav2Vec2ForPreTraining\n@@ -391,7 +390,6 @@ def test_run_image_classification(self):\n             --output_dir {tmp_dir}\n             --model_name_or_path google/vit-base-patch16-224-in21k\n             --dataset_name hf-internal-testing/cats_vs_dogs_sample\n-            --trust_remote_code\n             --do_train\n             --do_eval\n             --learning_rate 1e-4\n@@ -415,7 +413,6 @@ def test_run_image_classification(self):\n             result = get_results(tmp_dir)\n             self.assertGreaterEqual(result[\"eval_accuracy\"], 0.8)\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_run_speech_recognition_ctc(self):\n         tmp_dir = self.get_auto_remove_tmp_dir()\n         testargs = f\"\"\"\n@@ -426,7 +423,6 @@ def test_run_speech_recognition_ctc(self):\n             --dataset_config_name clean\n             --train_split_name validation\n             --eval_split_name validation\n-            --trust_remote_code\n             --do_train\n             --do_eval\n             --learning_rate 1e-4\n@@ -447,7 +443,6 @@ def test_run_speech_recognition_ctc(self):\n             result = get_results(tmp_dir)\n             self.assertLess(result[\"eval_loss\"], result[\"train_loss\"])\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_run_speech_recognition_ctc_adapter(self):\n         tmp_dir = self.get_auto_remove_tmp_dir()\n         testargs = f\"\"\"\n@@ -458,7 +453,6 @@ def test_run_speech_recognition_ctc_adapter(self):\n             --dataset_config_name clean\n             --train_split_name validation\n             --eval_split_name validation\n-            --trust_remote_code\n             --do_train\n             --do_eval\n             --learning_rate 1e-4\n@@ -481,7 +475,6 @@ def test_run_speech_recognition_ctc_adapter(self):\n             self.assertTrue(os.path.isfile(os.path.join(tmp_dir, \"./adapter.tur.safetensors\")))\n             self.assertLess(result[\"eval_loss\"], result[\"train_loss\"])\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_run_speech_recognition_seq2seq(self):\n         tmp_dir = self.get_auto_remove_tmp_dir()\n         testargs = f\"\"\"\n@@ -492,7 +485,6 @@ def test_run_speech_recognition_seq2seq(self):\n             --dataset_config_name clean\n             --train_split_name validation\n             --eval_split_name validation\n-            --trust_remote_code\n             --do_train\n             --do_eval\n             --learning_rate 1e-4\n@@ -520,7 +512,6 @@ def test_run_audio_classification(self):\n             --output_dir {tmp_dir}\n             --model_name_or_path hf-internal-testing/tiny-random-wav2vec2\n             --dataset_name anton-l/superb_demo\n-            --trust_remote_code\n             --dataset_config_name ks\n             --train_split_name test\n             --eval_split_name test\n@@ -555,7 +546,6 @@ def test_run_wav2vec2_pretraining(self):\n             --dataset_name hf-internal-testing/librispeech_asr_dummy\n             --dataset_config_names clean\n             --dataset_split_names validation\n-            --trust_remote_code\n             --learning_rate 1e-4\n             --per_device_train_batch_size 4\n             --per_device_eval_batch_size 4\n@@ -576,7 +566,6 @@ def test_run_vit_mae_pretraining(self):\n             run_mae.py\n             --output_dir {tmp_dir}\n             --dataset_name hf-internal-testing/cats_vs_dogs_sample\n-            --trust_remote_code\n             --do_train\n             --do_eval\n             --learning_rate 1e-4"
        },
        {
            "sha": "03d0e32def07f46cf7b982a2b4b55f32c243026b",
            "filename": "examples/tensorflow/test_tensorflow_examples.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/examples%2Ftensorflow%2Ftest_tensorflow_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/examples%2Ftensorflow%2Ftest_tensorflow_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftest_tensorflow_examples.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -315,7 +315,6 @@ def test_run_image_classification(self):\n         testargs = f\"\"\"\n             run_image_classification.py\n             --dataset_name hf-internal-testing/cats_vs_dogs_sample\n-            --trust_remote_code\n             --model_name_or_path microsoft/resnet-18\n             --do_train\n             --do_eval"
        },
        {
            "sha": "119114033c476e857534c5ba707e0416c9d5f99f",
            "filename": "src/transformers/models/audio_spectrogram_transformer/convert_audio_spectrogram_transformer_original_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -206,7 +206,7 @@ def convert_audio_spectrogram_transformer_checkpoint(model_name, pytorch_dump_fo\n \n     if \"speech-commands\" in model_name:\n         # TODO: Convert dataset to Parquet\n-        dataset = load_dataset(\"google/speech_commands\", \"v0.02\", split=\"validation\", trust_remote_code=True)\n+        dataset = load_dataset(\"google/speech_commands\", \"v0.02\", split=\"validation\")\n         waveform = dataset[0][\"audio\"][\"array\"]\n     else:\n         filepath = hf_hub_download("
        },
        {
            "sha": "c2e366d7dd024e86611dabbc01a5e7c21059c58d",
            "filename": "src/transformers/models/beit/convert_beit_unilm_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fbeit%2Fconvert_beit_unilm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fbeit%2Fconvert_beit_unilm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fconvert_beit_unilm_to_pytorch.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -266,7 +266,7 @@ def convert_beit_checkpoint(checkpoint_url, pytorch_dump_folder_path):\n     # Check outputs on an image\n     if is_semantic:\n         image_processor = BeitImageProcessor(size=config.image_size, do_center_crop=False)\n-        ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n+        ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n         image = Image.open(ds[0][\"file\"])\n     else:\n         image_processor = BeitImageProcessor("
        },
        {
            "sha": "dfbddef0a054769f8c3dad38dcdf7edf8641c426",
            "filename": "src/transformers/models/data2vec/convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconvert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -226,7 +226,7 @@ def load_data2vec(path):\n \n     processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-lv60\")\n \n-    ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\", trust_remote_code=True)\n+    ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n     input_audio = [x[\"array\"] for x in ds[:4][\"audio\"]]\n \n     inputs = processor(input_audio, return_tensors=\"pt\", padding=True)"
        },
        {
            "sha": "87dfed1a8c35be80473f0d4cb09b9886cf8d134a",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -1212,7 +1212,7 @@ def forward(\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)\n         >>> model = LayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", revision=\"1e3ebac\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd\", split=\"train\")\n         >>> example = dataset[0]\n         >>> question = \"what's his name?\"\n         >>> words = example[\"words\"]"
        },
        {
            "sha": "79c08b46d2aac37ff92a8a10d53d6f57c6a6a5c2",
            "filename": "src/transformers/models/layoutlm/modeling_tf_layoutlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -1601,7 +1601,7 @@ def call(\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)\n         >>> model = TFLayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", revision=\"1e3ebac\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd\", split=\"train\")\n         >>> example = dataset[0]\n         >>> question = \"what's his name?\"\n         >>> words = example[\"words\"]"
        },
        {
            "sha": "66637bedd8d2daa50a7ce9b1e553344227a10194",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -753,9 +753,8 @@ def forward(\n         >>> model = LayoutLMv2Model.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n \n \n-        >>> dataset = load_dataset(\"hf-internal-testing/fixtures_docvqa\", trust_remote_code=True)\n-        >>> image_path = dataset[\"test\"][0][\"file\"]\n-        >>> image = Image.open(image_path).convert(\"RGB\")\n+        >>> dataset = load_dataset(\"hf-internal-testing/fixtures_docvqa\")\n+        >>> image = dataset[\"test\"][0][\"image\"]\n \n         >>> encoding = processor(image, return_tensors=\"pt\")\n \n@@ -943,7 +942,7 @@ def forward(\n \n         >>> set_seed(0)\n \n-        >>> dataset = load_dataset(\"aharley/rvl_cdip\", split=\"train\", streaming=True, trust_remote_code=True)\n+        >>> dataset = load_dataset(\"aharley/rvl_cdip\", split=\"train\", streaming=True)\n         >>> data = next(iter(dataset))\n         >>> image = data[\"image\"].convert(\"RGB\")\n \n@@ -1145,7 +1144,7 @@ def forward(\n \n         >>> set_seed(0)\n \n-        >>> datasets = load_dataset(\"nielsr/funsd\", split=\"test\", trust_remote_code=True)\n+        >>> datasets = load_dataset(\"nielsr/funsd\", split=\"test\")\n         >>> labels = datasets.features[\"ner_tags\"].feature.names\n         >>> id2label = {v: k for v, k in enumerate(labels)}\n \n@@ -1302,9 +1301,8 @@ def forward(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n         >>> model = LayoutLMv2ForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n \n-        >>> dataset = load_dataset(\"hf-internal-testing/fixtures_docvqa\", trust_remote_code=True)\n-        >>> image_path = dataset[\"test\"][0][\"file\"]\n-        >>> image = Image.open(image_path).convert(\"RGB\")\n+        >>> dataset = load_dataset(\"hf-internal-testing/fixtures_docvqa\")\n+        >>> image = dataset[\"test\"][0][\"image\"]\n         >>> question = \"When is coffee break?\"\n         >>> encoding = processor(image, question, return_tensors=\"pt\")\n "
        },
        {
            "sha": "05f662b12a9f9bee1a96e25d8377edf62bd1f186",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -736,7 +736,7 @@ def forward(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n         >>> model = AutoModel.from_pretrained(\"microsoft/layoutlmv3-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> words = example[\"tokens\"]\n@@ -951,7 +951,7 @@ def forward(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n         >>> model = AutoModelForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\", num_labels=7)\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> words = example[\"tokens\"]\n@@ -1052,7 +1052,7 @@ def forward(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n         >>> model = AutoModelForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv3-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> question = \"what's his name?\"\n@@ -1172,7 +1172,7 @@ def forward(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n         >>> model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/layoutlmv3-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> words = example[\"tokens\"]"
        },
        {
            "sha": "bac5af8a9829f34d824636a5e1aeccdaac655fb3",
            "filename": "src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_tf_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_tf_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_tf_layoutlmv3.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -1296,7 +1296,7 @@ def call(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n         >>> model = TFAutoModel.from_pretrained(\"microsoft/layoutlmv3-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> words = example[\"tokens\"]\n@@ -1439,7 +1439,7 @@ def call(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n         >>> model = TFAutoModelForSequenceClassification.from_pretrained(\"microsoft/layoutlmv3-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> words = example[\"tokens\"]\n@@ -1566,7 +1566,7 @@ def call(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n         >>> model = TFAutoModelForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\", num_labels=7)\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> words = example[\"tokens\"]\n@@ -1703,7 +1703,7 @@ def call(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n         >>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"microsoft/layoutlmv3-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> question = \"what's his name?\""
        },
        {
            "sha": "d2dd1c75166419c8d23207faec9868a381529fba",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -644,7 +644,7 @@ def forward(\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n         >>> model = AutoModel.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> words = example[\"tokens\"]\n         >>> boxes = example[\"bboxes\"]\n@@ -784,7 +784,7 @@ def forward(\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n         >>> model = AutoModelForSequenceClassification.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> words = example[\"tokens\"]\n         >>> boxes = example[\"bboxes\"]\n@@ -899,7 +899,7 @@ def forward(\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n         >>> model = AutoModelForTokenClassification.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> words = example[\"tokens\"]\n         >>> boxes = example[\"bboxes\"]\n@@ -1016,7 +1016,7 @@ def forward(\n         >>> tokenizer = AutoTokenizer.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n         >>> model = AutoModelForQuestionAnswering.from_pretrained(\"SCUT-DLVCLab/lilt-roberta-en-base\")\n \n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> words = example[\"tokens\"]\n         >>> boxes = example[\"bboxes\"]"
        },
        {
            "sha": "8d26f7d790fbd9ca4eb10cdb1656f936c7d5ced0",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -2197,7 +2197,7 @@ def forward(\n         >>> from datasets import load_dataset\n \n         >>> dataset = load_dataset(\n-        ...     \"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True\n+        ...     \"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\"\n         ... )  # doctest: +IGNORE_RESULT\n         >>> dataset = dataset.sort(\"id\")\n         >>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n@@ -2878,7 +2878,7 @@ def forward(\n         >>> import torch\n \n         >>> dataset = load_dataset(\n-        ...     \"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True\n+        ...     \"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\"\n         ... )  # doctest: +IGNORE_RESULT\n         >>> dataset = dataset.sort(\"id\")\n         >>> sampling_rate = dataset.features[\"audio\"].sampling_rate"
        },
        {
            "sha": "8d4e368e945bf27b476c0ba6328515294e2e0846",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -1608,7 +1608,7 @@ def forward(\n \n         >>> # load an example image, along with the words and coordinates\n         >>> # which were extracted using an OCR engine\n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> words = example[\"tokens\"]\n@@ -1817,7 +1817,7 @@ def forward(\n \n         >>> # load an example image, along with the words and coordinates\n         >>> # which were extracted using an OCR engine\n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> words = example[\"tokens\"]\n@@ -2029,7 +2029,7 @@ def forward(\n \n         >>> # load an example image, along with the words and coordinates\n         >>> # which were extracted using an OCR engine\n-        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\", trust_remote_code=True)\n+        >>> dataset = load_dataset(\"nielsr/funsd-layoutlmv3\", split=\"train\")\n         >>> example = dataset[0]\n         >>> image = example[\"image\"]\n         >>> words = example[\"tokens\"]"
        },
        {
            "sha": "14e61ec5135a24b0559b0717fb5c4b5835827029",
            "filename": "src/transformers/models/wav2vec2/tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -590,7 +590,7 @@ def decode(\n         >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n \n         >>> # load first sample of English common_voice\n-        >>> dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train\", streaming=True, trust_remote_code=True)\n+        >>> dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train\", streaming=True)\n         >>> dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n         >>> dataset_iter = iter(dataset)\n         >>> sample = next(dataset_iter)"
        },
        {
            "sha": "beb22ca86749f01cde572cad7fe0b19e01f1e415",
            "filename": "src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -546,7 +546,7 @@ def decode(\n         >>> processor = AutoProcessor.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")\n \n         >>> # load first sample of English common_voice\n-        >>> dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train\", streaming=True, trust_remote_code=True)\n+        >>> dataset = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train\", streaming=True)\n         >>> dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n         >>> dataset_iter = iter(dataset)\n         >>> sample = next(dataset_iter)"
        },
        {
            "sha": "63b7f718536e4524a86b693bd260f4cda81508d6",
            "filename": "src/transformers/models/whisper/modeling_flax_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_flax_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_flax_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_flax_whisper.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -1670,7 +1670,7 @@ def __call__(\n     >>> model = FlaxWhisperForAudioClassification.from_pretrained(\n     ...     \"sanchit-gandhi/whisper-medium-fleurs-lang-id\", from_pt=True\n     ... )\n-    >>> ds = load_dataset(\"google/fleurs\", \"all\", split=\"validation\", streaming=True, trust_remote_code=True)\n+    >>> ds = load_dataset(\"google/fleurs\", \"all\", split=\"validation\", streaming=True)\n \n     >>> sample = next(iter(ds))\n "
        },
        {
            "sha": "6488c6d16bdd1b1f983451cb08827d7c3268dc1d",
            "filename": "src/transformers/utils/doc.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Futils%2Fdoc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/src%2Ftransformers%2Futils%2Fdoc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdoc.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -423,7 +423,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> import torch\n     >>> from datasets import load_dataset\n \n-    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n     >>> dataset = dataset.sort(\"id\")\n     >>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n@@ -449,7 +449,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> from datasets import load_dataset\n     >>> import torch\n \n-    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n     >>> dataset = dataset.sort(\"id\")\n     >>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n@@ -484,7 +484,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> from datasets import load_dataset\n     >>> import torch\n \n-    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n     >>> dataset = dataset.sort(\"id\")\n     >>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n@@ -520,7 +520,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> from datasets import load_dataset\n     >>> import torch\n \n-    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n     >>> dataset = dataset.sort(\"id\")\n     >>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n@@ -549,7 +549,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> from datasets import load_dataset\n     >>> import torch\n \n-    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n     >>> dataset = dataset.sort(\"id\")\n     >>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n@@ -584,7 +584,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> import torch\n     >>> from datasets import load_dataset\n \n-    >>> dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"huggingface/cats-image\")\n     >>> image = dataset[\"test\"][\"image\"][0]\n \n     >>> image_processor = AutoImageProcessor.from_pretrained(\"{checkpoint}\")\n@@ -609,7 +609,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> import torch\n     >>> from datasets import load_dataset\n \n-    >>> dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"huggingface/cats-image\")\n     >>> image = dataset[\"test\"][\"image\"][0]\n \n     >>> image_processor = AutoImageProcessor.from_pretrained(\"{checkpoint}\")\n@@ -1194,7 +1194,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> from transformers import AutoProcessor, {model_class}\n     >>> from datasets import load_dataset\n \n-    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n     >>> dataset = dataset.sort(\"id\")\n     >>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n@@ -1219,7 +1219,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> from datasets import load_dataset\n     >>> import tensorflow as tf\n \n-    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n     >>> dataset = dataset.sort(\"id\")\n     >>> sampling_rate = dataset.features[\"audio\"].sampling_rate\n \n@@ -1254,7 +1254,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> from transformers import AutoImageProcessor, {model_class}\n     >>> from datasets import load_dataset\n \n-    >>> dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"huggingface/cats-image\")\n     >>> image = dataset[\"test\"][\"image\"][0]\n \n     >>> image_processor = AutoImageProcessor.from_pretrained(\"{checkpoint}\")\n@@ -1277,7 +1277,7 @@ def _prepare_output_docstrings(output_type, config_class, min_indent=None, add_i\n     >>> import tensorflow as tf\n     >>> from datasets import load_dataset\n \n-    >>> dataset = load_dataset(\"huggingface/cats-image\", trust_remote_code=True)\n+    >>> dataset = load_dataset(\"huggingface/cats-image\"))\n     >>> image = dataset[\"test\"][\"image\"][0]\n \n     >>> image_processor = AutoImageProcessor.from_pretrained(\"{checkpoint}\")"
        },
        {
            "sha": "2195bee01ccfc6d41685f947c27d11822715ab3b",
            "filename": "tests/deepspeed/test_model_zoo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fdeepspeed%2Ftest_model_zoo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fdeepspeed%2Ftest_model_zoo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_model_zoo.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -270,7 +270,6 @@ def make_task_cmds():\n         \"img_clas\": f\"\"\"\n         {scripts_dir}/image-classification/run_image_classification.py\n             --dataset_name hf-internal-testing/cats_vs_dogs_sample\n-            --trust_remote_code\n             --remove_unused_columns False\n             --max_steps 10\n             --image_processor_name {DS_TESTS_DIRECTORY}/vit_feature_extractor.json"
        },
        {
            "sha": "51a72beeb5e5c8dce614a47f04b42f1fd4336614",
            "filename": "tests/models/beit/test_image_processing_beit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 19,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fbeit%2Ftest_image_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fbeit%2Ftest_image_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_image_processing_beit.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -27,8 +27,6 @@\n     import torch\n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import BeitImageProcessor\n \n     if is_torchvision_available():\n@@ -98,23 +96,14 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n \n \n def prepare_semantic_single_inputs():\n-    dataset = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-\n-    image = Image.open(dataset[0][\"file\"])\n-    map = Image.open(dataset[1][\"file\"])\n-\n-    return image, map\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n \n \n def prepare_semantic_batch_inputs():\n-    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-\n-    image1 = Image.open(ds[0][\"file\"])\n-    map1 = Image.open(ds[1][\"file\"])\n-    image2 = Image.open(ds[2][\"file\"])\n-    map2 = Image.open(ds[3][\"file\"])\n-\n-    return [image1, image2], [map1, map2]\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n \n \n @require_torch\n@@ -157,7 +146,6 @@ def test_image_processor_from_dict_with_kwargs(self):\n             self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n             self.assertEqual(image_processor.do_reduce_labels, True)\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_call_segmentation_maps(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n@@ -265,7 +253,6 @@ def test_call_segmentation_maps(self):\n             self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n             self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_reduce_labels(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n@@ -282,7 +269,6 @@ def test_reduce_labels(self):\n             self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n             self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_slow_fast_equivalence(self):\n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test\")"
        },
        {
            "sha": "4804cb08b66abd1b44ae66e9fb71e77358a1ec4c",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 12,
            "deletions": 27,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -16,7 +16,6 @@\n import unittest\n \n from datasets import load_dataset\n-from packaging import version\n \n from transformers import BeitConfig\n from transformers.testing_utils import (\n@@ -53,7 +52,6 @@\n \n \n if is_vision_available():\n-    import PIL\n     from PIL import Image\n \n     from transformers import BeitImageProcessor\n@@ -504,8 +502,8 @@ def test_inference_semantic_segmentation(self):\n \n         image_processor = BeitImageProcessor(do_resize=True, size=640, do_center_crop=False)\n \n-        ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-        image = Image.open(ds[0][\"file\"])\n+        ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+        image = ds[0][\"image\"].convert(\"RGB\")\n         inputs = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n \n         # forward pass\n@@ -517,27 +515,14 @@ def test_inference_semantic_segmentation(self):\n         expected_shape = torch.Size((1, 150, 160, 160))\n         self.assertEqual(logits.shape, expected_shape)\n \n-        is_pillow_less_than_9 = version.parse(PIL.__version__) < version.parse(\"9.0.0\")\n-\n-        if is_pillow_less_than_9:\n-            expected_slice = torch.tensor(\n-                [\n-                    [[-4.9225, -2.3954, -3.0522], [-2.8822, -1.0046, -1.7561], [-2.9549, -1.3228, -2.1347]],\n-                    [[-5.8168, -3.4129, -4.0778], [-3.8651, -2.2214, -3.0277], [-3.8356, -2.4643, -3.3535]],\n-                    [[-0.0078, 3.9952, 4.0754], [2.9856, 4.6944, 5.0035], [3.2413, 4.7813, 4.9969]],\n-                ],\n-                device=torch_device,\n-            )\n-        else:\n-            expected_slice = torch.tensor(\n-                [\n-                    [[-4.8960, -2.3688, -3.0355], [-2.8478, -0.9836, -1.7418], [-2.9449, -1.3332, -2.1456]],\n-                    [[-5.8081, -3.4124, -4.1006], [-3.8561, -2.2081, -3.0323], [-3.8365, -2.4601, -3.3669]],\n-                    [[-0.0309, 3.9868, 4.0540], [2.9640, 4.6877, 4.9976], [3.2081, 4.7690, 4.9942]],\n-                ],\n-                device=torch_device,\n-            )\n-\n+        expected_slice = torch.tensor(\n+            [\n+                [[-4.8963, -2.3696, -3.0359], [-2.8485, -0.9842, -1.7426], [-2.9453, -1.3338, -2.1463]],\n+                [[-5.8099, -3.4140, -4.1025], [-3.8578, -2.2100, -3.0337], [-3.8383, -2.4615, -3.3681]],\n+                [[-0.0314, 3.9864, 4.0536], [2.9637, 4.6879, 4.9976], [3.2074, 4.7690, 4.9946]],\n+            ],\n+            device=torch_device,\n+        )\n         torch.testing.assert_close(logits[0, :3, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n@@ -547,8 +532,8 @@ def test_post_processing_semantic_segmentation(self):\n \n         image_processor = BeitImageProcessor(do_resize=True, size=640, do_center_crop=False)\n \n-        ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-        image = Image.open(ds[0][\"file\"])\n+        ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+        image = ds[0][\"image\"].convert(\"RGB\")\n         inputs = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n \n         # forward pass"
        },
        {
            "sha": "e275b8d681be4515991e6dc2805d5b7b73f30717",
            "filename": "tests/models/data2vec/test_modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -669,7 +669,7 @@ def _load_datasamples(self, num_samples):\n         return [x[\"array\"] for x in speech_samples]\n \n     def _load_superb(self, task, num_samples):\n-        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\", trust_remote_code=True)\n+        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\")\n \n         return ds[:num_samples]\n "
        },
        {
            "sha": "538ec08dc1ca2b159b9b3c7d7e6d239603c91b7e",
            "filename": "tests/models/dpt/test_image_processing_dpt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 20,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -29,8 +29,6 @@\n     import torch\n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import DPTImageProcessor\n \n     if is_torchvision_available():\n@@ -94,24 +92,15 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n \n # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs\n def prepare_semantic_single_inputs():\n-    dataset = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-\n-    image = Image.open(dataset[0][\"file\"])\n-    map = Image.open(dataset[1][\"file\"])\n-\n-    return image, map\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n \n \n # Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs\n def prepare_semantic_batch_inputs():\n-    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-\n-    image1 = Image.open(ds[0][\"file\"])\n-    map1 = Image.open(ds[1][\"file\"])\n-    image2 = Image.open(ds[2][\"file\"])\n-    map2 = Image.open(ds[3][\"file\"])\n-\n-    return [image1, image2], [map1, map2]\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n \n \n @require_torch\n@@ -187,7 +176,6 @@ def test_keep_aspect_ratio(self):\n \n             self.assertEqual(list(pixel_values.shape), [1, 3, 512, 672])\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     # Copied from transformers.tests.models.beit.test_image_processing_beit.BeitImageProcessingTest.test_call_segmentation_maps\n     def test_call_segmentation_maps(self):\n         for image_processing_class in self.image_processor_list:\n@@ -296,7 +284,6 @@ def test_call_segmentation_maps(self):\n             self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n             self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_reduce_labels(self):\n         for image_processing_class in self.image_processor_list:\n             image_processor = image_processing_class(**self.image_processor_dict)\n@@ -319,7 +306,6 @@ def test_reduce_labels(self):\n             # Compare with non-reduced label to see if it's reduced by 1\n             self.assertEqual(encoding[\"labels\"][first_non_zero_coords].item(), first_non_zero_value - 1)\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_slow_fast_equivalence(self):\n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n@@ -341,7 +327,6 @@ def test_slow_fast_equivalence(self):\n         )\n         self.assertTrue(torch.allclose(image_encoding_slow.labels, image_encoding_fast.labels, atol=1e-1))\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_slow_fast_equivalence_batched(self):\n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test\")"
        },
        {
            "sha": "67ef91db7850e213a51f16c97a807aa570af1eea",
            "filename": "tests/models/granite_speech/test_modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -391,7 +391,7 @@ def test_small_model_integration_test_batch(self):\n \n         EXPECTED_DECODED_TEXT = [\n             \"systemKnowledge Cutoff Date: April 2024.\\nToday's Date: December 19, 2024.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\\nusercan you transcribe the speech into a written format?\\nassistantmister quilter is the apostle of the middle classes and we are glad to welcome his gospel\",\n-            \"systemKnowledge Cutoff Date: April 2024.\\nToday's Date: December 19, 2024.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\\nusercan you transcribe the speech into a written format?\\nassistantnor is mister quilp's manner less interesting than his matter\"\n+            \"systemKnowledge Cutoff Date: April 2024.\\nToday's Date: December 19, 2024.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\\nusercan you transcribe the speech into a written format?\\nassistantnor is mister quilter's manner less interesting than his matter\"\n         ]  # fmt: skip\n \n         self.assertEqual("
        },
        {
            "sha": "905b435bb598c630c876693e088215e53913f430",
            "filename": "tests/models/hubert/test_modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -767,7 +767,7 @@ def _load_datasamples(self, num_samples):\n     def _load_superb(self, task, num_samples):\n         from datasets import load_dataset\n \n-        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\", trust_remote_code=True)\n+        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\")\n \n         return ds[:num_samples]\n "
        },
        {
            "sha": "f574f6751104ee8ecd1e28346e87dc7e59cc9263",
            "filename": "tests/models/layoutlmv2/test_image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -123,13 +123,13 @@ def test_image_processor_from_dict_with_kwargs(self):\n     def test_layoutlmv2_integration_test(self):\n         from datasets import load_dataset\n \n-        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\", trust_remote_code=True)\n+        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\")\n \n         for image_processing_class in self.image_processor_list:\n             # with apply_OCR = True\n             image_processing = image_processing_class()\n \n-            image = Image.open(ds[0][\"file\"]).convert(\"RGB\")\n+            image = ds[0][\"image\"]\n \n             encoding = image_processing(image, return_tensors=\"pt\")\n "
        },
        {
            "sha": "e9f76cfbd7167e3c3ed0d25c3debbec867ad83a1",
            "filename": "tests/models/layoutlmv2/test_processor_layoutlmv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutlmv2%2Ftest_processor_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutlmv2%2Ftest_processor_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_processor_layoutlmv2.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -28,8 +28,6 @@\n \n \n if is_pytesseract_available():\n-    from PIL import Image\n-\n     from transformers import LayoutLMv2ImageProcessor\n \n \n@@ -156,11 +154,11 @@ def test_overflowing_tokens(self):\n         from datasets import load_dataset\n \n         # set up\n-        datasets = load_dataset(\"nielsr/funsd\", trust_remote_code=True)\n+        datasets = load_dataset(\"nielsr/funsd\")\n         processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\", revision=\"no_ocr\")\n \n         def preprocess_data(examples):\n-            images = [Image.open(path).convert(\"RGB\") for path in examples[\"image_path\"]]\n+            images = [image.convert(\"RGB\") for image in examples[\"image\"]]\n             words = examples[\"words\"]\n             boxes = examples[\"bboxes\"]\n             word_labels = examples[\"ner_tags\"]\n@@ -192,12 +190,8 @@ def get_images(self):\n         # we verify our implementation on 2 document images from the DocVQA dataset\n         from datasets import load_dataset\n \n-        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\", trust_remote_code=True)\n-\n-        image_1 = Image.open(ds[0][\"file\"]).convert(\"RGB\")\n-        image_2 = Image.open(ds[1][\"file\"]).convert(\"RGB\")\n-\n-        return image_1, image_2\n+        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\")\n+        return ds[0][\"image\"].convert(\"RGB\"), ds[1][\"image\"].convert(\"RGB\")\n \n     @cached_property\n     def get_tokenizers(self):"
        },
        {
            "sha": "eb4b4f1d9acfe417fb31364012bf91ef3f65acb4",
            "filename": "tests/models/layoutlmv3/test_image_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutlmv3%2Ftest_image_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutlmv3%2Ftest_image_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_image_processing_layoutlmv3.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -22,8 +22,6 @@\n \n \n if is_pytesseract_available():\n-    from PIL import Image\n-\n     from transformers import LayoutLMv3ImageProcessor\n \n     if is_torchvision_available():\n@@ -103,17 +101,16 @@ def test_image_processor_from_dict_with_kwargs(self):\n             image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n             self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_LayoutLMv3_integration_test(self):\n         from datasets import load_dataset\n \n-        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\", trust_remote_code=True)\n+        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\")\n \n         # with apply_OCR = True\n         for image_processing_class in self.image_processor_list:\n             image_processor = image_processing_class()\n \n-            image = Image.open(ds[0][\"file\"]).convert(\"RGB\")\n+            image = ds[0][\"image\"].convert(\"RGB\")\n \n             encoding = image_processor(image, return_tensors=\"pt\")\n "
        },
        {
            "sha": "cf367c615ea0a88d818ee5aad64c4e6b38b839d5",
            "filename": "tests/models/layoutlmv3/test_processor_layoutlmv3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutlmv3%2Ftest_processor_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutlmv3%2Ftest_processor_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_processor_layoutlmv3.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -28,8 +28,6 @@\n \n \n if is_pytesseract_available():\n-    from PIL import Image\n-\n     from transformers import LayoutLMv3ImageProcessor\n \n \n@@ -172,12 +170,8 @@ def get_images(self):\n         # we verify our implementation on 2 document images from the DocVQA dataset\n         from datasets import load_dataset\n \n-        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\", trust_remote_code=True)\n-\n-        image_1 = Image.open(ds[0][\"file\"]).convert(\"RGB\")\n-        image_2 = Image.open(ds[1][\"file\"]).convert(\"RGB\")\n-\n-        return image_1, image_2\n+        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\")\n+        return ds[0][\"image\"].convert(\"RGB\"), ds[1][\"image\"].convert(\"RGB\")\n \n     @cached_property\n     def get_tokenizers(self):"
        },
        {
            "sha": "2fc7a273b96cea66f869d9cc95deb42fe027a16f",
            "filename": "tests/models/layoutxlm/test_processor_layoutxlm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutxlm%2Ftest_processor_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Flayoutxlm%2Ftest_processor_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_processor_layoutxlm.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -33,8 +33,6 @@\n \n \n if is_pytesseract_available():\n-    from PIL import Image\n-\n     from transformers import LayoutLMv2ImageProcessor\n \n \n@@ -162,11 +160,11 @@ def test_overflowing_tokens(self):\n         from datasets import load_dataset\n \n         # set up\n-        datasets = load_dataset(\"nielsr/funsd\", trust_remote_code=True)\n+        datasets = load_dataset(\"nielsr/funsd\")\n         processor = LayoutXLMProcessor.from_pretrained(\"microsoft/layoutxlm-base\", apply_ocr=False)\n \n         def preprocess_data(examples):\n-            images = [Image.open(path).convert(\"RGB\") for path in examples[\"image_path\"]]\n+            images = [image.convert(\"RGB\") for image in examples[\"image\"]]\n             words = examples[\"words\"]\n             boxes = examples[\"bboxes\"]\n             word_labels = examples[\"ner_tags\"]\n@@ -200,12 +198,8 @@ def get_images(self):\n         # we verify our implementation on 2 document images from the DocVQA dataset\n         from datasets import load_dataset\n \n-        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\", trust_remote_code=True)\n-\n-        image_1 = Image.open(ds[0][\"file\"]).convert(\"RGB\")\n-        image_2 = Image.open(ds[1][\"file\"]).convert(\"RGB\")\n-\n-        return image_1, image_2\n+        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\")\n+        return ds[0][\"image\"].convert(\"RGB\"), ds[1][\"image\"].convert(\"RGB\")\n \n     @cached_property\n     def get_tokenizers(self):"
        },
        {
            "sha": "7df498176d71750c92725f4799128786bb8e558c",
            "filename": "tests/models/mobilevit/test_image_processing_mobilevit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -27,8 +27,6 @@\n     import torch\n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import MobileViTImageProcessor\n \n \n@@ -86,23 +84,14 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n \n \n def prepare_semantic_single_inputs():\n-    dataset = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-\n-    image = Image.open(dataset[0][\"file\"])\n-    map = Image.open(dataset[1][\"file\"])\n-\n-    return image, map\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n \n \n def prepare_semantic_batch_inputs():\n-    dataset = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-\n-    image1 = Image.open(dataset[0][\"file\"])\n-    map1 = Image.open(dataset[1][\"file\"])\n-    image2 = Image.open(dataset[2][\"file\"])\n-    map2 = Image.open(dataset[3][\"file\"])\n-\n-    return [image1, image2], [map1, map2]\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n \n \n @require_torch\n@@ -135,7 +124,6 @@ def test_image_processor_from_dict_with_kwargs(self):\n         self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n         self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_call_segmentation_maps(self):\n         # Initialize image_processing\n         image_processing = self.image_processing_class(**self.image_processor_dict)"
        },
        {
            "sha": "996860da6edeeaa0e4d70aaadb7d0d8bb5b8d48d",
            "filename": "tests/models/nougat/test_image_processing_nougat.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -86,8 +86,12 @@ def expected_output_image_shape(self, images):\n         return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n \n     def prepare_dummy_image(self):\n+        revision = \"ec57bf8c8b1653a209c13f6e9ee66b12df0fc2db\"\n         filepath = hf_hub_download(\n-            repo_id=\"hf-internal-testing/fixtures_docvqa\", filename=\"nougat_pdf.png\", repo_type=\"dataset\"\n+            repo_id=\"hf-internal-testing/fixtures_docvqa\",\n+            filename=\"nougat_pdf.png\",\n+            repo_type=\"dataset\",\n+            revision=revision,\n         )\n         image = Image.open(filepath).convert(\"RGB\")\n         return image\n@@ -136,7 +140,6 @@ def test_image_processor_from_dict_with_kwargs(self):\n         image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n         self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_expected_output(self):\n         dummy_image = self.image_processor_tester.prepare_dummy_image()\n         image_processor = self.image_processor\n@@ -180,13 +183,16 @@ def test_align_long_axis_data_format(self):\n         self.assertEqual((3, 100, 200), aligned_image.shape)\n \n     def prepare_dummy_np_image(self):\n+        revision = \"ec57bf8c8b1653a209c13f6e9ee66b12df0fc2db\"\n         filepath = hf_hub_download(\n-            repo_id=\"hf-internal-testing/fixtures_docvqa\", filename=\"nougat_pdf.png\", repo_type=\"dataset\"\n+            repo_id=\"hf-internal-testing/fixtures_docvqa\",\n+            filename=\"nougat_pdf.png\",\n+            repo_type=\"dataset\",\n+            revision=revision,\n         )\n         image = Image.open(filepath).convert(\"RGB\")\n         return np.array(image)\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_crop_margin_equality_cv2_python(self):\n         image = self.prepare_dummy_np_image()\n         image_processor = self.image_processor"
        },
        {
            "sha": "6c2aceea53fd2b090e01d8d23aeb22af4cbffff7",
            "filename": "tests/models/perceiver/test_modeling_perceiver.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -842,11 +842,8 @@ def prepare_img():\n \n # Helper functions for optical flow integration test\n def prepare_optical_flow_images():\n-    dataset = load_dataset(\"hf-internal-testing/fixtures_sintel\", split=\"test\", trust_remote_code=True)\n-    image1 = Image.open(dataset[0][\"file\"]).convert(\"RGB\")\n-    image2 = Image.open(dataset[0][\"file\"]).convert(\"RGB\")\n-\n-    return image1, image2\n+    ds = load_dataset(\"hf-internal-testing/fixtures_sintel\", split=\"test\")\n+    return list(ds[\"image\"][:2])\n \n \n def normalize(img):"
        },
        {
            "sha": "f03d9c4fd60d62ab5d020d64c8259316ee8faafc",
            "filename": "tests/models/segformer/test_image_processing_segformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 18,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fsegformer%2Ftest_image_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fsegformer%2Ftest_image_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsegformer%2Ftest_image_processing_segformer.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -27,8 +27,6 @@\n     import torch\n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import SegformerImageProcessor\n \n \n@@ -86,23 +84,14 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n \n \n def prepare_semantic_single_inputs():\n-    dataset = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-\n-    image = Image.open(dataset[0][\"file\"])\n-    map = Image.open(dataset[1][\"file\"])\n-\n-    return image, map\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n \n \n def prepare_semantic_batch_inputs():\n-    dataset = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-\n-    image1 = Image.open(dataset[0][\"file\"])\n-    map1 = Image.open(dataset[1][\"file\"])\n-    image2 = Image.open(dataset[2][\"file\"])\n-    map2 = Image.open(dataset[3][\"file\"])\n-\n-    return [image1, image2], [map1, map2]\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n \n \n @require_torch\n@@ -138,7 +127,6 @@ def test_image_processor_from_dict_with_kwargs(self):\n         self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n         self.assertEqual(image_processor.do_reduce_labels, True)\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_call_segmentation_maps(self):\n         # Initialize image_processing\n         image_processing = self.image_processing_class(**self.image_processor_dict)\n@@ -245,7 +233,6 @@ def test_call_segmentation_maps(self):\n         self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n         self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n \n-    @unittest.skip(\"temporary to avoid failing on circleci\")\n     def test_reduce_labels(self):\n         # Initialize image_processing\n         image_processing = self.image_processing_class(**self.image_processor_dict)"
        },
        {
            "sha": "92dd47c39201d956db9a4101cd3b468de1d28038",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 12,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -16,9 +16,9 @@\n import inspect\n import unittest\n \n-from huggingface_hub import hf_hub_download\n+from datasets import load_dataset\n \n-from transformers import UdopConfig, is_torch_available, is_vision_available\n+from transformers import UdopConfig, is_torch_available\n from transformers.testing_utils import (\n     require_sentencepiece,\n     require_tokenizers,\n@@ -42,10 +42,6 @@\n     from transformers import UdopEncoderModel, UdopForConditionalGeneration, UdopModel, UdopProcessor\n \n \n-if is_vision_available():\n-    from PIL import Image\n-\n-\n class UdopModelTester:\n     def __init__(\n         self,\n@@ -618,12 +614,8 @@ def test_custom_4d_attention_mask(self):\n class UdopModelIntegrationTests(unittest.TestCase):\n     @cached_property\n     def image(self):\n-        filepath = hf_hub_download(\n-            repo_id=\"hf-internal-testing/fixtures_docvqa\", filename=\"document_2.png\", repo_type=\"dataset\"\n-        )\n-        image = Image.open(filepath).convert(\"RGB\")\n-\n-        return image\n+        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\")\n+        return ds[1][\"image\"]\n \n     @cached_property\n     def processor(self):"
        },
        {
            "sha": "2fc3f59d2dbaef60e3a903216a0c146a922241e0",
            "filename": "tests/models/udop/test_processor_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fudop%2Ftest_processor_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fudop%2Ftest_processor_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_processor_udop.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -41,8 +41,6 @@\n \n \n if is_pytesseract_available():\n-    from PIL import Image\n-\n     from transformers import LayoutLMv3ImageProcessor\n \n \n@@ -184,11 +182,11 @@ def test_overflowing_tokens(self):\n         from datasets import load_dataset\n \n         # set up\n-        datasets = load_dataset(\"nielsr/funsd\", trust_remote_code=True)\n+        datasets = load_dataset(\"nielsr/funsd\")\n         processor = UdopProcessor.from_pretrained(\"microsoft/udop-large\", apply_ocr=False)\n \n         def preprocess_data(examples):\n-            images = [Image.open(path).convert(\"RGB\") for path in examples[\"image_path\"]]\n+            images = [image.convert(\"RGB\") for image in examples[\"image\"]]\n             words = examples[\"words\"]\n             boxes = examples[\"bboxes\"]\n             word_labels = examples[\"ner_tags\"]\n@@ -222,12 +220,8 @@ def get_images(self):\n         # we verify our implementation on 2 document images from the DocVQA dataset\n         from datasets import load_dataset\n \n-        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\", trust_remote_code=True)\n-\n-        image_1 = Image.open(ds[0][\"file\"]).convert(\"RGB\")\n-        image_2 = Image.open(ds[1][\"file\"]).convert(\"RGB\")\n-\n-        return image_1, image_2\n+        ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\")\n+        return ds[0][\"image\"].convert(\"RGB\"), ds[1][\"image\"].convert(\"RGB\")\n \n     @cached_property\n     def get_tokenizers(self):"
        },
        {
            "sha": "37da494a9655f59726063aba4a326b0f7bdbed45",
            "filename": "tests/models/unispeech/test_modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -566,7 +566,7 @@ def _load_datasamples(self, num_samples):\n         return [x[\"array\"] for x in speech_samples]\n \n     def _load_superb(self, task, num_samples):\n-        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\", trust_remote_code=True)\n+        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\")\n \n         return ds[:num_samples]\n "
        },
        {
            "sha": "1b6a1cb80428c0a06463d127e7d030201b3dd5d9",
            "filename": "tests/models/unispeech_sat/test_modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -820,7 +820,7 @@ def _load_datasamples(self, num_samples):\n         return [x[\"array\"] for x in speech_samples]\n \n     def _load_superb(self, task, num_samples):\n-        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\", trust_remote_code=True)\n+        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\")\n \n         return ds[:num_samples]\n "
        },
        {
            "sha": "ed0a982efd869cfa95c0f97e64bf09598b053782",
            "filename": "tests/models/upernet/test_modeling_upernet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -15,7 +15,7 @@\n \n import unittest\n \n-from huggingface_hub import hf_hub_download\n+from datasets import load_dataset\n \n from transformers import ConvNextConfig, UperNetConfig\n from transformers.testing_utils import (\n@@ -41,8 +41,6 @@\n \n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import AutoImageProcessor\n \n \n@@ -277,11 +275,8 @@ def test_model_from_pretrained(self):\n \n # We will verify our results on an image of ADE20k\n def prepare_img():\n-    filepath = hf_hub_download(\n-        repo_id=\"hf-internal-testing/fixtures_ade20k\", repo_type=\"dataset\", filename=\"ADE_val_00000001.jpg\"\n-    )\n-    image = Image.open(filepath).convert(\"RGB\")\n-    return image\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return ds[0][\"image\"].convert(\"RGB\")\n \n \n @require_torch\n@@ -302,7 +297,7 @@ def test_inference_swin_backbone(self):\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n         expected_slice = torch.tensor(\n-            [[-7.5958, -7.5958, -7.4302], [-7.5958, -7.5958, -7.4302], [-7.4797, -7.4797, -7.3068]]\n+            [[-7.5969, -7.5969, -7.4313], [-7.5969, -7.5969, -7.4313], [-7.4808, -7.4808, -7.3080]]\n         ).to(torch_device)\n         torch.testing.assert_close(outputs.logits[0, 0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n "
        },
        {
            "sha": "ec3fff698befc60dc122453fd0b44510300d8e40",
            "filename": "tests/models/vilt/test_modeling_vilt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -637,9 +637,9 @@ def test_inference_natural_language_visual_reasoning(self):\n \n         processor = self.default_processor\n \n-        dataset = load_dataset(\"hf-internal-testing/fixtures_nlvr2\", split=\"test\", trust_remote_code=True)\n-        image1 = Image.open(dataset[0][\"file\"]).convert(\"RGB\")\n-        image2 = Image.open(dataset[1][\"file\"]).convert(\"RGB\")\n+        dataset = load_dataset(\"hf-internal-testing/fixtures_nlvr2\", split=\"train\")\n+        image1 = dataset[0][\"image\"]\n+        image2 = dataset[1][\"image\"]\n \n         text = (\n             \"The left image contains twice the number of dogs as the right image, and at least two dogs in total are\""
        },
        {
            "sha": "93264feab2c6053753266ce315861634e3222e85",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -1149,8 +1149,8 @@ def default_processor(self):\n     def test_inference_handwritten(self):\n         model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\").to(torch_device)\n \n-        dataset = load_dataset(\"hf-internal-testing/fixtures_ocr\", split=\"test\", trust_remote_code=True)\n-        image = Image.open(dataset[0][\"file\"]).convert(\"RGB\")\n+        dataset = load_dataset(\"hf-internal-testing/fixtures_ocr\", split=\"train\")\n+        image = dataset[1][\"image\"].convert(\"RGB\")\n \n         processor = self.default_processor\n         pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(torch_device)\n@@ -1174,8 +1174,8 @@ def test_inference_handwritten(self):\n     def test_inference_printed(self):\n         model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\").to(torch_device)\n \n-        dataset = load_dataset(\"hf-internal-testing/fixtures_ocr\", split=\"test\", trust_remote_code=True)\n-        image = Image.open(dataset[1][\"file\"]).convert(\"RGB\")\n+        dataset = load_dataset(\"hf-internal-testing/fixtures_ocr\", split=\"train\")\n+        image = dataset[0][\"image\"].convert(\"RGB\")\n \n         processor = self.default_processor\n         pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(torch_device)"
        },
        {
            "sha": "087664f4d26cbc5f9f3e1ffe3a5c97e68f19d031",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -97,9 +97,7 @@ def _test_wav2vec2_with_lm_invalid_pool(in_queue, out_queue, timeout):\n     try:\n         _ = in_queue.get(timeout=timeout)\n \n-        ds = load_dataset(\n-            \"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", streaming=True, trust_remote_code=True\n-        )\n+        ds = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", streaming=True)\n         sample = next(iter(ds))\n \n         resampled_audio = torchaudio.functional.resample(\n@@ -1470,7 +1468,7 @@ def _load_datasamples(self, num_samples):\n         return [x[\"array\"] for x in speech_samples]\n \n     def _load_superb(self, task, num_samples):\n-        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\", trust_remote_code=True)\n+        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\")\n \n         return ds[:num_samples]\n \n@@ -1836,9 +1834,7 @@ def test_phoneme_recognition(self):\n     @require_pyctcdecode\n     @require_torchaudio\n     def test_wav2vec2_with_lm(self):\n-        ds = load_dataset(\n-            \"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", streaming=True, trust_remote_code=True\n-        )\n+        ds = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", streaming=True)\n         sample = next(iter(ds))\n \n         resampled_audio = torchaudio.functional.resample(\n@@ -1862,9 +1858,7 @@ def test_wav2vec2_with_lm(self):\n     @require_pyctcdecode\n     @require_torchaudio\n     def test_wav2vec2_with_lm_pool(self):\n-        ds = load_dataset(\n-            \"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", streaming=True, trust_remote_code=True\n-        )\n+        ds = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"es\", split=\"test\", streaming=True)\n         sample = next(iter(ds))\n \n         resampled_audio = torchaudio.functional.resample(\n@@ -1963,9 +1957,7 @@ def test_inference_mms_1b_all(self):\n         LANG_MAP = {\"it\": \"ita\", \"es\": \"spa\", \"fr\": \"fra\", \"en\": \"eng\"}\n \n         def run_model(lang):\n-            ds = load_dataset(\n-                \"mozilla-foundation/common_voice_11_0\", lang, split=\"test\", streaming=True, trust_remote_code=True\n-            )\n+            ds = load_dataset(\"mozilla-foundation/common_voice_11_0\", lang, split=\"test\", streaming=True)\n             sample = next(iter(ds))\n \n             wav2vec2_lang = LANG_MAP[lang]"
        },
        {
            "sha": "66fc8665cb5e328a11ccba7dc9737091823c778b",
            "filename": "tests/models/wav2vec2_with_lm/test_processor_wav2vec2_with_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processor_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processor_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processor_wav2vec2_with_lm.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -463,9 +463,7 @@ def test_offsets_integration_fast_batch(self):\n     def test_word_time_stamp_integration(self):\n         import torch\n \n-        ds = load_dataset(\n-            \"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train\", streaming=True, trust_remote_code=True\n-        )\n+        ds = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"train\", streaming=True)\n         ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n         ds_iter = iter(ds)\n         sample = next(ds_iter)"
        },
        {
            "sha": "84855613dd6e21689079f7c4c3dbed3659c1e48f",
            "filename": "tests/models/wavlm/test_modeling_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fwavlm%2Ftest_modeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fwavlm%2Ftest_modeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwavlm%2Ftest_modeling_wavlm.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -473,7 +473,7 @@ def _load_datasamples(self, num_samples):\n         return [x[\"array\"] for x in speech_samples]\n \n     def _load_superb(self, task, num_samples):\n-        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\", trust_remote_code=True)\n+        ds = load_dataset(\"anton-l/superb_dummy\", task, split=\"test\")\n \n         return ds[:num_samples]\n "
        },
        {
            "sha": "3e1b42fde90a6cd43a93d858c04e8075546051b5",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -1645,9 +1645,7 @@ def test_large_generation_multilingual(self):\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n         model.to(torch_device)\n \n-        ds = load_dataset(\n-            \"facebook/multilingual_librispeech\", \"german\", split=\"test\", streaming=True, trust_remote_code=True\n-        )\n+        ds = load_dataset(\"facebook/multilingual_librispeech\", \"german\", split=\"test\", streaming=True)\n         ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n \n         input_speech = next(iter(ds))[\"audio\"][\"array\"]\n@@ -1714,11 +1712,10 @@ def test_large_batched_generation_multilingual(self):\n \n         token = os.getenv(\"HF_HUB_READ_TOKEN\", True)\n         ds = load_dataset(\n-            \"mozilla-foundation/common_voice_6_1\",\n+            \"hf-internal-testing/fixtures_common_voice\",\n             \"ja\",\n             split=\"test\",\n             streaming=True,\n-            trust_remote_code=True,\n             token=token,\n         )\n         ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n@@ -1728,7 +1725,10 @@ def test_large_batched_generation_multilingual(self):\n             torch_device\n         )\n \n-        EXPECTED_TRANSCRIPTS = [\"\", \" Kimura-san called me.\"]\n+        EXPECTED_TRANSCRIPTS = [\n+            \"\",\n+            \" It was the time of day and all of the pens left during the summer.\",\n+        ]\n \n         generated_ids = model.generate(\n             input_features.repeat(2, 1, 1),"
        },
        {
            "sha": "bbad033d13849174832b7c90cc3a59029e1f7d7b",
            "filename": "tests/pipelines/test_pipelines_audio_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -179,7 +179,7 @@ def test_large_model_pt(self):\n         model = \"superb/wav2vec2-base-superb-ks\"\n \n         audio_classifier = pipeline(\"audio-classification\", model=model)\n-        dataset = datasets.load_dataset(\"anton-l/superb_dummy\", \"ks\", split=\"test\", trust_remote_code=True)\n+        dataset = datasets.load_dataset(\"anton-l/superb_dummy\", \"ks\", split=\"test\")\n \n         audio = np.array(dataset[3][\"speech\"], dtype=np.float32)\n         output = audio_classifier(audio, top_k=4)"
        },
        {
            "sha": "d48caf161378873fa26c23901080d8987dc28a78",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 6,
            "deletions": 12,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -265,9 +265,7 @@ def test_small_model_pt_seq2seq_gen_kwargs(self):\n     @require_torch\n     @require_pyctcdecode\n     def test_large_model_pt_with_lm(self):\n-        dataset = load_dataset(\"Narsil/asr_dummy\", streaming=True, trust_remote_code=True)\n-        third_item = next(iter(dataset[\"test\"].skip(3)))\n-        filename = third_item[\"file\"]\n+        filename = hf_hub_download(\"Narsil/asr_dummy\", filename=\"4.flac\", repo_type=\"dataset\")\n \n         speech_recognizer = pipeline(\n             task=\"automatic-speech-recognition\",\n@@ -388,7 +386,7 @@ def test_return_timestamps_in_preprocess(self):\n             chunk_length_s=8,\n             stride_length_s=1,\n         )\n-        data = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"test\", streaming=True, trust_remote_code=True)\n+        data = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"test\", streaming=True)\n         sample = next(iter(data))\n \n         res = pipe(sample[\"audio\"][\"array\"])\n@@ -434,7 +432,7 @@ def test_return_timestamps_and_language_in_preprocess(self):\n             stride_length_s=1,\n             return_language=True,\n         )\n-        data = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"test\", streaming=True, trust_remote_code=True)\n+        data = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"test\", streaming=True)\n         sample = next(iter(data))\n \n         res = pipe(sample[\"audio\"][\"array\"])\n@@ -489,7 +487,7 @@ def test_return_timestamps_in_preprocess_longform(self):\n             task=\"automatic-speech-recognition\",\n             model=\"openai/whisper-tiny.en\",\n         )\n-        data = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"test\", streaming=True, trust_remote_code=True)\n+        data = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=\"test\", streaming=True)\n         samples = [next(iter(data)) for _ in range(8)]\n         audio = np.concatenate([sample[\"audio\"][\"array\"] for sample in samples])\n \n@@ -1125,9 +1123,7 @@ def test_whisper_language(self):\n     @slow\n     def test_speculative_decoding_whisper_non_distil(self):\n         # Load data:\n-        dataset = load_dataset(\n-            \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\", trust_remote_code=True\n-        )\n+        dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\")\n         sample = dataset[0][\"audio\"]\n \n         # Load model:\n@@ -1169,9 +1165,7 @@ def test_speculative_decoding_whisper_non_distil(self):\n     @slow\n     def test_speculative_decoding_whisper_distil(self):\n         # Load data:\n-        dataset = load_dataset(\n-            \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\", trust_remote_code=True\n-        )\n+        dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\")\n         sample = dataset[0][\"audio\"]\n \n         # Load model:"
        },
        {
            "sha": "215a6180379e13c9a6e8233a613822ba84da49c8",
            "filename": "tests/pipelines/test_pipelines_image_segmentation.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/858f9b71a8bc39b8ba64f9ca88194b195215aae9/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py?ref=858f9b71a8bc39b8ba64f9ca88194b195215aae9",
            "patch": "@@ -601,9 +601,9 @@ def test_maskformer(self):\n \n         image_segmenter = pipeline(\"image-segmentation\", model=model, image_processor=image_processor)\n \n-        image = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-        file = image[0][\"file\"]\n-        outputs = image_segmenter(file, threshold=threshold)\n+        ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+        image = ds[0][\"image\"].convert(\"RGB\")\n+        outputs = image_segmenter(image, threshold=threshold)\n \n         # Shortening by hashing\n         for o in outputs:\n@@ -655,9 +655,9 @@ def test_maskformer(self):\n     def test_oneformer(self):\n         image_segmenter = pipeline(model=\"shi-labs/oneformer_ade20k_swin_tiny\")\n \n-        image = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n-        file = image[0][\"file\"]\n-        outputs = image_segmenter(file, threshold=0.99)\n+        ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+        image = ds[0][\"image\"].convert(\"RGB\")\n+        outputs = image_segmenter(image, threshold=0.99)\n         # Shortening by hashing\n         for o in outputs:\n             o[\"mask\"] = mask_to_test_readable(o[\"mask\"])\n@@ -679,7 +679,7 @@ def test_oneformer(self):\n         )\n \n         # Different task\n-        outputs = image_segmenter(file, threshold=0.99, subtask=\"instance\")\n+        outputs = image_segmenter(image, threshold=0.99, subtask=\"instance\")\n         # Shortening by hashing\n         for o in outputs:\n             o[\"mask\"] = mask_to_test_readable(o[\"mask\"])\n@@ -701,7 +701,7 @@ def test_oneformer(self):\n         )\n \n         # Different task\n-        outputs = image_segmenter(file, subtask=\"semantic\")\n+        outputs = image_segmenter(image, subtask=\"semantic\")\n         # Shortening by hashing\n         for o in outputs:\n             o[\"mask\"] = mask_to_test_readable(o[\"mask\"])"
        }
    ],
    "stats": {
        "total": 447,
        "additions": 154,
        "deletions": 293
    }
}