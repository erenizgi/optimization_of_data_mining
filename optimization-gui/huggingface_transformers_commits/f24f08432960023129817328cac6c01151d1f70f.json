{
    "author": "LysandreJik",
    "message": "Import structure & first three model refactors (#31329)\n\n* Import structure & first three model refactors\r\n\r\n* Register -> Export. Export all in __all__. Sensible defaults according to filename.\r\n\r\n* Apply most comments from Amy and some comments from Lucain\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\nCo-authored-by: Lucain Pouget <lucainp@gmail.com>\r\n\r\n* Style\r\n\r\n* Add comment\r\n\r\n* Clearer .py management\r\n\r\n* Raise if not in backend mapping\r\n\r\n* More specific type\r\n\r\n* More efficient listdir\r\n\r\n* Misc fixes\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\nCo-authored-by: Lucain Pouget <lucainp@gmail.com>",
    "sha": "f24f08432960023129817328cac6c01151d1f70f",
    "files": [
        {
            "sha": "d3998327cc71f1c535c9ab38315b21a431c23756",
            "filename": "Makefile",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/Makefile",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/Makefile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/Makefile?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -53,7 +53,6 @@ quality:\n \t@python -c \"from transformers import *\" || (echo 'üö® import failed, this means you introduced unprotected imports! üö®'; exit 1)\n \truff check $(check_dirs) setup.py conftest.py\n \truff format --check $(check_dirs) setup.py conftest.py\n-\tpython utils/custom_init_isort.py --check_only\n \tpython utils/sort_auto_mappings.py --check_only\n \tpython utils/check_doc_toc.py\n \tpython utils/check_docstrings.py --check_all\n@@ -62,7 +61,6 @@ quality:\n # Format source code automatically and check is there are any problems left that need manual fixing\n \n extra_style_checks:\n-\tpython utils/custom_init_isort.py\n \tpython utils/sort_auto_mappings.py\n \tpython utils/check_doc_toc.py --fix_and_overwrite\n "
        },
        {
            "sha": "71295978fd4dd1bcbc95cf83e580878f1eca36bb",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 65,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -1500,7 +1500,6 @@\n             \"BertForQuestionAnswering\",\n             \"BertForSequenceClassification\",\n             \"BertForTokenClassification\",\n-            \"BertLayer\",\n             \"BertLMHeadModel\",\n             \"BertModel\",\n             \"BertPreTrainedModel\",\n@@ -1524,7 +1523,6 @@\n             \"BigBirdForQuestionAnswering\",\n             \"BigBirdForSequenceClassification\",\n             \"BigBirdForTokenClassification\",\n-            \"BigBirdLayer\",\n             \"BigBirdModel\",\n             \"BigBirdPreTrainedModel\",\n             \"load_tf_weights_in_big_bird\",\n@@ -1643,7 +1641,6 @@\n             \"CanineForQuestionAnswering\",\n             \"CanineForSequenceClassification\",\n             \"CanineForTokenClassification\",\n-            \"CanineLayer\",\n             \"CanineModel\",\n             \"CaninePreTrainedModel\",\n             \"load_tf_weights_in_canine\",\n@@ -1730,7 +1727,6 @@\n             \"ConvBertForQuestionAnswering\",\n             \"ConvBertForSequenceClassification\",\n             \"ConvBertForTokenClassification\",\n-            \"ConvBertLayer\",\n             \"ConvBertModel\",\n             \"ConvBertPreTrainedModel\",\n             \"load_tf_weights_in_convbert\",\n@@ -1959,7 +1955,6 @@\n             \"QDQBertForQuestionAnswering\",\n             \"QDQBertForSequenceClassification\",\n             \"QDQBertForTokenClassification\",\n-            \"QDQBertLayer\",\n             \"QDQBertLMHeadModel\",\n             \"QDQBertModel\",\n             \"QDQBertPreTrainedModel\",\n@@ -2211,7 +2206,6 @@\n             \"FNetForQuestionAnswering\",\n             \"FNetForSequenceClassification\",\n             \"FNetForTokenClassification\",\n-            \"FNetLayer\",\n             \"FNetModel\",\n             \"FNetPreTrainedModel\",\n         ]\n@@ -2312,15 +2306,13 @@\n             \"GPTNeoXForQuestionAnswering\",\n             \"GPTNeoXForSequenceClassification\",\n             \"GPTNeoXForTokenClassification\",\n-            \"GPTNeoXLayer\",\n             \"GPTNeoXModel\",\n             \"GPTNeoXPreTrainedModel\",\n         ]\n     )\n     _import_structure[\"models.gpt_neox_japanese\"].extend(\n         [\n             \"GPTNeoXJapaneseForCausalLM\",\n-            \"GPTNeoXJapaneseLayer\",\n             \"GPTNeoXJapaneseModel\",\n             \"GPTNeoXJapanesePreTrainedModel\",\n         ]\n@@ -2552,7 +2544,6 @@\n             \"LongformerForTokenClassification\",\n             \"LongformerModel\",\n             \"LongformerPreTrainedModel\",\n-            \"LongformerSelfAttention\",\n         ]\n     )\n     _import_structure[\"models.longt5\"].extend(\n@@ -2585,7 +2576,6 @@\n             \"LxmertModel\",\n             \"LxmertPreTrainedModel\",\n             \"LxmertVisualFeatureEncoder\",\n-            \"LxmertXLayer\",\n         ]\n     )\n     _import_structure[\"models.m2m_100\"].extend(\n@@ -2609,7 +2599,9 @@\n             \"Mamba2PreTrainedModel\",\n         ]\n     )\n-    _import_structure[\"models.marian\"].extend([\"MarianForCausalLM\", \"MarianModel\", \"MarianMTModel\"])\n+    _import_structure[\"models.marian\"].extend(\n+        [\"MarianForCausalLM\", \"MarianModel\", \"MarianMTModel\", \"MarianPreTrainedModel\"]\n+    )\n     _import_structure[\"models.markuplm\"].extend(\n         [\n             \"MarkupLMForQuestionAnswering\",\n@@ -2692,7 +2684,6 @@\n             \"MobileBertForQuestionAnswering\",\n             \"MobileBertForSequenceClassification\",\n             \"MobileBertForTokenClassification\",\n-            \"MobileBertLayer\",\n             \"MobileBertModel\",\n             \"MobileBertPreTrainedModel\",\n             \"load_tf_weights_in_mobilebert\",\n@@ -2738,7 +2729,6 @@\n             \"MPNetForQuestionAnswering\",\n             \"MPNetForSequenceClassification\",\n             \"MPNetForTokenClassification\",\n-            \"MPNetLayer\",\n             \"MPNetModel\",\n             \"MPNetPreTrainedModel\",\n         ]\n@@ -2828,7 +2818,6 @@\n             \"NystromformerForQuestionAnswering\",\n             \"NystromformerForSequenceClassification\",\n             \"NystromformerForTokenClassification\",\n-            \"NystromformerLayer\",\n             \"NystromformerModel\",\n             \"NystromformerPreTrainedModel\",\n         ]\n@@ -2942,7 +2931,6 @@\n             \"PerceiverForMultimodalAutoencoding\",\n             \"PerceiverForOpticalFlow\",\n             \"PerceiverForSequenceClassification\",\n-            \"PerceiverLayer\",\n             \"PerceiverModel\",\n             \"PerceiverPreTrainedModel\",\n         ]\n@@ -3078,11 +3066,9 @@\n     )\n     _import_structure[\"models.reformer\"].extend(\n         [\n-            \"ReformerAttention\",\n             \"ReformerForMaskedLM\",\n             \"ReformerForQuestionAnswering\",\n             \"ReformerForSequenceClassification\",\n-            \"ReformerLayer\",\n             \"ReformerModel\",\n             \"ReformerModelWithLMHead\",\n             \"ReformerPreTrainedModel\",\n@@ -3103,7 +3089,6 @@\n             \"RemBertForQuestionAnswering\",\n             \"RemBertForSequenceClassification\",\n             \"RemBertForTokenClassification\",\n-            \"RemBertLayer\",\n             \"RemBertModel\",\n             \"RemBertPreTrainedModel\",\n             \"load_tf_weights_in_rembert\",\n@@ -3150,7 +3135,6 @@\n             \"RoCBertForQuestionAnswering\",\n             \"RoCBertForSequenceClassification\",\n             \"RoCBertForTokenClassification\",\n-            \"RoCBertLayer\",\n             \"RoCBertModel\",\n             \"RoCBertPreTrainedModel\",\n             \"load_tf_weights_in_roc_bert\",\n@@ -3164,7 +3148,6 @@\n             \"RoFormerForQuestionAnswering\",\n             \"RoFormerForSequenceClassification\",\n             \"RoFormerForTokenClassification\",\n-            \"RoFormerLayer\",\n             \"RoFormerModel\",\n             \"RoFormerPreTrainedModel\",\n             \"load_tf_weights_in_roformer\",\n@@ -3221,7 +3204,6 @@\n             \"SegformerDecodeHead\",\n             \"SegformerForImageClassification\",\n             \"SegformerForSemanticSegmentation\",\n-            \"SegformerLayer\",\n             \"SegformerModel\",\n             \"SegformerPreTrainedModel\",\n         ]\n@@ -3280,7 +3262,6 @@\n         [\n             \"SplinterForPreTraining\",\n             \"SplinterForQuestionAnswering\",\n-            \"SplinterLayer\",\n             \"SplinterModel\",\n             \"SplinterPreTrainedModel\",\n         ]\n@@ -3293,7 +3274,6 @@\n             \"SqueezeBertForSequenceClassification\",\n             \"SqueezeBertForTokenClassification\",\n             \"SqueezeBertModel\",\n-            \"SqueezeBertModule\",\n             \"SqueezeBertPreTrainedModel\",\n         ]\n     )\n@@ -3492,7 +3472,6 @@\n             \"ViltForMaskedLM\",\n             \"ViltForQuestionAnswering\",\n             \"ViltForTokenClassification\",\n-            \"ViltLayer\",\n             \"ViltModel\",\n             \"ViltPreTrainedModel\",\n         ]\n@@ -3512,7 +3491,6 @@\n             \"VisualBertForQuestionAnswering\",\n             \"VisualBertForRegionToPhraseAlignment\",\n             \"VisualBertForVisualReasoning\",\n-            \"VisualBertLayer\",\n             \"VisualBertModel\",\n             \"VisualBertPreTrainedModel\",\n         ]\n@@ -3528,7 +3506,6 @@\n     _import_structure[\"models.vit_mae\"].extend(\n         [\n             \"ViTMAEForPreTraining\",\n-            \"ViTMAELayer\",\n             \"ViTMAEModel\",\n             \"ViTMAEPreTrainedModel\",\n         ]\n@@ -3708,7 +3685,6 @@\n             \"YosoForQuestionAnswering\",\n             \"YosoForSequenceClassification\",\n             \"YosoForTokenClassification\",\n-            \"YosoLayer\",\n             \"YosoModel\",\n             \"YosoPreTrainedModel\",\n         ]\n@@ -3855,7 +3831,6 @@\n     )\n     _import_structure[\"models.bert\"].extend(\n         [\n-            \"TFBertEmbeddings\",\n             \"TFBertForMaskedLM\",\n             \"TFBertForMultipleChoice\",\n             \"TFBertForNextSentencePrediction\",\n@@ -3921,7 +3896,6 @@\n             \"TFConvBertForQuestionAnswering\",\n             \"TFConvBertForSequenceClassification\",\n             \"TFConvBertForTokenClassification\",\n-            \"TFConvBertLayer\",\n             \"TFConvBertModel\",\n             \"TFConvBertPreTrainedModel\",\n         ]\n@@ -4152,7 +4126,6 @@\n             \"TFLongformerForTokenClassification\",\n             \"TFLongformerModel\",\n             \"TFLongformerPreTrainedModel\",\n-            \"TFLongformerSelfAttention\",\n         ]\n     )\n     _import_structure[\"models.lxmert\"].extend(\n@@ -4253,7 +4226,6 @@\n             \"TFRemBertForQuestionAnswering\",\n             \"TFRemBertForSequenceClassification\",\n             \"TFRemBertForTokenClassification\",\n-            \"TFRemBertLayer\",\n             \"TFRemBertModel\",\n             \"TFRemBertPreTrainedModel\",\n         ]\n@@ -4299,7 +4271,6 @@\n             \"TFRoFormerForQuestionAnswering\",\n             \"TFRoFormerForSequenceClassification\",\n             \"TFRoFormerForTokenClassification\",\n-            \"TFRoFormerLayer\",\n             \"TFRoFormerModel\",\n             \"TFRoFormerPreTrainedModel\",\n         ]\n@@ -5829,7 +5800,8 @@\n         from .models.llama import LlamaTokenizer\n         from .models.m2m_100 import M2M100Tokenizer\n         from .models.marian import MarianTokenizer\n-        from .models.mbart import MBart50Tokenizer, MBartTokenizer\n+        from .models.mbart import MBartTokenizer\n+        from .models.mbart50 import MBart50Tokenizer\n         from .models.mluke import MLukeTokenizer\n         from .models.mt5 import MT5Tokenizer\n         from .models.nllb import NllbTokenizer\n@@ -6300,7 +6272,6 @@\n             BertForQuestionAnswering,\n             BertForSequenceClassification,\n             BertForTokenClassification,\n-            BertLayer,\n             BertLMHeadModel,\n             BertModel,\n             BertPreTrainedModel,\n@@ -6320,7 +6291,6 @@\n             BigBirdForQuestionAnswering,\n             BigBirdForSequenceClassification,\n             BigBirdForTokenClassification,\n-            BigBirdLayer,\n             BigBirdModel,\n             BigBirdPreTrainedModel,\n             load_tf_weights_in_big_bird,\n@@ -6415,7 +6385,6 @@\n             CanineForQuestionAnswering,\n             CanineForSequenceClassification,\n             CanineForTokenClassification,\n-            CanineLayer,\n             CanineModel,\n             CaninePreTrainedModel,\n             load_tf_weights_in_canine,\n@@ -6488,7 +6457,6 @@\n             ConvBertForQuestionAnswering,\n             ConvBertForSequenceClassification,\n             ConvBertForTokenClassification,\n-            ConvBertLayer,\n             ConvBertModel,\n             ConvBertPreTrainedModel,\n             load_tf_weights_in_convbert,\n@@ -6673,7 +6641,6 @@\n             QDQBertForQuestionAnswering,\n             QDQBertForSequenceClassification,\n             QDQBertForTokenClassification,\n-            QDQBertLayer,\n             QDQBertLMHeadModel,\n             QDQBertModel,\n             QDQBertPreTrainedModel,\n@@ -6872,7 +6839,6 @@\n             FNetForQuestionAnswering,\n             FNetForSequenceClassification,\n             FNetForTokenClassification,\n-            FNetLayer,\n             FNetModel,\n             FNetPreTrainedModel,\n         )\n@@ -6960,13 +6926,11 @@\n             GPTNeoXForQuestionAnswering,\n             GPTNeoXForSequenceClassification,\n             GPTNeoXForTokenClassification,\n-            GPTNeoXLayer,\n             GPTNeoXModel,\n             GPTNeoXPreTrainedModel,\n         )\n         from .models.gpt_neox_japanese import (\n             GPTNeoXJapaneseForCausalLM,\n-            GPTNeoXJapaneseLayer,\n             GPTNeoXJapaneseModel,\n             GPTNeoXJapanesePreTrainedModel,\n         )\n@@ -7142,7 +7106,6 @@\n             LongformerForTokenClassification,\n             LongformerModel,\n             LongformerPreTrainedModel,\n-            LongformerSelfAttention,\n         )\n         from .models.longt5 import (\n             LongT5EncoderModel,\n@@ -7169,7 +7132,6 @@\n             LxmertModel,\n             LxmertPreTrainedModel,\n             LxmertVisualFeatureEncoder,\n-            LxmertXLayer,\n         )\n         from .models.m2m_100 import (\n             M2M100ForConditionalGeneration,\n@@ -7186,7 +7148,7 @@\n             Mamba2Model,\n             Mamba2PreTrainedModel,\n         )\n-        from .models.marian import MarianForCausalLM, MarianModel, MarianMTModel\n+        from .models.marian import MarianForCausalLM, MarianModel, MarianMTModel, MarianPreTrainedModel\n         from .models.markuplm import (\n             MarkupLMForQuestionAnswering,\n             MarkupLMForSequenceClassification,\n@@ -7252,7 +7214,6 @@\n             MobileBertForQuestionAnswering,\n             MobileBertForSequenceClassification,\n             MobileBertForTokenClassification,\n-            MobileBertLayer,\n             MobileBertModel,\n             MobileBertPreTrainedModel,\n             load_tf_weights_in_mobilebert,\n@@ -7288,7 +7249,6 @@\n             MPNetForQuestionAnswering,\n             MPNetForSequenceClassification,\n             MPNetForTokenClassification,\n-            MPNetLayer,\n             MPNetModel,\n             MPNetPreTrainedModel,\n         )\n@@ -7360,7 +7320,6 @@\n             NystromformerForQuestionAnswering,\n             NystromformerForSequenceClassification,\n             NystromformerForTokenClassification,\n-            NystromformerLayer,\n             NystromformerModel,\n             NystromformerPreTrainedModel,\n         )\n@@ -7448,7 +7407,6 @@\n             PerceiverForMultimodalAutoencoding,\n             PerceiverForOpticalFlow,\n             PerceiverForSequenceClassification,\n-            PerceiverLayer,\n             PerceiverModel,\n             PerceiverPreTrainedModel,\n         )\n@@ -7550,11 +7508,9 @@\n             RecurrentGemmaPreTrainedModel,\n         )\n         from .models.reformer import (\n-            ReformerAttention,\n             ReformerForMaskedLM,\n             ReformerForQuestionAnswering,\n             ReformerForSequenceClassification,\n-            ReformerLayer,\n             ReformerModel,\n             ReformerModelWithLMHead,\n             ReformerPreTrainedModel,\n@@ -7571,7 +7527,6 @@\n             RemBertForQuestionAnswering,\n             RemBertForSequenceClassification,\n             RemBertForTokenClassification,\n-            RemBertLayer,\n             RemBertModel,\n             RemBertPreTrainedModel,\n             load_tf_weights_in_rembert,\n@@ -7610,7 +7565,6 @@\n             RoCBertForQuestionAnswering,\n             RoCBertForSequenceClassification,\n             RoCBertForTokenClassification,\n-            RoCBertLayer,\n             RoCBertModel,\n             RoCBertPreTrainedModel,\n             load_tf_weights_in_roc_bert,\n@@ -7622,7 +7576,6 @@\n             RoFormerForQuestionAnswering,\n             RoFormerForSequenceClassification,\n             RoFormerForTokenClassification,\n-            RoFormerLayer,\n             RoFormerModel,\n             RoFormerPreTrainedModel,\n             load_tf_weights_in_roformer,\n@@ -7667,7 +7620,6 @@\n             SegformerDecodeHead,\n             SegformerForImageClassification,\n             SegformerForSemanticSegmentation,\n-            SegformerLayer,\n             SegformerModel,\n             SegformerPreTrainedModel,\n         )\n@@ -7712,7 +7664,6 @@\n         from .models.splinter import (\n             SplinterForPreTraining,\n             SplinterForQuestionAnswering,\n-            SplinterLayer,\n             SplinterModel,\n             SplinterPreTrainedModel,\n         )\n@@ -7723,7 +7674,6 @@\n             SqueezeBertForSequenceClassification,\n             SqueezeBertForTokenClassification,\n             SqueezeBertModel,\n-            SqueezeBertModule,\n             SqueezeBertPreTrainedModel,\n         )\n         from .models.stablelm import (\n@@ -7872,7 +7822,6 @@\n             ViltForMaskedLM,\n             ViltForQuestionAnswering,\n             ViltForTokenClassification,\n-            ViltLayer,\n             ViltModel,\n             ViltPreTrainedModel,\n         )\n@@ -7888,7 +7837,6 @@\n             VisualBertForQuestionAnswering,\n             VisualBertForRegionToPhraseAlignment,\n             VisualBertForVisualReasoning,\n-            VisualBertLayer,\n             VisualBertModel,\n             VisualBertPreTrainedModel,\n         )\n@@ -7900,7 +7848,6 @@\n         )\n         from .models.vit_mae import (\n             ViTMAEForPreTraining,\n-            ViTMAELayer,\n             ViTMAEModel,\n             ViTMAEPreTrainedModel,\n         )\n@@ -8042,7 +7989,6 @@\n             YosoForQuestionAnswering,\n             YosoForSequenceClassification,\n             YosoForTokenClassification,\n-            YosoLayer,\n             YosoModel,\n             YosoPreTrainedModel,\n         )\n@@ -8176,7 +8122,6 @@\n             TFBartPretrainedModel,\n         )\n         from .models.bert import (\n-            TFBertEmbeddings,\n             TFBertForMaskedLM,\n             TFBertForMultipleChoice,\n             TFBertForNextSentencePrediction,\n@@ -8230,7 +8175,6 @@\n             TFConvBertForQuestionAnswering,\n             TFConvBertForSequenceClassification,\n             TFConvBertForTokenClassification,\n-            TFConvBertLayer,\n             TFConvBertModel,\n             TFConvBertPreTrainedModel,\n         )\n@@ -8415,7 +8359,6 @@\n             TFLongformerForTokenClassification,\n             TFLongformerModel,\n             TFLongformerPreTrainedModel,\n-            TFLongformerSelfAttention,\n         )\n         from .models.lxmert import (\n             TFLxmertForPreTraining,\n@@ -8505,7 +8448,6 @@\n             TFRemBertForQuestionAnswering,\n             TFRemBertForSequenceClassification,\n             TFRemBertForTokenClassification,\n-            TFRemBertLayer,\n             TFRemBertModel,\n             TFRemBertPreTrainedModel,\n         )\n@@ -8543,7 +8485,6 @@\n             TFRoFormerForQuestionAnswering,\n             TFRoFormerForSequenceClassification,\n             TFRoFormerForTokenClassification,\n-            TFRoFormerLayer,\n             TFRoFormerModel,\n             TFRoFormerPreTrainedModel,\n         )"
        },
        {
            "sha": "57b5747909e091ede05ff07c98254224fbebed97",
            "filename": "src/transformers/models/albert/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 154,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2F__init__.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -11,165 +11,21 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_flax_available,\n-    is_sentencepiece_available,\n-    is_tf_available,\n-    is_tokenizers_available,\n-    is_torch_available,\n-)\n-\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n-_import_structure = {\n-    \"configuration_albert\": [\"AlbertConfig\", \"AlbertOnnxConfig\"],\n-}\n-\n-try:\n-    if not is_sentencepiece_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_albert\"] = [\"AlbertTokenizer\"]\n-\n-try:\n-    if not is_tokenizers_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"tokenization_albert_fast\"] = [\"AlbertTokenizerFast\"]\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_albert\"] = [\n-        \"AlbertForMaskedLM\",\n-        \"AlbertForMultipleChoice\",\n-        \"AlbertForPreTraining\",\n-        \"AlbertForQuestionAnswering\",\n-        \"AlbertForSequenceClassification\",\n-        \"AlbertForTokenClassification\",\n-        \"AlbertModel\",\n-        \"AlbertPreTrainedModel\",\n-        \"load_tf_weights_in_albert\",\n-    ]\n-\n-try:\n-    if not is_tf_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_tf_albert\"] = [\n-        \"TFAlbertForMaskedLM\",\n-        \"TFAlbertForMultipleChoice\",\n-        \"TFAlbertForPreTraining\",\n-        \"TFAlbertForQuestionAnswering\",\n-        \"TFAlbertForSequenceClassification\",\n-        \"TFAlbertForTokenClassification\",\n-        \"TFAlbertMainLayer\",\n-        \"TFAlbertModel\",\n-        \"TFAlbertPreTrainedModel\",\n-    ]\n-\n-try:\n-    if not is_flax_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_flax_albert\"] = [\n-        \"FlaxAlbertForMaskedLM\",\n-        \"FlaxAlbertForMultipleChoice\",\n-        \"FlaxAlbertForPreTraining\",\n-        \"FlaxAlbertForQuestionAnswering\",\n-        \"FlaxAlbertForSequenceClassification\",\n-        \"FlaxAlbertForTokenClassification\",\n-        \"FlaxAlbertModel\",\n-        \"FlaxAlbertPreTrainedModel\",\n-    ]\n \n if TYPE_CHECKING:\n-    from .configuration_albert import AlbertConfig, AlbertOnnxConfig\n-\n-    try:\n-        if not is_sentencepiece_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_albert import AlbertTokenizer\n-\n-    try:\n-        if not is_tokenizers_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .tokenization_albert_fast import AlbertTokenizerFast\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_albert import (\n-            AlbertForMaskedLM,\n-            AlbertForMultipleChoice,\n-            AlbertForPreTraining,\n-            AlbertForQuestionAnswering,\n-            AlbertForSequenceClassification,\n-            AlbertForTokenClassification,\n-            AlbertModel,\n-            AlbertPreTrainedModel,\n-            load_tf_weights_in_albert,\n-        )\n-\n-    try:\n-        if not is_tf_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_tf_albert import (\n-            TFAlbertForMaskedLM,\n-            TFAlbertForMultipleChoice,\n-            TFAlbertForPreTraining,\n-            TFAlbertForQuestionAnswering,\n-            TFAlbertForSequenceClassification,\n-            TFAlbertForTokenClassification,\n-            TFAlbertMainLayer,\n-            TFAlbertModel,\n-            TFAlbertPreTrainedModel,\n-        )\n-\n-    try:\n-        if not is_flax_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_flax_albert import (\n-            FlaxAlbertForMaskedLM,\n-            FlaxAlbertForMultipleChoice,\n-            FlaxAlbertForPreTraining,\n-            FlaxAlbertForQuestionAnswering,\n-            FlaxAlbertForSequenceClassification,\n-            FlaxAlbertForTokenClassification,\n-            FlaxAlbertModel,\n-            FlaxAlbertPreTrainedModel,\n-        )\n+    from .configuration_albert import *\n+    from .modeling_albert import *\n+    from .modeling_flax_albert import *\n+    from .modeling_tf_albert import *\n+    from .tokenization_albert import *\n+    from .tokenization_albert_fast import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e1e2d4547cc4e285634a8e9fcfac68092c8ded68",
            "filename": "src/transformers/models/albert/configuration_albert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -165,3 +165,6 @@ def inputs(self) -> Mapping[str, Mapping[int, str]]:\n                 (\"token_type_ids\", dynamic_axis),\n             ]\n         )\n+\n+\n+__all__ = [\"AlbertConfig\", \"AlbertOnnxConfig\"]"
        },
        {
            "sha": "dca1fe7f6002956c912b32d43760dca5d7a0c8cb",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -1466,3 +1466,16 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+__all__ = [\n+    \"load_tf_weights_in_albert\",\n+    \"AlbertPreTrainedModel\",\n+    \"AlbertModel\",\n+    \"AlbertForPreTraining\",\n+    \"AlbertForMaskedLM\",\n+    \"AlbertForSequenceClassification\",\n+    \"AlbertForTokenClassification\",\n+    \"AlbertForQuestionAnswering\",\n+    \"AlbertForMultipleChoice\",\n+]"
        },
        {
            "sha": "b5b49219aebf6393620ba07d17c15e64889f9c89",
            "filename": "src/transformers/models/albert/modeling_flax_albert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_flax_albert.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -1119,3 +1119,14 @@ class FlaxAlbertForQuestionAnswering(FlaxAlbertPreTrainedModel):\n     FlaxQuestionAnsweringModelOutput,\n     _CONFIG_FOR_DOC,\n )\n+\n+__all__ = [\n+    \"FlaxAlbertPreTrainedModel\",\n+    \"FlaxAlbertModel\",\n+    \"FlaxAlbertForPreTraining\",\n+    \"FlaxAlbertForMaskedLM\",\n+    \"FlaxAlbertForSequenceClassification\",\n+    \"FlaxAlbertForMultipleChoice\",\n+    \"FlaxAlbertForTokenClassification\",\n+    \"FlaxAlbertForQuestionAnswering\",\n+]"
        },
        {
            "sha": "24a25658a4d41a942776404bbae301008f91b1ab",
            "filename": "src/transformers/models/albert/modeling_tf_albert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -1558,3 +1558,16 @@ def build(self, input_shape=None):\n         if getattr(self, \"classifier\", None) is not None:\n             with tf.name_scope(self.classifier.name):\n                 self.classifier.build([None, None, self.config.hidden_size])\n+\n+\n+__all__ = [\n+    \"TFAlbertPreTrainedModel\",\n+    \"TFAlbertModel\",\n+    \"TFAlbertForPreTraining\",\n+    \"TFAlbertForMaskedLM\",\n+    \"TFAlbertForSequenceClassification\",\n+    \"TFAlbertForTokenClassification\",\n+    \"TFAlbertForQuestionAnswering\",\n+    \"TFAlbertForMultipleChoice\",\n+    \"TFAlbertMainLayer\",\n+]"
        },
        {
            "sha": "4971d0511f47bd6b9a0dcb90441621fb19ae2c42",
            "filename": "src/transformers/models/albert/tokenization_albert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n+from ...utils.import_utils import export\n \n \n logger = logging.get_logger(__name__)\n@@ -32,6 +33,7 @@\n SPIECE_UNDERLINE = \"‚ñÅ\"\n \n \n+@export(backends=(\"sentencepiece\",))\n class AlbertTokenizer(PreTrainedTokenizer):\n     \"\"\"\n     Construct an ALBERT tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).\n@@ -343,3 +345,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n                 fi.write(content_spiece_model)\n \n         return (out_vocab_file,)\n+\n+\n+__all__ = [\"AlbertTokenizer\"]"
        },
        {
            "sha": "6e7b110b0afad7e65fba7be2d951ee7a7fba4788",
            "filename": "src/transformers/models/albert/tokenization_albert_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Ftokenization_albert_fast.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -207,3 +207,6 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n             copyfile(self.vocab_file, out_vocab_file)\n \n         return (out_vocab_file,)\n+\n+\n+__all__ = [\"AlbertTokenizerFast\"]"
        },
        {
            "sha": "aaa64dfb6064b10e820fca01e9e632aa7ecd68c6",
            "filename": "src/transformers/models/align/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 48,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falign%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falign%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2F__init__.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -13,57 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n-_import_structure = {\n-    \"configuration_align\": [\n-        \"AlignConfig\",\n-        \"AlignTextConfig\",\n-        \"AlignVisionConfig\",\n-    ],\n-    \"processing_align\": [\"AlignProcessor\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_align\"] = [\n-        \"AlignModel\",\n-        \"AlignPreTrainedModel\",\n-        \"AlignTextModel\",\n-        \"AlignVisionModel\",\n-    ]\n-\n if TYPE_CHECKING:\n-    from .configuration_align import (\n-        AlignConfig,\n-        AlignTextConfig,\n-        AlignVisionConfig,\n-    )\n-    from .processing_align import AlignProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_align import (\n-            AlignModel,\n-            AlignPreTrainedModel,\n-            AlignTextModel,\n-            AlignVisionModel,\n-        )\n-\n+    from .configuration_align import *\n+    from .modeling_align import *\n+    from .processing_align import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "99fa81b4a9350de1c7611f8eaf3ea6e6b90e09ba",
            "filename": "src/transformers/models/align/configuration_align.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -378,3 +378,6 @@ def from_text_vision_configs(cls, text_config: AlignTextConfig, vision_config: A\n         \"\"\"\n \n         return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n+\n+\n+__all__ = [\"AlignTextConfig\", \"AlignVisionConfig\", \"AlignConfig\"]"
        },
        {
            "sha": "dea035618a3341d96089cd3607a05c628ab47d61",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -1636,3 +1636,6 @@ def forward(\n             text_model_output=text_outputs,\n             vision_model_output=vision_outputs,\n         )\n+\n+\n+__all__ = [\"AlignPreTrainedModel\", \"AlignTextModel\", \"AlignVisionModel\", \"AlignModel\"]"
        },
        {
            "sha": "a5846a87d2369609f5bf8174b68d84820bbb8fd1",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -162,3 +162,6 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"AlignProcessor\"]"
        },
        {
            "sha": "a30de8a2527567b521be3e9b60e99d025571cb18",
            "filename": "src/transformers/models/altclip/__init__.py",
            "status": "modified",
            "additions": 7,
            "deletions": 46,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Faltclip%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Faltclip%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2F__init__.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -13,55 +13,16 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_tokenizers_available, is_torch_available\n-\n-\n-_import_structure = {\n-    \"configuration_altclip\": [\n-        \"AltCLIPConfig\",\n-        \"AltCLIPTextConfig\",\n-        \"AltCLIPVisionConfig\",\n-    ],\n-    \"processing_altclip\": [\"AltCLIPProcessor\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"modeling_altclip\"] = [\n-        \"AltCLIPPreTrainedModel\",\n-        \"AltCLIPModel\",\n-        \"AltCLIPTextModel\",\n-        \"AltCLIPVisionModel\",\n-    ]\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n \n \n if TYPE_CHECKING:\n-    from .configuration_altclip import (\n-        AltCLIPConfig,\n-        AltCLIPTextConfig,\n-        AltCLIPVisionConfig,\n-    )\n-    from .processing_altclip import AltCLIPProcessor\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .modeling_altclip import (\n-            AltCLIPModel,\n-            AltCLIPPreTrainedModel,\n-            AltCLIPTextModel,\n-            AltCLIPVisionModel,\n-        )\n-\n-\n+    from .configuration_altclip import *\n+    from .modeling_altclip import *\n+    from .processing_altclip import *\n else:\n     import sys\n \n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "7333fa63a35280820f3822b104d3ddecb2bdbb0b",
            "filename": "src/transformers/models/altclip/configuration_altclip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -398,3 +398,6 @@ def from_text_vision_configs(cls, text_config: AltCLIPTextConfig, vision_config:\n         \"\"\"\n \n         return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n+\n+\n+__all__ = [\"AltCLIPTextConfig\", \"AltCLIPVisionConfig\", \"AltCLIPConfig\"]"
        },
        {
            "sha": "4ed0930605e8996591f1ca725d1d9968f9a83f92",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -1694,3 +1694,6 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n     mask = input_ids.ne(padding_idx).int()\n     incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n     return incremental_indices.long() + padding_idx\n+\n+\n+__all__ = [\"AltCLIPPreTrainedModel\", \"AltCLIPVisionModel\", \"AltCLIPTextModel\", \"AltCLIPModel\"]"
        },
        {
            "sha": "5343498842832c0da8f0c1989efa590e4ac43853",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -130,3 +130,6 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"AltCLIPProcessor\"]"
        },
        {
            "sha": "5c84f97319ecbb1d8d400b1776fdbfa883b3c0ba",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 182,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -1262,13 +1262,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class BertLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class BertLMHeadModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -1368,13 +1361,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class BigBirdLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class BigBirdModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -1862,13 +1848,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class CanineLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class CanineModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -2230,13 +2209,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class ConvBertLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class ConvBertModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -3144,13 +3116,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class QDQBertLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class QDQBertLMHeadModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -4133,13 +4098,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class FNetLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class FNetModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -4572,13 +4530,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class GPTNeoXLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class GPTNeoXModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -4600,13 +4551,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class GPTNeoXJapaneseLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class GPTNeoXJapaneseModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -5437,13 +5381,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class LongformerSelfAttention(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class LongT5EncoderModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -5584,13 +5521,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class LxmertXLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class M2M100ForConditionalGeneration(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -5675,6 +5605,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class MarianPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class MarkupLMForQuestionAnswering(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -6011,13 +5948,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class MobileBertLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class MobileBertModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -6184,13 +6114,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class MPNetLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class MPNetModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -6562,13 +6485,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class NystromformerLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class NystromformerModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -6993,13 +6909,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class PerceiverLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class PerceiverModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -7469,13 +7378,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class ReformerAttention(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class ReformerForMaskedLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -7497,13 +7399,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class ReformerLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class ReformerModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -7588,13 +7483,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class RemBertLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class RemBertModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -7802,13 +7690,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class RoCBertLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class RoCBertModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -7869,13 +7750,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class RoFormerLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class RoFormerModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -8097,13 +7971,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class SegformerLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class SegformerModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -8314,13 +8181,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class SplinterLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class SplinterModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -8377,13 +8237,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class SqueezeBertModule(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class SqueezeBertPreTrainedModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -9092,13 +8945,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class ViltLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class ViltModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -9176,13 +9022,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class VisualBertLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class VisualBertModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -9232,13 +9071,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class ViTMAELayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class ViTMAEModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -9957,13 +9789,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class YosoLayer(metaclass=DummyObject):\n-    _backends = [\"torch\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"torch\"])\n-\n-\n class YosoModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "7931e0fe6584bbe85ed966911a14bc57c0d43c48",
            "filename": "src/transformers/utils/dummy_sentencepiece_objects.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Futils%2Fdummy_sentencepiece_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Futils%2Fdummy_sentencepiece_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_sentencepiece_objects.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -128,14 +128,14 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"sentencepiece\"])\n \n \n-class MBart50Tokenizer(metaclass=DummyObject):\n+class MBartTokenizer(metaclass=DummyObject):\n     _backends = [\"sentencepiece\"]\n \n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"sentencepiece\"])\n \n \n-class MBartTokenizer(metaclass=DummyObject):\n+class MBart50Tokenizer(metaclass=DummyObject):\n     _backends = [\"sentencepiece\"]\n \n     def __init__(self, *args, **kwargs):"
        },
        {
            "sha": "6e1674c9173e788578d4db61b96c92498ede1e36",
            "filename": "src/transformers/utils/dummy_tf_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Futils%2Fdummy_tf_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Futils%2Fdummy_tf_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_tf_objects.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -478,13 +478,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"tf\"])\n \n \n-class TFBertEmbeddings(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n class TFBertForMaskedLM(metaclass=DummyObject):\n     _backends = [\"tf\"]\n \n@@ -772,13 +765,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"tf\"])\n \n \n-class TFConvBertLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n class TFConvBertModel(metaclass=DummyObject):\n     _backends = [\"tf\"]\n \n@@ -1717,13 +1703,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"tf\"])\n \n \n-class TFLongformerSelfAttention(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n class TFLxmertForPreTraining(metaclass=DummyObject):\n     _backends = [\"tf\"]\n \n@@ -2179,13 +2158,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"tf\"])\n \n \n-class TFRemBertLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n class TFRemBertModel(metaclass=DummyObject):\n     _backends = [\"tf\"]\n \n@@ -2389,13 +2361,6 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"tf\"])\n \n \n-class TFRoFormerLayer(metaclass=DummyObject):\n-    _backends = [\"tf\"]\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"tf\"])\n-\n-\n class TFRoFormerModel(metaclass=DummyObject):\n     _backends = [\"tf\"]\n "
        },
        {
            "sha": "9ebaf5864f3f80fedf175624ac1aef5f7f4de52c",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 471,
            "deletions": 15,
            "changes": 486,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -15,6 +15,7 @@\n Import utilities: Utilities related to imports and our lazy inits.\n \"\"\"\n \n+import importlib.machinery\n import importlib.metadata\n import importlib.util\n import json\n@@ -27,7 +28,7 @@\n from functools import lru_cache\n from itertools import chain\n from types import ModuleType\n-from typing import Any, Optional, Tuple, Union\n+from typing import Any, Dict, FrozenSet, Optional, Set, Tuple, Union\n \n from packaging import version\n \n@@ -1386,6 +1387,11 @@ def is_liger_kernel_available():\n Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n+# docstyle-ignore\n+TORCHAUDIO_IMPORT_ERROR = \"\"\"\n+{0} requires the torchaudio library but it was not found in your environment. Please install it and restart your\n+runtime.\n+\"\"\"\n \n # docstyle-ignore\n PANDAS_IMPORT_ERROR = \"\"\"\n@@ -1561,6 +1567,7 @@ def is_liger_kernel_available():\n         (\"tf\", (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n         (\"tensorflow_text\", (is_tensorflow_text_available, TENSORFLOW_TEXT_IMPORT_ERROR)),\n         (\"timm\", (is_timm_available, TIMM_IMPORT_ERROR)),\n+        (\"torchaudio\", (is_torchaudio_available, TORCHAUDIO_IMPORT_ERROR)),\n         (\"natten\", (is_natten_available, NATTEN_IMPORT_ERROR)),\n         (\"nltk\", (is_nltk_available, NLTK_IMPORT_ERROR)),\n         (\"tokenizers\", (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n@@ -1619,28 +1626,82 @@ def is_torch_fx_proxy(x):\n     return False\n \n \n+BACKENDS_T = FrozenSet[str]\n+IMPORT_STRUCTURE_T = Dict[BACKENDS_T, Dict[str, Set[str]]]\n+\n+\n class _LazyModule(ModuleType):\n     \"\"\"\n     Module class that surfaces all objects but only performs associated imports when the objects are requested.\n     \"\"\"\n \n     # Very heavily inspired by optuna.integration._IntegrationModule\n     # https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py\n-    def __init__(self, name, module_file, import_structure, module_spec=None, extra_objects=None):\n+    def __init__(\n+        self,\n+        name: str,\n+        module_file: str,\n+        import_structure: IMPORT_STRUCTURE_T,\n+        module_spec: importlib.machinery.ModuleSpec = None,\n+        extra_objects: Dict[str, object] = None,\n+    ):\n         super().__init__(name)\n-        self._modules = set(import_structure.keys())\n-        self._class_to_module = {}\n-        for key, values in import_structure.items():\n-            for value in values:\n-                self._class_to_module[value] = key\n-        # Needed for autocompletion in an IDE\n-        self.__all__ = list(import_structure.keys()) + list(chain(*import_structure.values()))\n-        self.__file__ = module_file\n-        self.__spec__ = module_spec\n-        self.__path__ = [os.path.dirname(module_file)]\n-        self._objects = {} if extra_objects is None else extra_objects\n-        self._name = name\n-        self._import_structure = import_structure\n+\n+        self._object_missing_backend = {}\n+        if any(isinstance(key, frozenset) for key in import_structure.keys()):\n+            self._modules = set()\n+            self._class_to_module = {}\n+            self.__all__ = []\n+\n+            _import_structure = {}\n+\n+            for backends, module in import_structure.items():\n+                missing_backends = []\n+                for backend in backends:\n+                    if backend not in BACKENDS_MAPPING:\n+                        raise ValueError(\n+                            f\"Error: the following backend: '{backend}' was specified around object {module} but isn't specified in the backends mapping.\"\n+                        )\n+                    callable, error = BACKENDS_MAPPING[backend]\n+                    if not callable():\n+                        missing_backends.append(backend)\n+                self._modules = self._modules.union(set(module.keys()))\n+\n+                for key, values in module.items():\n+                    if len(missing_backends):\n+                        self._object_missing_backend[key] = missing_backends\n+\n+                    for value in values:\n+                        self._class_to_module[value] = key\n+                        if len(missing_backends):\n+                            self._object_missing_backend[value] = missing_backends\n+                    _import_structure.setdefault(key, []).extend(values)\n+\n+                # Needed for autocompletion in an IDE\n+                self.__all__.extend(list(module.keys()) + list(chain(*module.values())))\n+\n+            self.__file__ = module_file\n+            self.__spec__ = module_spec\n+            self.__path__ = [os.path.dirname(module_file)]\n+            self._objects = {} if extra_objects is None else extra_objects\n+            self._name = name\n+            self._import_structure = _import_structure\n+\n+        # This can be removed once every exportable object has a `export()` export.\n+        else:\n+            self._modules = set(import_structure.keys())\n+            self._class_to_module = {}\n+            for key, values in import_structure.items():\n+                for value in values:\n+                    self._class_to_module[value] = key\n+            # Needed for autocompletion in an IDE\n+            self.__all__ = list(import_structure.keys()) + list(chain(*import_structure.values()))\n+            self.__file__ = module_file\n+            self.__spec__ = module_spec\n+            self.__path__ = [os.path.dirname(module_file)]\n+            self._objects = {} if extra_objects is None else extra_objects\n+            self._name = name\n+            self._import_structure = import_structure\n \n     # Needed for autocompletion in an IDE\n     def __dir__(self):\n@@ -1657,6 +1718,19 @@ def __getattr__(self, name: str) -> Any:\n             return self._objects[name]\n         if name in self._modules:\n             value = self._get_module(name)\n+        elif name in self._object_missing_backend.keys():\n+            missing_backends = self._object_missing_backend[name]\n+\n+            class Placeholder(metaclass=DummyObject):\n+                _backends = missing_backends\n+\n+                def __init__(self, *args, **kwargs):\n+                    requires_backends(self, missing_backends)\n+\n+            Placeholder.__name__ = name\n+            Placeholder.__module__ = self.__spec__\n+\n+            value = Placeholder\n         elif name in self._class_to_module.keys():\n             module = self._get_module(self._class_to_module[name])\n             value = getattr(module, name)\n@@ -1700,3 +1774,385 @@ def direct_transformers_import(path: str, file=\"__init__.py\") -> ModuleType:\n     spec.loader.exec_module(module)\n     module = sys.modules[name]\n     return module\n+\n+\n+def export(*, backends=()):\n+    \"\"\"\n+    This decorator enables two things:\n+    - Attaching a `__backends` tuple to an object to see what are the necessary backends for it\n+      to execute correctly without instantiating it\n+    - The '@export' string is used to dynamically import objects\n+    \"\"\"\n+    for backend in backends:\n+        if backend not in BACKENDS_MAPPING:\n+            raise ValueError(f\"Backend should be defined in the BACKENDS_MAPPING. Offending backend: {backend}\")\n+\n+    if not isinstance(backends, tuple):\n+        raise ValueError(\"Backends should be a tuple.\")\n+\n+    def inner_fn(fun):\n+        fun.__backends = backends\n+        return fun\n+\n+    return inner_fn\n+\n+\n+BASE_FILE_REQUIREMENTS = {\n+    lambda e: \"modeling_tf_\" in e: (\"tf\",),\n+    lambda e: \"modeling_flax_\" in e: (\"flax\",),\n+    lambda e: \"modeling_\" in e: (\"torch\",),\n+    lambda e: e.startswith(\"tokenization_\") and e.endswith(\"_fast\"): (\"tokenizers\",),\n+}\n+\n+\n+def fetch__all__(file_content):\n+    \"\"\"\n+    Returns the content of the __all__ variable in the file content.\n+    Returns None if not defined, otherwise returns a list of strings.\n+    \"\"\"\n+\n+    if \"__all__\" not in file_content:\n+        return []\n+\n+    lines = file_content.splitlines()\n+    for index, line in enumerate(lines):\n+        if line.startswith(\"__all__\"):\n+            start_index = index\n+\n+    lines = lines[start_index:]\n+\n+    if not lines[0].startswith(\"__all__\"):\n+        raise ValueError(\n+            \"fetch__all__ accepts a list of lines, with the first line being the __all__ variable declaration\"\n+        )\n+\n+    # __all__ is defined on a single line\n+    if lines[0].endswith(\"]\"):\n+        return [obj.strip(\"\\\"' \") for obj in lines[0].split(\"=\")[1].strip(\" []\").split(\",\")]\n+\n+    # __all__ is defined on multiple lines\n+    else:\n+        _all = []\n+        for __all__line_index in range(1, len(lines)):\n+            if lines[__all__line_index].strip() == \"]\":\n+                return _all\n+            else:\n+                _all.append(lines[__all__line_index].strip(\"\\\"', \"))\n+\n+        return _all\n+\n+\n+@lru_cache()\n+def create_import_structure_from_path(module_path):\n+    \"\"\"\n+    This method takes the path to a file/a folder and returns the import structure.\n+    If a file is given, it will return the import structure of the parent folder.\n+\n+    Import structures are designed to be digestible by `_LazyModule` objects. They are\n+    created from the __all__ definitions in each files as well as the `@export` decorators\n+    above methods and objects.\n+\n+    The import structure allows explicit display of the required backends for a given object.\n+    These backends are specified in two ways:\n+\n+    1. Through their `@export`, if they are exported with that decorator. This `@export` decorator\n+       accepts a `backend` tuple kwarg mentioning which backends are required to run this object.\n+\n+    2. If an object is defined in a file with \"default\" backends, it will have, at a minimum, this\n+       backend specified. The default backends are defined according to the filename:\n+\n+       - If a file is named like `modeling_*.py`, it will have a `torch` backend\n+       - If a file is named like `modeling_tf_*.py`, it will have a `tf` backend\n+       - If a file is named like `modeling_flax_*.py`, it will have a `flax` backend\n+       - If a file is named like `tokenization_*_fast.py`, it will have a `tokenizers` backend\n+\n+    Backends serve the purpose of displaying a clear error message to the user in case the backends are not installed.\n+    Should an object be imported without its required backends being in the environment, any attempt to use the\n+    object will raise an error mentioning which backend(s) should be added to the environment in order to use\n+    that object.\n+\n+    Here's an example of an input import structure at the src.transformers.models level:\n+\n+    {\n+        'albert': {\n+            frozenset(): {\n+                'configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'}\n+            },\n+            frozenset({'tokenizers'}): {\n+                'tokenization_albert_fast': {'AlbertTokenizerFast'}\n+            },\n+        },\n+        'align': {\n+            frozenset(): {\n+                'configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},\n+                'processing_align': {'AlignProcessor'}\n+            },\n+        },\n+        'altclip': {\n+            frozenset(): {\n+                'configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},\n+                'processing_altclip': {'AltCLIPProcessor'},\n+            }\n+        }\n+    }\n+    \"\"\"\n+    import_structure = {}\n+    if os.path.isdir(module_path):\n+        directory = module_path\n+        adjacent_modules = []\n+\n+        for f in os.listdir(module_path):\n+            if f != \"__pycache__\" and os.path.isdir(os.path.join(module_path, f)):\n+                import_structure[f] = create_import_structure_from_path(os.path.join(module_path, f))\n+\n+            elif not os.path.isdir(os.path.join(directory, f)):\n+                adjacent_modules.append(f)\n+\n+    else:\n+        directory = os.path.dirname(module_path)\n+        adjacent_modules = [f for f in os.listdir(directory) if not os.path.isdir(os.path.join(directory, f))]\n+\n+    # We're only taking a look at files different from __init__.py\n+    # We could theoretically export things directly from the __init__.py\n+    # files, but this is not supported at this time.\n+    if \"__init__.py\" in adjacent_modules:\n+        adjacent_modules.remove(\"__init__.py\")\n+\n+    module_requirements = {}\n+    for module_name in adjacent_modules:\n+        # Only modules ending in `.py` are accepted here.\n+        if not module_name.endswith(\".py\"):\n+            continue\n+\n+        with open(os.path.join(directory, module_name)) as f:\n+            file_content = f.read()\n+\n+        # Remove the .py suffix\n+        module_name = module_name[:-3]\n+\n+        previous_line = \"\"\n+        previous_index = 0\n+\n+        # Some files have some requirements by default.\n+        # For example, any file named `modeling_tf_xxx.py`\n+        # should have TensorFlow as a required backend.\n+        base_requirements = ()\n+        for string_check, requirements in BASE_FILE_REQUIREMENTS.items():\n+            if string_check(module_name):\n+                base_requirements = requirements\n+                break\n+\n+        # Objects that have a `@export` assigned to them will get exported\n+        # with the backends specified in the decorator as well as the file backends.\n+        exported_objects = set()\n+        if \"@export\" in file_content:\n+            lines = file_content.split(\"\\n\")\n+            for index, line in enumerate(lines):\n+                # This allows exporting items with other decorators. We'll take a look\n+                # at the line that follows at the same indentation level.\n+                if line.startswith((\" \", \"\\t\", \"@\", \")\")) and not line.startswith(\"@export\"):\n+                    continue\n+\n+                # Skipping line enables putting whatever we want between the\n+                # export() call and the actual class/method definition.\n+                # This is what enables having # Copied from statements, docs, etc.\n+                skip_line = False\n+\n+                if \"@export\" in previous_line:\n+                    skip_line = False\n+\n+                    # Backends are defined on the same line as export\n+                    if \"backends\" in previous_line:\n+                        backends_string = previous_line.split(\"backends=\")[1].split(\"(\")[1].split(\")\")[0]\n+                        backends = tuple(sorted([b.strip(\"'\\\",\") for b in backends_string.split(\", \") if b]))\n+\n+                    # Backends are defined in the lines following export, for example such as:\n+                    # @export(\n+                    #     backends=(\n+                    #             \"sentencepiece\",\n+                    #             \"torch\",\n+                    #             \"tf\",\n+                    #     )\n+                    # )\n+                    #\n+                    # or\n+                    #\n+                    # @export(\n+                    #     backends=(\n+                    #             \"sentencepiece\", \"tf\"\n+                    #     )\n+                    # )\n+                    elif \"backends\" in lines[previous_index + 1]:\n+                        backends = []\n+                        for backend_line in lines[previous_index:index]:\n+                            if \"backends\" in backend_line:\n+                                backend_line = backend_line.split(\"=\")[1]\n+                            if '\"' in backend_line or \"'\" in backend_line:\n+                                if \", \" in backend_line:\n+                                    backends.extend(backend.strip(\"()\\\"', \") for backend in backend_line.split(\", \"))\n+                                else:\n+                                    backends.append(backend_line.strip(\"()\\\"', \"))\n+\n+                            # If the line is only a ')', then we reached the end of the backends and we break.\n+                            if backend_line.strip() == \")\":\n+                                break\n+                        backends = tuple(backends)\n+\n+                    # No backends are registered for export\n+                    else:\n+                        backends = ()\n+\n+                    backends = frozenset(backends + base_requirements)\n+                    if backends not in module_requirements:\n+                        module_requirements[backends] = {}\n+                    if module_name not in module_requirements[backends]:\n+                        module_requirements[backends][module_name] = set()\n+\n+                    if not line.startswith(\"class\") and not line.startswith(\"def\"):\n+                        skip_line = True\n+                    else:\n+                        start_index = 6 if line.startswith(\"class\") else 4\n+                        object_name = line[start_index:].split(\"(\")[0].strip(\":\")\n+                        module_requirements[backends][module_name].add(object_name)\n+                        exported_objects.add(object_name)\n+\n+                if not skip_line:\n+                    previous_line = line\n+                    previous_index = index\n+\n+        # All objects that are in __all__ should be exported by default.\n+        # These objects are exported with the file backends.\n+        if \"__all__\" in file_content:\n+            for _all_object in fetch__all__(file_content):\n+                if _all_object not in exported_objects:\n+                    backends = frozenset(base_requirements)\n+                    if backends not in module_requirements:\n+                        module_requirements[backends] = {}\n+                    if module_name not in module_requirements[backends]:\n+                        module_requirements[backends][module_name] = set()\n+\n+                    module_requirements[backends][module_name].add(_all_object)\n+\n+    import_structure = {**module_requirements, **import_structure}\n+    return import_structure\n+\n+\n+def spread_import_structure(nested_import_structure):\n+    \"\"\"\n+    This method takes as input an unordered import structure and brings the required backends at the top-level,\n+    aggregating modules and objects under their required backends.\n+\n+    Here's an example of an input import structure at the src.transformers.models level:\n+\n+    {\n+        'albert': {\n+            frozenset(): {\n+                'configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'}\n+            },\n+            frozenset({'tokenizers'}): {\n+                'tokenization_albert_fast': {'AlbertTokenizerFast'}\n+            },\n+        },\n+        'align': {\n+            frozenset(): {\n+                'configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},\n+                'processing_align': {'AlignProcessor'}\n+            },\n+        },\n+        'altclip': {\n+            frozenset(): {\n+                'configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},\n+                'processing_altclip': {'AltCLIPProcessor'},\n+            }\n+        }\n+    }\n+\n+    Here's an example of an output import structure at the src.transformers.models level:\n+\n+    {\n+        frozenset({'tokenizers'}): {\n+            'albert.tokenization_albert_fast': {'AlbertTokenizerFast'}\n+        },\n+        frozenset(): {\n+            'albert.configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'},\n+            'align.processing_align': {'AlignProcessor'},\n+            'align.configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},\n+            'altclip.configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},\n+            'altclip.processing_altclip': {'AltCLIPProcessor'}\n+        }\n+    }\n+\n+    \"\"\"\n+\n+    def propagate_frozenset(unordered_import_structure):\n+        tuple_first_import_structure = {}\n+        for _key, _value in unordered_import_structure.items():\n+            if not isinstance(_value, dict):\n+                tuple_first_import_structure[_key] = _value\n+\n+            elif any(isinstance(v, frozenset) for v in _value.keys()):\n+                # Here we want to switch around key and v\n+                for k, v in _value.items():\n+                    if isinstance(k, frozenset):\n+                        if k not in tuple_first_import_structure:\n+                            tuple_first_import_structure[k] = {}\n+                        tuple_first_import_structure[k][_key] = v\n+\n+            else:\n+                tuple_first_import_structure[_key] = propagate_frozenset(_value)\n+\n+        return tuple_first_import_structure\n+\n+    def flatten_dict(_dict, previous_key=None):\n+        items = []\n+        for _key, _value in _dict.items():\n+            _key = f\"{previous_key}.{_key}\" if previous_key is not None else _key\n+            if isinstance(_value, dict):\n+                items.extend(flatten_dict(_value, _key).items())\n+            else:\n+                items.append((_key, _value))\n+        return dict(items)\n+\n+    # The tuples contain the necessary backends. We want these first, so we propagate them up the\n+    # import structure.\n+    ordered_import_structure = nested_import_structure\n+\n+    # 6 is a number that gives us sufficient depth to go through all files and foreseeable folder depths\n+    # while not taking too long to parse.\n+    for i in range(6):\n+        ordered_import_structure = propagate_frozenset(ordered_import_structure)\n+\n+    # We then flatten the dict so that it references a module path.\n+    flattened_import_structure = {}\n+    for key, value in ordered_import_structure.copy().items():\n+        if isinstance(key, str):\n+            del ordered_import_structure[key]\n+        else:\n+            flattened_import_structure[key] = flatten_dict(value)\n+\n+    return flattened_import_structure\n+\n+\n+def define_import_structure(module_path: str) -> IMPORT_STRUCTURE_T:\n+    \"\"\"\n+    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\n+\n+    Here's an example of an output import structure at the src.transformers.models level:\n+\n+    {\n+        frozenset({'tokenizers'}): {\n+            'albert.tokenization_albert_fast': {'AlbertTokenizerFast'}\n+        },\n+        frozenset(): {\n+            'albert.configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'},\n+            'align.processing_align': {'AlignProcessor'},\n+            'align.configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},\n+            'altclip.configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},\n+            'altclip.processing_altclip': {'AltCLIPProcessor'}\n+        }\n+    }\n+\n+    The import structure is a dict defined with frozensets as keys, and dicts of strings to sets of objects.\n+    \"\"\"\n+    import_structure = create_import_structure_from_path(module_path)\n+    return spread_import_structure(import_structure)"
        },
        {
            "sha": "e7f2f67cc23236a62587583c32bc435fbeb77ab1",
            "filename": "tests/models/longformer/test_modeling_longformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -34,8 +34,8 @@\n         LongformerForSequenceClassification,\n         LongformerForTokenClassification,\n         LongformerModel,\n-        LongformerSelfAttention,\n     )\n+    from transformers.models.longformer.modeling_longformer import LongformerSelfAttention\n \n \n class LongformerModelTester:"
        },
        {
            "sha": "131c077653458f987f75ffa93632e58b4f10c43f",
            "filename": "tests/models/longformer/test_modeling_tf_longformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/tests%2Fmodels%2Flongformer%2Ftest_modeling_tf_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/tests%2Fmodels%2Flongformer%2Ftest_modeling_tf_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_modeling_tf_longformer.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -37,8 +37,8 @@\n         TFLongformerForSequenceClassification,\n         TFLongformerForTokenClassification,\n         TFLongformerModel,\n-        TFLongformerSelfAttention,\n     )\n+    from transformers.models.longformer.modeling_tf_longformer import TFLongformerSelfAttention\n     from transformers.tf_utils import shape_list\n \n "
        },
        {
            "sha": "ba0a9232847a74a2d487923f879d7d58a0b5857e",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -40,11 +40,11 @@\n         ReformerForMaskedLM,\n         ReformerForQuestionAnswering,\n         ReformerForSequenceClassification,\n-        ReformerLayer,\n         ReformerModel,\n         ReformerModelWithLMHead,\n         ReformerTokenizer,\n     )\n+    from transformers.models.reformer.modeling_reformer import ReformerLayer\n \n \n class ReformerModelTester:"
        },
        {
            "sha": "d635619b60758a1ff869299c55424d331b0a25fd",
            "filename": "tests/utils/import_structures/failing_export.py",
            "status": "added",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Fimport_structures%2Ffailing_export.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Fimport_structures%2Ffailing_export.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Fimport_structures%2Ffailing_export.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -0,0 +1,23 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# fmt: off\n+\n+from transformers.utils.import_utils import export\n+\n+\n+@export(backends=(\"random_item_that_should_not_exist\",))\n+class A0:\n+    def __init__(self):\n+        pass"
        },
        {
            "sha": "47f2ba84f1ef0d7358e1293ce2c86c6b0a0cf614",
            "filename": "tests/utils/import_structures/import_structure_raw_register.py",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Fimport_structures%2Fimport_structure_raw_register.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Fimport_structures%2Fimport_structure_raw_register.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Fimport_structures%2Fimport_structure_raw_register.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -0,0 +1,80 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# fmt: off\n+\n+from transformers.utils.import_utils import export\n+\n+\n+@export()\n+class A0:\n+    def __init__(self):\n+        pass\n+\n+\n+@export()\n+def a0():\n+    pass\n+\n+\n+@export(backends=(\"torch\", \"tf\"))\n+class A1:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(backends=(\"torch\", \"tf\"))\n+def a1():\n+    pass\n+\n+\n+@export(\n+    backends=(\"torch\", \"tf\")\n+)\n+class A2:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(\n+    backends=(\"torch\", \"tf\")\n+)\n+def a2():\n+    pass\n+\n+\n+@export(\n+    backends=(\n+        \"torch\",\n+        \"tf\"\n+    )\n+)\n+class A3:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(\n+    backends=(\n+            \"torch\",\n+            \"tf\"\n+    )\n+)\n+def a3():\n+    pass\n+\n+@export(backends=())\n+class A4:\n+    def __init__(self):\n+        pass"
        },
        {
            "sha": "18dfd40193c1ff833b0c6fb54a712a0cc0bc9f1b",
            "filename": "tests/utils/import_structures/import_structure_register_with_comments.py",
            "status": "added",
            "additions": 79,
            "deletions": 0,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_comments.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_comments.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_comments.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -0,0 +1,79 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# fmt: off\n+\n+from transformers.utils.import_utils import export\n+\n+\n+@export()\n+# That's a statement\n+class B0:\n+    def __init__(self):\n+        pass\n+\n+\n+@export()\n+# That's a statement\n+def b0():\n+    pass\n+\n+\n+@export(backends=(\"torch\", \"tf\"))\n+# That's a statement\n+class B1:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(backends=(\"torch\", \"tf\"))\n+# That's a statement\n+def b1():\n+    pass\n+\n+\n+@export(backends=(\"torch\", \"tf\"))\n+# That's a statement\n+class B2:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(backends=(\"torch\", \"tf\"))\n+# That's a statement\n+def b2():\n+    pass\n+\n+\n+@export(\n+    backends=(\n+        \"torch\",\n+        \"tf\"\n+    )\n+)\n+# That's a statement\n+class B3:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(\n+    backends=(\n+        \"torch\",\n+        \"tf\"\n+    )\n+)\n+# That's a statement\n+def b3():\n+    pass"
        },
        {
            "sha": "01842c71a1ff49de6109609b56d1b659572e4d3d",
            "filename": "tests/utils/import_structures/import_structure_register_with_duplicates.py",
            "status": "added",
            "additions": 77,
            "deletions": 0,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_duplicates.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_duplicates.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Fimport_structures%2Fimport_structure_register_with_duplicates.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -0,0 +1,77 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# fmt: off\n+\n+from transformers.utils.import_utils import export\n+\n+\n+@export(backends=(\"torch\", \"torch\"))\n+class C0:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(backends=(\"torch\", \"torch\"))\n+def c0():\n+    pass\n+\n+\n+@export(backends=(\"torch\", \"torch\"))\n+# That's a statement\n+class C1:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(backends=(\"torch\", \"torch\"))\n+# That's a statement\n+def c1():\n+    pass\n+\n+\n+@export(backends=(\"torch\", \"torch\"))\n+# That's a statement\n+class C2:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(backends=(\"torch\", \"torch\"))\n+# That's a statement\n+def c2():\n+    pass\n+\n+\n+@export(\n+    backends=(\n+        \"torch\",\n+        \"torch\"\n+    )\n+)\n+# That's a statement\n+class C3:\n+    def __init__(self):\n+        pass\n+\n+\n+@export(\n+    backends=(\n+        \"torch\",\n+        \"torch\"\n+    )\n+)\n+# That's a statement\n+def c3():\n+    pass"
        },
        {
            "sha": "18f4b8400886534dc9ce0dbfb8728f4fb442c19e",
            "filename": "tests/utils/test_import_structure.py",
            "status": "added",
            "additions": 98,
            "deletions": 0,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Ftest_import_structure.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/tests%2Futils%2Ftest_import_structure.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_import_structure.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -0,0 +1,98 @@\n+import os\n+import unittest\n+from pathlib import Path\n+\n+from transformers.utils.import_utils import define_import_structure, spread_import_structure\n+\n+\n+import_structures = Path(\"import_structures\")\n+\n+\n+def fetch__all__(file_content):\n+    \"\"\"\n+    Returns the content of the __all__ variable in the file content.\n+    Returns None if not defined, otherwise returns a list of strings.\n+    \"\"\"\n+    lines = file_content.split(\"\\n\")\n+    for line_index in range(len(lines)):\n+        line = lines[line_index]\n+        if line.startswith(\"__all__ = \"):\n+            # __all__ is defined on a single line\n+            if line.endswith(\"]\"):\n+                return [obj.strip(\"\\\"' \") for obj in line.split(\"=\")[1].strip(\" []\").split(\",\")]\n+\n+            # __all__ is defined on multiple lines\n+            else:\n+                _all = []\n+                for __all__line_index in range(line_index + 1, len(lines)):\n+                    if lines[__all__line_index].strip() == \"]\":\n+                        return _all\n+                    else:\n+                        _all.append(lines[__all__line_index].strip(\"\\\"', \"))\n+\n+\n+class TestImportStructures(unittest.TestCase):\n+    base_transformers_path = Path(__file__).parent.parent.parent\n+    models_path = base_transformers_path / \"src\" / \"transformers\" / \"models\"\n+    models_import_structure = spread_import_structure(define_import_structure(models_path))\n+\n+    def test_definition(self):\n+        import_structure = define_import_structure(import_structures)\n+        import_structure_definition = {\n+            frozenset(()): {\n+                \"import_structure_raw_register\": {\"A0\", \"a0\", \"A4\"},\n+                \"import_structure_register_with_comments\": {\"B0\", \"b0\"},\n+            },\n+            frozenset((\"tf\", \"torch\")): {\n+                \"import_structure_raw_register\": {\"A1\", \"a1\", \"A2\", \"a2\", \"A3\", \"a3\"},\n+                \"import_structure_register_with_comments\": {\"B1\", \"b1\", \"B2\", \"b2\", \"B3\", \"b3\"},\n+            },\n+            frozenset((\"torch\",)): {\n+                \"import_structure_register_with_duplicates\": {\"C0\", \"c0\", \"C1\", \"c1\", \"C2\", \"c2\", \"C3\", \"c3\"},\n+            },\n+        }\n+\n+        self.assertDictEqual(import_structure, import_structure_definition)\n+\n+    def test_transformers_specific_model_import(self):\n+        \"\"\"\n+        This test ensures that there is equivalence between what is written down in __all__ and what is\n+        written down with register().\n+\n+        It doesn't test the backends attributed to register().\n+        \"\"\"\n+        for architecture in os.listdir(self.models_path):\n+            if (\n+                os.path.isfile(self.models_path / architecture)\n+                or architecture.startswith(\"_\")\n+                or architecture == \"deprecated\"\n+            ):\n+                continue\n+\n+            with self.subTest(f\"Testing arch {architecture}\"):\n+                import_structure = define_import_structure(self.models_path / architecture)\n+                backend_agnostic_import_structure = {}\n+                for requirement, module_object_mapping in import_structure.items():\n+                    for module, objects in module_object_mapping.items():\n+                        if module not in backend_agnostic_import_structure:\n+                            backend_agnostic_import_structure[module] = []\n+\n+                        backend_agnostic_import_structure[module].extend(objects)\n+\n+                for module, objects in backend_agnostic_import_structure.items():\n+                    with open(self.models_path / architecture / f\"{module}.py\") as f:\n+                        content = f.read()\n+                        _all = fetch__all__(content)\n+\n+                        if _all is None:\n+                            raise ValueError(f\"{module} doesn't have __all__ defined.\")\n+\n+                        error_message = (\n+                            f\"self.models_path / architecture / f'{module}.py doesn't seem to be defined correctly:\\n\"\n+                            f\"Defined in __all__: {sorted(_all)}\\nDefined with register: {sorted(objects)}\"\n+                        )\n+                        self.assertListEqual(sorted(objects), sorted(_all), msg=error_message)\n+\n+    def test_export_backend_should_be_defined(self):\n+        with self.assertRaisesRegex(ValueError, \"Backend should be defined in the BACKENDS_MAPPING\"):\n+            pass"
        },
        {
            "sha": "82bf07ce43a9c526131fa4fb3ac884cada8394b6",
            "filename": "utils/custom_init_isort.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f24f08432960023129817328cac6c01151d1f70f/utils%2Fcustom_init_isort.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f24f08432960023129817328cac6c01151d1f70f/utils%2Fcustom_init_isort.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcustom_init_isort.py?ref=f24f08432960023129817328cac6c01151d1f70f",
            "patch": "@@ -244,7 +244,7 @@ def sort_imports(file: str, check_only: bool = True):\n         code = f.read()\n \n     # If the file is not a custom init, there is nothing to do.\n-    if \"_import_structure\" not in code:\n+    if \"_import_structure\" not in code or \"define_import_structure\" in code:\n         return\n \n     # Blocks of indent level 0"
        }
    ],
    "stats": {
        "total": 1490,
        "additions": 937,
        "deletions": 553
    }
}