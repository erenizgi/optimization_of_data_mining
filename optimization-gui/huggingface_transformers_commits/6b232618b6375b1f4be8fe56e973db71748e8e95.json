{
    "author": "merveenoyan",
    "message": "Keypoint matching docs (#40541)\n\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: StevenBucaille <steven.bucaille@gmail.com>",
    "sha": "6b232618b6375b1f4be8fe56e973db71748e8e95",
    "files": [
        {
            "sha": "8fafeaa97f8f52f56032cccaa7a8380e86333e94",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6b232618b6375b1f4be8fe56e973db71748e8e95/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6b232618b6375b1f4be8fe56e973db71748e8e95/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=6b232618b6375b1f4be8fe56e973db71748e8e95",
            "patch": "@@ -277,6 +277,8 @@\n         title: Keypoint detection\n       - local: tasks/knowledge_distillation_for_image_classification\n         title: Knowledge Distillation for Computer Vision\n+      - local: tasks/keypoint_matching\n+        title: Keypoint matching\n       title: Computer vision\n     - sections:\n       - local: tasks/image_captioning"
        },
        {
            "sha": "f7065f3152118ca50be90f5c183150ab0840af61",
            "filename": "docs/source/en/tasks/keypoint_matching.md",
            "status": "added",
            "additions": 129,
            "deletions": 0,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/6b232618b6375b1f4be8fe56e973db71748e8e95/docs%2Fsource%2Fen%2Ftasks%2Fkeypoint_matching.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6b232618b6375b1f4be8fe56e973db71748e8e95/docs%2Fsource%2Fen%2Ftasks%2Fkeypoint_matching.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fkeypoint_matching.md?ref=6b232618b6375b1f4be8fe56e973db71748e8e95",
            "patch": "@@ -0,0 +1,129 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Keypoint matching\n+\n+Keypoint matching matches different points of interests that belong to same object appearing in two different images. Most modern keypoint matchers take images as input and output the following:\n+\n+- **Keypoint coordinates (x,y):** one-to-one mapping of pixel coordinates between the first and the second image using two lists. Each keypoint at a given index in the first list is matched to the keypoint at the same index in the second list.\n+- **Matching scores:** Scores assigned to the keypoint matches.\n+\n+In this tutorial, you will extract keypoint matches with the [`EfficientLoFTR`] model trained with the [MatchAnything framework](https://huggingface.co/zju-community/matchanything_eloftr), and refine the matches. This model is only 16M parameters and can be run on a CPU. You will use the [`AutoModelForKeypointMatching`] class.\n+\n+```python\n+from transformers import AutoImageProcessor, AutoModelForKeypointMatching\n+import torch\n+\n+processor = AutoImageProcessor.from_pretrained(\"zju-community/matchanything_eloftr\")\n+model = AutoModelForKeypointMatching.from_pretrained(\"zju-community/matchanything_eloftr\"))\n+```\n+\n+Load two images that have the same object of interest. The second photo is taken a second apart, it's colors are edited, and it is further cropped and rotated.\n+\n+<div style=\"display: flex; align-items: center;\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\" \n+         alt=\"Bee\" \n+         style=\"height: 200px; object-fit: contain; margin-right: 10px;\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee_edited.jpg\" \n+         alt=\"Bee edited\" \n+         style=\"height: 200px; object-fit: contain;\">\n+</div>\n+\n+```python \n+from transformers.image_utils import load_image\n+image1 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\")\n+image2 = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee_edited.jpg\")\n+\n+images = [image1, image2]\n+```\n+\n+We can pass the images to the processor and infer.\n+\n+```python\n+inputs = processor(images, return_tensors=\"pt\")\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+```\n+\n+We can postprocess the outputs. The threshold parameter is used to refine noise (lower confidence thresholds) in the output matches.\n+\n+```python\n+image_sizes = [[(image.height, image.width) for image in images]]\n+\n+outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n+print(outputs)\n+```\n+\n+Here's the outputs.\n+\n+```\n+[{'keypoints0': tensor([[4514,  550],\n+          [4813,  683],\n+          [1972, 1547],\n+          ...\n+          [3916, 3408]], dtype=torch.int32),\n+  'keypoints1': tensor([[2280,  463],\n+          [2378,  613],\n+          [2231,  887],\n+          ...\n+          [1521, 2560]], dtype=torch.int32),\n+  'matching_scores': tensor([0.2189, 0.2073, 0.2414, ...\n+    ])}]\n+``` \n+\n+We have trimmed the output but there's 401 matches!\n+\n+```python\n+len(outputs[0][\"keypoints0\"])\n+# 401\n+``` \n+\n+We can visualize them using the processor's [`~EfficientLoFTRImageProcessor.visualize_keypoint_matching`] method. \n+\n+```python\n+plot_images = processor.visualize_keypoint_matching(images, outputs)\n+plot_images\n+```\n+\n+![Matched Image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/matched_bees.png)\n+\n+Optionally, you can use the [`Pipeline`] API and set the task to `keypoint-matching`. \n+\n+```python\n+from transformers import pipeline \n+\n+image_1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n+image_2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee_edited.jpg\"\n+\n+pipe = pipeline(\"keypoint-matching\", model=\"zju-community/matchanything_eloftr\")\n+pipe([image_1, image_2])\n+```\n+\n+The output looks like following.\n+\n+```bash\n+[{'keypoint_image_0': {'x': 2444, 'y': 2869},\n+  'keypoint_image_1': {'x': 837, 'y': 1500},\n+  'score': 0.9756593704223633},\n+ {'keypoint_image_0': {'x': 1248, 'y': 2819},\n+  'keypoint_image_1': {'x': 862, 'y': 866},\n+  'score': 0.9735618829727173},\n+ {'keypoint_image_0': {'x': 1547, 'y': 3317},\n+  'keypoint_image_1': {'x': 1436, 'y': 1500},\n+  ...\n+ }\n+]\n+```"
        }
    ],
    "stats": {
        "total": 131,
        "additions": 131,
        "deletions": 0
    }
}