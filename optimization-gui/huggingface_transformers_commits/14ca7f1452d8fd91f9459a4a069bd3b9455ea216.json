{
    "author": "faaany",
    "message": "[docs] fix typo (#36080)\n\ntypo fix",
    "sha": "14ca7f1452d8fd91f9459a4a069bd3b9455ea216",
    "files": [
        {
            "sha": "177e26144589916cd5a089f6a65ee0ab45f61e0c",
            "filename": "docs/source/en/quantization/compressed_tensors.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/14ca7f1452d8fd91f9459a4a069bd3b9455ea216/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/14ca7f1452d8fd91f9459a4a069bd3b9455ea216/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md?ref=14ca7f1452d8fd91f9459a4a069bd3b9455ea216",
            "patch": "@@ -61,7 +61,7 @@ ct_model = AutoModelForCausalLM.from_pretrained(\"nm-testing/Meta-Llama-3.1-8B-In\n \n # Measure memory usage\n mem_params = sum([param.nelement()*param.element_size() for param in ct_model.parameters()])\n-print(f\"{mem/2**30:.4f} GB\")\n+print(f\"{mem_params/2**30:.4f} GB\")\n # 8.4575 GB\n ```\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}