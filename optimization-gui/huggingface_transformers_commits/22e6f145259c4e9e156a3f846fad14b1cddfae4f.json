{
    "author": "Cyrilvallez",
    "message": "Reducing memory usage: removing useless logits computation in generate() (#31292)\n\n* Add .float() in all generation methods logit outputs\r\n\r\n* Switch float-casting of logits to training only for main models\r\n\r\n* Add `num_logits_to_keep` in Llama and add it by default in generate\r\n\r\n* Apply style\r\n\r\n* Add num_logits_to_keep as arg in prepare_input_for_generation\r\n\r\n* Add support for Mistral\r\n\r\n* Revert models except llama and mistral\r\n\r\n* Fix default None value in _supports_num_logits_to_keep()\r\n\r\n* Fix dimension of dummy input\r\n\r\n* Add exception for prophetnet in _supports_num_logits_to_keep()\r\n\r\n* Update _supports_num_logits_to_keep() to use inspect.signature()\r\n\r\n* Add deprecation cycle + remove modification with pretraining_tp\r\n\r\n* Apply style\r\n\r\n* Add most used models\r\n\r\n* Apply style\r\n\r\n* Make `num_logits_to_keep` an int in all cases to remove if-else clause\r\n\r\n* Add compile check for the warning\r\n\r\n* Fix torch versions\r\n\r\n* style\r\n\r\n* Add gemma2\r\n\r\n* Update warning version\r\n\r\n* Add comment about .float operations in generation utils\r\n\r\n* Add tests in GenerationTesterMixin and ModelTesterMixin\r\n\r\n* Fix batch size for assisted decoding in tests\r\n\r\n* fix small issues in test\r\n\r\n* refacor test\r\n\r\n* fix slicing removing dim issue\r\n\r\n* Add nemotron support (should fix check-copy issue in CIs)\r\n\r\n* Trigger new CIs\r\n\r\n* Trigger new CIs\r\n\r\n* Bump version\r\n\r\n* Bump version in TODO\r\n\r\n* Trigger CIs\r\n\r\n* remove blank space\r\n\r\n* Trigger CIs",
    "sha": "22e6f145259c4e9e156a3f846fad14b1cddfae4f",
    "files": [
        {
            "sha": "bf55ae3e2b061efb887fb3b8fde52c2f271f6c96",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -119,6 +119,10 @@ def __init__(\n                     value.detach().to(device) if isinstance(value, torch.Tensor) else copy.deepcopy(value)\n                 )\n \n+        # Remove potential default \"num_logits_to_keep\" key\n+        if \"num_logits_to_keep\" in assistant_kwargs.keys() and not assistant_model._supports_num_logits_to_keep():\n+            del assistant_kwargs[\"num_logits_to_keep\"]\n+\n         if \"assistant_encoder_outputs\" in model_kwargs:\n             assistant_kwargs[\"encoder_outputs\"] = model_kwargs[\"assistant_encoder_outputs\"]\n         elif assistant_model.config.is_encoder_decoder:"
        },
        {
            "sha": "319873e3f7e8e3cd4b9d1eb18dcf102a35cc1448",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 31,
            "deletions": 9,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -1601,6 +1601,13 @@ def _prepare_cache_for_generation(\n                 else EncoderDecoderCache(DynamicCache(), DynamicCache())\n             )\n \n+    def _supports_num_logits_to_keep(self) -> bool:\n+        \"\"\"\n+        Return True if the current model supports the keyword argument `num_logits_to_keep` in forward()\n+        to save memory. Checking it in this way allows to avoid using a new model attribute.\n+        \"\"\"\n+        return \"num_logits_to_keep\" in set(inspect.signature(self.forward).parameters.keys())\n+\n     def _prepare_special_tokens(\n         self,\n         generation_config: GenerationConfig,\n@@ -1876,6 +1883,13 @@ def generate(\n             inputs_tensor=inputs_tensor,\n             input_ids_length=input_ids_length,\n         )\n+\n+        # If the model supports `num_logits_to_keep` in forward(), set it to 1 to avoid computing the whole\n+        # logit matrix. This can save a lot of memory during the first forward pass. Note that assisted decoding\n+        # dynamically overrides this value as it can need more than the last token logits\n+        if self._supports_num_logits_to_keep() and \"num_logits_to_keep\" not in model_kwargs:\n+            model_kwargs[\"num_logits_to_keep\"] = 1\n+\n         self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n \n         # 7. Prepare the cache.\n@@ -2412,8 +2426,9 @@ def _dola_decoding(\n                 output_hidden_states=True,\n             )\n \n-            final_layer_next_token_logits = outputs.logits[:, -1, :].detach().clone()\n-            final_logits = outputs.logits[:, -1, :]\n+            # .float() is needed to retain precision for later logits manipulations\n+            final_layer_next_token_logits = outputs.logits[:, -1, :].detach().clone().float()\n+            final_logits = outputs.logits[:, -1, :].float()\n             candidate_premature_logits = {}\n             for candidate_premature_layer in candidate_premature_layers:\n                 candidate_premature_logits[candidate_premature_layer] = lm_head(\n@@ -2590,7 +2605,8 @@ def _contrastive_search(\n                 # next logit for contrastive search to select top-k candidate tokens\n                 # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for this first iteration\n                 # (the clone itself is always small)\n-                logit_for_next_step = outputs.logits[:, -1, :].clone()\n+                # .float() is needed to retain precision for later logits manipulations\n+                logit_for_next_step = outputs.logits[:, -1, :].clone().float()\n \n                 model_kwargs = self._update_model_kwargs_for_generation(\n                     outputs,\n@@ -2720,7 +2736,8 @@ def _contrastive_search(\n                 next_hidden = outputs.hidden_states[-1]\n                 full_hidden_states = outputs.hidden_states\n \n-            logits = outputs.logits[:, -1, :]\n+            # .float() is needed to retain precision for later logits manipulations\n+            logits = outputs.logits[:, -1, :].float()\n             context_hidden = last_hidden_states.repeat_interleave(top_k, dim=0)\n \n             # compute the degeneration penalty and re-rank the candidates based on the degeneration penalty and the\n@@ -2966,7 +2983,8 @@ def _sample(\n \n             # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n-            next_token_logits = outputs.logits[:, -1, :].clone()\n+            # .float() is needed to retain precision for later logits manipulations\n+            next_token_logits = outputs.logits[:, -1, :].clone().float()\n \n             # pre-process distribution\n             next_token_scores = logits_processor(input_ids, next_token_logits)\n@@ -3210,7 +3228,8 @@ def _beam_search(\n \n             # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n-            next_token_logits = outputs.logits[:, -1, :].clone()\n+            # .float() is needed to retain precision for later logits manipulations\n+            next_token_logits = outputs.logits[:, -1, :].clone().float()\n             next_token_scores = nn.functional.log_softmax(\n                 next_token_logits, dim=-1\n             )  # (batch_size * num_beams, vocab_size)\n@@ -3484,7 +3503,8 @@ def _group_beam_search(\n \n                 # select outputs of beams of current group only\n                 # No need to clone() the logits here as they will not retain outputs.logits at the end of the loop\n-                next_token_logits = outputs.logits[batch_group_indices, -1, :]\n+                # .float() is needed to retain precision for later logits manipulations\n+                next_token_logits = outputs.logits[batch_group_indices, -1, :].float()\n \n                 next_token_scores = nn.functional.log_softmax(\n                     next_token_logits, dim=-1\n@@ -3739,7 +3759,8 @@ def _constrained_beam_search(\n \n             # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n-            next_token_logits = outputs.logits[:, -1, :].clone()\n+            # .float() is needed to retain precision for later logits manipulations\n+            next_token_logits = outputs.logits[:, -1, :].clone().float()\n             next_token_scores = nn.functional.log_softmax(\n                 next_token_logits, dim=-1\n             )  # (batch_size * num_beams, vocab_size)\n@@ -3998,7 +4019,8 @@ def _assisted_decoding(\n             outputs = self(**model_inputs)\n \n             # 2.3. Process the new logits\n-            new_logits = outputs.logits[:, -candidate_length - 1 :]  # excludes the input prompt if present\n+            # .float() is needed to retain precision for later logits manipulations\n+            new_logits = outputs.logits[:, -candidate_length - 1 :].float()  # excludes the input prompt if present\n             next_token_logits = new_logits.clone()\n             if len(logits_processor) > 0:\n                 for i in range(candidate_length + 1):"
        },
        {
            "sha": "ea5fd6749e6ba28c284e9d2c805b94b0976945d7",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -44,6 +44,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1024,6 +1025,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1032,6 +1034,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1071,12 +1078,19 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n         logits = logits * self.logit_scale\n-        logits = logits.float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1109,6 +1123,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1166,6 +1181,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "d2b76b8ef8357666414063f7b367c41c14914807",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -1275,6 +1275,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"Forward function for causal language modeling.\n \n@@ -1284,6 +1285,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1328,7 +1334,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        # No upscaling to float was ever done for Dbrx\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:\n@@ -1380,6 +1387,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1437,6 +1445,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "ea3448e033132a737151a8225e0483ae0e99f403",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 19,
            "deletions": 2,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -43,6 +43,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1038,6 +1039,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1046,6 +1048,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1085,10 +1092,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+\n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1121,6 +1136,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1177,6 +1193,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "bf6ff76189d4a37af09b9ccb08096dd6b8a6d606",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -41,6 +41,7 @@\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -987,6 +988,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -995,6 +997,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1038,15 +1045,23 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n         if self.config.final_logit_softcapping is not None:\n             logits = logits / self.config.final_logit_softcapping\n             logits = torch.tanh(logits)\n             logits = logits * self.config.final_logit_softcapping\n \n+        # TODO: remove the float() operation in v4.46\n         logits = logits.float()\n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1079,6 +1094,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1139,6 +1155,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "4d3db3208d773feef6bc0c19f0252b32f230a80a",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 25,
            "deletions": 3,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -33,6 +33,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1520,6 +1521,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, Idefics2CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1528,6 +1530,12 @@ def forward(\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics2ForConditionalGeneration`).\n                 Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n                 computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1591,11 +1599,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             labels = labels.to(logits.device)\n             # Shift so that tokens < n predict n\n             if attention_mask is not None:\n@@ -1623,7 +1638,13 @@ def forward(\n         )\n \n     def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        num_logits_to_keep=0,\n+        **kwargs,\n     ):\n         past_length = 0\n         # Omit tokens covered by past_key_values\n@@ -1682,6 +1703,7 @@ def prepare_inputs_for_generation(\n                 \"pixel_values\": pixel_values,\n                 \"pixel_attention_mask\": pixel_attention_mask,\n                 \"image_hidden_states\": image_hidden_states,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "2722a5e06909cc5c73433d009dd0356505f6d703",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -50,6 +50,7 @@\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n     is_mamba_ssm_available,\n+    is_torchdynamo_compiling,\n )\n from .configuration_jamba import JambaConfig\n \n@@ -1497,10 +1498,17 @@ def forward(\n             logits = self.lm_head(hidden_states)\n         else:\n             logits = self.lm_head(hidden_states[..., -num_logits_to_keep:, :])\n+        if labels is None and not is_torchdynamo_compiling:\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # TODO: remove the float() operations in v4.46\n         logits = logits.float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()"
        },
        {
            "sha": "244f7471799040543f591f421883029ed5f01659",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -37,6 +37,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1253,6 +1254,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1261,6 +1263,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n         \"\"\"\n \n@@ -1285,11 +1292,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1339,6 +1353,7 @@ def prepare_inputs_for_generation(\n         output_router_logits=False,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1371,6 +1386,7 @@ def prepare_inputs_for_generation(\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n                 \"output_router_logits\": output_router_logits,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "7ce9d2b028b7dfa2268dd8be9618022eb1d09b80",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -44,6 +44,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1146,6 +1147,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1154,6 +1156,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1198,11 +1205,18 @@ def forward(\n             logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n             logits = torch.cat(logits, dim=-1)\n         else:\n-            logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+            if labels is None and not is_torchdynamo_compiling():\n+                logger.warning_once(\n+                    \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+                )\n+            # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+            # TODO: remove the float() operation in v4.46\n+            logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1235,6 +1249,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1292,6 +1307,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "438d8e8a568a948d104f392855a512097ff4b2e3",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -42,6 +42,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -996,6 +997,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1004,6 +1006,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1044,11 +1051,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1081,6 +1095,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1115,6 +1130,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "247b5e10f76231724cc0ca4708e84a47bb87836a",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -43,6 +43,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1233,6 +1234,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1241,6 +1243,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1286,11 +1293,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1339,6 +1353,7 @@ def prepare_inputs_for_generation(\n         output_router_logits=False,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1371,6 +1386,7 @@ def prepare_inputs_for_generation(\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n                 \"output_router_logits\": output_router_logits,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "76edefcf3025f771e9fb7f8b82eafc07c386fd1d",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -42,6 +42,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1029,6 +1030,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1037,6 +1039,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1076,11 +1083,19 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        # TODO: remove the float() operation in v4.46\n         logits = logits.float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1113,6 +1128,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1170,6 +1186,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "edf3eb0ab3fe718dd95dce3e4b480834518ae443",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -42,6 +42,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1068,6 +1069,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1076,6 +1078,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1116,11 +1123,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1153,6 +1167,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1210,6 +1225,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "7d43ff552c373ed257b0612e0a2be0da25cdf136",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -885,6 +885,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -893,6 +894,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -933,7 +939,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        # No upscaling to float was ever done for Persimmon\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:\n@@ -970,6 +977,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1027,6 +1035,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "fbc8720c89c8cf47a4323a38a706845c7a266926",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -41,6 +41,7 @@\n     get_torch_version,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1169,6 +1170,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1177,6 +1179,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1217,11 +1224,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1255,6 +1269,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1312,6 +1327,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "fea30dc191d273638a7291bd06048cf8bd745d3b",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -40,6 +40,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1210,6 +1211,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1218,6 +1220,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1257,11 +1264,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1295,6 +1309,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1352,6 +1367,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "1db1da30f96c67c0e542b7d25dbe2ee4a4b4e90b",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -42,6 +42,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1067,6 +1068,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1075,6 +1077,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1115,11 +1122,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1153,6 +1167,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1210,6 +1225,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "31ea5564461189da5b08c3c07ac31e177aee95e4",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -43,6 +43,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1244,6 +1245,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1252,6 +1254,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1296,11 +1303,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1349,6 +1363,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1406,6 +1421,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "14e7d5cc65fc7ae703e5f7e979a80e785ffaac3f",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -1164,6 +1164,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1172,6 +1173,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1211,7 +1217,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        # No upscaling to float was ever done for StableLm\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:\n@@ -1248,6 +1255,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1305,6 +1313,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "ea3f3be9d8610811559bb41932c2b0aef46249a9",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -42,6 +42,7 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1043,6 +1044,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1051,6 +1053,11 @@ def forward(\n                 config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                 (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n         Returns:\n \n         Example:\n@@ -1091,11 +1098,18 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n+        if labels is None and not is_torchdynamo_compiling():\n+            logger.warning_once(\n+                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n+            )\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        # TODO: remove the float() operation in v4.46\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()\n@@ -1129,6 +1143,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        num_logits_to_keep=0,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1186,6 +1201,7 @@ def prepare_inputs_for_generation(\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n+                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "b0cf08d0530c9bf1478d4c42b5df7e11c6d23da4",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -1828,6 +1828,62 @@ def test_generate_compile_fullgraph(self):\n                 output_compiled = compiled_generate(model_inputs, generation_config=generation_config)\n                 self.assertListEqual(output_dynamic.tolist(), output_compiled.tolist())\n \n+    def test_generate_methods_with_num_logits_to_keep(self):\n+        for model_class in self.all_generative_model_classes:\n+            if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n+                self.skipTest(reason=\"This model does not support `num_logits_to_keep` argument.\")\n+\n+            config, input_ids, attention_mask = self._get_input_ids_and_config()\n+            config.use_cache = True\n+            config.is_decoder = True\n+\n+            model = model_class(config).to(torch_device).eval()\n+            # All generation methods (except assisted decoding) rely on always extracting the last token logits of the\n+            # full logits matrix, so testing out only greedy search and assisted decoding is enough (if it works,\n+            # other methods will work as well)\n+            generation_kwargs = {\n+                \"max_new_tokens\": 10,\n+                \"do_sample\": False,\n+            }\n+\n+            # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n+            with_all_logits = model.generate(\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, num_logits_to_keep=0\n+            )\n+            # By default, num_logits_to_keep is automatically set to 1 if not provided (new behavior)\n+            without_all_logits = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n+\n+    def test_assisted_decoding_with_num_logits_to_keep(self):\n+        for model_class in self.all_generative_model_classes:\n+            if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n+                self.skipTest(reason=\"This model does not support `num_logits_to_keep` argument.\")\n+            if model_class._is_stateful:\n+                self.skipTest(reason=\"Stateful models don't support assisted generation\")\n+\n+            config, input_ids, attention_mask = self._get_input_ids_and_config(batch_size=1)\n+            config.use_cache = True\n+            config.is_decoder = True\n+\n+            model = model_class(config).to(torch_device).eval()\n+            assistant_model = model\n+            # All generation methods (except assisted decoding) rely on always extracting the last token logits of the\n+            # full logits matrix, so testing out only greedy search and assisted decoding is enough (if it works,\n+            # other methods will work as well)\n+            generation_kwargs = {\n+                \"max_new_tokens\": 10,\n+                \"do_sample\": False,\n+                \"assistant_model\": assistant_model,\n+            }\n+\n+            # Setting num_logits_to_keep at 0 keeps all logits (old behavior)\n+            with_all_logits = model.generate(\n+                input_ids, attention_mask=attention_mask, **generation_kwargs, num_logits_to_keep=0\n+            )\n+            # By default, num_logits_to_keep is automatically set to 1 if not provided (new behavior)\n+            without_all_logits = model.generate(input_ids, attention_mask=attention_mask, **generation_kwargs)\n+            self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n+\n     def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n         batch_size, seq_length = input_ids.shape\n         num_sequences_in_output = batch_size * num_return_sequences"
        },
        {
            "sha": "7617c15efabf7bb98eeaa7edc85e247d05e7836f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/22e6f145259c4e9e156a3f846fad14b1cddfae4f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/22e6f145259c4e9e156a3f846fad14b1cddfae4f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=22e6f145259c4e9e156a3f846fad14b1cddfae4f",
            "patch": "@@ -4824,6 +4824,27 @@ def test_compile_cuda_graph_time(self):\n         self.assertTrue(record_time < 0.15 * graph_warmup_time)\n         self.assertTrue(opt_time < record_time)\n \n+    def test_forward_with_num_logits_to_keep(self):\n+        for model_class in self.all_generative_model_classes:\n+            if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n+                self.skipTest(reason=\"This model does not support `num_logits_to_keep` argument.\")\n+\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+            batch_size, sequence_length = inputs[\"input_ids\"].shape\n+            vocab_size = config.vocab_size\n+            model = model_class(config).to(device=torch_device).eval()\n+\n+            # num_logits_to_keep=0 is a special case meaning \"keep all logits\"\n+            all_logits = model(**inputs, num_logits_to_keep=0).logits\n+            last_token_logits = model(**inputs, num_logits_to_keep=1).logits\n+\n+            # Assert all shapes are correct\n+            self.assertEqual(tuple(all_logits.shape), (batch_size, sequence_length, vocab_size))\n+            self.assertEqual(tuple(last_token_logits.shape), (batch_size, 1, vocab_size))\n+\n+            # Assert the last tokens are actually the same\n+            self.assertTrue(torch.allclose(all_logits[:, -1:, :], last_token_logits))\n+\n \n global_rng = random.Random()\n "
        }
    ],
    "stats": {
        "total": 469,
        "additions": 428,
        "deletions": 41
    }
}