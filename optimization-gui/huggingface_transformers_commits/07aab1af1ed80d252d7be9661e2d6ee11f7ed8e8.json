{
    "author": "Cyrilvallez",
    "message": "Remove dead protected imports (#38980)\n\n* remove them\n\n* more",
    "sha": "07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
    "files": [
        {
            "sha": "76193bf7a2ba5ffa3202fdcb8fdd749dd93b89a8",
            "filename": "src/transformers/models/align/configuration_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -14,12 +14,6 @@\n # limitations under the License.\n \"\"\"ALIGN model configuration\"\"\"\n \n-from typing import TYPE_CHECKING\n-\n-\n-if TYPE_CHECKING:\n-    pass\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n "
        },
        {
            "sha": "8b9255d4540fe3c0d53b3234cfadae9e3879cb52",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -52,13 +52,10 @@\n     can_return_tuple,\n     logging,\n )\n-from ...utils.import_utils import is_causal_conv1d_available, is_flash_attn_2_available, is_mamba_2_ssm_available\n+from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n \n-if is_flash_attn_2_available():\n-    pass\n-\n if is_mamba_2_ssm_available():\n     from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n     from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined"
        },
        {
            "sha": "d1ee5c9fb7990146da24fa005b2ca805f6aee9dc",
            "filename": "src/transformers/models/clvp/configuration_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fconfiguration_clvp.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -15,11 +15,7 @@\n \"\"\"CLVP model configuration\"\"\"\n \n import os\n-from typing import TYPE_CHECKING, Union\n-\n-\n-if TYPE_CHECKING:\n-    pass\n+from typing import Union\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging"
        },
        {
            "sha": "98127283a62bdb6310da604f40b182977508ade9",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -33,13 +33,10 @@\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, is_vision_available, logging\n+from ...utils import auto_docstring, can_return_tuple, logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n-if is_vision_available():\n-    pass\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "f3468ca8fac4e0eb69cb378f2e762bba9b9e12d8",
            "filename": "src/transformers/models/mpt/configuration_mpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fmpt%2Fconfiguration_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fmpt%2Fconfiguration_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fconfiguration_mpt.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"Mpt configuration\"\"\"\n \n-from typing import TYPE_CHECKING, Optional, Union\n-\n-\n-if TYPE_CHECKING:\n-    pass\n+from typing import Optional, Union\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import logging"
        },
        {
            "sha": "3447c0ab15168e493218c92520d45cd242765291",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -41,16 +41,12 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...utils.import_utils import is_cv2_available, is_vision_available\n+from ...utils.import_utils import is_vision_available\n \n \n logger = logging.get_logger(__name__)\n \n \n-if is_cv2_available():\n-    pass\n-\n-\n if is_vision_available():\n     import PIL\n "
        },
        {
            "sha": "0629fe2ad192244feff474106feb1c786db27201",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -25,7 +25,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, is_flash_attn_available\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -44,10 +44,6 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-if is_flash_attn_available():\n-    pass\n-\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "310a46508b84ac1952008f67a1a0845ad9512e18",
            "filename": "src/transformers/models/owlv2/configuration_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fowlv2%2Fconfiguration_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fowlv2%2Fconfiguration_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fconfiguration_owlv2.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -14,12 +14,6 @@\n # limitations under the License.\n \"\"\"OWLv2 model configuration\"\"\"\n \n-from typing import TYPE_CHECKING\n-\n-\n-if TYPE_CHECKING:\n-    pass\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n "
        },
        {
            "sha": "b05206713ca9b80a675ce8b4915ebbe765625ba5",
            "filename": "src/transformers/models/seggpt/image_processing_seggpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -32,15 +32,12 @@\n     to_numpy_array,\n     valid_images,\n )\n-from ...utils import TensorType, is_torch_available, is_vision_available, logging, requires_backends\n+from ...utils import TensorType, is_torch_available, logging, requires_backends\n \n \n if is_torch_available():\n     import torch\n \n-if is_vision_available():\n-    pass\n-\n \n logger = logging.get_logger(__name__)\n "
        },
        {
            "sha": "eaa5aebe8463ca9bded292c386fcae8366da022f",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -22,7 +22,6 @@\n     AutoProcessor,\n     AyaVisionConfig,\n     is_torch_available,\n-    is_vision_available,\n )\n from transformers.testing_utils import (\n     Expectations,\n@@ -51,10 +50,6 @@\n     )\n \n \n-if is_vision_available():\n-    pass\n-\n-\n class AyaVisionVisionText2TextModelTester:\n     def __init__(\n         self,"
        },
        {
            "sha": "f385ab4830e94f4ee318aac449f2356cd7ccc0a1",
            "filename": "tests/models/internvl/test_video_processor_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Finternvl%2Ftest_video_processor_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Finternvl%2Ftest_video_processor_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_video_processor_internvl.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -17,14 +17,11 @@\n \n from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n \n \n-if is_torch_available():\n-    pass\n-\n if is_vision_available():\n     if is_torchvision_available():\n         from transformers import InternVLVideoProcessor"
        },
        {
            "sha": "88fcc8888dbd72494273f285a57814ccd34d2de7",
            "filename": "tests/models/janus/test_processor_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fjanus%2Ftest_processor_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fjanus%2Ftest_processor_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_processor_janus.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -20,15 +20,10 @@\n import numpy as np\n \n from transformers import AutoProcessor, AutoTokenizer, JanusProcessor\n-from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n-if is_vision_available():\n-    pass\n-\n-\n class JanusProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = JanusProcessor\n "
        },
        {
            "sha": "95f2b658b759e2a5523faa625330f28ea83292f5",
            "filename": "tests/models/llama4/test_image_processing_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllama4%2Ftest_image_processing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllama4%2Ftest_image_processing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_image_processing_llama4.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -16,14 +16,11 @@\n import unittest\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n-if is_torch_available():\n-    pass\n-\n if is_vision_available() and is_torchvision_available():\n     from transformers import Llama4ImageProcessorFast\n "
        },
        {
            "sha": "d89601d78bdabfcdedf36b8d763d3ebec8459675",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -18,17 +18,14 @@\n \n from transformers import AutoProcessor, AutoTokenizer, LlamaTokenizerFast, LlavaProcessor\n from transformers.testing_utils import require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n     from transformers import CLIPImageProcessor\n \n-if is_torch_available:\n-    pass\n-\n \n @require_vision\n class LlavaProcessorTest(ProcessorTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "b902b8c496c2f1187ba88fa615387bf5fc3c7856",
            "filename": "tests/models/llava_next_video/test_processor_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -21,7 +21,7 @@\n \n from transformers import AutoProcessor, LlamaTokenizerFast, LlavaNextVideoProcessor\n from transformers.testing_utils import require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n@@ -32,9 +32,6 @@\n     if is_torchvision_available():\n         from transformers import LlavaNextVideoVideoProcessor\n \n-if is_torch_available:\n-    pass\n-\n \n @require_vision\n class LlavaNextVideoProcessorTest(ProcessorTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "d0ee9658b43035600f89cfbf92e7c8ab871ae21c",
            "filename": "tests/models/llava_next_video/test_video_processing_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava_next_video%2Ftest_video_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava_next_video%2Ftest_video_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_video_processing_llava_next_video.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -17,14 +17,11 @@\n \n from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n \n \n-if is_torch_available():\n-    pass\n-\n if is_vision_available():\n     if is_torchvision_available():\n         from transformers import LlavaNextVideoVideoProcessor"
        },
        {
            "sha": "52f2b99f92ac7bc71b5942908b5ea1c4477b77ab",
            "filename": "tests/models/llava_onevision/test_processor_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -20,7 +20,7 @@\n import torch\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n@@ -36,9 +36,6 @@\n     if is_torchvision_available():\n         from transformers import LlavaOnevisionVideoProcessor\n \n-if is_torch_available:\n-    pass\n-\n \n @require_vision\n @require_torch"
        },
        {
            "sha": "386b288ff0a628623f30e5a8a0a9ccdb9dc8e6a6",
            "filename": "tests/models/llava_onevision/test_video_processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava_onevision%2Ftest_video_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fllava_onevision%2Ftest_video_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_video_processing_llava_onevision.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -17,14 +17,11 @@\n \n from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n \n \n-if is_torch_available():\n-    pass\n-\n if is_vision_available():\n     if is_torchvision_available():\n         from transformers import LlavaOnevisionVideoProcessor"
        },
        {
            "sha": "d000b7854cb346cf8082088822d19ae4c5730158",
            "filename": "tests/models/mistral3/test_processor_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -20,7 +20,7 @@\n \n from transformers import PixtralProcessor\n from transformers.testing_utils import require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n@@ -29,10 +29,6 @@\n     import torch\n \n \n-if is_vision_available():\n-    pass\n-\n-\n @require_vision\n class Mistral3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     \"\"\"This tests Pixtral processor with the new `spatial_merge_size` argument in Mistral3.\"\"\""
        },
        {
            "sha": "d40a6ec17e075a89ef8cfd4fc945fafe670693d3",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -23,7 +23,6 @@\n     PaliGemmaConfig,\n     PaliGemmaForConditionalGeneration,\n     is_torch_available,\n-    is_vision_available,\n )\n from transformers.testing_utils import (\n     is_flaky,\n@@ -40,10 +39,6 @@\n     import torch\n \n \n-if is_vision_available():\n-    pass\n-\n-\n class PaliGemma2VisionText2TextModelTester:\n     def __init__(\n         self,"
        },
        {
            "sha": "95832c72c2f275434e8cfd926209056cb4fec3e7",
            "filename": "tests/models/qwen2_audio/test_processor_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -17,15 +17,10 @@\n \n from transformers import AutoProcessor, AutoTokenizer, Qwen2AudioProcessor, WhisperFeatureExtractor\n from transformers.testing_utils import require_torch, require_torchaudio\n-from transformers.utils import is_torch_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n-if is_torch_available:\n-    pass\n-\n-\n @require_torch\n @require_torchaudio\n class Qwen2AudioProcessorTest(ProcessorTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "c38f38f3d8bbfac8433a624bc7d8f3433590229a",
            "filename": "tests/models/timesfm/test_modeling_timesfm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimesfm%2Ftest_modeling_timesfm.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -21,15 +21,11 @@\n \n from transformers import TimesFmConfig, is_torch_available\n from transformers.testing_utils import require_torch, slow, torch_device\n-from transformers.utils import is_torch_fx_available\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin\n \n \n-if is_torch_fx_available():\n-    pass\n-\n if is_torch_available():\n     from transformers import TimesFmModelForPrediction\n "
        },
        {
            "sha": "70784beca4ceb9e059f922c9f2329a690b65ed3e",
            "filename": "tests/models/video_llava/test_video_processing_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fvideo_llava%2Ftest_video_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fvideo_llava%2Ftest_video_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_video_processing_video_llava.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -17,14 +17,11 @@\n \n from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n \n \n-if is_torch_available():\n-    pass\n-\n if is_vision_available():\n     if is_torchvision_available():\n         from transformers import VideoLlavaVideoProcessor"
        },
        {
            "sha": "64ff79a68eca11e3c1df8da3c9e84792d0da3851",
            "filename": "tests/models/vitpose_backbone/test_modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py?ref=07aab1af1ed80d252d7be9661e2d6ee11f7ed8e8",
            "patch": "@@ -18,7 +18,7 @@\n \n from transformers import VitPoseBackboneConfig\n from transformers.testing_utils import require_torch, torch_device\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available\n \n from ...test_backbone_common import BackboneTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -31,10 +31,6 @@\n     from transformers import VitPoseBackbone\n \n \n-if is_vision_available():\n-    pass\n-\n-\n class VitPoseBackboneModelTester:\n     def __init__(\n         self,"
        }
    ],
    "stats": {
        "total": 127,
        "additions": 17,
        "deletions": 110
    }
}