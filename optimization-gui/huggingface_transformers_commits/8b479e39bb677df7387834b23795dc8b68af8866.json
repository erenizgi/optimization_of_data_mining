{
    "author": "unknown",
    "message": "Saving `Trainer.collator.tokenizer` in when `Trainer.processing_class` is `None` (#36552)\n\n* feat: Saving tokenizer in collator when processing_class is None\n\n* chore: Style issue\n\n* chore: Typo\n\n* dbg: Check why test failed\n\n* dbg: Remove logics and another test failed which successed before, so should be the stablibility issue\n\n* test: Init unit-test\n\n* chore: Style\n\n* chore: Add err log\n\n* fix: Case\n\n* Update tests/trainer/test_trainer.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* chore: Try to use get_regression_trainer\n\n* fix: Impl and style\n\n* fix: Style\n\n* fix: Case\n\n* fix: Import err\n\n* fix: Missed import\n\n* fix: Import block un-sorted problem\n\n* fix: Try another tokenizer\n\n* fix: Test logic\n\n* chore: Light updates\n\n* chore: Reformat\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "8b479e39bb677df7387834b23795dc8b68af8866",
    "files": [
        {
            "sha": "fe61498635c8a3804d8d6ad282de71fb2809802f",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b479e39bb677df7387834b23795dc8b68af8866/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b479e39bb677df7387834b23795dc8b68af8866/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=8b479e39bb677df7387834b23795dc8b68af8866",
            "patch": "@@ -3992,6 +3992,13 @@ def _save(self, output_dir: Optional[str] = None, state_dict=None):\n \n         if self.processing_class is not None:\n             self.processing_class.save_pretrained(output_dir)\n+        elif (\n+            self.data_collator is not None\n+            and hasattr(self.data_collator, \"tokenizer\")\n+            and self.data_collator.tokenizer is not None\n+        ):\n+            logger.info(\"Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\")\n+            self.data_collator.tokenizer.save_pretrained(output_dir)\n \n         # Good practice: save your training arguments together with the trained model\n         torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))"
        },
        {
            "sha": "254202142565341d2427de75ff61c2062b387b56",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b479e39bb677df7387834b23795dc8b68af8866/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b479e39bb677df7387834b23795dc8b68af8866/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=8b479e39bb677df7387834b23795dc8b68af8866",
            "patch": "@@ -28,7 +28,7 @@\n from functools import partial\n from itertools import product\n from pathlib import Path\n-from typing import Dict, List\n+from typing import Any, Dict, List\n from unittest.mock import Mock, patch\n \n import numpy as np\n@@ -46,6 +46,7 @@\n     PretrainedConfig,\n     TrainerCallback,\n     TrainingArguments,\n+    default_data_collator,\n     enable_full_determinism,\n     get_polynomial_decay_schedule_with_warmup,\n     is_torch_available,\n@@ -2975,6 +2976,24 @@ def test_safe_checkpoints(self):\n                 tmp_dir, 5, int(self.n_epochs * 64 / self.batch_size), False, safe_weights=save_safetensors\n             )\n \n+    def test_save_collator_tokenizer_by_default(self):\n+        class FakeCollator:\n+            def __init__(self):\n+                self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n+                self.tokenizer.add_tokens([\"<NEW_TOKEN1>\", \"<NEW_TOKEN2>\"])\n+\n+            def __call__(self, features: List[Any], return_tensors=\"pt\") -> Dict[str, Any]:\n+                return default_data_collator(features, return_tensors)\n+\n+        data_collator = FakeCollator()\n+        tmp_dir = self.get_auto_remove_tmp_dir()\n+        trainer = get_regression_trainer(\n+            output_dir=tmp_dir, save_steps=5, save_safetensors=True, data_collator=data_collator\n+        )\n+        trainer.train()\n+        loaded_tokenizer = AutoTokenizer.from_pretrained(os.path.join(tmp_dir, os.listdir(tmp_dir)[0]))\n+        assert len(loaded_tokenizer) == len(trainer.data_collator.tokenizer), \"Failed to load updated tokenizer\"\n+\n     def test_load_best_model_with_save(self):\n         tmp_dir = self.get_auto_remove_tmp_dir()\n         trainer = get_regression_trainer("
        }
    ],
    "stats": {
        "total": 28,
        "additions": 27,
        "deletions": 1
    }
}