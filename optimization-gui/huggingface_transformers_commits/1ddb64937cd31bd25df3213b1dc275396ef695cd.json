{
    "author": "qubvel",
    "message": "Fix fp16 ONNX export for RT-DETR and RT-DETRv2 (#36460)\n\n* Fix FP16 ONNX export\n\n* Fix typo\n\n* Sync omdet-turbo\n\n* Refactor encoder for better readability\n\n* Fix _no_split_modules\n\n* Fix int -> torch_int\n\n* Fix rt_detr\n\n* Apply to rt-detr-v2\n\n* Fixup\n\n* Fix copies",
    "sha": "1ddb64937cd31bd25df3213b1dc275396ef695cd",
    "files": [
        {
            "sha": "39849d5584477bc73273ef15180b9588fb9f953e",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ddb64937cd31bd25df3213b1dc275396ef695cd/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ddb64937cd31bd25df3213b1dc275396ef695cd/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=1ddb64937cd31bd25df3213b1dc275396ef695cd",
            "patch": "@@ -575,10 +575,9 @@ def __init__(self, config: OmDetTurboConfig):\n             self.conv3 = nn.Identity()\n \n     def forward(self, hidden_state):\n-        device = hidden_state.device\n         hidden_state_1 = self.conv1(hidden_state)\n-        hidden_state_1 = self.bottlenecks(hidden_state_1).to(device)\n-        hidden_state_2 = self.conv2(hidden_state).to(device)\n+        hidden_state_1 = self.bottlenecks(hidden_state_1)\n+        hidden_state_2 = self.conv2(hidden_state)\n         return self.conv3(hidden_state_1 + hidden_state_2)\n \n "
        },
        {
            "sha": "7a2ee51aa652ca2205a5c1ab863353269a44b322",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 76,
            "deletions": 54,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ddb64937cd31bd25df3213b1dc275396ef695cd/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ddb64937cd31bd25df3213b1dc275396ef695cd/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=1ddb64937cd31bd25df3213b1dc275396ef695cd",
            "patch": "@@ -42,6 +42,7 @@\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ...utils.backbone_utils import load_backbone\n from .configuration_rt_detr import RTDetrConfig\n@@ -500,7 +501,7 @@ def get_contrastive_denoising_training_group(\n         denoise_positive_idx, [n * num_groups_denoising_queries for n in num_ground_truths]\n     )\n     # total denoising queries\n-    num_denoising_queries = int(max_gt_num * 2 * num_groups_denoising_queries)\n+    num_denoising_queries = torch_int(max_gt_num * 2 * num_groups_denoising_queries)\n \n     if label_noise_ratio > 0:\n         mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)\n@@ -721,10 +722,9 @@ def __init__(self, config: RTDetrConfig):\n             self.conv3 = nn.Identity()\n \n     def forward(self, hidden_state):\n-        device = hidden_state.device\n         hidden_state_1 = self.conv1(hidden_state)\n-        hidden_state_1 = self.bottlenecks(hidden_state_1).to(device)\n-        hidden_state_2 = self.conv2(hidden_state).to(device)\n+        hidden_state_1 = self.bottlenecks(hidden_state_1)\n+        hidden_state_2 = self.conv2(hidden_state)\n         return self.conv3(hidden_state_1 + hidden_state_2)\n \n \n@@ -1129,7 +1129,7 @@ class RTDetrPreTrainedModel(PreTrainedModel):\n     config_class = RTDetrConfig\n     base_model_prefix = \"rt_detr\"\n     main_input_name = \"pixel_values\"\n-    _no_split_modules = [r\"RTDetrConvEncoder\", r\"RTDetrEncoderLayer\", r\"RTDetrDecoderLayer\"]\n+    _no_split_modules = [r\"RTDetrHybridEncoder\", r\"RTDetrDecoderLayer\"]\n \n     def _init_weights(self, module):\n         \"\"\"Initalize the weights\"\"\"\n@@ -1280,43 +1280,56 @@ def __init__(self, config: RTDetrConfig):\n         self.eval_size = config.eval_size\n         self.out_channels = [self.encoder_hidden_dim for _ in self.in_channels]\n         self.out_strides = self.feat_strides\n-        activation_function = config.activation_function\n+        self.num_fpn_stages = len(self.in_channels) - 1\n+        self.num_pan_stages = len(self.in_channels) - 1\n+        activation = config.activation_function\n \n         # encoder transformer\n         self.encoder = nn.ModuleList([RTDetrEncoder(config) for _ in range(len(self.encode_proj_layers))])\n-        # top-down fpn\n+\n+        # top-down FPN\n         self.lateral_convs = nn.ModuleList()\n         self.fpn_blocks = nn.ModuleList()\n-        for _ in range(len(self.in_channels) - 1, 0, -1):\n-            self.lateral_convs.append(\n-                RTDetrConvNormLayer(\n-                    config, self.encoder_hidden_dim, self.encoder_hidden_dim, 1, 1, activation=activation_function\n-                )\n+        for _ in range(self.num_fpn_stages):\n+            lateral_conv = RTDetrConvNormLayer(\n+                config,\n+                in_channels=self.encoder_hidden_dim,\n+                out_channels=self.encoder_hidden_dim,\n+                kernel_size=1,\n+                stride=1,\n+                activation=activation,\n             )\n-            self.fpn_blocks.append(RTDetrCSPRepLayer(config))\n+            fpn_block = RTDetrCSPRepLayer(config)\n+            self.lateral_convs.append(lateral_conv)\n+            self.fpn_blocks.append(fpn_block)\n \n-        # bottom-up pan\n+        # bottom-up PAN\n         self.downsample_convs = nn.ModuleList()\n         self.pan_blocks = nn.ModuleList()\n-        for _ in range(len(self.in_channels) - 1):\n-            self.downsample_convs.append(\n-                RTDetrConvNormLayer(\n-                    config, self.encoder_hidden_dim, self.encoder_hidden_dim, 3, 2, activation=activation_function\n-                )\n+        for _ in range(self.num_pan_stages):\n+            downsample_conv = RTDetrConvNormLayer(\n+                config,\n+                in_channels=self.encoder_hidden_dim,\n+                out_channels=self.encoder_hidden_dim,\n+                kernel_size=3,\n+                stride=2,\n+                activation=activation,\n             )\n-            self.pan_blocks.append(RTDetrCSPRepLayer(config))\n+            pan_block = RTDetrCSPRepLayer(config)\n+            self.downsample_convs.append(downsample_conv)\n+            self.pan_blocks.append(pan_block)\n \n     @staticmethod\n     def build_2d_sincos_position_embedding(\n         width, height, embed_dim=256, temperature=10000.0, device=\"cpu\", dtype=torch.float32\n     ):\n-        grid_w = torch.arange(int(width), dtype=dtype, device=device)\n-        grid_h = torch.arange(int(height), dtype=dtype, device=device)\n+        grid_w = torch.arange(torch_int(width), device=device).to(dtype)\n+        grid_h = torch.arange(torch_int(height), device=device).to(dtype)\n         grid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing=\"ij\")\n         if embed_dim % 4 != 0:\n             raise ValueError(\"Embed dimension must be divisible by 4 for 2D sin-cos position embedding\")\n         pos_dim = embed_dim // 4\n-        omega = torch.arange(pos_dim, dtype=dtype, device=device) / pos_dim\n+        omega = torch.arange(pos_dim, device=device).to(dtype) / pos_dim\n         omega = 1.0 / (temperature**omega)\n \n         out_w = grid_w.flatten()[..., None] @ omega[None]\n@@ -1372,6 +1385,7 @@ def forward(\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n+\n         # encoder\n         if self.config.encoder_layers > 0:\n             for i, enc_ind in enumerate(self.encode_proj_layers):\n@@ -1407,30 +1421,37 @@ def forward(\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states[enc_ind],)\n \n-        # broadcasting and fusion\n+        # top-down FPN\n         fpn_feature_maps = [hidden_states[-1]]\n-        for idx in range(len(self.in_channels) - 1, 0, -1):\n-            feat_high = fpn_feature_maps[0]\n-            feat_low = hidden_states[idx - 1]\n-            feat_high = self.lateral_convs[len(self.in_channels) - 1 - idx](feat_high)\n-            fpn_feature_maps[0] = feat_high\n-            upsample_feat = F.interpolate(feat_high, scale_factor=2.0, mode=\"nearest\")\n-            fps_map = self.fpn_blocks[len(self.in_channels) - 1 - idx](torch.concat([upsample_feat, feat_low], dim=1))\n-            fpn_feature_maps.insert(0, fps_map)\n-\n-        fpn_states = [fpn_feature_maps[0]]\n-        for idx in range(len(self.in_channels) - 1):\n-            feat_low = fpn_states[-1]\n-            feat_high = fpn_feature_maps[idx + 1]\n-            downsample_feat = self.downsample_convs[idx](feat_low)\n-            hidden_states = self.pan_blocks[idx](\n-                torch.concat([downsample_feat, feat_high.to(downsample_feat.device)], dim=1)\n-            )\n-            fpn_states.append(hidden_states)\n+        for idx, (lateral_conv, fpn_block) in enumerate(zip(self.lateral_convs, self.fpn_blocks)):\n+            backbone_feature_map = hidden_states[self.num_fpn_stages - idx - 1]\n+            top_fpn_feature_map = fpn_feature_maps[-1]\n+            # apply lateral block\n+            top_fpn_feature_map = lateral_conv(top_fpn_feature_map)\n+            fpn_feature_maps[-1] = top_fpn_feature_map\n+            # apply fpn block\n+            top_fpn_feature_map = F.interpolate(top_fpn_feature_map, scale_factor=2.0, mode=\"nearest\")\n+            fused_feature_map = torch.concat([top_fpn_feature_map, backbone_feature_map], dim=1)\n+            new_fpn_feature_map = fpn_block(fused_feature_map)\n+            fpn_feature_maps.append(new_fpn_feature_map)\n+\n+        fpn_feature_maps = fpn_feature_maps[::-1]\n+\n+        # bottom-up PAN\n+        pan_feature_maps = [fpn_feature_maps[0]]\n+        for idx, (downsample_conv, pan_block) in enumerate(zip(self.downsample_convs, self.pan_blocks)):\n+            top_pan_feature_map = pan_feature_maps[-1]\n+            fpn_feature_map = fpn_feature_maps[idx + 1]\n+            downsampled_feature_map = downsample_conv(top_pan_feature_map)\n+            fused_feature_map = torch.concat([downsampled_feature_map, fpn_feature_map], dim=1)\n+            new_pan_feature_map = pan_block(fused_feature_map)\n+            pan_feature_maps.append(new_pan_feature_map)\n \n         if not return_dict:\n-            return tuple(v for v in [fpn_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(last_hidden_state=fpn_states, hidden_states=encoder_states, attentions=all_attentions)\n+            return tuple(v for v in [pan_feature_maps, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=pan_feature_maps, hidden_states=encoder_states, attentions=all_attentions\n+        )\n \n \n class RTDetrDecoder(RTDetrPreTrainedModel):\n@@ -1719,13 +1740,14 @@ def generate_anchors(self, spatial_shapes=None, grid_size=0.05, device=\"cpu\", dt\n         anchors = []\n         for level, (height, width) in enumerate(spatial_shapes):\n             grid_y, grid_x = torch.meshgrid(\n-                torch.arange(end=height, dtype=dtype, device=device),\n-                torch.arange(end=width, dtype=dtype, device=device),\n+                torch.arange(end=height, device=device).to(dtype),\n+                torch.arange(end=width, device=device).to(dtype),\n                 indexing=\"ij\",\n             )\n             grid_xy = torch.stack([grid_x, grid_y], -1)\n-            valid_wh = torch.tensor([width, height], device=device).to(dtype)\n-            grid_xy = (grid_xy.unsqueeze(0) + 0.5) / valid_wh\n+            grid_xy = grid_xy.unsqueeze(0) + 0.5\n+            grid_xy[..., 0] /= width\n+            grid_xy[..., 1] /= height\n             wh = torch.ones_like(grid_xy) * grid_size * (2.0**level)\n             anchors.append(torch.concat([grid_xy, wh], -1).reshape(-1, height * width, 4))\n         # define the valid range for anchor coordinates\n@@ -1826,14 +1848,15 @@ def forward(\n         # Prepare encoder inputs (by flattening)\n         source_flatten = []\n         spatial_shapes_list = []\n+        spatial_shapes = torch.empty((len(sources), 2), device=device, dtype=torch.long)\n         for level, source in enumerate(sources):\n-            batch_size, num_channels, height, width = source.shape\n-            spatial_shape = (height, width)\n-            spatial_shapes_list.append(spatial_shape)\n+            height, width = source.shape[-2:]\n+            spatial_shapes[level, 0] = height\n+            spatial_shapes[level, 1] = width\n+            spatial_shapes_list.append((height, width))\n             source = source.flatten(2).transpose(1, 2)\n             source_flatten.append(source)\n         source_flatten = torch.cat(source_flatten, 1)\n-        spatial_shapes = torch.as_tensor(spatial_shapes_list, dtype=torch.long, device=source_flatten.device)\n         level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n \n         # prepare denoising training\n@@ -1867,8 +1890,7 @@ def forward(\n             anchors, valid_mask = self.generate_anchors(spatial_shapes_tuple, device=device, dtype=dtype)\n         else:\n             anchors, valid_mask = self.anchors, self.valid_mask\n-\n-        anchors, valid_mask = anchors.to(device, dtype), valid_mask.to(device, dtype)\n+            anchors, valid_mask = anchors.to(device, dtype), valid_mask.to(device, dtype)\n \n         # use the valid_mask to selectively retain values in the feature map where the mask is `True`\n         memory = valid_mask.to(source_flatten.dtype) * source_flatten"
        },
        {
            "sha": "514ea362eb8f6d55eaa5c5a8aad8caa8ccf80dbf",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 76,
            "deletions": 54,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ddb64937cd31bd25df3213b1dc275396ef695cd/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ddb64937cd31bd25df3213b1dc275396ef695cd/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=1ddb64937cd31bd25df3213b1dc275396ef695cd",
            "patch": "@@ -39,6 +39,7 @@\n     add_start_docstrings_to_model_forward,\n     is_torchdynamo_compiling,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ...utils.backbone_utils import load_backbone\n from .configuration_rt_detr_v2 import RTDetrV2Config\n@@ -899,10 +900,9 @@ def __init__(self, config: RTDetrV2Config):\n             self.conv3 = nn.Identity()\n \n     def forward(self, hidden_state):\n-        device = hidden_state.device\n         hidden_state_1 = self.conv1(hidden_state)\n-        hidden_state_1 = self.bottlenecks(hidden_state_1).to(device)\n-        hidden_state_2 = self.conv2(hidden_state).to(device)\n+        hidden_state_1 = self.bottlenecks(hidden_state_1)\n+        hidden_state_2 = self.conv2(hidden_state)\n         return self.conv3(hidden_state_1 + hidden_state_2)\n \n \n@@ -944,43 +944,56 @@ def __init__(self, config: RTDetrV2Config):\n         self.eval_size = config.eval_size\n         self.out_channels = [self.encoder_hidden_dim for _ in self.in_channels]\n         self.out_strides = self.feat_strides\n-        activation_function = config.activation_function\n+        self.num_fpn_stages = len(self.in_channels) - 1\n+        self.num_pan_stages = len(self.in_channels) - 1\n+        activation = config.activation_function\n \n         # encoder transformer\n         self.encoder = nn.ModuleList([RTDetrV2Encoder(config) for _ in range(len(self.encode_proj_layers))])\n-        # top-down fpn\n+\n+        # top-down FPN\n         self.lateral_convs = nn.ModuleList()\n         self.fpn_blocks = nn.ModuleList()\n-        for _ in range(len(self.in_channels) - 1, 0, -1):\n-            self.lateral_convs.append(\n-                RTDetrV2ConvNormLayer(\n-                    config, self.encoder_hidden_dim, self.encoder_hidden_dim, 1, 1, activation=activation_function\n-                )\n+        for _ in range(self.num_fpn_stages):\n+            lateral_conv = RTDetrV2ConvNormLayer(\n+                config,\n+                in_channels=self.encoder_hidden_dim,\n+                out_channels=self.encoder_hidden_dim,\n+                kernel_size=1,\n+                stride=1,\n+                activation=activation,\n             )\n-            self.fpn_blocks.append(RTDetrV2CSPRepLayer(config))\n+            fpn_block = RTDetrV2CSPRepLayer(config)\n+            self.lateral_convs.append(lateral_conv)\n+            self.fpn_blocks.append(fpn_block)\n \n-        # bottom-up pan\n+        # bottom-up PAN\n         self.downsample_convs = nn.ModuleList()\n         self.pan_blocks = nn.ModuleList()\n-        for _ in range(len(self.in_channels) - 1):\n-            self.downsample_convs.append(\n-                RTDetrV2ConvNormLayer(\n-                    config, self.encoder_hidden_dim, self.encoder_hidden_dim, 3, 2, activation=activation_function\n-                )\n+        for _ in range(self.num_pan_stages):\n+            downsample_conv = RTDetrV2ConvNormLayer(\n+                config,\n+                in_channels=self.encoder_hidden_dim,\n+                out_channels=self.encoder_hidden_dim,\n+                kernel_size=3,\n+                stride=2,\n+                activation=activation,\n             )\n-            self.pan_blocks.append(RTDetrV2CSPRepLayer(config))\n+            pan_block = RTDetrV2CSPRepLayer(config)\n+            self.downsample_convs.append(downsample_conv)\n+            self.pan_blocks.append(pan_block)\n \n     @staticmethod\n     def build_2d_sincos_position_embedding(\n         width, height, embed_dim=256, temperature=10000.0, device=\"cpu\", dtype=torch.float32\n     ):\n-        grid_w = torch.arange(int(width), dtype=dtype, device=device)\n-        grid_h = torch.arange(int(height), dtype=dtype, device=device)\n+        grid_w = torch.arange(torch_int(width), device=device).to(dtype)\n+        grid_h = torch.arange(torch_int(height), device=device).to(dtype)\n         grid_w, grid_h = torch.meshgrid(grid_w, grid_h, indexing=\"ij\")\n         if embed_dim % 4 != 0:\n             raise ValueError(\"Embed dimension must be divisible by 4 for 2D sin-cos position embedding\")\n         pos_dim = embed_dim // 4\n-        omega = torch.arange(pos_dim, dtype=dtype, device=device) / pos_dim\n+        omega = torch.arange(pos_dim, device=device).to(dtype) / pos_dim\n         omega = 1.0 / (temperature**omega)\n \n         out_w = grid_w.flatten()[..., None] @ omega[None]\n@@ -1036,6 +1049,7 @@ def forward(\n \n         encoder_states = () if output_hidden_states else None\n         all_attentions = () if output_attentions else None\n+\n         # encoder\n         if self.config.encoder_layers > 0:\n             for i, enc_ind in enumerate(self.encode_proj_layers):\n@@ -1071,30 +1085,37 @@ def forward(\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states[enc_ind],)\n \n-        # broadcasting and fusion\n+        # top-down FPN\n         fpn_feature_maps = [hidden_states[-1]]\n-        for idx in range(len(self.in_channels) - 1, 0, -1):\n-            feat_high = fpn_feature_maps[0]\n-            feat_low = hidden_states[idx - 1]\n-            feat_high = self.lateral_convs[len(self.in_channels) - 1 - idx](feat_high)\n-            fpn_feature_maps[0] = feat_high\n-            upsample_feat = F.interpolate(feat_high, scale_factor=2.0, mode=\"nearest\")\n-            fps_map = self.fpn_blocks[len(self.in_channels) - 1 - idx](torch.concat([upsample_feat, feat_low], dim=1))\n-            fpn_feature_maps.insert(0, fps_map)\n-\n-        fpn_states = [fpn_feature_maps[0]]\n-        for idx in range(len(self.in_channels) - 1):\n-            feat_low = fpn_states[-1]\n-            feat_high = fpn_feature_maps[idx + 1]\n-            downsample_feat = self.downsample_convs[idx](feat_low)\n-            hidden_states = self.pan_blocks[idx](\n-                torch.concat([downsample_feat, feat_high.to(downsample_feat.device)], dim=1)\n-            )\n-            fpn_states.append(hidden_states)\n+        for idx, (lateral_conv, fpn_block) in enumerate(zip(self.lateral_convs, self.fpn_blocks)):\n+            backbone_feature_map = hidden_states[self.num_fpn_stages - idx - 1]\n+            top_fpn_feature_map = fpn_feature_maps[-1]\n+            # apply lateral block\n+            top_fpn_feature_map = lateral_conv(top_fpn_feature_map)\n+            fpn_feature_maps[-1] = top_fpn_feature_map\n+            # apply fpn block\n+            top_fpn_feature_map = F.interpolate(top_fpn_feature_map, scale_factor=2.0, mode=\"nearest\")\n+            fused_feature_map = torch.concat([top_fpn_feature_map, backbone_feature_map], dim=1)\n+            new_fpn_feature_map = fpn_block(fused_feature_map)\n+            fpn_feature_maps.append(new_fpn_feature_map)\n+\n+        fpn_feature_maps = fpn_feature_maps[::-1]\n+\n+        # bottom-up PAN\n+        pan_feature_maps = [fpn_feature_maps[0]]\n+        for idx, (downsample_conv, pan_block) in enumerate(zip(self.downsample_convs, self.pan_blocks)):\n+            top_pan_feature_map = pan_feature_maps[-1]\n+            fpn_feature_map = fpn_feature_maps[idx + 1]\n+            downsampled_feature_map = downsample_conv(top_pan_feature_map)\n+            fused_feature_map = torch.concat([downsampled_feature_map, fpn_feature_map], dim=1)\n+            new_pan_feature_map = pan_block(fused_feature_map)\n+            pan_feature_maps.append(new_pan_feature_map)\n \n         if not return_dict:\n-            return tuple(v for v in [fpn_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(last_hidden_state=fpn_states, hidden_states=encoder_states, attentions=all_attentions)\n+            return tuple(v for v in [pan_feature_maps, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=pan_feature_maps, hidden_states=encoder_states, attentions=all_attentions\n+        )\n \n \n def inverse_sigmoid(x, eps=1e-5):\n@@ -1184,7 +1205,7 @@ def get_contrastive_denoising_training_group(\n         denoise_positive_idx, [n * num_groups_denoising_queries for n in num_ground_truths]\n     )\n     # total denoising queries\n-    num_denoising_queries = int(max_gt_num * 2 * num_groups_denoising_queries)\n+    num_denoising_queries = torch_int(max_gt_num * 2 * num_groups_denoising_queries)\n \n     if label_noise_ratio > 0:\n         mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)\n@@ -1289,7 +1310,7 @@ class RTDetrV2PreTrainedModel(PreTrainedModel):\n     config_class = RTDetrV2Config\n     base_model_prefix = \"rt_detr_v2\"\n     main_input_name = \"pixel_values\"\n-    _no_split_modules = [r\"RTDetrV2ConvEncoder\", r\"RTDetrV2EncoderLayer\", r\"RTDetrV2DecoderLayer\"]\n+    _no_split_modules = [r\"RTDetrV2HybridEncoder\", r\"RTDetrV2DecoderLayer\"]\n \n     def _init_weights(self, module):\n         \"\"\"Initalize the weights\"\"\"\n@@ -1610,13 +1631,14 @@ def generate_anchors(self, spatial_shapes=None, grid_size=0.05, device=\"cpu\", dt\n         anchors = []\n         for level, (height, width) in enumerate(spatial_shapes):\n             grid_y, grid_x = torch.meshgrid(\n-                torch.arange(end=height, dtype=dtype, device=device),\n-                torch.arange(end=width, dtype=dtype, device=device),\n+                torch.arange(end=height, device=device).to(dtype),\n+                torch.arange(end=width, device=device).to(dtype),\n                 indexing=\"ij\",\n             )\n             grid_xy = torch.stack([grid_x, grid_y], -1)\n-            valid_wh = torch.tensor([width, height], device=device).to(dtype)\n-            grid_xy = (grid_xy.unsqueeze(0) + 0.5) / valid_wh\n+            grid_xy = grid_xy.unsqueeze(0) + 0.5\n+            grid_xy[..., 0] /= width\n+            grid_xy[..., 1] /= height\n             wh = torch.ones_like(grid_xy) * grid_size * (2.0**level)\n             anchors.append(torch.concat([grid_xy, wh], -1).reshape(-1, height * width, 4))\n         # define the valid range for anchor coordinates\n@@ -1717,14 +1739,15 @@ def forward(\n         # Prepare encoder inputs (by flattening)\n         source_flatten = []\n         spatial_shapes_list = []\n+        spatial_shapes = torch.empty((len(sources), 2), device=device, dtype=torch.long)\n         for level, source in enumerate(sources):\n-            batch_size, num_channels, height, width = source.shape\n-            spatial_shape = (height, width)\n-            spatial_shapes_list.append(spatial_shape)\n+            height, width = source.shape[-2:]\n+            spatial_shapes[level, 0] = height\n+            spatial_shapes[level, 1] = width\n+            spatial_shapes_list.append((height, width))\n             source = source.flatten(2).transpose(1, 2)\n             source_flatten.append(source)\n         source_flatten = torch.cat(source_flatten, 1)\n-        spatial_shapes = torch.as_tensor(spatial_shapes_list, dtype=torch.long, device=source_flatten.device)\n         level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n \n         # prepare denoising training\n@@ -1758,8 +1781,7 @@ def forward(\n             anchors, valid_mask = self.generate_anchors(spatial_shapes_tuple, device=device, dtype=dtype)\n         else:\n             anchors, valid_mask = self.anchors, self.valid_mask\n-\n-        anchors, valid_mask = anchors.to(device, dtype), valid_mask.to(device, dtype)\n+            anchors, valid_mask = anchors.to(device, dtype), valid_mask.to(device, dtype)\n \n         # use the valid_mask to selectively retain values in the feature map where the mask is `True`\n         memory = valid_mask.to(source_flatten.dtype) * source_flatten"
        }
    ],
    "stats": {
        "total": 265,
        "additions": 154,
        "deletions": 111
    }
}