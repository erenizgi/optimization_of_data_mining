{
    "author": "gante",
    "message": "[gpt2] fix generation tests (#35822)\n\nfix gpt2 generation tests",
    "sha": "36c9181f5cc748b174f02f133ff17376e3969e4c",
    "files": [
        {
            "sha": "ba8eb90c5ea1a7ce82cb04c6f6f52371cf12fdc5",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/36c9181f5cc748b174f02f133ff17376e3969e4c/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/36c9181f5cc748b174f02f133ff17376e3969e4c/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=36c9181f5cc748b174f02f133ff17376e3969e4c",
            "patch": "@@ -651,16 +651,18 @@ def test_batch_generation(self):\n         outputs = model.generate(\n             input_ids=input_ids,\n             attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            max_length=20,\n         )\n \n         outputs_tt = model.generate(\n             input_ids=input_ids,\n             attention_mask=inputs[\"attention_mask\"].to(torch_device),\n             token_type_ids=token_type_ids,\n+            max_length=20,\n         )\n \n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_non_padded = model.generate(input_ids=inputs_non_padded)\n+        output_non_padded = model.generate(input_ids=inputs_non_padded, max_length=20)\n \n         num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().cpu().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n@@ -711,16 +713,18 @@ def test_batch_generation_2heads(self):\n         outputs = model.generate(\n             input_ids=input_ids,\n             attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            max_length=20,\n         )\n \n         outputs_tt = model.generate(\n             input_ids=input_ids,\n             attention_mask=inputs[\"attention_mask\"].to(torch_device),\n             token_type_ids=token_type_ids,\n+            max_length=20,\n         )\n \n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_non_padded = model.generate(input_ids=inputs_non_padded)\n+        output_non_padded = model.generate(input_ids=inputs_non_padded, max_length=20)\n \n         num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().cpu().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n@@ -776,7 +780,7 @@ def _test_lm_generate_gpt2_helper(\n \n         # The dog was found in a field near the intersection of West and West Streets.\\n\\nThe dog\n         expected_output_ids = [464, 3290, 373, 1043, 287, 257, 2214, 1474, 262, 16246, 286, 2688, 290, 2688, 27262, 13, 198, 198, 464, 3290,]  # fmt: skip\n-        output_ids = model.generate(input_ids, do_sample=False)\n+        output_ids = model.generate(input_ids, do_sample=False, max_length=20)\n         if verify_outputs:\n             self.assertListEqual(output_ids[0].tolist(), expected_output_ids)\n \n@@ -805,13 +809,13 @@ def test_gpt2_sample(self):\n         torch.manual_seed(0)\n         tokenized = tokenizer(\"Today is a nice day and\", return_tensors=\"pt\", return_token_type_ids=True)\n         input_ids = tokenized.input_ids.to(torch_device)\n-        output_ids = model.generate(input_ids, do_sample=True)\n+        output_ids = model.generate(input_ids, do_sample=True, max_length=20)\n         output_str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n \n         token_type_ids = tokenized.token_type_ids.to(torch_device)\n-        output_seq = model.generate(input_ids=input_ids, do_sample=True, num_return_sequences=5)\n+        output_seq = model.generate(input_ids=input_ids, do_sample=True, num_return_sequences=5, max_length=20)\n         output_seq_tt = model.generate(\n-            input_ids=input_ids, token_type_ids=token_type_ids, do_sample=True, num_return_sequences=5\n+            input_ids=input_ids, token_type_ids=token_type_ids, do_sample=True, num_return_sequences=5, max_length=20\n         )\n         output_seq_strs = tokenizer.batch_decode(output_seq, skip_special_tokens=True)\n         output_seq_tt_strs = tokenizer.batch_decode(output_seq_tt, skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 10,
        "deletions": 6
    }
}