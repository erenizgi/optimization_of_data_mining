{
    "author": "benniekiss",
    "message": "[whisper] pass attention_mask to generate_with_fallback() (#33145)\n\npass attention_mask to generate_with_fallback",
    "sha": "e0b87b0f408206b317ee6cc7af3de3aa88ef7c39",
    "files": [
        {
            "sha": "7da4e49de98c9ae807513f7a858db38408165d7e",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0b87b0f408206b317ee6cc7af3de3aa88ef7c39/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0b87b0f408206b317ee6cc7af3de3aa88ef7c39/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=e0b87b0f408206b317ee6cc7af3de3aa88ef7c39",
            "patch": "@@ -686,6 +686,7 @@ def generate(\n                 do_condition_on_prev_tokens=do_condition_on_prev_tokens,\n                 is_shortform=is_shortform,\n                 batch_size=batch_size,\n+                attention_mask=attention_mask,\n                 kwargs=kwargs,\n             )\n \n@@ -790,6 +791,7 @@ def generate_with_fallback(\n         do_condition_on_prev_tokens,\n         is_shortform,\n         batch_size,\n+        attention_mask,\n         kwargs,\n     ):\n         kwargs = copy.copy(kwargs)\n@@ -837,6 +839,7 @@ def generate_with_fallback(\n                 prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                 synced_gpus=synced_gpus,\n                 decoder_input_ids=decoder_input_ids,\n+                attention_mask=attention_mask,\n                 **generate_kwargs,\n             )\n "
        }
    ],
    "stats": {
        "total": 3,
        "additions": 3,
        "deletions": 0
    }
}