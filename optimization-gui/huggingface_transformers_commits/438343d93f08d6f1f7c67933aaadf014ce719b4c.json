{
    "author": "cyyever",
    "message": "Don't list dropout in eager_paged_attention_forward (#40924)\n\nRemove dropout argument\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "438343d93f08d6f1f7c67933aaadf014ce719b4c",
    "files": [
        {
            "sha": "29d078ad2cad5c81b6de87950a6e88e0d47662cd",
            "filename": "src/transformers/integrations/eager_paged.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/438343d93f08d6f1f7c67933aaadf014ce719b4c/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/438343d93f08d6f1f7c67933aaadf014ce719b4c/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feager_paged.py?ref=438343d93f08d6f1f7c67933aaadf014ce719b4c",
            "patch": "@@ -23,7 +23,6 @@ def eager_paged_attention_forward(\n     value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],  # shape [seqlen_q, seqlen_k]\n     scaling: float,\n-    dropout: float = 0.0,\n     **kwargs,\n ):\n     # Add KV cache to the key and value tensors"
        }
    ],
    "stats": {
        "total": 1,
        "additions": 0,
        "deletions": 1
    }
}