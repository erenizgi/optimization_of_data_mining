{
    "author": "Cyrilvallez",
    "message": "Fix generate outputs and simplify cache tests (#41440)\n\n* start refactoring\n\n* simplify\n\n* tests\n\n* tests\n\n* fix\n\n* zamba\n\n* final fix\n\n* fix",
    "sha": "bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
    "files": [
        {
            "sha": "83da0c5e77b00396d0d23e4bd60985285248404f",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 6,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -2812,6 +2812,10 @@ def _sample(\n             streamer.end()\n \n         if return_dict_in_generate:\n+            cache = None\n+            if any(cache_key in model_kwargs for cache_key in ALL_CACHE_NAMES):\n+                cache_key = next(cache_key for cache_key in ALL_CACHE_NAMES if cache_key in model_kwargs)\n+                cache = model_kwargs[cache_key]\n             if self.config.is_encoder_decoder:\n                 return GenerateEncoderDecoderOutput(\n                     sequences=input_ids,\n@@ -2822,7 +2826,7 @@ def _sample(\n                     decoder_attentions=decoder_attentions,\n                     cross_attentions=cross_attentions,\n                     decoder_hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n+                    past_key_values=cache,\n                 )\n             else:\n                 return GenerateDecoderOnlyOutput(\n@@ -2831,7 +2835,7 @@ def _sample(\n                     logits=raw_logits,\n                     attentions=decoder_attentions,\n                     hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n+                    past_key_values=cache,\n                 )\n         else:\n             return input_ids\n@@ -3375,6 +3379,11 @@ def _beam_search(\n             if not output_scores:\n                 beam_scores = None\n \n+            cache = None\n+            if any(cache_key in model_kwargs for cache_key in ALL_CACHE_NAMES):\n+                cache_key = next(cache_key for cache_key in ALL_CACHE_NAMES if cache_key in model_kwargs)\n+                cache = model_kwargs[cache_key]\n+\n             if self.config.is_encoder_decoder:\n                 return GenerateBeamEncoderDecoderOutput(\n                     sequences=sequences,\n@@ -3387,7 +3396,7 @@ def _beam_search(\n                     decoder_attentions=decoder_attentions,\n                     cross_attentions=cross_attentions,\n                     decoder_hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n+                    past_key_values=cache,\n                 )\n             else:\n                 return GenerateBeamDecoderOnlyOutput(\n@@ -3398,7 +3407,7 @@ def _beam_search(\n                     beam_indices=beam_indices,\n                     attentions=decoder_attentions,\n                     hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n+                    past_key_values=cache,\n                 )\n         else:\n             return sequences\n@@ -3673,6 +3682,10 @@ def _assisted_decoding(\n                 candidate_generator.num_assistant_tokens\n             )\n         if return_dict_in_generate:\n+            cache = None\n+            if any(cache_key in model_kwargs for cache_key in ALL_CACHE_NAMES):\n+                cache_key = next(cache_key for cache_key in ALL_CACHE_NAMES if cache_key in model_kwargs)\n+                cache = model_kwargs[cache_key]\n             if self.config.is_encoder_decoder:\n                 return GenerateEncoderDecoderOutput(\n                     sequences=input_ids,\n@@ -3683,7 +3696,7 @@ def _assisted_decoding(\n                     decoder_attentions=decoder_attentions,\n                     cross_attentions=cross_attentions,\n                     decoder_hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n+                    past_key_values=cache,\n                 )\n             else:\n                 return GenerateDecoderOnlyOutput(\n@@ -3692,7 +3705,7 @@ def _assisted_decoding(\n                     logits=raw_logits,\n                     attentions=decoder_attentions,\n                     hidden_states=decoder_hidden_states,\n-                    past_key_values=model_kwargs.get(\"past_key_values\"),\n+                    past_key_values=cache,\n                 )\n         else:\n             return input_ids"
        },
        {
            "sha": "bbc0c68b0e4e66290e39e8b8ad16800bfee3e2b3",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -143,7 +143,7 @@ def __init__(\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.sliding_window = sliding_window\n-        self.head_dim = head_dim\n+        self.head_dim = head_dim if head_dim is not None else hidden_size // num_attention_heads\n \n         # for backward compatibility\n         if num_key_value_heads is None:"
        },
        {
            "sha": "dff3b49fc864b563f9f25afa9676a3ac0cb8806b",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 61,
            "deletions": 128,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -1027,16 +1027,15 @@ def _prepare_model_kwargs(model_inputs, signature):\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n     @pytest.mark.generate\n-    def test_past_key_values_format(self, custom_all_cache_shapes=None):\n+    def test_past_key_values_format(self):\n         \"\"\"\n-        Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test, or pass the\n-        expected cache shapes.\n+        Test that the KV cache is formatted correctly.\n         Having a standard KV cache format is important for a consistent API (and for advanced generation methods).\n         \"\"\"\n         for model_class in self.all_generative_model_classes:\n             config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n \n-            # 1. If it doesn't support cache, skip the test\n+            # If it doesn't support cache, skip the test\n             decoder_config = config.get_text_config(decoder=True)\n             if not hasattr(decoder_config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n@@ -1050,82 +1049,14 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n             if \"past_key_values\" not in outputs:\n                 self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n \n-            # 2. retrieve the KV cache and compute its default expected shapes (if no custom shapes are provided)\n-            past_kv = outputs[\"past_key_values\"]\n-\n-            num_decoder_layers = decoder_config.num_hidden_layers\n-            if custom_all_cache_shapes is None:\n-                num_query_attention_heads = decoder_config.num_attention_heads\n-                embed_dim = getattr(decoder_config, \"d_model\", decoder_config.hidden_size)\n-                per_head_embed_dim = (\n-                    getattr(decoder_config, \"head_dim\", None) or embed_dim // num_query_attention_heads\n-                )\n-                num_key_value_heads = getattr(decoder_config, \"num_key_value_heads\", None) or num_query_attention_heads\n-                if config.is_encoder_decoder:\n-                    batch_size, seq_length = inputs[\"decoder_input_ids\"].shape[:2]\n-                    # The sequence length for the encoder K V depends on the model. Since it is not manipulated in\n-                    # autoregressive generation, we're keeping the test general and not checking the 3rd dim\n-                    default_cross_attention_shape = (batch_size, num_key_value_heads, per_head_embed_dim)\n-                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n-                    all_cache_shapes = [\n-                        [\n-                            default_self_attention_shape,\n-                            default_self_attention_shape,\n-                            default_cross_attention_shape,\n-                            default_cross_attention_shape,\n-                        ]\n-                        for _ in range(num_decoder_layers)\n-                    ]\n-                else:\n-                    batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n-                    default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n-                    all_cache_shapes = [\n-                        [default_self_attention_shape, default_self_attention_shape] for _ in range(num_decoder_layers)\n-                    ]\n-\n-            else:\n-                all_cache_shapes = custom_all_cache_shapes\n-\n-            # 3. Check cache shapes\n-            # 3.1. Encoder-Decoder checks\n+            cache = outputs[\"past_key_values\"]\n             if config.is_encoder_decoder:\n-                num_cache_decoder_layers = len(past_kv)\n-                self.assertEqual(num_cache_decoder_layers, num_decoder_layers)\n-\n-                for i in range(num_decoder_layers):\n-                    # Self attention\n-                    self_attention_layer_keys = past_kv.self_attention_cache.layers[i].keys\n-                    self_attention_layer_values = past_kv.self_attention_cache.layers[i].values\n-                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n-\n-                    # Cross attention (ignore 3rd dim, see default shape preparation)\n-                    cross_attention_layer_keys = past_kv.cross_attention_cache.layers[i].keys\n-                    cross_attention_layer_values = past_kv.cross_attention_cache.layers[i].values\n-                    cross_attention_layer_keys = cross_attention_layer_keys[:, :, 0, :]\n-                    cross_attention_layer_values = cross_attention_layer_values[:, :, 0, :]\n-                    self.assertEqual(cross_attention_layer_keys.shape, all_cache_shapes[i][2])\n-                    self.assertEqual(cross_attention_layer_values.shape, all_cache_shapes[i][3])\n-\n-            # 3.2. Decoder-only checks\n+                batch_size, seq_length = inputs[\"decoder_input_ids\"].shape[:2]\n             else:\n-                num_cache_decoder_layers = len(past_kv)\n-                self.assertEqual(\n-                    # we may have skipped layers\n-                    num_cache_decoder_layers + getattr(decoder_config, \"num_kv_shared_layers\", 0),\n-                    num_decoder_layers,\n-                )\n+                batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n \n-                for i in range(num_cache_decoder_layers):\n-                    # Cache is lot layered (i.e, Mamba derivatives)\n-                    if getattr(past_kv, \"layers\", None) is None:\n-                        self_attention_layer_keys = past_kv.key_cache[i]\n-                        self_attention_layer_values = past_kv.value_cache[i]\n-                    else:\n-                        self_attention_layer_keys = past_kv.layers[i].keys\n-                        self_attention_layer_values = past_kv.layers[i].values\n-                    self.assertEqual(self_attention_layer_keys.shape, all_cache_shapes[i][0])\n-                    self.assertEqual(self_attention_layer_values.shape, all_cache_shapes[i][1])\n+            # Check the format\n+            self._check_past_key_values_for_generate(batch_size, cache, seq_length, decoder_config)\n \n     @pytest.mark.generate\n     def test_generate_from_random_inputs_embeds(self):\n@@ -2281,9 +2212,9 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n         generated_length = (\n             output.sequences.shape[1] - 1 if config.is_encoder_decoder else output.sequences.shape[1] - prompt_length\n         )\n-        decoder_past_key_values = getattr(output, \"past_key_values\", None)\n-        if config.is_encoder_decoder and isinstance(decoder_past_key_values, EncoderDecoderCache):\n-            decoder_past_key_values = decoder_past_key_values.self_attention_cache\n+\n+        cache = getattr(output, \"past_key_values\", None)\n+        decoder_past_key_values = cache.self_attention_cache if isinstance(cache, EncoderDecoderCache) else cache\n \n         # in some models we subsample the sequence length in inner layers\n         if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n@@ -2354,36 +2285,18 @@ def _check_generate_outputs(self, output, config, use_cache=False, num_return_se\n                 use_cache=use_cache,\n             )\n \n-        # Past Key Value States -- a few notes here:\n-        # 1. Its inner sequence length is with respect to the inputs of the latest forward pass, hence the \"-1\"\n-        # 2. We ignore models that have unique cache structures (e.g. mamba) or are in need of refatoring to match the\n-        #    standard cache format (e.g.mamba architecture )\n-        models_without_standard_cache = (\n-            \"bamba\",\n-            \"granitemoehybrid\",\n-            \"reformer\",\n-            \"jamba\",\n-            \"mamba\",\n-            \"xlnet\",\n-            \"zamba\",\n-            \"zamba2\",\n-            \"lfm2\",\n-            \"lfm2-vl\",\n-        )\n-        has_standard_cache = not any(\n-            model_name in config.__class__.__name__.lower() for model_name in models_without_standard_cache\n-        )\n-        if has_standard_cache:\n-            if use_cache:\n-                cache_length = output.sequences.shape[1] - 1\n-                self._check_past_key_values_for_generate(\n-                    batch_size=internal_batch_size,\n-                    decoder_past_key_values=decoder_past_key_values,\n-                    cache_length=cache_length,\n-                    config=config,\n-                )\n-            elif use_cache is False:\n-                self.assertTrue(decoder_past_key_values is None)\n+        # Check the cache shape\n+        if use_cache:\n+            cache_length = output.sequences.shape[1] - 1\n+            self._check_past_key_values_for_generate(\n+                batch_size=internal_batch_size,\n+                past_key_values=cache,\n+                seq_length=cache_length,\n+                config=config,\n+            )\n+        # xlnet has a weird list cache, which is returned even with `use_cache=False`...\n+        elif \"xlnet\" not in config.__class__.__name__.lower():\n+            self.assertTrue(cache is None)\n \n     def _check_scores(self, batch_size, scores, generated_length, config):\n         vocab_size = config.get_text_config(decoder=True).vocab_size\n@@ -2483,28 +2396,48 @@ def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, c\n             [encoder_expected_shape] * len(hidden_states),\n         )\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        # Raise a useful error, asking to explicitly override the method\n+        if not isinstance(past_key_values, Cache):\n+            raise ValueError(\"The cache is not standard! Please overwrite `_check_past_key_values_for_generate`\")\n+\n+        # In this case, we simply call recursively the function on both internal caches\n+        if isinstance(past_key_values, EncoderDecoderCache):\n+            self._check_past_key_values_for_generate(\n+                batch_size, past_key_values.self_attention_cache, seq_length, config\n+            )\n+            # For cross attention cache, the seq_length depends on the model, so we remove that dim\n+            self._check_past_key_values_for_generate(batch_size, past_key_values.cross_attention_cache, None, config)\n+            return\n+\n+        # Use the correct config\n+        config = config.get_text_config(decoder=True)\n+\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        hidden_size = getattr(config, \"d_model\", config.hidden_size)\n+        head_dim = getattr(config, \"head_dim\", hidden_size // config.num_attention_heads)\n \n-        # (batch, # kv heads, seq_length, head_features)\n+        # For cross attention cache, the seq_length depends on the model, so we remove that dim\n         expected_shape = (\n-            batch_size,\n-            getattr(config, \"num_key_value_heads\", None) or config.num_attention_heads,\n-            cache_length,\n-            getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads,\n+            (batch_size, num_heads, seq_length, head_dim)\n+            if seq_length is not None\n+            else (batch_size, num_heads, head_dim)\n         )\n \n-        if isinstance(decoder_past_key_values, Cache):\n-            self.assertListEqual(\n-                [layer.keys.shape for layer in decoder_past_key_values.layers],\n-                [expected_shape] * len(decoder_past_key_values.layers),\n-            )\n-            self.assertListEqual(\n-                [layer.values.shape for layer in decoder_past_key_values.layers],\n-                [expected_shape] * len(decoder_past_key_values.layers),\n-            )\n-        else:\n-            raise ValueError(\"The cache is not standard! Please overwrite `_check_past_key_values_for_generate`\")\n+        # Check the size is coherent\n+        num_hidden_layers = config.num_hidden_layers\n+        if getattr(config, \"num_kv_shared_layers\", None) is not None:\n+            num_hidden_layers -= config.num_kv_shared_layers\n+        self.assertEqual(num_hidden_layers, len(past_key_values))\n+\n+        # Check each layer has the correct shape\n+        for layer in past_key_values.layers:\n+            # Remove the seq_length dim for cross-attention cache (it changes based on the model)\n+            keys = layer.keys if seq_length is not None else layer.keys[:, :, 0, :]\n+            values = layer.values if seq_length is not None else layer.values[:, :, 0, :]\n+            self.assertEqual(keys.shape, expected_shape)\n+            self.assertEqual(values.shape, expected_shape)\n \n     def _check_sequence_inside_sequence(self, tensor_1, tensor_2):\n         # check if tensor_1 inside tensor_2 or tensor_2 inside tensor_1."
        },
        {
            "sha": "bc634af7829a8335bdf9155292e34248d13d14ee",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -52,9 +52,7 @@\n         BambaForCausalLM,\n         BambaModel,\n     )\n-    from transformers.models.bamba.modeling_bamba import (\n-        HybridMambaAttentionDynamicCache,\n-    )\n+    from transformers.models.bamba.modeling_bamba import HybridMambaAttentionDynamicCache\n \n \n class BambaModelTester:\n@@ -296,25 +294,30 @@ class BambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     # This is because we are hitting edge cases with the causal_mask buffer\n     model_split_percents = [0.5, 0.7, 0.8]\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, HybridMambaAttentionDynamicCache)\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, HybridMambaAttentionDynamicCache)\n+\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        attention_shape = (batch_size, num_heads, seq_length, head_dim)\n \n-        # (batch, head, seq_length, head_features)\n-        expected_shape = (\n+        conv_shape = (\n             batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-            config.hidden_size // config.num_attention_heads,\n+            config.mamba_expand * config.hidden_size + 2 * config.mamba_n_groups * config.mamba_d_state,\n+            config.mamba_d_conv,\n         )\n+        ssm_shape = (batch_size, config.mamba_n_heads, config.mamba_d_head, config.mamba_d_state)\n \n-        self.assertListEqual(\n-            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n-            [expected_shape] * len(decoder_past_key_values.key_cache),\n-        )\n-        self.assertListEqual(\n-            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n-            [expected_shape] * len(decoder_past_key_values.value_cache),\n-        )\n+        self.assertTrue(config.num_hidden_layers, len(past_key_values))\n+\n+        for idx in range(len(past_key_values)):\n+            if config.layers_block_type[idx] == \"mamba\":\n+                self.assertEqual(past_key_values.conv_states[idx].shape, conv_shape)\n+                self.assertEqual(past_key_values.ssm_states[idx].shape, ssm_shape)\n+            else:\n+                self.assertEqual(past_key_values.key_cache[idx].shape, attention_shape)\n+                self.assertEqual(past_key_values.value_cache[idx].shape, attention_shape)\n \n     def _check_caches_are_equal(\n         self, cache1: HybridMambaAttentionDynamicCache, cache2: HybridMambaAttentionDynamicCache"
        },
        {
            "sha": "a24badfac50ac5927212b354e75c4962d063f1d3",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 36,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -73,6 +73,23 @@ class DeepseekV2ModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = DeepseekV2ForCausalLM if is_torch_available() else None\n \n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        \"\"\"Needs to be overridden as deepseek has special MLA cache format (though we don't really use the MLA)\"\"\"\n+        self.assertIsInstance(past_key_values, Cache)\n+\n+        # (batch, head, seq_length, head_features)\n+        expected_common_shape = (\n+            batch_size,\n+            getattr(config, \"num_key_value_heads\", config.num_attention_heads),\n+            seq_length,\n+        )\n+        expected_key_shape = expected_common_shape + (config.qk_nope_head_dim + config.qk_rope_head_dim,)\n+        expected_value_shape = expected_common_shape + (config.v_head_dim,)\n+\n+        for layer in past_key_values.layers:\n+            self.assertEqual(layer.keys.shape, expected_key_shape)\n+            self.assertEqual(layer.values.shape, expected_value_shape)\n+\n     def test_model_rope_scaling_frequencies(self):\n         \"\"\"\n         Overwritten: DeepseekV2 implements RoPE in the complex domain, as opposed to in the real domain with\n@@ -130,42 +147,6 @@ def test_model_rope_scaling_frequencies(self):\n         with self.assertRaises(AssertionError):\n             torch.testing.assert_close(yarn_freqs_cis_long, original_freqs_cis_long)\n \n-    def test_past_key_values_format(self):\n-        \"\"\"\n-        Overwriting to pass the expected cache shapes (Deepseek-V3 uses MLA so the cache shapes are non-standard)\n-        \"\"\"\n-        config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-        batch_size, seq_length = inputs[\"input_ids\"].shape\n-        # difference: last dim\n-        k_embed_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n-        v_embed_dim = config.v_head_dim\n-        self_attention_key_cache_shape = (batch_size, config.num_key_value_heads, seq_length, k_embed_dim)\n-        self_attention_value_cache_shape = (batch_size, config.num_key_value_heads, seq_length, v_embed_dim)\n-        # build the full cache shapes\n-        num_hidden_layers = config.num_hidden_layers\n-        all_cache_shapes = [\n-            [self_attention_key_cache_shape, self_attention_value_cache_shape] for _ in range(num_hidden_layers)\n-        ]\n-        super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n-\n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        \"\"\"Needs to be overridden as deepseek has special MLA cache format (though we don't really use the MLA)\"\"\"\n-        self.assertIsInstance(decoder_past_key_values, Cache)\n-\n-        # (batch, head, seq_length, head_features)\n-        expected_common_shape = (\n-            batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-        )\n-        expected_key_shape = expected_common_shape + (config.qk_nope_head_dim + config.qk_rope_head_dim,)\n-        expected_value_shape = expected_common_shape + (config.v_head_dim,)\n-\n-        if isinstance(decoder_past_key_values, Cache):\n-            for layer in decoder_past_key_values.layers:\n-                self.assertEqual(layer.keys.shape, expected_key_shape)\n-                self.assertEqual(layer.values.shape, expected_value_shape)\n-\n     @unittest.skip(\"Dynamic control flow in MoE\")\n     @pytest.mark.torch_compile_test\n     def test_torch_compile_for_training(self):"
        },
        {
            "sha": "e7a1de3b25edf85bed750ae112846bba041a0b19",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -40,6 +40,7 @@\n     import torch\n \n     from transformers import (\n+        Cache,\n         DeepseekV3ForCausalLM,\n         DeepseekV3ForSequenceClassification,\n         DeepseekV3ForTokenClassification,\n@@ -248,6 +249,23 @@ def setUp(self):\n         self.model_tester = DeepseekV3ModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=DeepseekV3Config, hidden_size=37)\n \n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        \"\"\"Needs to be overridden as deepseek has special MLA cache format (though we don't really use the MLA)\"\"\"\n+        self.assertIsInstance(past_key_values, Cache)\n+\n+        # (batch, head, seq_length, head_features)\n+        expected_common_shape = (\n+            batch_size,\n+            getattr(config, \"num_key_value_heads\", config.num_attention_heads),\n+            seq_length,\n+        )\n+        expected_key_shape = expected_common_shape + (config.qk_nope_head_dim + config.qk_rope_head_dim,)\n+        expected_value_shape = expected_common_shape + (config.v_head_dim,)\n+\n+        for layer in past_key_values.layers:\n+            self.assertEqual(layer.keys.shape, expected_key_shape)\n+            self.assertEqual(layer.values.shape, expected_value_shape)\n+\n     @parameterized.expand([(\"random\",), (\"same\",)])\n     @unittest.skip(\"DeepseekV3 is not compatible with assisted decoding\")\n     def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n@@ -378,22 +396,6 @@ def test_model_rope_scaling(self):\n         with self.assertRaises(AssertionError):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n-    def test_past_key_values_format(self):\n-        \"\"\"\n-        Overwriting to pass the expected cache shapes (Deepseek-V3 uses MLA so the cache shapes are non-standard)\n-        \"\"\"\n-        config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-        batch_size, seq_length = inputs[\"input_ids\"].shape\n-        # difference: last dim\n-        k_embed_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n-        v_embed_dim = config.v_head_dim\n-        self_attention_keys_shape = (batch_size, config.num_key_value_heads, seq_length, k_embed_dim)\n-        self_attention_values_shape = (batch_size, config.num_key_value_heads, seq_length, v_embed_dim)\n-        # build the full cache shapes\n-        num_hidden_layers = config.num_hidden_layers\n-        all_cache_shapes = [[self_attention_keys_shape, self_attention_values_shape] for _ in range(num_hidden_layers)]\n-        super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n-\n     @require_torch_large_accelerator\n     @slow\n     def test_eager_matches_sdpa_generate(self):"
        },
        {
            "sha": "64f00f644e2c01c48d97672ff11ddc7114f94267",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -52,7 +52,6 @@\n         PreTrainedModel,\n     )\n     from transformers.cache_utils import (\n-        Cache,\n         StaticCache,\n     )\n     from transformers.models.dia.modeling_dia import DiaDecoder, DiaEncoder\n@@ -377,30 +376,6 @@ def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, c\n             [encoder_expected_shape] * len(hidden_states),\n         )\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n-\n-        # we need the decoder config here\n-        config = config.decoder_config\n-\n-        # (batch, head, seq_length, head_features)\n-        expected_shape = (\n-            batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-            config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads,\n-        )\n-\n-        if isinstance(decoder_past_key_values, Cache):\n-            self.assertListEqual(\n-                [layer.keys.shape for layer in decoder_past_key_values.layers],\n-                [expected_shape] * len(decoder_past_key_values.layers),\n-            )\n-            self.assertListEqual(\n-                [layer.values.shape for layer in decoder_past_key_values.layers],\n-                [expected_shape] * len(decoder_past_key_values.layers),\n-            )\n-\n     def _check_scores(self, batch_size, scores, generated_length, config):\n         # Special case where Dia keeps score in a 2D mesh of (bsz * channels, vocab)\n         vocab_size = config.decoder_config.vocab_size"
        },
        {
            "sha": "b3c10867ee4dc556bc3693b4b4278571648d242c",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -270,24 +270,21 @@ class FalconH1ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n         {\"feature-extraction\": FalconH1Model, \"text-generation\": FalconH1ForCausalLM} if is_torch_available() else {}\n     )\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, FalconHybridMambaAttentionDynamicCache)\n-\n-        # (batch, head, seq_length, head_features)\n-        expected_shape = (\n-            batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-            config.hidden_size // config.num_attention_heads,\n-        )\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, FalconHybridMambaAttentionDynamicCache)\n+\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        expected_shape = (batch_size, num_heads, seq_length, head_dim)\n \n         self.assertListEqual(\n-            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n-            [expected_shape] * len(decoder_past_key_values.key_cache),\n+            [key_tensor.shape for key_tensor in past_key_values.key_cache],\n+            [expected_shape] * len(past_key_values.key_cache),\n         )\n         self.assertListEqual(\n-            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n-            [expected_shape] * len(decoder_past_key_values.value_cache),\n+            [value_cache.shape for value_cache in past_key_values.value_cache],\n+            [expected_shape] * len(past_key_values.value_cache),\n         )\n \n     def _check_caches_are_equal(self, cache1, cache2):"
        },
        {
            "sha": "4bc5de03e9513cfbea7a03b6f0d0d0dcc7f713e0",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 14,
            "deletions": 5,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -41,11 +41,8 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import (\n-        FalconMambaCache,\n-        FalconMambaForCausalLM,\n-        FalconMambaModel,\n-    )\n+    from transformers import FalconMambaForCausalLM, FalconMambaModel\n+    from transformers.models.falcon_mamba.modeling_falcon_mamba import FalconMambaCache\n \n \n # Copied from transformers.tests.models.mamba.MambaModelTester with Mamba->FalconMamba,mamba->falcon_mamba\n@@ -286,6 +283,18 @@ def setUp(self):\n             self, config_class=FalconMambaConfig, n_embd=37, common_properties=[\"hidden_size\", \"num_hidden_layers\"]\n         )\n \n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, FalconMambaCache)\n+\n+        conv_shape = (batch_size, config.intermediate_size, config.conv_kernel)\n+        ssm_shape = (batch_size, config.intermediate_size, config.state_size)\n+\n+        self.assertTrue(config.num_hidden_layers, len(past_key_values.conv_states))\n+\n+        for idx in range(len(past_key_values.conv_states)):\n+            self.assertEqual(past_key_values.conv_states[idx].shape, conv_shape)\n+            self.assertEqual(past_key_values.ssm_states[idx].shape, ssm_shape)\n+\n     def assertInterval(self, member, container, msg=None):\n         r\"\"\"\n         Simple utility function to check if a member is inside an interval."
        },
        {
            "sha": "951558b19e5402c8db1a13bbe95b92cd5fcfaf05",
            "filename": "tests/models/granitemoehybrid/test_modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoehybrid%2Ftest_modeling_granitemoehybrid.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -41,6 +41,7 @@\n         GraniteMoeHybridForCausalLM,\n         GraniteMoeHybridModel,\n     )\n+    from transformers.models.granitemoehybrid.modeling_granitemoehybrid import HybridMambaAttentionDynamicCache\n \n \n class GraniteMoeHybridModelTester(BambaModelTester):\n@@ -95,6 +96,31 @@ class GraniteMoeHybridModelTest(BambaModelTest, GenerationTesterMixin, unittest.\n         else {}\n     )\n \n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, HybridMambaAttentionDynamicCache)\n+\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        attention_shape = (batch_size, num_heads, seq_length, head_dim)\n+\n+        conv_shape = (\n+            batch_size,\n+            config.mamba_expand * config.hidden_size + 2 * config.mamba_n_groups * config.mamba_d_state,\n+            config.mamba_d_conv,\n+        )\n+        ssm_shape = (batch_size, config.mamba_n_heads, config.mamba_d_head, config.mamba_d_state)\n+\n+        self.assertTrue(config.num_hidden_layers, len(past_key_values))\n+\n+        for idx in range(len(past_key_values)):\n+            if config.layers_block_type[idx] == \"mamba\":\n+                self.assertEqual(past_key_values.conv_states[idx].shape, conv_shape)\n+                self.assertEqual(past_key_values.ssm_states[idx].shape, ssm_shape)\n+            else:\n+                self.assertEqual(past_key_values.key_cache[idx].shape, attention_shape)\n+                self.assertEqual(past_key_values.value_cache[idx].shape, attention_shape)\n+\n     def test_config_requires_mamba_or_attention_layers(self):\n         \"\"\"Ensure we can't create a config with disallowed layers.\"\"\"\n         with pytest.raises(ValueError):"
        },
        {
            "sha": "1ed068bb500f900dc7b32f17a5a73f4483caa761",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 19,
            "deletions": 23,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -341,25 +341,25 @@ class JambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         else {}\n     )\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, HybridMambaAttentionDynamicCache)\n-\n-        # (batch, head, seq_length, head_features)\n-        expected_shape = (\n-            batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-            config.hidden_size // config.num_attention_heads,\n-        )\n-\n-        self.assertListEqual(\n-            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n-            [expected_shape] * len(decoder_past_key_values.key_cache),\n-        )\n-        self.assertListEqual(\n-            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n-            [expected_shape] * len(decoder_past_key_values.value_cache),\n-        )\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, HybridMambaAttentionDynamicCache)\n+\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        attention_shape = (batch_size, num_heads, seq_length, head_dim)\n+        conv_shape = (batch_size, config.mamba_expand * config.hidden_size, config.mamba_d_conv)\n+        ssm_shape = (batch_size, config.mamba_expand * config.hidden_size, config.mamba_d_state)\n+\n+        self.assertTrue(config.num_hidden_layers, len(past_key_values))\n+\n+        for idx in range(len(past_key_values)):\n+            if config.layers_block_type[idx] == \"mamba\":\n+                self.assertEqual(past_key_values.conv_states[idx].shape, conv_shape)\n+                self.assertEqual(past_key_values.ssm_states[idx].shape, ssm_shape)\n+            else:\n+                self.assertEqual(past_key_values.key_cache[idx].shape, attention_shape)\n+                self.assertEqual(past_key_values.value_cache[idx].shape, attention_shape)\n \n     def _check_caches_are_equal(\n         self, cache1: HybridMambaAttentionDynamicCache, cache2: HybridMambaAttentionDynamicCache\n@@ -550,10 +550,6 @@ def test_flash_attn_2_fp32_ln(self):\n                 # with attention mask\n                 _ = model(dummy_input, attention_mask=dummy_attention_mask)\n \n-    @unittest.skip(\"Jamba has a non standard cache format (mamba cache)\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @unittest.skip(\"Jamba has a non standard cache which is not compatible with dp/ddp\")\n     def test_multi_gpu_data_parallel_forward(self):\n         pass"
        },
        {
            "sha": "4dd5a32ef983d569fbcce7acc816b8b0cc211522",
            "filename": "tests/models/lfm2/test_modeling_lfm2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 49,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -15,8 +15,6 @@\n \n import unittest\n \n-import pytest\n-\n from transformers import is_torch_available\n from transformers.testing_utils import (\n     require_read_token,\n@@ -64,24 +62,21 @@ class Lfm2ModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = Lfm2ForCausalLM if is_torch_available() else None\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, Lfm2HybridConvCache)\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, Lfm2HybridConvCache)\n \n-        # (batch, head, seq_length, head_features)\n-        attention_shape = (\n-            batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-            config.hidden_size // config.num_attention_heads,\n-        )\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        attention_shape = (batch_size, num_heads, seq_length, head_dim)\n         conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n \n         for i in range(config.num_hidden_layers):\n             if config.layer_types[i] == \"full_attention\":\n-                self.assertEqual(decoder_past_key_values.key_cache[i].shape, attention_shape)\n-                self.assertEqual(decoder_past_key_values.value_cache[i].shape, attention_shape)\n+                self.assertEqual(past_key_values.key_cache[i].shape, attention_shape)\n+                self.assertEqual(past_key_values.value_cache[i].shape, attention_shape)\n             else:\n-                self.assertEqual(decoder_past_key_values.conv_cache[i], conv_shape)\n+                self.assertEqual(past_key_values.conv_cache[i].shape, conv_shape)\n \n     def _check_caches_are_equal(self, cache1: Lfm2HybridConvCache, cache2: Lfm2HybridConvCache):\n         if not isinstance(cache1, Lfm2HybridConvCache) or not isinstance(cache2, Lfm2HybridConvCache):\n@@ -138,41 +133,6 @@ def test_attention_outputs(self):\n             self.assertEqual(len(self_attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n             self.assertListEqual(list(self_attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n \n-    @pytest.mark.generate\n-    def test_past_key_values_format(self):\n-        \"\"\"Lfm2Moe has a special cache format as it alternates between attention and conv layers\"\"\"\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            model = model_class(config).to(torch_device).eval()\n-            if \"use_cache\" not in inputs:\n-                inputs[\"use_cache\"] = True\n-            outputs = model(**inputs)\n-\n-            past_kv = outputs[\"past_key_values\"]\n-\n-            num_query_attention_heads = config.num_attention_heads\n-            embed_dim = config.hidden_size\n-            per_head_embed_dim = embed_dim // num_query_attention_heads\n-            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n-\n-            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n-            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n-            default_conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n-\n-            num_cache_decoder_layers = len(past_kv)\n-            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n-\n-            for i in range(config.num_hidden_layers):\n-                if config.layer_types[i] == \"full_attention\":\n-                    self_attention_layer_keys = past_kv.key_cache[i]\n-                    self_attention_layer_values = past_kv.value_cache[i]\n-                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n-                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n-                else:\n-                    conv_layer = past_kv.conv_cache[i]\n-                    self.assertEqual(conv_layer.shape, default_conv_shape)\n-\n \n @require_torch_accelerator\n @require_read_token"
        },
        {
            "sha": "81f2ae752140f8d467221f307c876e7c733230e7",
            "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 49,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -16,8 +16,6 @@\n \n import unittest\n \n-import pytest\n-\n from transformers import AutoTokenizer, is_torch_available, set_seed\n from transformers.testing_utils import (\n     cleanup,\n@@ -71,24 +69,21 @@ class Lfm2MoeModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = Lfm2MoeForCausalLM if is_torch_available() else None\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, Lfm2MoeHybridConvCache)\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, Lfm2MoeHybridConvCache)\n \n-        # (batch, head, seq_length, head_features)\n-        attention_shape = (\n-            batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-            config.hidden_size // config.num_attention_heads,\n-        )\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        attention_shape = (batch_size, num_heads, seq_length, head_dim)\n         conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n \n         for i in range(config.num_hidden_layers):\n             if config.layer_types[i] == \"full_attention\":\n-                self.assertEqual(decoder_past_key_values.key_cache[i].shape, attention_shape)\n-                self.assertEqual(decoder_past_key_values.value_cache[i].shape, attention_shape)\n+                self.assertEqual(past_key_values.key_cache[i].shape, attention_shape)\n+                self.assertEqual(past_key_values.value_cache[i].shape, attention_shape)\n             else:\n-                self.assertEqual(decoder_past_key_values.conv_cache[i], conv_shape)\n+                self.assertEqual(past_key_values.conv_cache[i].shape, conv_shape)\n \n     def _check_caches_are_equal(self, cache1: Lfm2MoeHybridConvCache, cache2: Lfm2MoeHybridConvCache):\n         if not isinstance(cache1, Lfm2MoeHybridConvCache) or not isinstance(cache2, Lfm2MoeHybridConvCache):\n@@ -145,41 +140,6 @@ def test_attention_outputs(self):\n             self.assertEqual(len(self_attentions), sum(layer == \"full_attention\" for layer in config.layer_types))\n             self.assertListEqual(list(self_attentions[0].shape[-3:]), [config.num_attention_heads, seq_len, seq_len])\n \n-    @pytest.mark.generate\n-    def test_past_key_values_format(self):\n-        \"\"\"Lfm2Moe has a special cache format as it alternates between attention and conv layers\"\"\"\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            model = model_class(config).to(torch_device).eval()\n-            if \"use_cache\" not in inputs:\n-                inputs[\"use_cache\"] = True\n-            outputs = model(**inputs)\n-\n-            past_kv = outputs[\"past_key_values\"]\n-\n-            num_query_attention_heads = config.num_attention_heads\n-            embed_dim = config.hidden_size\n-            per_head_embed_dim = embed_dim // num_query_attention_heads\n-            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n-\n-            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n-            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n-            default_conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n-\n-            num_cache_decoder_layers = len(past_kv)\n-            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n-\n-            for i in range(config.num_hidden_layers):\n-                if config.layer_types[i] == \"full_attention\":\n-                    self_attention_layer_keys = past_kv.key_cache[i]\n-                    self_attention_layer_values = past_kv.value_cache[i]\n-                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n-                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n-                else:\n-                    conv_layer = past_kv.conv_cache[i]\n-                    self.assertEqual(conv_layer.shape, default_conv_shape)\n-\n \n @require_torch_accelerator\n @require_read_token"
        },
        {
            "sha": "85aa9cb72c06513a3db69f0312dea2f18647d093",
            "filename": "tests/models/lfm2_vl/test_modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 16,
            "deletions": 4,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -173,6 +173,22 @@ def setUp(self):\n             self, config_class=Lfm2VlConfig, has_text_modality=False, common_properties=common_properties\n         )\n \n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, Lfm2HybridConvCache)\n+\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        attention_shape = (batch_size, num_heads, seq_length, head_dim)\n+        conv_shape = (batch_size, config.hidden_size, config.conv_L_cache)\n+\n+        for i in range(config.num_hidden_layers):\n+            if config.layer_types[i] == \"full_attention\":\n+                self.assertEqual(past_key_values.key_cache[i].shape, attention_shape)\n+                self.assertEqual(past_key_values.value_cache[i].shape, attention_shape)\n+            else:\n+                self.assertEqual(past_key_values.conv_cache[i].shape, conv_shape)\n+\n     def _check_caches_are_equal(self, cache1: Lfm2HybridConvCache, cache2: Lfm2HybridConvCache):\n         \"\"\"Text model uses lfm2, which has non-standard cache\"\"\"\n         if not isinstance(cache1, Lfm2HybridConvCache) or not isinstance(cache2, Lfm2HybridConvCache):\n@@ -196,10 +212,6 @@ def test_config(self):\n     def test_attention_outputs(self):\n         pass\n \n-    @unittest.skip(\"Lfm2 backbone has a special cache format as it alternates between attention and conv layers\")\n-    def test_past_key_values_format(self):\n-        pass\n-\n     @unittest.skip(\n         \"Lfm2 backbone has a special cache format which is not compatible with compile as it has static address for conv cache\"\n     )"
        },
        {
            "sha": "d5fd63a2612b6bb93d34d1b870c0d6b8f5692ac6",
            "filename": "tests/models/longcat_flash/test_modeling_longcat_flash.py",
            "status": "modified",
            "additions": 8,
            "deletions": 30,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -38,7 +38,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import AutoTokenizer, LongcatFlashForCausalLM, LongcatFlashModel\n+    from transformers import AutoTokenizer, Cache, LongcatFlashForCausalLM, LongcatFlashModel\n \n \n class LongcatFlashModelTester(CausalLMModelTester):\n@@ -231,40 +231,18 @@ def test_save_load_fast_init_from_base(self):\n     def test_save_load_fast_init_to_base(self):\n         pass\n \n-    def test_past_key_values_format(self):\n-        config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-        batch_size, seq_length = inputs[\"input_ids\"].shape\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, Cache)\n \n         k_embed_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n         v_embed_dim = config.v_head_dim\n \n-        self_attention_keys_shape = (batch_size, config.num_key_value_heads, seq_length, k_embed_dim)\n-        self_attention_values_shape = (batch_size, config.num_key_value_heads, seq_length, v_embed_dim)\n+        expected_key_shape = (batch_size, config.num_key_value_heads, seq_length, k_embed_dim)\n+        expected_value_shape = (batch_size, config.num_key_value_heads, seq_length, v_embed_dim)\n \n-        num_hidden_layers = config.num_hidden_layers\n-        all_cache_shapes = [[self_attention_keys_shape, self_attention_values_shape] for _ in range(num_hidden_layers)]\n-\n-        super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n-\n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        from transformers.cache_utils import Cache\n-\n-        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n-\n-        k_embed_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n-        v_embed_dim = config.v_head_dim\n-\n-        expected_key_shape = (batch_size, config.num_key_value_heads, cache_length, k_embed_dim)\n-        expected_value_shape = (batch_size, config.num_key_value_heads, cache_length, v_embed_dim)\n-\n-        if isinstance(decoder_past_key_values, Cache):\n-            for layer_idx in range(config.num_hidden_layers):\n-                self.assertEqual(decoder_past_key_values.layers[layer_idx].keys.shape, expected_key_shape)\n-                self.assertEqual(decoder_past_key_values.layers[layer_idx].values.shape, expected_value_shape)\n-        else:\n-            for layer_past in decoder_past_key_values:\n-                self.assertEqual(layer_past[0].shape, expected_key_shape)\n-                self.assertEqual(layer_past[1].shape, expected_value_shape)\n+        for layer_idx in range(config.num_hidden_layers):\n+            self.assertEqual(past_key_values.layers[layer_idx].keys.shape, expected_key_shape)\n+            self.assertEqual(past_key_values.layers[layer_idx].values.shape, expected_value_shape)\n \n     @unittest.skip(\"MoE experts may not receive gradients with small test data\")\n     def test_training_gradient_checkpointing(self):"
        },
        {
            "sha": "af52713c4fb791fa35bd5cb9318815bcd755514e",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -32,10 +32,10 @@\n     import torch\n \n     from transformers import (\n-        MambaCache,\n         MambaForCausalLM,\n         MambaModel,\n     )\n+    from transformers.models.mamba.modeling_mamba import MambaCache\n \n \n class MambaModelTester:\n@@ -252,6 +252,18 @@ def setUp(self):\n             self, config_class=MambaConfig, n_embd=37, common_properties=[\"hidden_size\", \"num_hidden_layers\"]\n         )\n \n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, MambaCache)\n+\n+        conv_shape = (batch_size, config.intermediate_size, config.conv_kernel)\n+        ssm_shape = (batch_size, config.intermediate_size, config.state_size)\n+\n+        self.assertTrue(config.num_hidden_layers, len(past_key_values.conv_states))\n+\n+        for idx in range(len(past_key_values.conv_states)):\n+            self.assertEqual(past_key_values.conv_states[idx].shape, conv_shape)\n+            self.assertEqual(past_key_values.ssm_states[idx].shape, ssm_shape)\n+\n     def assertInterval(self, member, container, msg=None):\n         r\"\"\"\n         Simple utility function to check if a member is inside an interval."
        },
        {
            "sha": "5471acd86b60bbd0c5f31bba75190fc7094d645e",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -255,6 +255,21 @@ def setUp(self):\n             self, config_class=Mamba2Config, n_embd=37, common_properties=[\"hidden_size\", \"num_hidden_layers\"]\n         )\n \n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, Mamba2Cache)\n+\n+        intermediate_size = config.expand * config.hidden_size\n+        conv_shape = (\n+            config.num_hidden_layers,\n+            batch_size,\n+            intermediate_size + 2 * config.n_groups * config.state_size,\n+            config.conv_kernel,\n+        )\n+        ssm_shape = (config.num_hidden_layers, batch_size, config.num_heads, config.head_dim, config.state_size)\n+\n+        self.assertEqual(past_key_values.conv_states.shape, conv_shape)\n+        self.assertEqual(past_key_values.ssm_states.shape, ssm_shape)\n+\n     def test_mamba2_caching(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_mamba2_caching(*config_and_inputs)"
        },
        {
            "sha": "26aafc8d0589dd4e6427981467e5d42f43055a02",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 6,
            "deletions": 29,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -15,8 +15,6 @@\n \n import unittest\n \n-import pytest\n-\n from transformers import is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n@@ -141,14 +139,14 @@ def _check_attentions_for_generate(\n                 if config.layer_types[layer_idx] == \"full_attention\":\n                     self.assertEqual(layer_attention.shape, expected_shape)\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, MiniMaxCache)\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, MiniMaxCache)\n \n         # (batch, head, seq_length, head_features)\n         key_value_cache_expected_shape = (\n             batch_size,\n             config.num_key_value_heads,\n-            cache_length,\n+            seq_length,\n             config.hidden_size // config.num_attention_heads,\n         )\n         # (batch, head, head_features, head_features)\n@@ -161,12 +159,10 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n \n         for layer_idx in range(config.num_hidden_layers):\n             if config.layer_types[layer_idx] == \"full_attention\":\n-                self.assertEqual(decoder_past_key_values.layers[layer_idx].keys.shape, key_value_cache_expected_shape)\n-                self.assertEqual(\n-                    decoder_past_key_values.layers[layer_idx].values.shape, key_value_cache_expected_shape\n-                )\n+                self.assertEqual(past_key_values.layers[layer_idx].keys.shape, key_value_cache_expected_shape)\n+                self.assertEqual(past_key_values.layers[layer_idx].values.shape, key_value_cache_expected_shape)\n             else:\n-                self.assertEqual(decoder_past_key_values.linear_cache[layer_idx].shape, linear_cache_expected_shape)\n+                self.assertEqual(past_key_values.linear_cache[layer_idx].shape, linear_cache_expected_shape)\n \n     def _check_caches_are_equal(self, cache1: MiniMaxCache, cache2: MiniMaxCache):\n         if not isinstance(cache1, MiniMaxCache) or not isinstance(cache2, MiniMaxCache):\n@@ -183,25 +179,6 @@ def _check_caches_are_equal(self, cache1: MiniMaxCache, cache2: MiniMaxCache):\n                 torch.testing.assert_close(cache1.layers[idx].values, cache1.layers[idx].values)\n             torch.testing.assert_close(cache1.linear_cache[idx], cache2.linear_cache[idx])\n \n-    @pytest.mark.generate\n-    def test_past_key_values_format(self, custom_all_cache_shapes=None):\n-        \"\"\"\n-        Test that the KV cache is formatted correctly.\n-        \"\"\"\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            model = model_class(config).to(torch_device)\n-            model = model.eval()\n-            if \"use_cache\" not in inputs:\n-                inputs[\"use_cache\"] = True\n-            outputs = model(**inputs)\n-\n-            past_kv = outputs[\"past_key_values\"]\n-\n-            batch_size, seq_length = inputs[\"input_ids\"].shape\n-            self._check_past_key_values_for_generate(batch_size, past_kv, seq_length, config)\n-\n     @unittest.skip(reason=\"MiniMaxCache does not support `crop()` method\")\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         pass"
        },
        {
            "sha": "06c46044f57ab939dae63ced4abb2e319859bb9e",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 75,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -397,86 +397,21 @@ def test_eager_padding_matches_padding_free_with_position_ids(self):\n     def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    @pytest.mark.generate\n-    # overridden because mllama is not an encoder-decoder model, but has encoder-decoder-like cache\n-    def test_past_key_values_format(self):\n-        # Test that the KV cache is formatted correctly. Exceptions need to explicitly overwrite this test. Having a\n-        # standard KV cache format is important for a consistent API (and for advanced generation methods).\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            model = model_class(config).to(torch_device)\n-            if \"use_cache\" not in inputs:\n-                inputs[\"use_cache\"] = True\n-            outputs = model(**inputs)\n-\n-            text_config = config.get_text_config()\n-            num_hidden_layers = (\n-                getattr(text_config, \"decoder_layers\", None)\n-                or getattr(text_config, \"num_decoder_layers\", None)\n-                or text_config.num_hidden_layers\n-            )\n-            num_attention_heads = getattr(text_config, \"decoder_attention_heads\", text_config.num_attention_heads)\n-            embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n-            per_head_embed_dim = embed_dim // num_attention_heads\n-\n-            # some models have different num-head for query vs key/value so we need to assign correct value\n-            # BUT only after `per_head_embed_dim` is set\n-            num_attention_heads = (\n-                text_config.num_key_value_heads\n-                if getattr(text_config, \"num_key_value_heads\", None) is not None\n-                else num_attention_heads\n-            )\n-\n-            past_kv = outputs[\"past_key_values\"]\n-            self.assertEqual(len(past_kv), num_hidden_layers)\n-            batch_size, seq_length = inputs[\"input_ids\"].shape\n-            for i in range(num_hidden_layers):\n-                if i in self.model_tester.text_config[\"cross_attention_layers\"]:\n-                    self.assertEqual(\n-                        past_kv.layers[i].keys.shape,\n-                        (batch_size, num_attention_heads, self.model_tester.image_length, per_head_embed_dim),\n-                    )\n-                    self.assertEqual(\n-                        past_kv.layers[i].values.shape,\n-                        (batch_size, num_attention_heads, self.model_tester.image_length, per_head_embed_dim),\n-                    )\n-                else:\n-                    self.assertEqual(\n-                        past_kv.layers[i].keys.shape, (batch_size, num_attention_heads, seq_length, per_head_embed_dim)\n-                    )\n-                    self.assertEqual(\n-                        past_kv.layers[i].values.shape,\n-                        (batch_size, num_attention_heads, seq_length, per_head_embed_dim),\n-                    )\n-\n     # overridden because mllama has special cache for self and cross attentions\n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, Cache)\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, Cache)\n+\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n \n-        for layer_idx in range(len(decoder_past_key_values)):\n+        for layer_idx in range(len(past_key_values)):\n             if layer_idx in self.model_tester.text_config[\"cross_attention_layers\"]:\n-                expected_shape = (\n-                    batch_size,\n-                    config.num_key_value_heads\n-                    if hasattr(config, \"num_key_value_heads\")\n-                    else config.num_attention_heads,\n-                    self.model_tester.image_length,\n-                    config.hidden_size // config.num_attention_heads,\n-                )\n+                expected_shape = (batch_size, num_heads, self.model_tester.image_length, head_dim)\n             else:\n-                # (batch, head, cache_length, head_features)\n-                expected_shape = (\n-                    batch_size,\n-                    config.num_key_value_heads\n-                    if hasattr(config, \"num_key_value_heads\")\n-                    else config.num_attention_heads,\n-                    cache_length,\n-                    config.hidden_size // config.num_attention_heads,\n-                )\n+                expected_shape = (batch_size, num_heads, seq_length, head_dim)\n             # check shape key, value\n-            self.assertListEqual([decoder_past_key_values.layers[layer_idx].keys.shape], [expected_shape])\n-            self.assertListEqual([decoder_past_key_values.layers[layer_idx].values.shape], [expected_shape])\n+            self.assertEqual(past_key_values.layers[layer_idx].keys.shape, expected_shape)\n+            self.assertEqual(past_key_values.layers[layer_idx].values.shape, expected_shape)\n \n     def test_generate_text_only_with_cache(self):\n         \"\"\""
        },
        {
            "sha": "15bb2336e4b16e0a5f161f5532b8ce13a571d69f",
            "filename": "tests/models/qwen3_next/test_modeling_qwen3_next.py",
            "status": "modified",
            "additions": 9,
            "deletions": 45,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -16,7 +16,6 @@\n import tempfile\n import unittest\n \n-import pytest\n from parameterized import parameterized\n \n from transformers import is_torch_available\n@@ -73,25 +72,22 @@ class Qwen3NextModelTest(CausalLMModelTest, unittest.TestCase):\n \n     model_tester_class = Qwen3NextModelTester\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n         \"Qwen3-Next has a special Cache as it alternates with gated deltanet layers\"\n-        self.assertIsInstance(decoder_past_key_values, Qwen3NextDynamicCache)\n+        self.assertIsInstance(past_key_values, Qwen3NextDynamicCache)\n \n-        # (batch, head, seq_length, head_features)\n-        expected_shape = (\n-            batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-            config.hidden_size // config.num_attention_heads,\n-        )\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        expected_shape = (batch_size, num_heads, seq_length, head_dim)\n \n-        attention_layer_indices = decoder_past_key_values.transformer_layers\n+        attention_layer_indices = past_key_values.transformer_layers\n         self.assertListEqual(\n-            [decoder_past_key_values.key_cache[idx].shape for idx in attention_layer_indices],\n+            [past_key_values.key_cache[idx].shape for idx in attention_layer_indices],\n             [expected_shape] * len(attention_layer_indices),\n         )\n         self.assertListEqual(\n-            [decoder_past_key_values.value_cache[idx].shape for idx in attention_layer_indices],\n+            [past_key_values.value_cache[idx].shape for idx in attention_layer_indices],\n             [expected_shape] * len(attention_layer_indices),\n         )\n \n@@ -106,38 +102,6 @@ def _check_caches_are_equal(self, cache1: Cache, cache2: Cache):\n                 torch.testing.assert_close(cache1.key_cache[idx], cache2.key_cache[idx])\n                 torch.testing.assert_close(cache1.value_cache[idx], cache2.value_cache[idx])\n \n-    @pytest.mark.generate\n-    def test_past_key_values_format(self):\n-        \"Needs to be overwritten as Qwen3-Next alternates between attention layers and gated deltanet layers.\"\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            model = model_class(config).to(torch_device)\n-            model = model.eval()\n-            if \"use_cache\" not in inputs:\n-                inputs[\"use_cache\"] = True\n-            outputs = model(**inputs)\n-\n-            past_kv = outputs[\"past_key_values\"]\n-\n-            num_query_attention_heads = config.num_attention_heads\n-            embed_dim = config.hidden_size\n-            per_head_embed_dim = embed_dim // num_query_attention_heads\n-            num_key_value_heads = getattr(config, \"num_key_value_heads\", num_query_attention_heads)\n-\n-            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n-            default_self_attention_shape = (batch_size, num_key_value_heads, seq_length, per_head_embed_dim)\n-\n-            num_cache_decoder_layers = len(past_kv)\n-            self.assertEqual(num_cache_decoder_layers, config.num_hidden_layers)\n-\n-            for i in range(config.num_hidden_layers):\n-                if config.layer_types[i] == \"full_attention\":\n-                    self_attention_layer_keys = past_kv.key_cache[i]\n-                    self_attention_layer_values = past_kv.value_cache[i]\n-                    self.assertEqual(self_attention_layer_keys.shape, default_self_attention_shape)\n-                    self.assertEqual(self_attention_layer_values.shape, default_self_attention_shape)\n-\n     def test_attention_outputs(self):\n         \"Needs to be overwritten as Qwen3-Next alternates between attention layers and gated deltanet layers.\"\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "bf6e11ecebb9e81c7222fafdd9eaa67e1596ef07",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 38,
            "deletions": 1,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -43,7 +43,7 @@\n         ReformerModelWithLMHead,\n         ReformerTokenizer,\n     )\n-    from transformers.models.reformer.modeling_reformer import ReformerLayer\n+    from transformers.models.reformer.modeling_reformer import ReformerDynamicCache, ReformerLayer\n \n \n class ReformerModelTester:\n@@ -689,6 +689,25 @@ def _check_hidden_states_for_generate(\n                 [expected_shape] * len(iter_hidden_states),\n             )\n \n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, ReformerDynamicCache)\n+\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        hidden_size = getattr(config, \"d_model\", config.hidden_size)\n+        head_dim = getattr(config, \"head_dim\", hidden_size // config.num_attention_heads)\n+\n+        # For cross attention cache, the seq_length depends on the model, so we remove that dim\n+        expected_shape = (batch_size, seq_length, num_heads * head_dim)\n+\n+        # Check the size is coherent\n+        self.assertEqual(config.num_hidden_layers, len(past_key_values))\n+\n+        # Check each layer has the correct shape\n+        for idx in range(len(past_key_values)):\n+            self.assertEqual(past_key_values.states_cache[idx].shape, expected_shape)\n+            self.assertEqual(past_key_values.buckets_cache[idx].shape, (0,))\n+\n     @unittest.skip(reason=\"The model doesn't support left padding\")  # and it's not used enough to be worth fixing :)\n     def test_left_padding_compatibility(self):\n         pass\n@@ -866,6 +885,24 @@ def _check_hidden_states_for_generate(\n                 [expected_shape] * len(iter_hidden_states),\n             )\n \n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, ReformerDynamicCache)\n+\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        hidden_size = getattr(config, \"d_model\", config.hidden_size)\n+        head_dim = getattr(config, \"head_dim\", hidden_size // config.num_attention_heads)\n+\n+        # For cross attention cache, the seq_length depends on the model, so we remove that dim\n+        expected_shape = (batch_size, seq_length, num_heads * head_dim)\n+\n+        # Check the size is coherent\n+        self.assertEqual(config.num_hidden_layers, len(past_key_values))\n+\n+        # Check each layer has the correct shape\n+        for idx in range(len(past_key_values)):\n+            self.assertEqual(past_key_values.states_cache[idx].shape, expected_shape)\n+\n     @unittest.skip(reason=\"Fails because the sequence length is not a multiple of 4\")\n     def test_problem_types(self):\n         pass"
        },
        {
            "sha": "a8c8c23b3d12e43adedc0711f293a8e16e42d5e8",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 21,
            "deletions": 19,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -301,25 +301,27 @@ class ZambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         else {}\n     )\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, ZambaHybridDynamicCache)\n-\n-        # (batch, head, seq_length, head_features)\n-        expected_shape = (\n-            batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-            config.hidden_size // config.num_attention_heads,\n-        )\n-\n-        self.assertListEqual(\n-            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n-            [expected_shape] * len(decoder_past_key_values.key_cache),\n-        )\n-        self.assertListEqual(\n-            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n-            [expected_shape] * len(decoder_past_key_values.value_cache),\n-        )\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, ZambaHybridDynamicCache)\n+\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"attention_head_dim\")\n+        attention_shape = (batch_size, num_heads, seq_length, head_dim)\n+\n+        intermediate_size = config.mamba_expand * config.hidden_size\n+        conv_shape = (batch_size, intermediate_size, config.mamba_d_conv)\n+        ssm_shape = (batch_size, config.n_mamba_heads, intermediate_size // config.n_mamba_heads, config.mamba_d_state)\n+\n+        self.assertTrue(config.num_hidden_layers, len(past_key_values))\n+\n+        for idx in range(len(past_key_values)):\n+            if config.layers_block_type[idx] == \"mamba\":\n+                self.assertEqual(past_key_values.conv_states[idx].shape, conv_shape)\n+                self.assertEqual(past_key_values.ssm_states[idx].shape, ssm_shape)\n+            else:\n+                self.assertEqual(past_key_values.key_cache[idx].shape, attention_shape)\n+                self.assertEqual(past_key_values.value_cache[idx].shape, attention_shape)\n \n     def _check_caches_are_equal(self, cache1: ZambaHybridDynamicCache, cache2: ZambaHybridDynamicCache):\n         if not isinstance(cache1, ZambaHybridDynamicCache) or not isinstance(cache2, ZambaHybridDynamicCache):"
        },
        {
            "sha": "6c6c5a42bed6deb8bf84e3d245c6407788798d7e",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 32,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=bfe2b623ef9bd301e57463258ffcc5e6fb19e2ac",
            "patch": "@@ -313,25 +313,31 @@ class Zamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n \n-    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n-        self.assertIsInstance(decoder_past_key_values, Zamba2HybridDynamicCache)\n+    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config):\n+        self.assertIsInstance(past_key_values, Zamba2HybridDynamicCache)\n \n-        # (batch, head, seq_length, head_features)\n-        expected_shape = (\n+        # (batch, kv heads, seq_length, head_dim)\n+        num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n+        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        attention_shape = (batch_size, num_heads, seq_length, head_dim)\n+\n+        intermediate_size = config.mamba_expand * config.hidden_size\n+        conv_shape = (\n             batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            cache_length,\n-            config.hidden_size // config.num_attention_heads,\n+            intermediate_size + 2 * config.mamba_ngroups * config.mamba_d_state,\n+            config.mamba_d_conv,\n         )\n+        ssm_shape = (batch_size, config.n_mamba_heads, config.mamba_headdim, config.mamba_d_state)\n \n-        self.assertListEqual(\n-            [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n-            [expected_shape] * len(decoder_past_key_values.key_cache),\n-        )\n-        self.assertListEqual(\n-            [value_cache.shape for value_cache in decoder_past_key_values.value_cache],\n-            [expected_shape] * len(decoder_past_key_values.value_cache),\n-        )\n+        self.assertTrue(config.num_hidden_layers, len(past_key_values))\n+\n+        for idx in range(len(past_key_values)):\n+            if config.layers_block_type[idx] == \"mamba\":\n+                self.assertEqual(past_key_values.conv_states[idx].shape, conv_shape)\n+                self.assertEqual(past_key_values.ssm_states[idx].shape, ssm_shape)\n+            else:\n+                self.assertEqual(past_key_values.key_cache[idx].shape, attention_shape)\n+                self.assertEqual(past_key_values.value_cache[idx].shape, attention_shape)\n \n     def _check_caches_are_equal(self, cache1: Zamba2HybridDynamicCache, cache2: Zamba2HybridDynamicCache):\n         if not isinstance(cache1, Zamba2HybridDynamicCache) or not isinstance(cache2, Zamba2HybridDynamicCache):\n@@ -355,23 +361,6 @@ def setUp(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    def test_past_key_values_format(self):\n-        \"\"\"\n-        Overwriting to pass the expected cache shapes (Zamba2 has cache shape = [batch_size, 0] for mamba layers)\n-        \"\"\"\n-        config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-        batch_size, seq_length = inputs[\"input_ids\"].shape\n-        per_head_embed_dim = config.attention_head_dim  # note: this one is not a common attribute name\n-        self_attention_cache_shape = (batch_size, config.num_key_value_heads, seq_length, per_head_embed_dim)\n-        # build the full cache shapes, including mamba layers\n-        all_cache_shapes = []\n-        for i in range(config.num_hidden_layers):\n-            if config.layers_block_type[i] == \"mamba\":\n-                all_cache_shapes.append([torch.Size([batch_size, 0]), torch.Size([batch_size, 0])])\n-            else:\n-                all_cache_shapes.append([self_attention_cache_shape, self_attention_cache_shape])\n-        super().test_past_key_values_format(custom_all_cache_shapes=all_cache_shapes)\n-\n     @unittest.skip(reason=\"Zamba2 has hybrid cache.\")\n     def test_generate_continue_from_inputs_embeds(self):\n         pass"
        }
    ],
    "stats": {
        "total": 988,
        "additions": 382,
        "deletions": 606
    }
}