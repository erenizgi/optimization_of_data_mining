{
    "author": "zucchini-nlp",
    "message": "[Phi4] add multimodal chat template (#36996)\n\n* phi4 chat template\n\n* remove from valid kwargs",
    "sha": "98601cc818a727eb266f64549c1c2c8c9426c962",
    "files": [
        {
            "sha": "3fa2b61cc976b2c74d4b97d60958dc8e0d5b2ffd",
            "filename": "docs/source/en/model_doc/phi4_multimodal.md",
            "status": "modified",
            "additions": 36,
            "deletions": 29,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/98601cc818a727eb266f64549c1c2c8c9426c962/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/98601cc818a727eb266f64549c1c2c8c9426c962/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md?ref=98601cc818a727eb266f64549c1c2c8c9426c962",
            "patch": "@@ -30,14 +30,8 @@ found [here](https://github.com/huggingface/transformers/blob/main/src/transform\n In the following, we demonstrate how to use it for inference depending on the input modalities (text, image, audio).\n \n ```python\n-import requests\n import torch\n-import os\n-import io\n-from PIL import Image\n-import soundfile as sf\n from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n-from urllib.request import urlopen\n \n \n # Define model path\n@@ -52,21 +46,25 @@ model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,  tor\n model.load_adapter(model_path, adapter_name=\"speech\", device_map=device, adapter_kwargs={\"subfolder\": 'speech-lora'})\n model.load_adapter(model_path, adapter_name=\"vision\", device_map=device, adapter_kwargs={\"subfolder\": 'vision-lora'})\n \n-# Define prompt structure\n-user_prompt = '<|user|>'\n-assistant_prompt = '<|assistant|>'\n-prompt_suffix = '<|end|>'\n+# Part : Image Processing\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n \n-# Part 1: Image Processing\n model.set_adapter(\"vision\") # if loaded, activate the vision adapter\n-print(\"\\n--- IMAGE PROCESSING ---\")\n-image_url = 'https://www.ilankelman.org/stopsigns/australia.jpg'\n-prompt = f'{user_prompt}<|image_1|>What is shown in this image?{prompt_suffix}{assistant_prompt}'\n-print(f'>>> Prompt\\n{prompt}')\n-\n-# Download and open image\n-image = Image.open(requests.get(image_url, stream=True).raw)\n-inputs = processor(text=prompt, images=image, return_tensors='pt').to(device)\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(device, torch.float16)\n \n # Generate response\n generate_ids = model.generate(\n@@ -80,19 +78,28 @@ response = processor.batch_decode(\n )[0]\n print(f'>>> Response\\n{response}')\n \n+\n # Part 2: Audio Processing\n model.set_adapter(\"speech\") # if loaded, activate the speech adapter\n-print(\"\\n--- AUDIO PROCESSING ---\")\n audio_url = \"https://upload.wikimedia.org/wikipedia/commons/b/b0/Barbara_Sahakian_BBC_Radio4_The_Life_Scientific_29_May_2012_b01j5j24.flac\"\n-speech_prompt = \"Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the original transcript and the translation.\"\n-prompt = f'{user_prompt}<|audio_1|>{speech_prompt}{prompt_suffix}{assistant_prompt}'\n-print(f'>>> Prompt\\n{prompt}')\n-\n-# Downlowd and open audio file\n-audio, sample_rate = sf.read(io.BytesIO(urlopen(audio_url).read()))\n-\n-# Process with the model\n-inputs = processor(text=prompt, audios=audio, sample_rate=sample_rate, return_tensors='pt').to(device)\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"audio\", \"url\": audio_url},\n+            {\"type\": \"text\", \"text\": \"Transcribe the audio to text, and then translate the audio to French. Use <sep> as a separator between the origina transcript and the translation.\"},\n+        ],\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+    sample_rate=sample_rate,\n+).to(device, torch.float16)\n \n generate_ids = model.generate(\n     **inputs,"
        },
        {
            "sha": "813f89dbeaa6d29877c15c620d475e76aeb5528c",
            "filename": "src/transformers/models/phi4_multimodal/image_processing_phi4_multimodal_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/98601cc818a727eb266f64549c1c2c8c9426c962/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98601cc818a727eb266f64549c1c2c8c9426c962/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py?ref=98601cc818a727eb266f64549c1c2c8c9426c962",
            "patch": "@@ -29,7 +29,7 @@\n     Unpack,\n     convert_to_rgb,\n )\n-from ...image_utils import ImageInput, make_list_of_images, valid_images\n+from ...image_utils import ImageInput, make_flat_list_of_images, valid_images\n from ...utils import TensorType, logging\n \n \n@@ -175,7 +175,7 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n         if not valid_images(images):\n             raise ValueError(\n                 \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \""
        },
        {
            "sha": "f853865f7d5584123f3a757bf54f747cf02cd0dc",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 12,
            "deletions": 16,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/98601cc818a727eb266f64549c1c2c8c9426c962/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98601cc818a727eb266f64549c1c2c8c9426c962/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=98601cc818a727eb266f64549c1c2c8c9426c962",
            "patch": "@@ -62,26 +62,22 @@ class Phi4MultimodalProcessor(ProcessorMixin):\n     tokenizer_class = \"GPT2TokenizerFast\"\n     image_processor_class = \"Phi4MultimodalImageProcessorFast\"\n     audio_processor_class = \"Phi4MultimodalFeatureExtractor\"\n-    valid_kwargs = [\"chat_template\", \"fake_image_token_pattern\", \"fake_audio_token_pattern\"]\n+    valid_kwargs = [\"chat_template\"]\n \n     def __init__(\n         self,\n         image_processor,\n         audio_processor,\n         tokenizer,\n-        fake_image_token_pattern: str = r\"<\\|image_\\d+\\|>\",\n-        fake_audio_token_pattern: str = r\"<\\|audio_\\d+\\|>\",\n         **kwargs,\n     ):\n         super().__init__(image_processor, audio_processor, tokenizer, **kwargs)\n-        self.fake_image_token_pattern = fake_image_token_pattern\n-        self.fake_audio_token_pattern = fake_audio_token_pattern\n \n     def __call__(\n         self,\n         text: Union[TextInput, List[TextInput]],\n         images: Optional[ImageInput] = None,\n-        audios: Optional[AudioInput] = None,\n+        audio: Optional[AudioInput] = None,\n         **kwargs: Unpack[ProcessingKwargs],\n     ) -> BatchFeature:\n         \"\"\"\n@@ -99,7 +95,7 @@ def __call__(\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n-            audios (`List[Union[np.ndarray, torch.Tensor]]`):\n+            audio (`List[Union[np.ndarray, torch.Tensor]]`):\n                 List of the audios to be prepared.\n \n         Returns:\n@@ -120,7 +116,7 @@ def __call__(\n         text_kwargs = output_kwargs[\"text_kwargs\"]\n \n         image_inputs = self.image_processor(images, **image_kwargs) if images is not None else {}\n-        audio_inputs = self.audio_processor(audios, **audio_kwargs) if audios is not None else {}\n+        audio_inputs = self.audio_processor(audio, **audio_kwargs) if audio is not None else {}\n \n         # We pop here for images as we don't need it later\n         num_img_tokens = image_inputs.pop(\"num_img_tokens\", [])\n@@ -134,25 +130,25 @@ def __call__(\n \n         image_token = self.tokenizer.image_token\n         audio_token = self.tokenizer.audio_token\n-        processed_text = [re.sub(self.fake_image_token_pattern, image_token, t) for t in text]\n-        processed_text = [re.sub(self.fake_audio_token_pattern, audio_token, t) for t in processed_text]\n \n         # Check that the number of special tokens is sound\n-        concatenated_prompt = \"\".join(processed_text)\n-        if concatenated_prompt.count(self.tokenizer.image_token) != len(num_img_tokens):\n+        concatenated_prompt = \"\".join(text)\n+        if concatenated_prompt.count(image_token) != len(num_img_tokens):\n             raise ValueError(\n-                \"You should add as much image tokens `<|image_i|>` in your prompt as you pass `images` to the processor\"\n+                \"You should add as much image tokens `<|image|>` in your prompt as you pass `images` to the processor. \",\n+                f\"Input contains {concatenated_prompt.count(image_token)} tokens != {len(num_img_tokens)} images\",\n             )\n-        if concatenated_prompt.count(self.tokenizer.audio_token) != len(audio_embed_sizes):\n+        if concatenated_prompt.count(audio_token) != len(audio_embed_sizes):\n             raise ValueError(\n-                \"You should add as much audio tokens `<|audio_i|>` in your prompt as you pass `audios` to the processor\"\n+                \"You should add as much audio tokens `<|audio|>` in your prompt as you pass `audios` to the processor. \"\n+                f\"Input contains {concatenated_prompt.count(audio_token)} tokens != {len(audio_embed_sizes)} audios\"\n             )\n \n         # Add appropriate number of image/audio tokens (note that the count of replacement is dynamic)\n         image_count_iter = iter(num_img_tokens)\n         audio_count_iter = iter(audio_embed_sizes)\n         processed_text = [\n-            re.sub(re.escape(image_token), lambda _: image_token * next(image_count_iter), t) for t in processed_text\n+            re.sub(re.escape(image_token), lambda _: image_token * next(image_count_iter), t) for t in text\n         ]\n         processed_text = [\n             re.sub(re.escape(audio_token), lambda _: audio_token * next(audio_count_iter), t) for t in processed_text"
        }
    ],
    "stats": {
        "total": 97,
        "additions": 50,
        "deletions": 47
    }
}