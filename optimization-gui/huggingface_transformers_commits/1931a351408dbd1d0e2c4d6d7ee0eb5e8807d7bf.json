{
    "author": "zucchini-nlp",
    "message": "Chat template docs (#36163)\n\n* decompose chat template docs\r\n\r\n* add docs\r\n\r\n* update model docs\r\n\r\n* qwen2-5\r\n\r\n* pixtral\r\n\r\n* remove old chat template\r\n\r\n* also video as list frames supported\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_template_multimodal.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* remove audio for now\r\n\r\n---------\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
    "files": [
        {
            "sha": "dc259103ae2e34dae6487f358fe4a36a8ef1a37f",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -110,6 +110,17 @@\n     - local: kv_cache\n       title: Best Practices for Generation with Cache\n     title: Generation\n+  - isExpanded: false\n+    sections:\n+    - local: chat_template_basics\n+      title: Getting Started with Chat Templates for Text LLMs\n+    - local: chat_template_multimodal\n+      title: Multimodal Chat Templates for Vision and Audio LLMs\n+    - local: chat_template_tools_and_documents\n+      title: Expanding Chat Templates with Tools and Documents\n+    - local: chat_template_advanced\n+      title: Advanced Usage and Customizing Your Chat Templates\n+    title: Chat Templates\n   - isExpanded: false\n     sections:\n     - local: tasks/idefics\n@@ -127,8 +138,6 @@\n     title: Use model-specific APIs\n   - local: custom_models\n     title: Share a custom model\n-  - local: chat_templating\n-    title: Chat templates\n   - local: trainer\n     title: Trainer\n   - local: sagemaker"
        },
        {
            "sha": "5943709539e79ea2ee66f168c0abf49e9e8aa2fe",
            "filename": "docs/source/en/chat_template_advanced.md",
            "status": "added",
            "additions": 463,
            "deletions": 0,
            "changes": 463,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fchat_template_advanced.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fchat_template_advanced.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_template_advanced.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -0,0 +1,463 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Advanced Usage and Customizing Your Chat Templates\n+\n+In this page, we’ll explore more advanced techniques for working with chat templates in Transformers. Whether you’re looking to write your own templates, create custom components, or optimize your templates for efficiency, we’ll cover everything you need to take your templates to the next level. Let’s dive into the tools and strategies that will help you get the most out of your chat models.\n+\n+\n+## How do chat templates work?\n+\n+The chat template for a model is stored on the `tokenizer.chat_template` attribute. Let's take a look at a `Zephyr` chat template, though note this\n+one is a little simplified from the actual one!\n+\n+```\n+{%- for message in messages %}\n+    {{- '<|' + message['role'] + '|>\\n' }}\n+    {{- message['content'] + eos_token }}\n+{%- endfor %}\n+{%- if add_generation_prompt %}\n+    {{- '<|assistant|>\\n' }}\n+{%- endif %}\n+```\n+\n+If you've never seen one of these before, this is a [Jinja template](https://jinja.palletsprojects.com/en/3.1.x/templates/).\n+Jinja is a templating language that allows you to write simple code that generates text. In many ways, the code and\n+syntax resembles Python. In pure Python, this template would look something like this:\n+\n+```python\n+for message in messages:\n+    print(f'<|{message[\"role\"]}|>')\n+    print(message['content'] + eos_token)\n+if add_generation_prompt:\n+    print('<|assistant|>')\n+```\n+\n+Effectively, the template does three things:\n+1. For each message, print the role enclosed in `<|` and `|>`, like `<|user|>` or `<|assistant|>`.\n+2. Next, print the content of the message, followed by the end-of-sequence token.\n+3. Finally, if `add_generation_prompt` is set, print the assistant token, so that the model knows to start generating\n+   an assistant response.\n+\n+This is a pretty simple template but Jinja gives you a lot of flexibility to do more complex things! Let's see a Jinja\n+template that can format inputs similarly to the way LLaMA formats them (note that the real LLaMA template includes \n+handling for default system messages and slightly different system message handling in general - don't use this one \n+in your actual code!)\n+\n+```\n+{%- for message in messages %}\n+    {%- if message['role'] == 'user' %}\n+        {{- bos_token + '[INST] ' + message['content'] + ' [/INST]' }}\n+    {%- elif message['role'] == 'system' %}\n+        {{- '<<SYS>>\\\\n' + message['content'] + '\\\\n<</SYS>>\\\\n\\\\n' }}\n+    {%- elif message['role'] == 'assistant' %}\n+        {{- ' '  + message['content'] + ' ' + eos_token }}\n+    {%- endif %}\n+{%- endfor %}\n+```\n+\n+Hopefully if you stare at this for a little bit you can see what this template is doing - it adds specific tokens like\n+`[INST]` and `[/INST]` based on the role of each message. User, assistant and system messages are clearly\n+distinguishable to the model because of the tokens they're wrapped in.\n+\n+\n+## How do I create a chat template?\n+\n+Simple, just write a jinja template and set `tokenizer.chat_template`. You may find it easier to start with an \n+existing template from another model and simply edit it for your needs! For example, we could take the LLaMA template\n+above and add \"[ASST]\" and \"[/ASST]\" to assistant messages:\n+\n+```\n+{%- for message in messages %}\n+    {%- if message['role'] == 'user' %}\n+        {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n+    {%- elif message['role'] == 'system' %}\n+        {{- '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\n+    {%- elif message['role'] == 'assistant' %}\n+        {{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n+    {%- endif %}\n+{%- endfor %}\n+```\n+\n+Now, simply set the `tokenizer.chat_template` attribute. Next time you use [`~PreTrainedTokenizer.apply_chat_template`], it will\n+use your new template! This attribute will be saved in the `tokenizer_config.json` file, so you can use\n+[`~utils.PushToHubMixin.push_to_hub`] to upload your new template to the Hub and make sure everyone's using the right\n+template for your model!\n+\n+```python\n+template = tokenizer.chat_template\n+template = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\n+tokenizer.chat_template = template  # Set the new template\n+tokenizer.push_to_hub(\"model_name\")  # Upload your new template to the Hub!\n+```\n+\n+The method [`~PreTrainedTokenizer.apply_chat_template`] which uses your chat template is called by the [`TextGenerationPipeline`] class, so \n+once you set the correct chat template, your model will automatically become compatible with [`TextGenerationPipeline`].\n+\n+<Tip>\n+If you're fine-tuning a model for chat, in addition to setting a chat template, you should probably add any new chat\n+control tokens as special tokens in the tokenizer. Special tokens are never split, \n+ensuring that your control tokens are always handled as single tokens rather than being tokenized in pieces. You \n+should also set the tokenizer's `eos_token` attribute to the token that marks the end of assistant generations in your\n+template. This will ensure that text generation tools can correctly figure out when to stop generating text.\n+</Tip>\n+\n+\n+## Why do some models have multiple templates?\n+\n+Some models use different templates for different use cases. For example, they might use one template for normal chat\n+and another for tool-use, or retrieval-augmented generation. In these cases, `tokenizer.chat_template` is a dictionary.\n+This can cause some confusion, and where possible, we recommend using a single template for all use-cases. You can use\n+Jinja statements like `if tools is defined` and `{% macro %}` definitions to easily wrap multiple code paths in a\n+single template.\n+\n+When a tokenizer has multiple templates, `tokenizer.chat_template` will be a `dict`, where each key is the name\n+of a template. The `apply_chat_template` method has special handling for certain template names: Specifically, it will\n+look for a template named `default` in most cases, and will raise an error if it can't find one. However, if a template\n+named `tool_use` exists when the user has passed a `tools` argument, it will use that instead. To access templates\n+with other names, pass the name of the template you want to the `chat_template` argument of\n+`apply_chat_template()`.\n+\n+We find that this can be a bit confusing for users, though - so if you're writing a template yourself, we recommend\n+trying to put it all in a single template where possible!\n+\n+\n+## What template should I use?\n+\n+When setting the template for a model that's already been trained for chat, you should ensure that the template\n+exactly matches the message formatting that the model saw during training, or else you will probably experience\n+performance degradation. This is true even if you're training the model further - you will probably get the best \n+performance if you keep the chat tokens constant. This is very analogous to tokenization - you generally get the\n+best performance for inference or fine-tuning when you precisely match the tokenization used during training.\n+\n+If you're training a model from scratch, or fine-tuning a base language model for chat, on the other hand,\n+you have a lot of freedom to choose an appropriate template! LLMs are smart enough to learn to handle lots of different\n+input formats. One popular choice is the `ChatML` format, and this is a good, flexible choice for many use-cases. \n+It looks like this:\n+\n+```\n+{%- for message in messages %}\n+    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n+{%- endfor %}\n+```\n+\n+If you like this one, here it is in one-liner form, ready to copy into your code. The one-liner also includes\n+handy support for [generation prompts](#what-are-generation-prompts), but note that it doesn't add BOS or EOS tokens!\n+If your model expects those, they won't be added automatically by `apply_chat_template` - in other words, the\n+text will be tokenized with `add_special_tokens=False`. This is to avoid potential conflicts between the template and\n+the `add_special_tokens` logic. If your model expects special tokens, make sure to add them to the template!\n+\n+```python\n+tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n+```\n+\n+This template wraps each message in `<|im_start|>` and `<|im_end|>` tokens, and simply writes the role as a string, which\n+allows for flexibility in the roles you train with. The output looks like this:\n+\n+```text\n+<|im_start|>system\n+You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n+<|im_start|>user\n+How are you?<|im_end|>\n+<|im_start|>assistant\n+I'm doing great!<|im_end|>\n+```\n+\n+The \"user\", \"system\" and \"assistant\" roles are the standard for chat, and we recommend using them when it makes sense,\n+particularly if you want your model to operate well with [`TextGenerationPipeline`]. However, you are not limited\n+to these roles - templating is extremely flexible, and any string can be a role.\n+\n+## I want to add some chat templates! How should I get started?\n+\n+If you have any chat models, you should set their `tokenizer.chat_template` attribute and test it using\n+[`~PreTrainedTokenizer.apply_chat_template`], then push the updated tokenizer to the Hub. This applies even if you're\n+not the model owner - if you're using a model with an empty chat template, or one that's still using the default class\n+template, please open a [pull request](https://huggingface.co/docs/hub/repositories-pull-requests-discussions) to the model repository so that this attribute can be set properly!\n+\n+Once the attribute is set, that's it, you're done! `tokenizer.apply_chat_template` will now work correctly for that\n+model, which means it is also automatically supported in places like `TextGenerationPipeline`!\n+\n+By ensuring that models have this attribute, we can make sure that the whole community gets to use the full power of\n+open-source models. Formatting mismatches have been haunting the field and silently harming performance for too long - \n+it's time to put an end to them!\n+\n+\n+<Tip>\n+\n+The easiest way to get started with writing Jinja templates is to take a look at some existing ones. You can use\n+`print(tokenizer.chat_template)` for any chat model to see what template it's using. In general, models that support tool use have \n+much more complex templates than other models - so when you're just getting started, they're probably a bad example\n+to learn from! You can also take a look at the \n+[Jinja documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/#synopsis) for details\n+of general Jinja formatting and syntax.\n+\n+</Tip>\n+\n+Jinja templates in `transformers` are identical to Jinja templates elsewhere. The main thing to know is that \n+the conversation history will be accessible inside your template as a variable called `messages`.  \n+You will be able to access `messages` in your template just like you can in Python, which means you can loop over \n+it with `{% for message in messages %}` or access individual messages with `{{ messages[0] }}`, for example.\n+\n+You can also use the following tips to write clean, efficient Jinja templates:\n+\n+### Trimming whitespace\n+\n+By default, Jinja will print any whitespace that comes before or after a block. This can be a problem for chat\n+templates, which generally want to be very precise with whitespace! To avoid this, we strongly recommend writing\n+your templates like this:\n+\n+```\n+{%- for message in messages %}\n+    {{- message['role'] + message['content'] }}\n+{%- endfor %}\n+```\n+\n+rather than like this:\n+\n+```\n+{% for message in messages %}\n+    {{ message['role'] + message['content'] }}\n+{% endfor %}\n+```\n+\n+Adding `-` will strip any whitespace that comes before the block. The second example looks innocent, but the newline\n+and indentation may end up being included in the output, which is probably not what you want!\n+\n+### Special variables\n+\n+Inside your template, you will have access several special variables. The most important of these is `messages`, \n+which contains the chat history as a list of message dicts. However, there are several others. Not every\n+variable will be used in every template. The most common other variables are:\n+\n+- `tools` contains a list of tools in JSON schema format. Will be `None` or undefined if no tools are passed.\n+- `documents` contains a list of documents in the format `{\"title\": \"Title\", \"contents\": \"Contents\"}`, used for retrieval-augmented generation. Will be `None` or undefined if no documents are passed.\n+- `add_generation_prompt` is a bool that is `True` if the user has requested a generation prompt, and `False` otherwise. If this is set, your template should add the header for an assistant message to the end of the conversation. If your model doesn't have a specific header for assistant messages, you can ignore this flag.\n+- **Special tokens** like `bos_token` and `eos_token`. These are extracted from `tokenizer.special_tokens_map`. The exact tokens available inside each template will differ depending on the parent tokenizer.\n+\n+<Tip>\n+\n+You can actually pass any `kwarg` to `apply_chat_template`, and it will be accessible inside the template as a variable. In general,\n+we recommend trying to stick to the core variables above, as it will make your model harder to use if users have\n+to write custom code to pass model-specific `kwargs`. However, we're aware that this field moves quickly, so if you\n+have a new use-case that doesn't fit in the core API, feel free to use a new `kwarg` for it! If a new `kwarg`\n+becomes common we may promote it into the core API and create a standard, documented format for it.\n+\n+</Tip>\n+\n+### Callable functions\n+\n+There is also a short list of callable functions available to you inside your templates. These are:\n+\n+- `raise_exception(msg)`: Raises a `TemplateException`. This is useful for debugging, and for telling users when they're\n+doing something that your template doesn't support.\n+- `strftime_now(format_str)`: Equivalent to `datetime.now().strftime(format_str)` in Python. This is used for getting\n+the current date/time in a specific format, which is sometimes included in system messages.\n+\n+### Compatibility with non-Python Jinja\n+\n+There are multiple implementations of Jinja in various languages. They generally have the same syntax,\n+but a key difference is that when you're writing a template in Python you can use Python methods, such as\n+`.lower()` on strings or `.items()` on dicts. This will break if someone tries to use your template on a non-Python\n+implementation of Jinja. Non-Python implementations are particularly common in deployment environments, where JS\n+and Rust are very popular. \n+\n+Don't panic, though! There are a few easy changes you can make to your templates to ensure they're compatible across\n+all implementations of Jinja:\n+\n+- Replace Python methods with Jinja filters. These usually have the same name, for example `string.lower()` becomes\n+  `string|lower`, and `dict.items()` becomes `dict|items`. One notable change is that `string.strip()` becomes `string|trim`.\n+  See the [list of built-in filters](https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters)\n+  in the Jinja documentation for more.\n+- Replace `True`, `False` and `None`, which are Python-specific, with `true`, `false` and `none`.\n+- Directly rendering a dict or list may give different results in other implementations (for example, string entries\n+  might change from single-quoted to double-quoted). Adding the `tojson` filter can help to ensure consistency here.\n+\n+### Writing generation prompts\n+\n+We mentioned above that `add_generation_prompt` is a special variable that will be accessible inside your template,\n+and is controlled by the user setting the `add_generation_prompt` flag. If your model expects a header for\n+assistant messages, then your template must support adding the header when `add_generation_prompt` is set.\n+\n+Here is an example of a template that formats messages ChatML-style, with generation prompt support:\n+\n+```text\n+{{- bos_token }}\n+{%- for message in messages %}\n+    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n+{%- endfor %}\n+{%- if add_generation_prompt %}\n+    {{- '<|im_start|>assistant\\n' }}\n+{%- endif %}\n+```\n+\n+The exact content of the assistant header will depend on your specific model, but it should always be **the string\n+that represents the start of an assistant message**, so that if the user applies your template with \n+`add_generation_prompt=True` and then generates text, the model will write an assistant response. Also note that some\n+models do not need a generation prompt, because assistant messages always begin immediately after user messages. \n+This is particularly common for LLaMA and Mistral models, where assistant messages begin immediately after the `[/INST]`\n+token that ends user messages. In these cases, the template can ignore the `add_generation_prompt` flag.\n+\n+Generation prompts are important! If your model requires a generation prompt but it is not set in the template, then\n+model generations will likely be severely degraded, or the model may display unusual behaviour like continuing \n+the final user message! \n+\n+### Writing and debugging larger templates\n+\n+When this feature was introduced, most templates were quite small, the Jinja equivalent of a \"one-liner\" script. \n+However, with new models and features like tool-use and RAG, some templates can be 100 lines long or more. When\n+writing templates like these, it's a good idea to write them in a separate file, using a text editor. You can easily \n+extract a chat template to a file:\n+\n+```python\n+open(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n+```\n+\n+Or load the edited template back into the tokenizer:\n+\n+```python\n+tokenizer.chat_template = open(\"template.jinja\").read()\n+```\n+\n+As an added bonus, when you write a long, multi-line template in a separate file, line numbers in that file will\n+exactly correspond to line numbers in template parsing or execution errors. This will make it much easier to\n+identify the source of issues.\n+\n+\n+\n+## Writing templates for tools\n+\n+Although chat templates do not enforce a specific API for tools (or for anything, really), we recommend \n+template authors try to stick to a standard API where possible. The whole point of chat templates is to allow code\n+to be transferable across models, so deviating from the standard tools API means users will have to write\n+custom code to use tools with your model. Sometimes it's unavoidable, but often with clever templating you can\n+make the standard API work!\n+\n+Below, we'll list the elements of the standard API, and give tips on writing templates that will work well with it.\n+\n+### Tool definitions\n+\n+Your template should expect that the variable `tools` will either be null (if no tools are passed), or is a list \n+of JSON schema dicts. Our chat template methods allow users to pass tools as either JSON schema or Python functions, but when\n+functions are passed, we automatically generate JSON schema and pass that to your template. As a result, the \n+`tools` variable that your template receives will always be a list of JSON schema. Here is\n+a sample tool JSON schema:\n+\n+```json\n+{\n+  \"type\": \"function\", \n+  \"function\": {\n+    \"name\": \"multiply\", \n+    \"description\": \"A function that multiplies two numbers\", \n+    \"parameters\": {\n+      \"type\": \"object\", \n+      \"properties\": {\n+        \"a\": {\n+          \"type\": \"number\", \n+          \"description\": \"The first number to multiply\"\n+        }, \n+        \"b\": {\n+          \"type\": \"number\",\n+          \"description\": \"The second number to multiply\"\n+        }\n+      }, \n+      \"required\": [\"a\", \"b\"]\n+    }\n+  }\n+}\n+```\n+\n+And here is some example code for handling tools in your chat template. Remember, this is just an example for a\n+specific format - your model will probably need different formatting!\n+\n+```text\n+{%- if tools %}\n+    {%- for tool in tools %}\n+        {{- '<tool>' + tool['function']['name'] + '\\n' }}\n+        {%- for argument in tool['function']['parameters']['properties'] %}\n+            {{- argument + ': ' + tool['function']['parameters']['properties'][argument]['description'] + '\\n' }}\n+        {%- endfor %}\n+        {{- '\\n</tool>' }}\n+    {%- endif %}\n+{%- endif %}\n+```\n+\n+The specific tokens and tool descriptions your template renders should of course be chosen to match the ones your model\n+was trained with. There is no requirement that your **model** understands JSON schema input, only that your template can translate\n+JSON schema into your model's format. For example, [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024) \n+was trained with tools defined using Python function headers, but the Command-R tool template accepts JSON schema, \n+converts types internally and renders the input tools as Python headers. You can do a lot with templates!\n+\n+### Tool calls\n+\n+Tool calls, if present, will be a list attached to a message with the \"assistant\" role. Note that `tool_calls` is \n+always a list, even though most tool-calling models only support single tool calls at a time, which means\n+the list will usually only have a single element. Here is a sample message dict containing a tool call:\n+\n+```json\n+{\n+  \"role\": \"assistant\",\n+  \"tool_calls\": [\n+    {\n+      \"type\": \"function\",\n+      \"function\": {\n+        \"name\": \"multiply\",\n+        \"arguments\": {\n+          \"a\": 5,\n+          \"b\": 6\n+        }\n+      }\n+    }\n+  ]\n+}\n+```\n+\n+And a common pattern for handling them would be something like this:\n+\n+```text\n+{%- if message['role'] == 'assistant' and 'tool_calls' in message %}\n+    {%- for tool_call in message['tool_calls'] %}\n+            {{- '<tool_call>' + tool_call['function']['name'] + '\\n' + tool_call['function']['arguments']|tojson + '\\n</tool_call>' }}\n+        {%- endif %}\n+    {%- endfor %}\n+{%- endif %}\n+```\n+\n+Again, you should render the tool call with the formatting and special tokens that your model expects.\n+\n+### Tool responses\n+\n+Tool responses have a simple format: They are a message dict with the \"tool\" role, a \"name\" key giving the name\n+of the called function, and a \"content\" key containing the result of the tool call. Here is a sample tool response:\n+\n+```json\n+{\n+  \"role\": \"tool\",\n+  \"name\": \"multiply\",\n+  \"content\": \"30\"\n+}\n+```\n+\n+You don't need to use all of the keys in the tool response. For example, if your model doesn't expect the function\n+name to be included in the tool response, then rendering it can be as simple as:\n+\n+```text\n+{%- if message['role'] == 'tool' %}\n+    {{- \"<tool_result>\" + message['content'] + \"</tool_result>\" }}\n+{%- endif %}\n+```\n+\n+Again, remember that the actual formatting and special tokens are model-specific - you should take a lot of care\n+to ensure that tokens, whitespace and everything else exactly match the format your model was trained with!"
        },
        {
            "sha": "2179fa4779ad35830e0c146e38383a3dff79bb5e",
            "filename": "docs/source/en/chat_template_basics.md",
            "status": "added",
            "additions": 287,
            "deletions": 0,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fchat_template_basics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fchat_template_basics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_template_basics.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -0,0 +1,287 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Getting Started with Chat Templates for Text LLMs\n+\n+An increasingly common use case for LLMs is **chat**. In a chat context, rather than continuing a single string\n+of text (as is the case with a standard language model), the model instead continues a conversation that consists\n+of one or more **messages**, each of which includes a **role**, like \"user\" or \"assistant\", as well as message text.\n+\n+Much like tokenization, different models expect very different input formats for chat. This is the reason we added\n+**chat templates** as a feature. Chat templates are part of the tokenizer for text-only LLMs or processor for multimodal LLMs. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the format that the model expects. \n+\n+We'll explore the basic usage of chat templates with text-only LLMs in this page. For detailed guidance on multimodal models, we have a dedicated [documentation oage for multimodal models](./chat_template_multimodal), which covers how to work with image, video and audio inputs in your templates.\n+\n+Let's make this concrete with a quick example using the `mistralai/Mistral-7B-Instruct-v0.1` model:\n+\n+```python\n+>>> from transformers import AutoTokenizer\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n+\n+>>> chat = [\n+...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n+...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n+...   {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n+... ]\n+\n+>>> tokenizer.apply_chat_template(chat, tokenize=False)\n+\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n+```\n+\n+Notice how the tokenizer has added the control tokens [INST] and [/INST] to indicate the start and end of \n+user messages (but not assistant messages!), and the entire chat is condensed into a single string. \n+If we use `tokenize=True`, which is the default setting, that string will also be tokenized for us.\n+\n+Now, try the same code, but swap in the `HuggingFaceH4/zephyr-7b-beta` model instead, and you should get:\n+\n+```text\n+<|user|>\n+Hello, how are you?</s>\n+<|assistant|>\n+I'm doing great. How can I help you today?</s>\n+<|user|>\n+I'd like to show off how chat templating works!</s>\n+```\n+\n+Both Zephyr and Mistral-Instruct were fine-tuned from the same base model, `Mistral-7B-v0.1`. However, they were trained\n+with totally different chat formats. Without chat templates, you would have to write manual formatting code for each\n+model, and it's very easy to make minor errors that hurt performance! Chat templates handle the details of formatting \n+for you, allowing you to write universal code that works for any model.\n+\n+\n+## How do I use chat templates?\n+\n+As you can see in the example above, chat templates are easy to use. Simply build a list of messages, with `role`\n+and `content` keys, and then pass it to the [`~PreTrainedTokenizer.apply_chat_template`] or [`~ProcessorMixin.apply_chat_template`] method\n+depending on what type of model you are using. Once you do that,\n+you'll get output that's ready to go! When using chat templates as input for model generation, it's also a good idea\n+to use `add_generation_prompt=True` to add a [generation prompt](#what-are-generation-prompts). \n+\n+Here's an example of preparing input for `model.generate()`, using `Zephyr` again:\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+checkpoint = \"HuggingFaceH4/zephyr-7b-beta\"\n+tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n+model = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n+    },\n+    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n+ ]\n+tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n+print(tokenizer.decode(tokenized_chat[0]))\n+```\n+This will yield a string in the input format that Zephyr expects. \n+```text\n+<|system|>\n+You are a friendly chatbot who always responds in the style of a pirate</s> \n+<|user|>\n+How many helicopters can a human eat in one sitting?</s> \n+<|assistant|>\n+```\n+\n+Now that our input is formatted correctly for Zephyr, we can use the model to generate a response to the user's question:\n+\n+```python\n+outputs = model.generate(tokenized_chat, max_new_tokens=128) \n+print(tokenizer.decode(outputs[0]))\n+```\n+\n+This will yield:\n+\n+```text\n+<|system|>\n+You are a friendly chatbot who always responds in the style of a pirate</s> \n+<|user|>\n+How many helicopters can a human eat in one sitting?</s> \n+<|assistant|>\n+Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n+```\n+\n+Arr, 'twas easy after all!\n+\n+\n+## Is there an automated pipeline for chat?\n+\n+Yes, there is! Our text generation pipelines support chat inputs, which makes it easy to use chat models. In the past,\n+we used to use a dedicated \"ConversationalPipeline\" class, but this has now been deprecated and its functionality\n+has been merged into the [`TextGenerationPipeline`]. Let's try the `Zephyr` example again, but this time using \n+a pipeline:\n+\n+```python\n+from transformers import pipeline\n+\n+pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n+    },\n+    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n+]\n+print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response\n+```\n+\n+```text\n+{'role': 'assistant', 'content': \"Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\"}\n+```\n+\n+The pipeline will take care of all the details of tokenization and calling `apply_chat_template` for you -\n+once the model has a chat template, all you need to do is initialize the pipeline and pass it the list of messages!\n+\n+\n+## What are \"generation prompts\"?\n+\n+You may have noticed that the `apply_chat_template` method has an `add_generation_prompt` argument. This argument tells\n+the template to add tokens that indicate the start of a bot response. For example, consider the following chat:\n+\n+```python\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Hi there!\"},\n+    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},\n+    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}\n+]\n+```\n+\n+Here's what this will look like without a generation prompt, for a model that uses standard \"ChatML\" formatting:\n+\n+```python\n+tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n+\"\"\"<|im_start|>user\n+Hi there!<|im_end|>\n+<|im_start|>assistant\n+Nice to meet you!<|im_end|>\n+<|im_start|>user\n+Can I ask a question?<|im_end|>\n+\"\"\"\n+```\n+\n+And here's what it looks like **with** a generation prompt:\n+\n+```python\n+tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+\"\"\"<|im_start|>user\n+Hi there!<|im_end|>\n+<|im_start|>assistant\n+Nice to meet you!<|im_end|>\n+<|im_start|>user\n+Can I ask a question?<|im_end|>\n+<|im_start|>assistant\n+\"\"\"\n+```\n+\n+Note that this time, we've added the tokens that indicate the start of a bot response. This ensures that when the model\n+generates text it will write a bot response instead of doing something unexpected, like continuing the user's \n+message. Remember, chat models are still just language models - they're trained to continue text, and chat is just a \n+special kind of text to them! You need to guide them with appropriate control tokens, so they know what they're \n+supposed to be doing.\n+\n+Not all models require generation prompts. Some models, like LLaMA, don't have any\n+special tokens before bot responses. In these cases, the `add_generation_prompt` argument will have no effect. The exact\n+effect that `add_generation_prompt` has will depend on the template being used.\n+\n+\n+## What does \"continue_final_message\" do?\n+\n+When passing a list of messages to `apply_chat_template` or `TextGenerationPipeline`, you can choose\n+to format the chat so the model will continue the final message in the chat instead of starting a new one. This is done\n+by removing any end-of-sequence tokens that indicate the end of the final message, so that the model will simply\n+extend the final message when it begins to generate text. This is useful for \"prefilling\" the model's response. \n+\n+Here's an example:\n+\n+```python\n+chat = [\n+    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n+    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n+]\n+\n+formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)\n+model.generate(**formatted_chat)\n+```\n+\n+The model will generate text that continues the JSON string, rather than starting a new message. This approach\n+can be very useful for improving the accuracy of the model's instruction-following when you know how you want\n+it to start its replies.\n+\n+Because `add_generation_prompt` adds the tokens that start a new message, and `continue_final_message` removes any\n+end-of-message tokens from the final message, it does not make sense to use them together. As a result, you'll\n+get an error if you try!\n+\n+<Tip>\n+\n+The default behaviour of `TextGenerationPipeline` is to set `add_generation_prompt=True` so that it starts a new\n+message. However, if the final message in the input chat has the \"assistant\" role, it will assume that this message is \n+a prefill and switch to `continue_final_message=True` instead, because most models do not support multiple \n+consecutive assistant messages. You can override this behaviour by explicitly passing the `continue_final_message` \n+argument when calling the pipeline.\n+\n+</Tip>\n+\n+\n+## Can I use chat templates in training?\n+\n+Yes! This is a good way to ensure that the chat template matches the tokens the model sees during training.\n+We recommend that you apply the chat template as a preprocessing step for your dataset. After this, you\n+can simply continue like any other language model training task. When training, you should usually set \n+`add_generation_prompt=False`, because the added tokens to prompt an assistant response will not be helpful during \n+training. Let's see an example:\n+\n+```python\n+from transformers import AutoTokenizer\n+from datasets import Dataset\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n+\n+chat1 = [\n+    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n+    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n+]\n+chat2 = [\n+    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n+    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n+]\n+\n+dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n+dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n+print(dataset['formatted_chat'][0])\n+```\n+And we get:\n+```text\n+<|user|>\n+Which is bigger, the moon or the sun?</s>\n+<|assistant|>\n+The sun.</s>\n+```\n+\n+From here, just continue training like you would with a standard language modelling task, using the `formatted_chat` column.\n+\n+<Tip>\n+\n+By default, some tokenizers add special tokens like `<bos>` and `<eos>` to text they tokenize. Chat templates should \n+already include all the special tokens they need, and so additional special tokens will often be incorrect or \n+duplicated, which will hurt model performance.\n+\n+Therefore, if you format text with `apply_chat_template(tokenize=False)`, you should set the argument\n+`add_special_tokens=False` when you tokenize that text later. If you use `apply_chat_template(tokenize=True)`, you don't need to worry about this!\n+\n+</Tip>\n+"
        },
        {
            "sha": "1b283449605b71fbdbf670b6e5dc9a9f047dcaa4",
            "filename": "docs/source/en/chat_template_multimodal.md",
            "status": "added",
            "additions": 289,
            "deletions": 0,
            "changes": 289,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fchat_template_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fchat_template_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_template_multimodal.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -0,0 +1,289 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Multimodal Chat Templates for Vision and Audio LLMs\n+\n+In this section, we'll explore how to use chat templates with multimodal models, enabling your templates to handle a variety of inputs such as text, images, and audio. Multimodal models provide richer, more interactive experiences, and understanding how to effectively combine these inputs within your templates is key. We’ll walk through how to work with different modalities, configure your templates for optimal performance, and tackle common challenges along the way.\n+\n+Just like with text-only LLMs, multimodal models expect a chat with **messages**, each of which includes a **role** and **content**. However, for multimodal models, chat templates are a part of the [Processor](./main_cllasses/processors) class. Let's see how we can format our prompts when there are images or videos in the input along with text.\n+\n+\n+## Image inputs\n+\n+For models such as [LLaVA](https://huggingface.co/llava-hf) the prompts can be formatted as below. Notice that the only difference from text-only models is that we need to also pass a placeholder for input images. To accommodate for extra modalities, each **content** is a list containing either a text or an image **type**.\n+\n+Let's make this concrete with a quick example using the `llava-hf/llava-onevision-qwen2-0.5b-ov-hf` model:\n+\n+```python\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+\n+model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+      \"role\": \"user\",\n+      \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What are these?\"},\n+        ],\n+    },\n+]\n+\n+formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+print(formatted_prompt)\n+```\n+\n+This yields a string in LLaVA's expected input format with many `<image>` tokens prepended before the text.\n+```text\n+'<|im_start|>system \n+<|im_start|>system \n+You are a friendly chatbot who always responds in the style of a pirate<|im_end|><|im_start|>user <image>\n+What are these?<|im_end|>\n+```\n+\n+\n+### Image paths or URLs\n+\n+To incorporate images into your chat templates, you can pass them as file paths or URLs. This method automatically loads the image, processes it, and prepares the necessary pixel values to create ready-to-use inputs for the model. This approach simplifies the integration of images, enabling seamless multimodal functionality.\n+\n+Let's see how it works with an example using the same model as above. This time we'll indicate an image URL with `\"url\"` key in the message's **content** and ask the chat template to `tokenize` and `return_dict`. Currently, \"base64\", \"url\", and \"path\" are supported image sources.\n+\n+```python\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+\n+model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+      \"role\": \"user\",\n+      \"content\": [\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What are these?\"},\n+        ],\n+    },\n+]\n+\n+processed_chat = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n+print(processed_chat.keys())\n+```\n+\n+This yields a dictionary with inputs processed and ready to be further passed into [`~GenerationMixin.generate`] to generate text.\n+```text\n+dict_keys([\"input_ids\", \"attention_mask\", \"pixel_values\", \"image_sizes\"])\n+```\n+\n+\n+## Video inputs\n+\n+Some vision models support videos as inputs as well as images. The message format is very similar to the image-only models with tiny differences to handle loading videos from a URL. We can continue using the same model as before since it supports videos.\n+\n+### Sampling with fixed number of frames\n+\n+Here's an example of how to set up a conversation with video inputs. Notice the extra `kwargs` passed to `processor.apply_chat_template()`. The key parameter here is `num_frames`, which controls how many frames to sample uniformly from the video. Each model checkpoint has a maximum frame count it was trained with, and exceeding this limit can significantly impact generation quality. So, it’s important to choose a frame count that fits both the model's capacity and your computational resources. If you don't specify `num_frames`, the entire video will be loaded without any frame sampling.\n+\n+You also have the option to choose a specific framework to load the video, depending on your preferences or needs. Currently, we support `decord`, `pyav` (the default), `opencv`, and `torchvision`. For this example, we’ll use `decord`, as it's a bit faster than `pyav`.\n+\n+\n+<Tip>\n+\n+Note that if you are trying to load a video from URL, you can decode the video only with `pyav` or `decord` as backend.\n+\n+</Tip>\n+\n+\n+```python\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+\n+model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+      \"role\": \"user\",\n+      \"content\": [\n+            {\"type\": \"video\", \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\"},\n+            {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n+        ],\n+    },\n+]\n+\n+processed_chat = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+    num_frames=32,\n+    video_load_backend=\"decord\",\n+)\n+print(processed_chat.keys())\n+```\n+\n+### Sampling with FPS\n+\n+When working with long videos, you might want to sample more frames for better representation. Instead of a fixed number of frames, you can specify `video_fps`, which determines how many frames per second to extract. For example, if a video is **10 seconds long** and you set `video_fps=2`, the model will sample **20 frames** (2 per second, uniformly spaced). \n+\n+Using the above model, we need to apply chat template as follows to sample 2 frames per second.\n+\n+```python\n+processed_chat = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    video_fps=32,\n+    video_load_backend=\"decord\",\n+)\n+print(processed_chat.keys())\n+```\n+\n+\n+### Custom Frame Sampling with a Function  \n+\n+Not all models sample frames **uniformly** — some require more complex logic to determine which frames to use. If your model follows a different sampling strategy, you can **customize** frame selection by providing a function:  \n+\n+🔹 Use the `sample_indices_fn` argument to pass a **callable function** for sampling.  \n+🔹 If provided, this function **overrides** standard `num_frames` and `fps` methods.  \n+🔹 It receives all the arguments passed to `load_video` and must return **valid frame indices** to sample.  \n+\n+You should use `sample_indices_fn` when:\n+\n+- If you need a custom sampling strategy (e.g., **adaptive frame selection** instead of uniform sampling).  \n+- If your model prioritizes **key moments** in a video rather than evenly spaced frames.  \n+\n+Here’s an example of how to implement it:  \n+\n+\n+```python\n+\n+def sample_indices_fn(metadata, **kwargs):\n+    # samples only the first and the second frame\n+    return [0, 1]\n+\n+processed_chat = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    sample_indices_fn=sample_indices_fn,\n+    video_load_backend=\"decord\",\n+)\n+print(processed_chat.keys())\n+```\n+\n+By using `sample_indices_fn`, you gain **full control** over frame selection, making your model **more adaptable** to different video scenarios. 🚀  \n+\n+\n+### List of image frames as video\n+\n+Sometimes, instead of having a full video file, you might only have a set of sampled frames stored as images.\n+\n+You can pass a list of image file paths, and the processor will automatically concatenate them into a video. Just make sure that all images have the same size, as they are assumed to be from the same video.\n+\n+\n+```python\n+frames_paths = [\"/path/to/frame0.png\", \"/path/to/frame5.png\", \"/path/to/frame10.png\"]\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+      \"role\": \"user\",\n+      \"content\": [\n+            {\"type\": \"video\", \"path\": frames_paths},\n+            {\"type\": \"text\", \"text\": \"What do you see in this video?\"},\n+        ],\n+    },\n+]\n+\n+processed_chat = processor.apply_chat_template(\n+    messages,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+)\n+print(processed_chat.keys())\n+```\n+\n+\n+## Multimodal conversational pipeline\n+\n+[`ImageTextToTextPipeline`] currently accepts images as inputs but we are planning to add support for video inputs in the future. The pipeline supports chat inputs in the same format as we have seen above. Apart from that, the pipeline will accept chats in OpenAI format. This format is supported exclusively within the pipeline to make inference easier and more accessible. \n+\n+Here is how the OpenAI conversation format looks:\n+\n+```python\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"text\",\n+                \"text\": \"What is in this image?\",\n+            },\n+            {\n+                \"type\": \"image_url\",\n+                \"image_url\": {\"url\": f\"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+            },\n+        ],\n+    }\n+]\n+```\n+\n+## Best Practices for Multimodal Template Configuration\n+\n+\n+To add a custom chat template for your multimodal LLM, simply create your template using [Jinja](https://jinja.palletsprojects.com/en/3.1.x/templates/) and set it with `processor.chat_template`. If you're new to writing chat templates or need some tips, check out our [tutorial here](./chat_template_advanced) for helpful guidance.\n+\n+In some cases, you may want your template to handle a **list of content** from multiple modalities, while still supporting a plain string for text-only inference. Here's an example of how you can achieve that, using the [Llama-Vision](https://huggingface.co/collections/meta-llama/metas-llama-32-multimodal-models-675bfd70e574a62dd0e4059b) chat template.\n+\n+\n+```\n+{% for message in messages %}\n+{% if loop.index0 == 0 %}{{ bos_token }}{% endif %}\n+{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\n+{% if message['content'] is string %}\n+{{ message['content'] }}\n+{% else %}\n+{% for content in message['content'] %}\n+{% if content['type'] == 'image' %}\n+{{ '<|image|>' }}\n+{% elif content['type'] == 'text' %}\n+{{ content['text'] }}\n+{% endif %}\n+{% endfor %}\n+{% endif %}\n+{{ '<|eot_id|>' }}\n+{% endfor %}\n+{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\n+```"
        },
        {
            "sha": "6c5491a2484a50106773b4d31763a4c10cdf7fd7",
            "filename": "docs/source/en/chat_template_tools_and_documents.md",
            "status": "added",
            "additions": 410,
            "deletions": 0,
            "changes": 410,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fchat_template_tools_and_documents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fchat_template_tools_and_documents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_template_tools_and_documents.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -0,0 +1,410 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+# Expanding Chat Templates with Tools and Documents\n+\n+The only argument that `apply_chat_template` requires is `messages`. However, you can pass any keyword\n+argument to `apply_chat_template` and it will be accessible inside the template. This gives you a lot of freedom to use\n+chat templates for many things. There are no restrictions on the names or the format of these arguments - you can pass\n+strings, lists, dicts or whatever else you want. \n+\n+That said, there are some common use-cases for these extra arguments,\n+such as passing tools for function calling, or documents for retrieval-augmented generation. In these common cases,\n+we have some opinionated recommendations about what the names and formats of these arguments should be, which are\n+described in the sections below. We encourage model authors to make their chat templates compatible with this format,\n+to make it easy to transfer tool-calling code between models.\n+\n+## Tool use / function calling\n+\n+\"Tool use\" LLMs can choose to call functions as external tools before generating an answer. When passing tools\n+to a tool-use model, you can simply pass a list of functions to the `tools` argument:\n+\n+```python\n+import datetime\n+\n+def current_time():\n+    \"\"\"Get the current local time as a string.\"\"\"\n+    return str(datetime.now())\n+\n+def multiply(a: float, b: float):\n+    \"\"\"\n+    A function that multiplies two numbers\n+    \n+    Args:\n+        a: The first number to multiply\n+        b: The second number to multiply\n+    \"\"\"\n+    return a * b\n+\n+tools = [current_time, multiply]\n+\n+model_input = tokenizer.apply_chat_template(\n+    messages,\n+    tools=tools\n+)\n+```\n+\n+In order for this to work correctly, you should write your functions in the format above, so that they can be parsed\n+correctly as tools. Specifically, you should follow these rules:\n+\n+- The function should have a descriptive name\n+- Every argument must have a type hint\n+- The function must have a docstring in the standard Google style (in other words, an initial function description  \n+  followed by an `Args:` block that describes the arguments, unless the function does not have any arguments.) \n+- Do not include types in the `Args:` block. In other words, write `a: The first number to multiply`, not\n+  `a (int): The first number to multiply`. Type hints should go in the function header instead.\n+- The function can have a return type and a `Returns:` block in the docstring. However, these are optional\n+  because most tool-use models ignore them.\n+\n+### Passing tool results to the model\n+\n+The sample code above is enough to list the available tools for your model, but what happens if it wants to actually use\n+one? If that happens, you should:\n+\n+1. Parse the model's output to get the tool name(s) and arguments.\n+2. Add the model's tool call(s) to the conversation.\n+3. Call the corresponding function(s) with those arguments.\n+4. Add the result(s) to the conversation\n+\n+### A complete tool use example\n+\n+Let's walk through a tool use example, step by step. For this example, we will use an 8B `Hermes-2-Pro` model,\n+as it is one of the highest-performing tool-use models in its size category at the time of writing. If you have the\n+memory, you can consider using a larger model instead like [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-v01)\n+or [Mixtral-8x22B](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1), both of which also support tool use\n+and offer even stronger performance.\n+\n+First, let's load our model and tokenizer:\n+\n+```python\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+checkpoint = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n+model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+```\n+\n+Next, let's define a list of tools:\n+\n+```python\n+def get_current_temperature(location: str, unit: str) -> float:\n+    \"\"\"\n+    Get the current temperature at a location.\n+    \n+    Args:\n+        location: The location to get the temperature for, in the format \"City, Country\"\n+        unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n+    Returns:\n+        The current temperature at the specified location in the specified units, as a float.\n+    \"\"\"\n+    return 22.  # A real function should probably actually get the temperature!\n+\n+def get_current_wind_speed(location: str) -> float:\n+    \"\"\"\n+    Get the current wind speed in km/h at a given location.\n+    \n+    Args:\n+        location: The location to get the temperature for, in the format \"City, Country\"\n+    Returns:\n+        The current wind speed at the given location in km/h, as a float.\n+    \"\"\"\n+    return 6.  # A real function should probably actually get the wind speed!\n+\n+tools = [get_current_temperature, get_current_wind_speed]\n+```\n+\n+Now, let's set up a conversation for our bot:\n+\n+```python\n+messages = [\n+  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries. You should reply with the unit used in the queried location.\"},\n+  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n+]\n+```\n+\n+Now, let's apply the chat template and generate a response:\n+\n+```python\n+inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n+inputs = {k: v.to(model.device) for k, v in inputs.items()}\n+out = model.generate(**inputs, max_new_tokens=128)\n+print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n+```\n+\n+And we get:\n+\n+```text\n+<tool_call>\n+{\"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}, \"name\": \"get_current_temperature\"}\n+</tool_call><|im_end|>\n+```\n+\n+The model has called the function with valid arguments, in the format requested by the function docstring. It has\n+inferred that we're most likely referring to the Paris in France, and it remembered that, as the home of SI units,\n+the temperature in France should certainly be displayed in Celsius.\n+\n+<Tip>\n+\n+The output format above is specific to the `Hermes-2-Pro` model we're using in this example. Other models may emit different\n+tool call formats, and you may need to do some manual parsing at this step. For example, `Llama-3.1` models will emit\n+slightly different JSON, with `parameters` instead of `arguments`. Regardless of the format the model outputs, you \n+should add the tool call to the conversation in the format below, with `tool_calls`, `function` and `arguments` keys. \n+\n+</Tip>\n+\n+Next, let's append the model's tool call to the conversation.\n+\n+```python\n+tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n+messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n+```\n+\n+<Tip warning={true}>\n+\n+If you're familiar with the OpenAI API, you should pay attention to an important difference here - the `tool_call` is\n+a dict, but in the OpenAI API it's a JSON string. Passing a string may cause errors or strange model behaviour!\n+\n+</Tip>\n+\n+Now that we've added the tool call to the conversation, we can call the function and append the result to the\n+conversation. Since we're just using a dummy function for this example that always returns 22.0, we can just append \n+that result directly.\n+\n+```python\n+messages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n+```\n+\n+<Tip>\n+\n+Some model architectures, notably Mistral/Mixtral, also require a `tool_call_id` here, which should be\n+9 randomly-generated alphanumeric characters, and assigned to the `id` key of the tool call\n+dictionary. The same key should also be assigned to the `tool_call_id` key of the tool response dictionary below, so \n+that tool calls can be matched to tool responses. So, for Mistral/Mixtral models, the code above would be:\n+\n+```python\n+tool_call_id = \"9Ae3bDc2F\"  # Random ID, 9 alphanumeric characters\n+tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n+messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": tool_call}]})\n+```\n+\n+and\n+\n+```python\n+messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call_id, \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n+```\n+\n+</Tip>\n+\n+Finally, let's let the assistant read the function outputs and continue chatting with the user:\n+\n+```python\n+inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n+inputs = {k: v.to(model.device) for k, v in inputs.items()}\n+out = model.generate(**inputs, max_new_tokens=128)\n+print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n+```\n+\n+And we get:\n+\n+```text\n+The current temperature in Paris, France is 22.0 ° Celsius.<|im_end|>\n+```\n+\n+Although this was a simple demo with dummy tools and a single call, the same technique works with \n+multiple real tools and longer conversations. This can be a powerful way to extend the capabilities of conversational\n+agents with real-time information, computational tools like calculators, or access to large databases.\n+\n+### Understanding tool schemas\n+\n+Each function you pass to the `tools` argument of `apply_chat_template` is converted into a \n+[JSON schema](https://json-schema.org/learn/getting-started-step-by-step). These schemas\n+are then passed to the model chat template. In other words, tool-use models do not see your functions directly, and they\n+never see the actual code inside them. What they care about is the function **definitions** and the **arguments** they\n+need to pass to them - they care about what the tools do and how to use them, not how they work! It is up to you\n+to read their outputs, detect if they have requested to use a tool, pass their arguments to the tool function, and\n+return the response in the chat.\n+\n+Generating JSON schemas to pass to the template should be automatic and invisible as long as your functions\n+follow the specification above, but if you encounter problems, or you simply want more control over the conversion, \n+you can handle the conversion manually. Here is an example of a manual schema conversion.\n+\n+```python\n+from transformers.utils import get_json_schema\n+\n+def multiply(a: float, b: float):\n+    \"\"\"\n+    A function that multiplies two numbers\n+    \n+    Args:\n+        a: The first number to multiply\n+        b: The second number to multiply\n+    \"\"\"\n+    return a * b\n+\n+schema = get_json_schema(multiply)\n+print(schema)\n+```\n+\n+This will yield:\n+\n+```json\n+{\n+  \"type\": \"function\", \n+  \"function\": {\n+    \"name\": \"multiply\", \n+    \"description\": \"A function that multiplies two numbers\", \n+    \"parameters\": {\n+      \"type\": \"object\", \n+      \"properties\": {\n+        \"a\": {\n+          \"type\": \"number\", \n+          \"description\": \"The first number to multiply\"\n+        }, \n+        \"b\": {\n+          \"type\": \"number\",\n+          \"description\": \"The second number to multiply\"\n+        }\n+      }, \n+      \"required\": [\"a\", \"b\"]\n+    }\n+  }\n+}\n+```\n+\n+If you wish, you can edit these schemas, or even write them from scratch yourself without using `get_json_schema` at \n+all. JSON schemas can be passed directly to the `tools` argument of \n+`apply_chat_template` - this gives you a lot of power to define precise schemas for more complex functions. Be careful,\n+though - the more complex your schemas, the more likely the model is to get confused when dealing with them! We \n+recommend simple function signatures where possible, keeping arguments (and especially complex, nested arguments) \n+to a minimum.\n+\n+Here is an example of defining schemas by hand, and passing them directly to `apply_chat_template`:\n+\n+```python\n+# A simple function that takes no arguments\n+current_time = {\n+  \"type\": \"function\", \n+  \"function\": {\n+    \"name\": \"current_time\",\n+    \"description\": \"Get the current local time as a string.\",\n+    \"parameters\": {\n+      'type': 'object',\n+      'properties': {}\n+    }\n+  }\n+}\n+\n+# A more complete function that takes two numerical arguments\n+multiply = {\n+  'type': 'function',\n+  'function': {\n+    'name': 'multiply',\n+    'description': 'A function that multiplies two numbers', \n+    'parameters': {\n+      'type': 'object', \n+      'properties': {\n+        'a': {\n+          'type': 'number',\n+          'description': 'The first number to multiply'\n+        }, \n+        'b': {\n+          'type': 'number', 'description': 'The second number to multiply'\n+        }\n+      }, \n+      'required': ['a', 'b']\n+    }\n+  }\n+}\n+\n+model_input = tokenizer.apply_chat_template(\n+    messages,\n+    tools = [current_time, multiply]\n+)\n+```\n+\n+## Retrieval-augmented generation\n+\n+\"Retrieval-augmented generation\" or \"RAG\" LLMs can search a corpus of documents for information before responding\n+to a query. This allows models to vastly expand their knowledge base beyond their limited context size. Our \n+recommendation for RAG models is that their template\n+should accept a `documents` argument. This should be a list of documents, where each \"document\"\n+is a single dict with `title` and `contents` keys, both of which are strings. Because this format is much simpler\n+than the JSON schemas used for tools, no helper functions are necessary.\n+\n+Here's an example of a RAG template in action:\n+\n+```python\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+# Load the model and tokenizer\n+model_id = \"CohereForAI/c4ai-command-r-v01-4bit\"\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n+device = model.device # Get the device the model is loaded on\n+\n+# Define conversation input\n+conversation = [\n+    {\"role\": \"user\", \"content\": \"What has Man always dreamed of?\"}\n+]\n+\n+# Define documents for retrieval-based generation\n+documents = [\n+    {\n+        \"title\": \"The Moon: Our Age-Old Foe\", \n+        \"text\": \"Man has always dreamed of destroying the moon. In this essay, I shall...\"\n+    },\n+    {\n+        \"title\": \"The Sun: Our Age-Old Friend\",\n+        \"text\": \"Although often underappreciated, the sun provides several notable benefits...\"\n+    }\n+]\n+\n+# Tokenize conversation and documents using a RAG template, returning PyTorch tensors.\n+input_ids = tokenizer.apply_chat_template(\n+    conversation=conversation,\n+    documents=documents,\n+    chat_template=\"rag\",\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_tensors=\"pt\").to(device)\n+\n+# Generate a response \n+gen_tokens = model.generate(\n+    input_ids,\n+    max_new_tokens=100,\n+    do_sample=True,\n+    temperature=0.3,\n+    )\n+\n+# Decode and print the generated text along with generation prompt\n+gen_text = tokenizer.decode(gen_tokens[0])\n+print(gen_text)\n+```\n+\n+<Tip>\n+\n+The `documents` input for retrieval-augmented generation is not widely supported, and many models have chat templates which simply ignore this input.\n+\n+To verify if a model supports the `documents` input, you can read its model card, or `print(tokenizer.chat_template)` to see if the `documents` key is used anywhere.\n+\n+One model class that does support it, though, is Cohere's [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-08-2024) and [Command-R+](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024), through their `rag` chat template. You can see additional examples of grounded generation using this feature in their model cards.\n+\n+</Tip>\n+\n+"
        },
        {
            "sha": "3581487e130fb2eb2d65e4d48c45305ad8066676",
            "filename": "docs/source/en/chat_templating.md",
            "status": "removed",
            "additions": 0,
            "deletions": 1166,
            "changes": 1166,
            "blob_url": "https://github.com/huggingface/transformers/blob/3bf02cf44079c84d5d451ae152f6554d1e1de318/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3bf02cf44079c84d5d451ae152f6554d1e1de318/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=3bf02cf44079c84d5d451ae152f6554d1e1de318",
            "patch": "@@ -1,1166 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Chat Templates\n-\n-## Introduction\n-\n-An increasingly common use case for LLMs is **chat**. In a chat context, rather than continuing a single string\n-of text (as is the case with a standard language model), the model instead continues a conversation that consists\n-of one or more **messages**, each of which includes a **role**, like \"user\" or \"assistant\", as well as message text.\n-\n-Much like tokenization, different models expect very different input formats for chat. This is the reason we added\n-**chat templates** as a feature. Chat templates are part of the tokenizer for text-only LLMs or processor for multimodal LLMs. They specify how to convert conversations,\n-represented as lists of messages, into a single tokenizable string in the format that the model expects.\n-\n-Let's make this concrete with a quick example using the `mistralai/Mistral-7B-Instruct-v0.1` model:\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n-\n->>> chat = [\n-...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n-...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n-...   {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n-... ]\n-\n->>> tokenizer.apply_chat_template(chat, tokenize=False)\n-\"<s> [INST] Hello, how are you? [/INST] I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n-```\n-\n-Notice how the tokenizer has added the control tokens [INST] and [/INST] to indicate the start and end of\n-user messages (but not assistant messages!), and the entire chat is condensed into a single string.\n-If we use `tokenize=True`, which is the default setting, that string will also be tokenized for us.\n-\n-Now, try the same code, but swap in the `HuggingFaceH4/zephyr-7b-beta` model instead, and you should get:\n-\n-```text\n-<|user|>\n-Hello, how are you?</s>\n-<|assistant|>\n-I'm doing great. How can I help you today?</s>\n-<|user|>\n-I'd like to show off how chat templating works!</s>\n-```\n-\n-Both Zephyr and Mistral-Instruct were fine-tuned from the same base model, `Mistral-7B-v0.1`. However, they were trained\n-with totally different chat formats. Without chat templates, you would have to write manual formatting code for each\n-model, and it's very easy to make minor errors that hurt performance! Chat templates handle the details of formatting\n-for you, allowing you to write universal code that works for any model.\n-\n-<Tip>\n-\n-Chat templates are a critical component of our [chat CLI](quicktour#chat-with-text-generation-models).\n-You can apply the learnings of this guide there as well.\n-\n-</Tip>\n-\n-\n-## How do I use chat templates?\n-\n-As you can see in the example above, chat templates are easy to use. Simply build a list of messages, with `role`\n-and `content` keys, and then pass it to the [`~PreTrainedTokenizer.apply_chat_template`] or [`~ProcessorMixin.apply_chat_template`] method\n-depending on what type of model you are using. Once you do that,\n-you'll get output that's ready to go! When using chat templates as input for model generation, it's also a good idea\n-to use `add_generation_prompt=True` to add a [generation prompt](#what-are-generation-prompts).\n-\n-## Usage with text-only LLMs\n-Here's an example of preparing input for `model.generate()`, using `Zephyr` again:\n-\n-```python\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-checkpoint = \"HuggingFaceH4/zephyr-7b-beta\"\n-tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n-model = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here\n-\n-messages = [\n-    {\n-        \"role\": \"system\",\n-        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n-    },\n-    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n- ]\n-tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n-print(tokenizer.decode(tokenized_chat[0]))\n-```\n-This will yield a string in the input format that Zephyr expects.\n-```text\n-<|system|>\n-You are a friendly chatbot who always responds in the style of a pirate</s>\n-<|user|>\n-How many helicopters can a human eat in one sitting?</s>\n-<|assistant|>\n-```\n-\n-Now that our input is formatted correctly for Zephyr, we can use the model to generate a response to the user's question:\n-\n-```python\n-outputs = model.generate(tokenized_chat, max_new_tokens=128)\n-print(tokenizer.decode(outputs[0]))\n-```\n-\n-This will yield:\n-\n-```text\n-<|system|>\n-You are a friendly chatbot who always responds in the style of a pirate</s>\n-<|user|>\n-How many helicopters can a human eat in one sitting?</s>\n-<|assistant|>\n-Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n-```\n-\n-## Usage with multimodal LLMs\n-\n-For multimodal LLMs such as [LLaVA](https://huggingface.co/llava-hf) the prompts can be formatted in a similar way. The only difference is you need to pass input images/videos as well along with the text. Each `\"content\"`\n-has to be a list containing either a text or an image/video.\n-\n-Here's an example of preparing input for using `LLaVA` model:\n-\n-```python\n-from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n-\n-model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n-model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)  # You may want to use bfloat16 and/or move to GPU here\n-processor = AutoProcessor.from_pretrained(model_id)\n-\n-messages = [\n-    {\n-        \"role\": \"system\",\n-        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n-    },\n-    {\n-      \"role\": \"user\",\n-      \"content\": [\n-          {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n-          {\"type\": \"text\", \"text\": \"What are these?\"},\n-        ],\n-    },\n-]\n-\n-processed_chat = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n-print(processor.batch_decode(processed_chat[\"input_ids\"][:, :30]))\n-```\n-This yields a string in LLaVAs expected input format with many `<image>` tokens at the end.\n-The `<image>` tokens are placeholders and each one will be replaced by image embeddings when the mode is run in the forward call. The `processed_chat` can be further passed into [`~GenerationMixin.generate`] to generate text.\n-```text\n-'<|im_start|>system\n-You are a friendly chatbot who always responds in the style of a pirate<|im_end|><|im_start|>user <image><image><image><image><image><image><image><image>'\n-```\n-\n-Arr, 'twas easy after all!\n-\n-## Is there an automated pipeline for chat?\n-\n-Yes, there is! Our text generation pipelines support chat inputs, which makes it easy to use chat models. In the past,\n-we used to use a dedicated \"ConversationalPipeline\" class, but this has now been deprecated and its functionality\n-has been merged into the [`TextGenerationPipeline`]. Let's try the `Zephyr` example again, but this time using\n-a pipeline:\n-\n-```python\n-from transformers import pipeline\n-\n-pipe = pipeline(\"text-generation\", \"HuggingFaceH4/zephyr-7b-beta\")\n-messages = [\n-    {\n-        \"role\": \"system\",\n-        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n-    },\n-    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n-]\n-print(pipe(messages, max_new_tokens=128)[0]['generated_text'][-1])  # Print the assistant's response\n-```\n-\n-```text\n-{'role': 'assistant', 'content': \"Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\"}\n-```\n-\n-The pipeline will take care of all the details of tokenization and calling `apply_chat_template` for you -\n-once the model has a chat template, all you need to do is initialize the pipeline and pass it the list of messages!\n-\n-## What are \"generation prompts\"?\n-\n-You may have noticed that the `apply_chat_template` method has an `add_generation_prompt` argument. This argument tells\n-the template to add tokens that indicate the start of a bot response. For example, consider the following chat:\n-\n-```python\n-messages = [\n-    {\"role\": \"user\", \"content\": \"Hi there!\"},\n-    {\"role\": \"assistant\", \"content\": \"Nice to meet you!\"},\n-    {\"role\": \"user\", \"content\": \"Can I ask a question?\"}\n-]\n-```\n-\n-Here's what this will look like without a generation prompt, for a model that uses standard \"ChatML\" formatting:\n-\n-```python\n-tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n-\"\"\"<|im_start|>user\n-Hi there!<|im_end|>\n-<|im_start|>assistant\n-Nice to meet you!<|im_end|>\n-<|im_start|>user\n-Can I ask a question?<|im_end|>\n-\"\"\"\n-```\n-\n-And here's what it looks like **with** a generation prompt:\n-\n-```python\n-tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-\"\"\"<|im_start|>user\n-Hi there!<|im_end|>\n-<|im_start|>assistant\n-Nice to meet you!<|im_end|>\n-<|im_start|>user\n-Can I ask a question?<|im_end|>\n-<|im_start|>assistant\n-\"\"\"\n-```\n-\n-Note that this time, we've added the tokens that indicate the start of a bot response. This ensures that when the model\n-generates text it will write a bot response instead of doing something unexpected, like continuing the user's\n-message. Remember, chat models are still just language models - they're trained to continue text, and chat is just a\n-special kind of text to them! You need to guide them with appropriate control tokens, so they know what they're\n-supposed to be doing.\n-\n-Not all models require generation prompts. Some models, like LLaMA, don't have any\n-special tokens before bot responses. In these cases, the `add_generation_prompt` argument will have no effect. The exact\n-effect that `add_generation_prompt` has will depend on the template being used.\n-\n-## What does \"continue_final_message\" do?\n-\n-When passing a list of messages to `apply_chat_template` or `TextGenerationPipeline`, you can choose\n-to format the chat so the model will continue the final message in the chat instead of starting a new one. This is done\n-by removing any end-of-sequence tokens that indicate the end of the final message, so that the model will simply\n-extend the final message when it begins to generate text. This is useful for \"prefilling\" the model's response.\n-\n-Here's an example:\n-\n-```python\n-chat = [\n-    {\"role\": \"user\", \"content\": \"Can you format the answer in JSON?\"},\n-    {\"role\": \"assistant\", \"content\": '{\"name\": \"'},\n-]\n-\n-formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)\n-model.generate(**formatted_chat)\n-```\n-\n-The model will generate text that continues the JSON string, rather than starting a new message. This approach\n-can be very useful for improving the accuracy of the model's instruction-following when you know how you want\n-it to start its replies.\n-\n-Because `add_generation_prompt` adds the tokens that start a new message, and `continue_final_message` removes any\n-end-of-message tokens from the final message, it does not make sense to use them together. As a result, you'll\n-get an error if you try!\n-\n-<Tip>\n-\n-The default behaviour of `TextGenerationPipeline` is to set `add_generation_prompt=True` so that it starts a new\n-message. However, if the final message in the input chat has the \"assistant\" role, it will assume that this message is\n-a prefill and switch to `continue_final_message=True` instead, because most models do not support multiple\n-consecutive assistant messages. You can override this behaviour by explicitly passing the `continue_final_message`\n-argument when calling the pipeline.\n-\n-</Tip>\n-\n-## Can I use chat templates in training?\n-\n-Yes! This is a good way to ensure that the chat template matches the tokens the model sees during training.\n-We recommend that you apply the chat template as a preprocessing step for your dataset. After this, you\n-can simply continue like any other language model training task. When training, you should usually set\n-`add_generation_prompt=False`, because the added tokens to prompt an assistant response will not be helpful during\n-training. Let's see an example:\n-\n-```python\n-from transformers import AutoTokenizer\n-from datasets import Dataset\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n-\n-chat1 = [\n-    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n-    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n-]\n-chat2 = [\n-    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n-    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n-]\n-\n-dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n-dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n-print(dataset['formatted_chat'][0])\n-```\n-And we get:\n-```text\n-<|user|>\n-Which is bigger, the moon or the sun?</s>\n-<|assistant|>\n-The sun.</s>\n-```\n-\n-From here, just continue training like you would with a standard language modelling task, using the `formatted_chat` column.\n-\n-<Tip>\n-\n-By default, some tokenizers add special tokens like `<bos>` and `<eos>` to text they tokenize. Chat templates should\n-already include all the special tokens they need, and so additional special tokens will often be incorrect or\n-duplicated, which will hurt model performance.\n-\n-Therefore, if you format text with `apply_chat_template(tokenize=False)`, you should set the argument\n-`add_special_tokens=False` when you tokenize that text later. If you use `apply_chat_template(tokenize=True)`, you don't need to worry about this!\n-\n-</Tip>\n-\n-## Advanced: Extra inputs to chat templates\n-\n-The only argument that `apply_chat_template` requires is `messages`. However, you can pass any keyword\n-argument to `apply_chat_template` and it will be accessible inside the template. This gives you a lot of freedom to use\n-chat templates for many things. There are no restrictions on the names or the format of these arguments - you can pass\n-strings, lists, dicts or whatever else you want.\n-\n-That said, there are some common use-cases for these extra arguments,\n-such as passing tools for function calling, or documents for retrieval-augmented generation. In these common cases,\n-we have some opinionated recommendations about what the names and formats of these arguments should be, which are\n-described in the sections below. We encourage model authors to make their chat templates compatible with this format,\n-to make it easy to transfer tool-calling code between models.\n-\n-## Advanced: Tool use / function calling\n-\n-\"Tool use\" LLMs can choose to call functions as external tools before generating an answer. When passing tools\n-to a tool-use model, you can simply pass a list of functions to the `tools` argument:\n-\n-```python\n-import datetime\n-\n-def current_time():\n-    \"\"\"Get the current local time as a string.\"\"\"\n-    return str(datetime.now())\n-\n-def multiply(a: float, b: float):\n-    \"\"\"\n-    A function that multiplies two numbers\n-\n-    Args:\n-        a: The first number to multiply\n-        b: The second number to multiply\n-    \"\"\"\n-    return a * b\n-\n-tools = [current_time, multiply]\n-\n-model_input = tokenizer.apply_chat_template(\n-    messages,\n-    tools=tools\n-)\n-```\n-\n-In order for this to work correctly, you should write your functions in the format above, so that they can be parsed\n-correctly as tools. Specifically, you should follow these rules:\n-\n-- The function should have a descriptive name\n-- Every argument must have a type hint\n-- The function must have a docstring in the standard Google style (in other words, an initial function description\n-  followed by an `Args:` block that describes the arguments, unless the function does not have any arguments.\n-- Do not include types in the `Args:` block. In other words, write `a: The first number to multiply`, not\n-  `a (int): The first number to multiply`. Type hints should go in the function header instead.\n-- The function can have a return type and a `Returns:` block in the docstring. However, these are optional\n-  because most tool-use models ignore them.\n-\n-### Passing tool results to the model\n-\n-The sample code above is enough to list the available tools for your model, but what happens if it wants to actually use\n-one? If that happens, you should:\n-\n-1. Parse the model's output to get the tool name(s) and arguments.\n-2. Add the model's tool call(s) to the conversation.\n-3. Call the corresponding function(s) with those arguments.\n-4. Add the result(s) to the conversation\n-\n-### A complete tool use example\n-\n-Let's walk through a tool use example, step by step. For this example, we will use an 8B `Hermes-2-Pro` model,\n-as it is one of the highest-performing tool-use models in its size category at the time of writing. If you have the\n-memory, you can consider using a larger model instead like [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-v01)\n-or [Mixtral-8x22B](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1), both of which also support tool use\n-and offer even stronger performance.\n-\n-First, let's load our model and tokenizer:\n-\n-```python\n-import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n-\n-checkpoint = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n-\n-tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n-model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n-```\n-\n-Next, let's define a list of tools:\n-\n-```python\n-def get_current_temperature(location: str, unit: str) -> float:\n-    \"\"\"\n-    Get the current temperature at a location.\n-\n-    Args:\n-        location: The location to get the temperature for, in the format \"City, Country\"\n-        unit: The unit to return the temperature in. (choices: [\"celsius\", \"fahrenheit\"])\n-    Returns:\n-        The current temperature at the specified location in the specified units, as a float.\n-    \"\"\"\n-    return 22.  # A real function should probably actually get the temperature!\n-\n-def get_current_wind_speed(location: str) -> float:\n-    \"\"\"\n-    Get the current wind speed in km/h at a given location.\n-\n-    Args:\n-        location: The location to get the temperature for, in the format \"City, Country\"\n-    Returns:\n-        The current wind speed at the given location in km/h, as a float.\n-    \"\"\"\n-    return 6.  # A real function should probably actually get the wind speed!\n-\n-tools = [get_current_temperature, get_current_wind_speed]\n-```\n-\n-Now, let's set up a conversation for our bot:\n-\n-```python\n-messages = [\n-  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries. You should reply with the unit used in the queried location.\"},\n-  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris right now?\"}\n-]\n-```\n-\n-Now, let's apply the chat template and generate a response:\n-\n-```python\n-inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n-inputs = {k: v.to(model.device) for k, v in inputs.items()}\n-out = model.generate(**inputs, max_new_tokens=128)\n-print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n-```\n-\n-And we get:\n-\n-```text\n-<tool_call>\n-{\"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}, \"name\": \"get_current_temperature\"}\n-</tool_call><|im_end|>\n-```\n-\n-The model has called the function with valid arguments, in the format requested by the function docstring. It has\n-inferred that we're most likely referring to the Paris in France, and it remembered that, as the home of SI units,\n-the temperature in France should certainly be displayed in Celsius.\n-\n-<Tip>\n-\n-The output format above is specific to the `Hermes-2-Pro` model we're using in this example. Other models may emit different\n-tool call formats, and you may need to do some manual parsing at this step. For example, `Llama-3.1` models will emit\n-slightly different JSON, with `parameters` instead of `arguments`. Regardless of the format the model outputs, you\n-should add the tool call to the conversation in the format below, with `tool_calls`, `function` and `arguments` keys.\n-\n-</Tip>\n-\n-Next, let's append the model's tool call to the conversation.\n-\n-```python\n-tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n-messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": tool_call}]})\n-```\n-\n-<Tip warning={true}>\n-\n-If you're familiar with the OpenAI API, you should pay attention to an important difference here - the `tool_call` is\n-a dict, but in the OpenAI API it's a JSON string. Passing a string may cause errors or strange model behaviour!\n-\n-</Tip>\n-\n-Now that we've added the tool call to the conversation, we can call the function and append the result to the\n-conversation. Since we're just using a dummy function for this example that always returns 22.0, we can just append\n-that result directly.\n-\n-```python\n-messages.append({\"role\": \"tool\", \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n-```\n-\n-<Tip>\n-\n-Some model architectures, notably Mistral/Mixtral, also require a `tool_call_id` here, which should be\n-9 randomly-generated alphanumeric characters, and assigned to the `id` key of the tool call\n-dictionary. The same key should also be assigned to the `tool_call_id` key of the tool response dictionary below, so\n-that tool calls can be matched to tool responses. So, for Mistral/Mixtral models, the code above would be:\n-\n-```python\n-tool_call_id = \"9Ae3bDc2F\"  # Random ID, 9 alphanumeric characters\n-tool_call = {\"name\": \"get_current_temperature\", \"arguments\": {\"location\": \"Paris, France\", \"unit\": \"celsius\"}}\n-messages.append({\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"id\": tool_call_id, \"function\": tool_call}]})\n-```\n-\n-and\n-\n-```python\n-messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call_id, \"name\": \"get_current_temperature\", \"content\": \"22.0\"})\n-```\n-\n-</Tip>\n-\n-Finally, let's let the assistant read the function outputs and continue chatting with the user:\n-\n-```python\n-inputs = tokenizer.apply_chat_template(messages, tools=tools, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n-inputs = {k: v.to(model.device) for k, v in inputs.items()}\n-out = model.generate(**inputs, max_new_tokens=128)\n-print(tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):]))\n-```\n-\n-And we get:\n-\n-```text\n-The current temperature in Paris, France is 22.0 ° Celsius.<|im_end|>\n-```\n-\n-Although this was a simple demo with dummy tools and a single call, the same technique works with\n-multiple real tools and longer conversations. This can be a powerful way to extend the capabilities of conversational\n-agents with real-time information, computational tools like calculators, or access to large databases.\n-\n-### Understanding tool schemas\n-\n-Each function you pass to the `tools` argument of `apply_chat_template` is converted into a\n-[JSON schema](https://json-schema.org/learn/getting-started-step-by-step). These schemas\n-are then passed to the model chat template. In other words, tool-use models do not see your functions directly, and they\n-never see the actual code inside them. What they care about is the function **definitions** and the **arguments** they\n-need to pass to them - they care about what the tools do and how to use them, not how they work! It is up to you\n-to read their outputs, detect if they have requested to use a tool, pass their arguments to the tool function, and\n-return the response in the chat.\n-\n-Generating JSON schemas to pass to the template should be automatic and invisible as long as your functions\n-follow the specification above, but if you encounter problems, or you simply want more control over the conversion,\n-you can handle the conversion manually. Here is an example of a manual schema conversion.\n-\n-```python\n-from transformers.utils import get_json_schema\n-\n-def multiply(a: float, b: float):\n-    \"\"\"\n-    A function that multiplies two numbers\n-\n-    Args:\n-        a: The first number to multiply\n-        b: The second number to multiply\n-    \"\"\"\n-    return a * b\n-\n-schema = get_json_schema(multiply)\n-print(schema)\n-```\n-\n-This will yield:\n-\n-```json\n-{\n-  \"type\": \"function\",\n-  \"function\": {\n-    \"name\": \"multiply\",\n-    \"description\": \"A function that multiplies two numbers\",\n-    \"parameters\": {\n-      \"type\": \"object\",\n-      \"properties\": {\n-        \"a\": {\n-          \"type\": \"number\",\n-          \"description\": \"The first number to multiply\"\n-        },\n-        \"b\": {\n-          \"type\": \"number\",\n-          \"description\": \"The second number to multiply\"\n-        }\n-      },\n-      \"required\": [\"a\", \"b\"]\n-    }\n-  }\n-}\n-```\n-\n-If you wish, you can edit these schemas, or even write them from scratch yourself without using `get_json_schema` at\n-all. JSON schemas can be passed directly to the `tools` argument of\n-`apply_chat_template` - this gives you a lot of power to define precise schemas for more complex functions. Be careful,\n-though - the more complex your schemas, the more likely the model is to get confused when dealing with them! We\n-recommend simple function signatures where possible, keeping arguments (and especially complex, nested arguments)\n-to a minimum.\n-\n-Here is an example of defining schemas by hand, and passing them directly to `apply_chat_template`:\n-\n-```python\n-# A simple function that takes no arguments\n-current_time = {\n-  \"type\": \"function\",\n-  \"function\": {\n-    \"name\": \"current_time\",\n-    \"description\": \"Get the current local time as a string.\",\n-    \"parameters\": {\n-      'type': 'object',\n-      'properties': {}\n-    }\n-  }\n-}\n-\n-# A more complete function that takes two numerical arguments\n-multiply = {\n-  'type': 'function',\n-  'function': {\n-    'name': 'multiply',\n-    'description': 'A function that multiplies two numbers',\n-    'parameters': {\n-      'type': 'object',\n-      'properties': {\n-        'a': {\n-          'type': 'number',\n-          'description': 'The first number to multiply'\n-        },\n-        'b': {\n-          'type': 'number', 'description': 'The second number to multiply'\n-        }\n-      },\n-      'required': ['a', 'b']\n-    }\n-  }\n-}\n-\n-model_input = tokenizer.apply_chat_template(\n-    messages,\n-    tools = [current_time, multiply]\n-)\n-```\n-\n-## Advanced: Retrieval-augmented generation\n-\n-\"Retrieval-augmented generation\" or \"RAG\" LLMs can search a corpus of documents for information before responding\n-to a query. This allows models to vastly expand their knowledge base beyond their limited context size. Our\n-recommendation for RAG models is that their template\n-should accept a `documents` argument. This should be a list of documents, where each \"document\"\n-is a single dict with `title` and `contents` keys, both of which are strings. Because this format is much simpler\n-than the JSON schemas used for tools, no helper functions are necessary.\n-\n-Here's an example of a RAG template in action:\n-\n-```python\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-\n-# Load the model and tokenizer\n-model_id = \"CohereForAI/c4ai-command-r-v01-4bit\"\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n-device = model.device # Get the device the model is loaded on\n-\n-# Define conversation input\n-conversation = [\n-    {\"role\": \"user\", \"content\": \"What has Man always dreamed of?\"}\n-]\n-\n-# Define documents for retrieval-based generation\n-documents = [\n-    {\n-        \"title\": \"The Moon: Our Age-Old Foe\",\n-        \"text\": \"Man has always dreamed of destroying the moon. In this essay, I shall...\"\n-    },\n-    {\n-        \"title\": \"The Sun: Our Age-Old Friend\",\n-        \"text\": \"Although often underappreciated, the sun provides several notable benefits...\"\n-    }\n-]\n-\n-# Tokenize conversation and documents using a RAG template, returning PyTorch tensors.\n-input_ids = tokenizer.apply_chat_template(\n-    conversation=conversation,\n-    documents=documents,\n-    chat_template=\"rag\",\n-    tokenize=True,\n-    add_generation_prompt=True,\n-    return_tensors=\"pt\").to(device)\n-\n-# Generate a response\n-gen_tokens = model.generate(\n-    input_ids,\n-    max_new_tokens=100,\n-    do_sample=True,\n-    temperature=0.3,\n-    )\n-\n-# Decode and print the generated text along with generation prompt\n-gen_text = tokenizer.decode(gen_tokens[0])\n-print(gen_text)\n-```\n-\n-<Tip>\n-\n-The `documents` input for retrieval-augmented generation is not widely supported, and many models have chat templates which simply ignore this input.\n-\n-To verify if a model supports the `documents` input, you can read its model card, or `print(tokenizer.chat_template)` to see if the `documents` key is used anywhere.\n-\n-One model class that does support it, though, is Cohere's [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-08-2024) and [Command-R+](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024), through their `rag` chat template. You can see additional examples of grounded generation using this feature in their model cards.\n-\n-</Tip>\n-\n-\n-\n-## Advanced: How do chat templates work?\n-\n-The chat template for a model is stored on the `tokenizer.chat_template` attribute. If no chat template is set, the\n-default template for that model class is used instead. Let's take a look at a `Zephyr` chat template, though note this\n-one is a little simplified from the actual one!\n-\n-```\n-{%- for message in messages %}\n-    {{- '<|' + message['role'] + '|>\\n' }}\n-    {{- message['content'] + eos_token }}\n-{%- endfor %}\n-{%- if add_generation_prompt %}\n-    {{- '<|assistant|>\\n' }}\n-{%- endif %}\n-```\n-\n-If you've never seen one of these before, this is a [Jinja template](https://jinja.palletsprojects.com/en/3.1.x/templates/).\n-Jinja is a templating language that allows you to write simple code that generates text. In many ways, the code and\n-syntax resembles Python. In pure Python, this template would look something like this:\n-\n-```python\n-for message in messages:\n-    print(f'<|{message[\"role\"]}|>')\n-    print(message['content'] + eos_token)\n-if add_generation_prompt:\n-    print('<|assistant|>')\n-```\n-\n-Effectively, the template does three things:\n-1. For each message, print the role enclosed in `<|` and `|>`, like `<|user|>` or `<|assistant|>`.\n-2. Next, print the content of the message, followed by the end-of-sequence token.\n-3. Finally, if `add_generation_prompt` is set, print the assistant token, so that the model knows to start generating\n-   an assistant response.\n-\n-This is a pretty simple template but Jinja gives you a lot of flexibility to do more complex things! Let's see a Jinja\n-template that can format inputs similarly to the way LLaMA formats them (note that the real LLaMA template includes\n-handling for default system messages and slightly different system message handling in general - don't use this one\n-in your actual code!)\n-\n-```\n-{%- for message in messages %}\n-    {%- if message['role'] == 'user' %}\n-        {{- bos_token + '[INST] ' + message['content'] + ' [/INST]' }}\n-    {%- elif message['role'] == 'system' %}\n-        {{- '<<SYS>>\\\\n' + message['content'] + '\\\\n<</SYS>>\\\\n\\\\n' }}\n-    {%- elif message['role'] == 'assistant' %}\n-        {{- ' '  + message['content'] + ' ' + eos_token }}\n-    {%- endif %}\n-{%- endfor %}\n-```\n-\n-Hopefully if you stare at this for a little bit you can see what this template is doing - it adds specific tokens like\n-`[INST]` and `[/INST]` based on the role of each message. User, assistant and system messages are clearly\n-distinguishable to the model because of the tokens they're wrapped in.\n-\n-## Advanced: Adding and editing chat templates\n-\n-### How do I create a chat template?\n-\n-Simple, just write a jinja template and set `tokenizer.chat_template`. You may find it easier to start with an\n-existing template from another model and simply edit it for your needs! For example, we could take the LLaMA template\n-above and add \"[ASST]\" and \"[/ASST]\" to assistant messages:\n-\n-```\n-{%- for message in messages %}\n-    {%- if message['role'] == 'user' %}\n-        {{- bos_token + '[INST] ' + message['content'].strip() + ' [/INST]' }}\n-    {%- elif message['role'] == 'system' %}\n-        {{- '<<SYS>>\\\\n' + message['content'].strip() + '\\\\n<</SYS>>\\\\n\\\\n' }}\n-    {%- elif message['role'] == 'assistant' %}\n-        {{- '[ASST] '  + message['content'] + ' [/ASST]' + eos_token }}\n-    {%- endif %}\n-{%- endfor %}\n-```\n-\n-Now, simply set the `tokenizer.chat_template` attribute. Next time you use [`~PreTrainedTokenizer.apply_chat_template`], it will\n-use your new template! This attribute will be saved in the `tokenizer_config.json` file, so you can use\n-[`~utils.PushToHubMixin.push_to_hub`] to upload your new template to the Hub and make sure everyone's using the right\n-template for your model!\n-\n-```python\n-template = tokenizer.chat_template\n-template = template.replace(\"SYS\", \"SYSTEM\")  # Change the system token\n-tokenizer.chat_template = template  # Set the new template\n-tokenizer.push_to_hub(\"model_name\")  # Upload your new template to the Hub!\n-```\n-\n-The method [`~PreTrainedTokenizer.apply_chat_template`] which uses your chat template is called by the [`TextGenerationPipeline`] class, so\n-once you set the correct chat template, your model will automatically become compatible with [`TextGenerationPipeline`].\n-\n-<Tip>\n-If you're fine-tuning a model for chat, in addition to setting a chat template, you should probably add any new chat\n-control tokens as special tokens in the tokenizer. Special tokens are never split,\n-ensuring that your control tokens are always handled as single tokens rather than being tokenized in pieces. You\n-should also set the tokenizer's `eos_token` attribute to the token that marks the end of assistant generations in your\n-template. This will ensure that text generation tools can correctly figure out when to stop generating text.\n-</Tip>\n-\n-\n-### Why do some models have multiple templates?\n-\n-Some models use different templates for different use cases. For example, they might use one template for normal chat\n-and another for tool-use, or retrieval-augmented generation. In these cases, `tokenizer.chat_template` is a dictionary.\n-This can cause some confusion, and where possible, we recommend using a single template for all use-cases. You can use\n-Jinja statements like `if tools is defined` and `{% macro %}` definitions to easily wrap multiple code paths in a\n-single template.\n-\n-When a tokenizer has multiple templates, `tokenizer.chat_template` will be a `dict`, where each key is the name\n-of a template. The `apply_chat_template` method has special handling for certain template names: Specifically, it will\n-look for a template named `default` in most cases, and will raise an error if it can't find one. However, if a template\n-named `tool_use` exists when the user has passed a `tools` argument, it will use that instead. To access templates\n-with other names, pass the name of the template you want to the `chat_template` argument of\n-`apply_chat_template()`.\n-\n-We find that this can be a bit confusing for users, though - so if you're writing a template yourself, we recommend\n-trying to put it all in a single template where possible!\n-\n-### What template should I use?\n-\n-When setting the template for a model that's already been trained for chat, you should ensure that the template\n-exactly matches the message formatting that the model saw during training, or else you will probably experience\n-performance degradation. This is true even if you're training the model further - you will probably get the best\n-performance if you keep the chat tokens constant. This is very analogous to tokenization - you generally get the\n-best performance for inference or fine-tuning when you precisely match the tokenization used during training.\n-\n-If you're training a model from scratch, or fine-tuning a base language model for chat, on the other hand,\n-you have a lot of freedom to choose an appropriate template! LLMs are smart enough to learn to handle lots of different\n-input formats. One popular choice is the `ChatML` format, and this is a good, flexible choice for many use-cases.\n-It looks like this:\n-\n-```\n-{%- for message in messages %}\n-    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n-{%- endfor %}\n-```\n-\n-If you like this one, here it is in one-liner form, ready to copy into your code. The one-liner also includes\n-handy support for [generation prompts](#what-are-generation-prompts), but note that it doesn't add BOS or EOS tokens!\n-If your model expects those, they won't be added automatically by `apply_chat_template` - in other words, the\n-text will be tokenized with `add_special_tokens=False`. This is to avoid potential conflicts between the template and\n-the `add_special_tokens` logic. If your model expects special tokens, make sure to add them to the template!\n-\n-```python\n-tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n-```\n-\n-This template wraps each message in `<|im_start|>` and `<|im_end|>` tokens, and simply writes the role as a string, which\n-allows for flexibility in the roles you train with. The output looks like this:\n-\n-```text\n-<|im_start|>system\n-You are a helpful chatbot that will do its best not to say anything so stupid that people tweet about it.<|im_end|>\n-<|im_start|>user\n-How are you?<|im_end|>\n-<|im_start|>assistant\n-I'm doing great!<|im_end|>\n-```\n-\n-The \"user\", \"system\" and \"assistant\" roles are the standard for chat, and we recommend using them when it makes sense,\n-particularly if you want your model to operate well with [`TextGenerationPipeline`]. However, you are not limited\n-to these roles - templating is extremely flexible, and any string can be a role.\n-\n-### I want to add some chat templates! How should I get started?\n-\n-If you have any chat models, you should set their `tokenizer.chat_template` attribute and test it using\n-[`~PreTrainedTokenizer.apply_chat_template`], then push the updated tokenizer to the Hub. This applies even if you're\n-not the model owner - if you're using a model with an empty chat template, or one that's still using the default class\n-template, please open a [pull request](https://huggingface.co/docs/hub/repositories-pull-requests-discussions) to the model repository so that this attribute can be set properly!\n-\n-Once the attribute is set, that's it, you're done! `tokenizer.apply_chat_template` will now work correctly for that\n-model, which means it is also automatically supported in places like `TextGenerationPipeline`!\n-\n-By ensuring that models have this attribute, we can make sure that the whole community gets to use the full power of\n-open-source models. Formatting mismatches have been haunting the field and silently harming performance for too long -\n-it's time to put an end to them!\n-\n-## Advanced: Template writing tips\n-\n-<Tip>\n-\n-The easiest way to get started with writing Jinja templates is to take a look at some existing ones. You can use\n-`print(tokenizer.chat_template)` for any chat model to see what template it's using. In general, models that support tool use have\n-much more complex templates than other models - so when you're just getting started, they're probably a bad example\n-to learn from! You can also take a look at the\n-[Jinja documentation](https://jinja.palletsprojects.com/en/3.1.x/templates/#synopsis) for details\n-of general Jinja formatting and syntax.\n-\n-</Tip>\n-\n-Jinja templates in `transformers` are identical to Jinja templates elsewhere. The main thing to know is that\n-the conversation history will be accessible inside your template as a variable called `messages`.\n-You will be able to access `messages` in your template just like you can in Python, which means you can loop over\n-it with `{% for message in messages %}` or access individual messages with `{{ messages[0] }}`, for example.\n-\n-You can also use the following tips to write clean, efficient Jinja templates:\n-\n-### Trimming whitespace\n-\n-By default, Jinja will print any whitespace that comes before or after a block. This can be a problem for chat\n-templates, which generally want to be very precise with whitespace! To avoid this, we strongly recommend writing\n-your templates like this:\n-\n-```\n-{%- for message in messages %}\n-    {{- message['role'] + message['content'] }}\n-{%- endfor %}\n-```\n-\n-rather than like this:\n-\n-```\n-{% for message in messages %}\n-    {{ message['role'] + message['content'] }}\n-{% endfor %}\n-```\n-\n-Adding `-` will strip any whitespace that comes before the block. The second example looks innocent, but the newline\n-and indentation may end up being included in the output, which is probably not what you want!\n-\n-### Special variables\n-\n-Inside your template, you will have access several special variables. The most important of these is `messages`,\n-which contains the chat history as a list of message dicts. However, there are several others. Not every\n-variable will be used in every template. The most common other variables are:\n-\n-- `tools` contains a list of tools in JSON schema format. Will be `None` or undefined if no tools are passed.\n-- `documents` contains a list of documents in the format `{\"title\": \"Title\", \"contents\": \"Contents\"}`, used for retrieval-augmented generation. Will be `None` or undefined if no documents are passed.\n-- `add_generation_prompt` is a bool that is `True` if the user has requested a generation prompt, and `False` otherwise. If this is set, your template should add the header for an assistant message to the end of the conversation. If your model doesn't have a specific header for assistant messages, you can ignore this flag.\n-- **Special tokens** like `bos_token` and `eos_token`. These are extracted from `tokenizer.special_tokens_map`. The exact tokens available inside each template will differ depending on the parent tokenizer.\n-\n-<Tip>\n-\n-You can actually pass any `kwarg` to `apply_chat_template`, and it will be accessible inside the template as a variable. In general,\n-we recommend trying to stick to the core variables above, as it will make your model harder to use if users have\n-to write custom code to pass model-specific `kwargs`. However, we're aware that this field moves quickly, so if you\n-have a new use-case that doesn't fit in the core API, feel free to use a new `kwarg` for it! If a new `kwarg`\n-becomes common we may promote it into the core API and create a standard, documented format for it.\n-\n-</Tip>\n-\n-### Callable functions\n-\n-There is also a short list of callable functions available to you inside your templates. These are:\n-\n-- `raise_exception(msg)`: Raises a `TemplateException`. This is useful for debugging, and for telling users when they're\n-doing something that your template doesn't support.\n-- `strftime_now(format_str)`: Equivalent to `datetime.now().strftime(format_str)` in Python. This is used for getting\n-the current date/time in a specific format, which is sometimes included in system messages.\n-\n-### Compatibility with non-Python Jinja\n-\n-There are multiple implementations of Jinja in various languages. They generally have the same syntax,\n-but a key difference is that when you're writing a template in Python you can use Python methods, such as\n-`.lower()` on strings or `.items()` on dicts. This will break if someone tries to use your template on a non-Python\n-implementation of Jinja. Non-Python implementations are particularly common in deployment environments, where JS\n-and Rust are very popular.\n-\n-Don't panic, though! There are a few easy changes you can make to your templates to ensure they're compatible across\n-all implementations of Jinja:\n-\n-- Replace Python methods with Jinja filters. These usually have the same name, for example `string.lower()` becomes\n-  `string|lower`, and `dict.items()` becomes `dict|items`. One notable change is that `string.strip()` becomes `string|trim`.\n-  See the [list of built-in filters](https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters)\n-  in the Jinja documentation for more.\n-- Replace `True`, `False` and `None`, which are Python-specific, with `true`, `false` and `none`.\n-- Directly rendering a dict or list may give different results in other implementations (for example, string entries\n-  might change from single-quoted to double-quoted). Adding the `tojson` filter can help to ensure consistency here.\n-\n-### Writing generation prompts\n-\n-We mentioned above that `add_generation_prompt` is a special variable that will be accessible inside your template,\n-and is controlled by the user setting the `add_generation_prompt` flag. If your model expects a header for\n-assistant messages, then your template must support adding the header when `add_generation_prompt` is set.\n-\n-Here is an example of a template that formats messages ChatML-style, with generation prompt support:\n-\n-```text\n-{{- bos_token }}\n-{%- for message in messages %}\n-    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n-{%- endfor %}\n-{%- if add_generation_prompt %}\n-    {{- '<|im_start|>assistant\\n' }}\n-{%- endif %}\n-```\n-\n-The exact content of the assistant header will depend on your specific model, but it should always be **the string\n-that represents the start of an assistant message**, so that if the user applies your template with\n-`add_generation_prompt=True` and then generates text, the model will write an assistant response. Also note that some\n-models do not need a generation prompt, because assistant messages always begin immediately after user messages.\n-This is particularly common for LLaMA and Mistral models, where assistant messages begin immediately after the `[/INST]`\n-token that ends user messages. In these cases, the template can ignore the `add_generation_prompt` flag.\n-\n-Generation prompts are important! If your model requires a generation prompt but it is not set in the template, then\n-model generations will likely be severely degraded, or the model may display unusual behaviour like continuing\n-the final user message!\n-\n-### Writing and debugging larger templates\n-\n-When this feature was introduced, most templates were quite small, the Jinja equivalent of a \"one-liner\" script.\n-However, with new models and features like tool-use and RAG, some templates can be 100 lines long or more. When\n-writing templates like these, it's a good idea to write them in a separate file, using a text editor. You can easily\n-extract a chat template to a file:\n-\n-```python\n-open(\"template.jinja\", \"w\").write(tokenizer.chat_template)\n-```\n-\n-Or load the edited template back into the tokenizer:\n-\n-```python\n-tokenizer.chat_template = open(\"template.jinja\").read()\n-```\n-\n-As an added bonus, when you write a long, multi-line template in a separate file, line numbers in that file will\n-exactly correspond to line numbers in template parsing or execution errors. This will make it much easier to\n-identify the source of issues.\n-\n-### Writing templates for tools\n-\n-Although chat templates do not enforce a specific API for tools (or for anything, really), we recommend\n-template authors try to stick to a standard API where possible. The whole point of chat templates is to allow code\n-to be transferable across models, so deviating from the standard tools API means users will have to write\n-custom code to use tools with your model. Sometimes it's unavoidable, but often with clever templating you can\n-make the standard API work!\n-\n-Below, we'll list the elements of the standard API, and give tips on writing templates that will work well with it.\n-\n-#### Tool definitions\n-\n-Your template should expect that the variable `tools` will either be null (if no tools are passed), or is a list\n-of JSON schema dicts. Our chat template methods allow users to pass tools as either JSON schema or Python functions, but when\n-functions are passed, we automatically generate JSON schema and pass that to your template. As a result, the\n-`tools` variable that your template receives will always be a list of JSON schema. Here is\n-a sample tool JSON schema:\n-\n-```json\n-{\n-  \"type\": \"function\",\n-  \"function\": {\n-    \"name\": \"multiply\",\n-    \"description\": \"A function that multiplies two numbers\",\n-    \"parameters\": {\n-      \"type\": \"object\",\n-      \"properties\": {\n-        \"a\": {\n-          \"type\": \"number\",\n-          \"description\": \"The first number to multiply\"\n-        },\n-        \"b\": {\n-          \"type\": \"number\",\n-          \"description\": \"The second number to multiply\"\n-        }\n-      },\n-      \"required\": [\"a\", \"b\"]\n-    }\n-  }\n-}\n-```\n-\n-And here is some example code for handling tools in your chat template. Remember, this is just an example for a\n-specific format - your model will probably need different formatting!\n-\n-```text\n-{%- if tools %}\n-    {%- for tool in tools %}\n-        {{- '<tool>' + tool['function']['name'] + '\\n' }}\n-        {%- for argument in tool['function']['parameters']['properties'] %}\n-            {{- argument + ': ' + tool['function']['parameters']['properties'][argument]['description'] + '\\n' }}\n-        {%- endfor %}\n-        {{- '\\n</tool>' }}\n-    {%- endif %}\n-{%- endif %}\n-```\n-\n-The specific tokens and tool descriptions your template renders should of course be chosen to match the ones your model\n-was trained with. There is no requirement that your **model** understands JSON schema input, only that your template can translate\n-JSON schema into your model's format. For example, [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024)\n-was trained with tools defined using Python function headers, but the Command-R tool template accepts JSON schema,\n-converts types internally and renders the input tools as Python headers. You can do a lot with templates!\n-\n-#### Tool calls\n-\n-Tool calls, if present, will be a list attached to a message with the \"assistant\" role. Note that `tool_calls` is\n-always a list, even though most tool-calling models only support single tool calls at a time, which means\n-the list will usually only have a single element. Here is a sample message dict containing a tool call:\n-\n-```json\n-{\n-  \"role\": \"assistant\",\n-  \"tool_calls\": [\n-    {\n-      \"type\": \"function\",\n-      \"function\": {\n-        \"name\": \"multiply\",\n-        \"arguments\": {\n-          \"a\": 5,\n-          \"b\": 6\n-        }\n-      }\n-    }\n-  ]\n-}\n-```\n-\n-And a common pattern for handling them would be something like this:\n-\n-```text\n-{%- if message['role'] == 'assistant' and 'tool_calls' in message %}\n-    {%- for tool_call in message['tool_calls'] %}\n-            {{- '<tool_call>' + tool_call['function']['name'] + '\\n' + tool_call['function']['arguments']|tojson + '\\n</tool_call>' }}\n-        {%- endif %}\n-    {%- endfor %}\n-{%- endif %}\n-```\n-\n-Again, you should render the tool call with the formatting and special tokens that your model expects.\n-\n-#### Tool responses\n-\n-Tool responses have a simple format: They are a message dict with the \"tool\" role, a \"name\" key giving the name\n-of the called function, and a \"content\" key containing the result of the tool call. Here is a sample tool response:\n-\n-```json\n-{\n-  \"role\": \"tool\",\n-  \"name\": \"multiply\",\n-  \"content\": \"30\"\n-}\n-```\n-\n-You don't need to use all of the keys in the tool response. For example, if your model doesn't expect the function\n-name to be included in the tool response, then rendering it can be as simple as:\n-\n-```text\n-{%- if message['role'] == 'tool' %}\n-    {{- \"<tool_result>\" + message['content'] + \"</tool_result>\" }}\n-{%- endif %}\n-```\n-\n-Again, remember that the actual formatting and special tokens are model-specific - you should take a lot of care\n-to ensure that tokens, whitespace and everything else exactly match the format your model was trained with!"
        },
        {
            "sha": "d89ec57be1e79e59d2766d4796d8659e0a4de3b3",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 86,
            "deletions": 40,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -47,9 +47,19 @@ Adding these attributes means that LLaVA will try to infer the number of image t\n The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n \n \n-### Single image inference\n+### Formatting Prompts with Chat Templates  \n+\n+Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.  \n+\n+**Important:**  \n+- You must construct a conversation history — passing a plain string won't work.  \n+- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n+- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n+\n+\n+Here’s an example of how to structure your input. \n+We will use [llava-hf/llava-1.5-7b-hf](https://huggingface.co/llava-hf/llava-1.5-7b-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:\n \n-For best results, we recommend users to use the processor's `apply_chat_template()` method to format your prompt correctly. For that you need to construct a conversation history, passing in a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities, as follows:\n \n ```python\n from transformers import AutoProcessor\n@@ -84,33 +94,89 @@ print(text_prompt)\n >>> \"USER: <image>\\n<What’s shown in this image? ASSISTANT: This image shows a red stop sign.</s>USER: Describe the image in more details. ASSISTANT:\"\n ```\n \n+- If you want to construct a chat prompt yourself, below is a list of prompt formats accepted by each llava checkpoint:\n+\n+[llava-interleave models](https://huggingface.co/collections/llava-hf/llava-interleave-668e19a97da0036aad4a2f19) requires the following format:\n+```bash\n+\"<|im_start|>user <image>\\nWhat is shown in this image?<|im_end|><|im_start|>assistant\"\n+```\n+\n+For multiple turns conversation:\n+\n+```bash\n+\"<|im_start|>user <image>\\n<prompt1><|im_end|><|im_start|>assistant <answer1><|im_end|><|im_start|>user <image>\\n<prompt1><|im_end|><|im_start|>assistant \"\n+```\n+\n+[llava-1.5 models](https://huggingface.co/collections/llava-hf/llava-15-65f762d5b6941db5c2ba07e0) requires the following format:\n+```bash\n+\"USER: <image>\\n<prompt> ASSISTANT:\"\n+```\n+\n+For multiple turns conversation:\n+\n+```bash\n+\"USER: <image>\\n<prompt1> ASSISTANT: <answer1></s>USER: <prompt2> ASSISTANT: <answer2></s>USER: <prompt3> ASSISTANT:\"\n+```\n+\n+🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n+\n+\n+## Usage examples\n+\n+### Single input inference\n+\n+\n+```python\n+import torch\n+from transformers import AutoProcessor, LlavaForConditionalGeneration\n+\n+# Load the model in half-precision\n+model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device, torch.float16)\n+\n+# Generate\n+generate_ids = model.generate(**inputs, max_new_tokens=30)\n+processor.batch_decode(generate_ids, skip_special_tokens=True)\n+```\n+\n+\n ### Batched inference\n \n LLaVa also supports batched inference. Here is how you can do it:\n \n ```python\n-import requests\n-from PIL import Image\n import torch\n from transformers import AutoProcessor, LlavaForConditionalGeneration\n \n # Load the model in half-precision\n model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n \n-# Get two different images\n-url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-image_stop = Image.open(requests.get(url, stream=True).raw)\n-\n-url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-image_cats = Image.open(requests.get(url, stream=True).raw)\n \n # Prepare a batch of two prompts\n conversation_1 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n         ],\n     },\n@@ -120,47 +186,27 @@ conversation_2 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n         ],\n     },\n ]\n \n-prompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)\n-prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\n-prompts = [prompt_1, prompt_2]\n+inputs = processor.apply_chat_template(\n+    [conversation_1, conversation_2],\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    padding=True,\n+    return_tensors=\"pt\"\n+).to(model.device, torch.float16)\n \n-# We can simply feed images in the order they have to be used in the text prompt\n-inputs = processor(images=[image_stop, image_cats], text=prompts, padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n \n # Generate\n generate_ids = model.generate(**inputs, max_new_tokens=30)\n processor.batch_decode(generate_ids, skip_special_tokens=True)\n ```\n \n-- If you want to construct a chat prompt yourself, below is a list of prompt formats accepted by each llava checkpoint:\n-\n-[llava-interleave models](https://huggingface.co/collections/llava-hf/llava-interleave-668e19a97da0036aad4a2f19) requires the following format:\n-```bash\n-\"<|im_start|>user <image>\\nWhat is shown in this image?<|im_end|><|im_start|>assistant\"\n-```\n-\n-For multiple turns conversation:\n-\n-```bash\n-\"<|im_start|>user <image>\\n<prompt1><|im_end|><|im_start|>assistant <answer1><|im_end|><|im_start|>user <image>\\n<prompt1><|im_end|><|im_start|>assistant \"\n-```\n-\n-[llava-1.5 models](https://huggingface.co/collections/llava-hf/llava-15-65f762d5b6941db5c2ba07e0) requires the following format:\n-```bash\n-\"USER: <image>\\n<prompt> ASSISTANT:\"\n-```\n-\n-For multiple turns conversation:\n-\n-```bash\n-\"USER: <image>\\n<prompt1> ASSISTANT: <answer1></s>USER: <prompt2> ASSISTANT: <answer2></s>USER: <prompt3> ASSISTANT:\"\n-```\n \n ## Note regarding reproducing original implementation\n "
        },
        {
            "sha": "e62b9ba68c1e60ab6ebff78b51c7a7928883ba4b",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -59,9 +59,17 @@ Adding these attributes means that LLaVA will try to infer the number of image t\n The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n \n \n-- Note that each checkpoint has been trained with a specific prompt format, depending on which large language model (LLM) was used. You can use the processor's `apply_chat_template` to format your prompts correctly. For that you have to construct a conversation history, passing a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities. Below is an example of how to do that and the list of formats accepted by each checkpoint.\n+### Formatting Prompts with Chat Templates  \n \n-We will use [llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:\n+Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.  \n+\n+**Important:**  \n+- You must construct a conversation history — passing a plain string won't work.  \n+- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n+- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n+\n+\n+Here’s an example of how to structure your input. We will use [llava-v1.6-mistral-7b-hf](https://huggingface.co/llava-hf/llava-v1.6-mistral-7b-hf) and a conversation history of text and image.\n \n ```python\n from transformers import LlavaNextProcessor\n@@ -125,6 +133,10 @@ print(text_prompt)\n \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<image>\\nWhat is shown in this image?<|im_end|>\\n<|im_start|>assistant\\n\"\n ```\n \n+🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n+\n+\n+\n ## Usage example\n \n ### Single image inference"
        },
        {
            "sha": "ecd7b83a8b580ad252449594ec1c77eefb31692f",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 20,
            "deletions": 40,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -56,9 +56,17 @@ Adding these attributes means that LLaVA will try to infer the number of image t\n The attributes can be obtained from model config, as `model.config.vision_config.patch_size` or `model.config.vision_feature_select_strategy`. The `num_additional_image_tokens` should be `1` if the vision backbone adds a CLS token or `0` if nothing extra is added to the vision patches.\n \n \n-- Note that each checkpoint has been trained with a specific prompt format, depending on which large language model (LLM) was used. You can use tokenizer's `apply_chat_template` to format your prompts correctly. Below is an example of how to do that.\n+### Formatting Prompts with Chat Templates  \n \n-We will use [LLaVA-NeXT-Video-7B-hf](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf) and a conversation history of videos and images. Each content field has to be a list of dicts, as follows:\n+Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.  \n+\n+**Important:**  \n+- You must construct a conversation history — passing a plain string won't work.  \n+- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n+- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n+\n+\n+Here’s an example of how to structure your input. We will use [LLaVA-NeXT-Video-7B-hf](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf) and a conversation history of videos and images.\n \n ```python\n from transformers import LlavaNextVideoProcessor\n@@ -99,62 +107,40 @@ text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=\n print(text_prompt)\n ```\n \n+🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n+\n+\n+\n ## Usage example\n \n ### Single Media Mode\n \n The model can accept both images and videos as input. Here's an example code for inference in half-precision (`torch.float16`):\n \n ```python\n-import av\n+from huggingface_hub import hf_hub_download\n import torch\n-import numpy as np\n from transformers import LlavaNextVideoForConditionalGeneration, LlavaNextVideoProcessor\n \n-def read_video_pyav(container, indices):\n-    '''\n-    Decode the video with PyAV decoder.\n-    Args:\n-        container (`av.container.input.InputContainer`): PyAV container.\n-        indices (`List[int]`): List of frame indices to decode.\n-    Returns:\n-        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-    '''\n-    frames = []\n-    container.seek(0)\n-    start_index = indices[0]\n-    end_index = indices[-1]\n-    for i, frame in enumerate(container.decode(video=0)):\n-        if i > end_index:\n-            break\n-        if i >= start_index and i in indices:\n-            frames.append(frame)\n-    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n # Load the model in half-precision\n model = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n processor = LlavaNextVideoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n \n # Load the video as an np.array, sampling uniformly 8 frames (can sample more for longer videos)\n video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n-container = av.open(video_path)\n-total_frames = container.streams.video[0].frames\n-indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n-video = read_video_pyav(container, indices)\n \n conversation = [\n     {\n \n         \"role\": \"user\",\n         \"content\": [\n             {\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n-            {\"type\": \"video\"},\n+            {\"type\": \"video\", \"path\": video_path},\n             ],\n     },\n ]\n \n-prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-inputs = processor(text=prompt, videos=video, return_tensors=\"pt\")\n+inputs = processor.apply_chat_template(conversation, num_frames=8, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n \n out = model.generate(**inputs, max_new_tokens=60)\n processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n@@ -166,20 +152,15 @@ processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spac\n The model can also generate from an interleaved image-video inputs. However note, that it was not trained in interleaved image-video setting which might affect the performance. Below is an example usage for mixed media input, add the following lines to the above code snippet: \n \n ```python\n-from PIL import Image\n-import requests\n \n # Generate from image and video mixed inputs\n-# Load and image and write a new prompt\n-url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-image = Image.open(requests.get(url, stream=True).raw)\n conversation = [\n     {\n \n         \"role\": \"user\",\n         \"content\": [\n             {\"type\": \"text\", \"text\": \"How many cats are there in the image?\"},\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n             ],\n     },\n     {\n@@ -192,12 +173,11 @@ conversation = [\n         \"role\": \"user\",\n         \"content\": [\n             {\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n-            {\"type\": \"video\"},\n+            {\"type\": \"video\", \"path\": video_path},\n             ],\n     },\n ]\n-prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-inputs = processor(text=prompt, images=image, videos=clip, padding=True, return_tensors=\"pt\")\n+inputs = processor.apply_chat_template(conversation, num_frames=8, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True, return_tensors=\"pt\")\n \n # Generate\n generate_ids = model.generate(**inputs, max_length=50)"
        },
        {
            "sha": "785e6af74a4d8d0981b0aa1f42d031376f49a93d",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 44,
            "deletions": 64,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -47,8 +47,18 @@ Tips:\n \n </Tip>\n \n-- Note that the model should use a specific prompt format, on which the large language model (LLM) was trained. You can use the processor's `apply_chat_template` to format your prompts correctly. For that you have to construct a conversation history, passing a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities.\n \n+### Formatting Prompts with Chat Templates  \n+\n+Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.  \n+\n+**Important:**  \n+- You must construct a conversation history — passing a plain string won't work.  \n+- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n+- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n+\n+\n+Here’s an example of how to structure your input. \n We will use [llava-onevision-qwen2-7b-si-hf](https://huggingface.co/llava-hf/llava-onevision-qwen2-7b-si-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:\n \n ```python\n@@ -84,6 +94,9 @@ print(text_prompt)\n '<|im_start|>user\\n<image>What is shown in this image?<|im_end|>\\n<|im_start|>assistant\\nPage showing the list of options.<|im_end|>'\n ```\n \n+🚀 **Bonus:** If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n+\n+\n This model was contributed by [RaushanTurganbay](https://huggingface.co/RaushanTurganbay).\n The original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main).\n \n@@ -97,28 +110,28 @@ Here's how to load the model and perform inference in half-precision (`torch.flo\n ```python\n from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n import torch\n-from PIL import Image\n-import requests\n \n-processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n-model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n-model.to(\"cuda:0\")\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\") \n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+    \"llava-hf/llava-onevision-qwen2-7b-ov-hf\",\n+    torch_dtype=torch.float16,\n+    low_cpu_mem_usage=True,\n+    device_map=\"cuda:0\"\n+)\n \n # prepare image and text prompt, using the appropriate prompt template\n url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n-image = Image.open(requests.get(url, stream=True).raw)\n-\n conversation = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"url\": url},\n             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n         ],\n     },\n ]\n-prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(\"cuda:0\", torch.float16)\n+inputs = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n+inputs = inputs.to(\"cuda:0\", torch.float16)\n \n # autoregressively complete prompt\n output = model.generate(**inputs, max_new_tokens=100)\n@@ -140,22 +153,12 @@ from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n \n-# Get three different images\n-url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-image_stop = Image.open(requests.get(url, stream=True).raw)\n-\n-url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-image_cats = Image.open(requests.get(url, stream=True).raw)\n-\n-url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n-image_snowman = Image.open(requests.get(url, stream=True).raw)\n-\n # Prepare a batch of two prompts, where the first one is a multi-turn conversation and the second is not\n conversation_1 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n             ],\n     },\n@@ -168,7 +171,7 @@ conversation_1 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n             {\"type\": \"text\", \"text\": \"What about this image? How many cats do you see?\"},\n             ],\n     },\n@@ -178,18 +181,20 @@ conversation_2 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"url\": \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"},\n             {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n             ],\n     },\n ]\n \n-prompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)\n-prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\n-prompts = [prompt_1, prompt_2]\n-\n-# We can simply feed images in the order they have to be used in the text prompt\n-inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n+inputs = processor.apply_chat_template(\n+    [conversation_1, conversation_2],\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    padding=True,\n+    return_tensors=\"pt\"\n+).to(model.device, torch.float16)\n \n # Generate\n generate_ids = model.generate(**inputs, max_new_tokens=30)\n@@ -202,59 +207,34 @@ processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokeniza\n LLaVa-OneVision also can perform inference with videos as input, where video frames are treated as multiple images. Here is how you can do it:\n \n ```python\n-import av\n-import numpy as np\n from huggingface_hub import hf_hub_download\n-\n import torch\n from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n \n # Load the model in half-precision\n model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n \n-\n-def read_video_pyav(container, indices):\n-    '''\n-    Decode the video with PyAV decoder.\n-    Args:\n-        container (`av.container.input.InputContainer`): PyAV container.\n-        indices (`List[int]`): List of frame indices to decode.\n-    Returns:\n-        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-    '''\n-    frames = []\n-    container.seek(0)\n-    start_index = indices[0]\n-    end_index = indices[-1]\n-    for i, frame in enumerate(container.decode(video=0)):\n-        if i > end_index:\n-            break\n-        if i >= start_index and i in indices:\n-            frames.append(frame)\n-    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-# Load the video as an np.array, sampling uniformly 8 frames (can sample more for longer videos, up to 32 frames)\n video_path = hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n-container = av.open(video_path)\n-total_frames = container.streams.video[0].frames\n-indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n-video = read_video_pyav(container, indices)\n-\n-# For videos we have to feed a \"video\" type instead of \"image\"\n conversation = [\n     {\n \n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"video\"},\n+            {\"type\": \"video\", \"path\": video_path},\n             {\"type\": \"text\", \"text\": \"Why is this video funny?\"},\n             ],\n     },\n ]\n \n-prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-inputs = processor(videos=list(video), text=prompt, return_tensors=\"pt\").to(\"cuda:0\", torch.float16)\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    num_frames=8\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device, torch.float16)\n \n out = model.generate(**inputs, max_new_tokens=60)\n processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)"
        },
        {
            "sha": "64da42b38b0f95c92ed4854c02c2e774e75d80b8",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -28,7 +28,8 @@ The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a\n - For text-only inputs use `MllamaForCausalLM` for generation to avoid loading vision tower.\n - Each sample can contain multiple images, and the number of images can vary between samples. The processor will pad the inputs to the maximum number of images across samples and to a maximum number of tiles within each image.\n - The text passed to the processor should have the `\"<|image|>\"` tokens where the images should be inserted.\n-- The processor has its own `apply_chat_template` method to convert chat messages to text that can then be passed as text to the processor.\n+- The processor has its own `apply_chat_template` method to convert chat messages to text that can then be passed as text to the processor. If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the **Usage Examples** below for more details on how to use it.\n+\n \n \n <Tip warning={true}>\n@@ -53,9 +54,7 @@ model.set_output_embeddings(resized_embeddings)\n \n #### Instruct model\n ```python\n-import requests\n import torch\n-from PIL import Image\n from transformers import MllamaForConditionalGeneration, AutoProcessor\n \n model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n@@ -67,18 +66,13 @@ messages = [\n         {\n             \"role\": \"user\", \n             \"content\": [\n-                {\"type\": \"image\"},\n+                {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n                 {\"type\": \"text\", \"text\": \"What does the image show?\"}\n             ]\n         }\n     ],\n ]\n-text = processor.apply_chat_template(messages, add_generation_prompt=True)\n-\n-url = \"https://llava-vl.github.io/static/images/view.jpg\"\n-image = Image.open(requests.get(url, stream=True).raw)\n-\n-inputs = processor(text=text, images=image, return_tensors=\"pt\").to(model.device)\n+inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device)\n output = model.generate(**inputs, max_new_tokens=25)\n print(processor.decode(output[0]))\n ```"
        },
        {
            "sha": "6e7652bfdea3b09fe7b0c3c0c059d153226593d1",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -38,38 +38,42 @@ Tips:\n ```\n \"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n ```\n-Then, the processor will replace each `[IMG]` token with a number of `[IMG]` tokens that depend on the height and the width of each image. Each *row* of the image is separated by an `[IMG_BREAK]` token, and each image is separated by an `[IMG_END]` token. It's advised to use the `apply_chat_template` method of the processor, which takes care of all of this. See the [usage section](#usage) for more info.\n+Then, the processor will replace each `[IMG]` token with a number of `[IMG]` tokens that depend on the height and the width of each image. Each *row* of the image is separated by an `[IMG_BREAK]` token, and each image is separated by an `[IMG_END]` token. It's advised to use the `apply_chat_template` method of the processor, which takes care of all of this and formats the text for you. If you're using `transformers>=4.49.0`, you can also get a vectorized output from `apply_chat_template`. See the [usage section](#usage) for more info.\n+\n \n This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [ArthurZ](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/vllm-project/vllm/pull/8377).\n \n+\n ## Usage\n \n At inference time, it's advised to use the processor's `apply_chat_template` method, which correctly formats the prompt for the model:\n \n ```python\n from transformers import AutoProcessor, LlavaForConditionalGeneration\n-from PIL import Image\n \n model_id = \"mistral-community/pixtral-12b\"\n processor = AutoProcessor.from_pretrained(model_id)\n-model = LlavaForConditionalGeneration.from_pretrained(model_id).to(\"cuda\")\n-\n-url_dog = \"https://picsum.photos/id/237/200/300\"\n-url_mountain = \"https://picsum.photos/seed/picsum/200/300\"\n+model = LlavaForConditionalGeneration.from_pretrained(model_id, device_map=\"cuda\")\n \n chat = [\n     {\n       \"role\": \"user\", \"content\": [\n         {\"type\": \"text\", \"content\": \"Can this animal\"}, \n-        {\"type\": \"image\"}, \n+        {\"type\": \"image\", \"ur\": \"https://picsum.photos/id/237/200/300\"}, \n         {\"type\": \"text\", \"content\": \"live here?\"}, \n-        {\"type\": \"image\"}\n+        {\"type\": \"image\", \"url\": \"https://picsum.photos/seed/picsum/200/300\"}\n       ]\n     }\n ]\n \n-prompt = processor.apply_chat_template(chat)\n-inputs = processor(text=prompt, images=[url_dog, url_mountain], return_tensors=\"pt\").to(model.device)\n+inputs = processor.apply_chat_template(\n+    chat,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n+\n generate_ids = model.generate(**inputs, max_new_tokens=500)\n output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n ```"
        },
        {
            "sha": "df3b8fb8967e640ad5545a4079d57fe445aac90a",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 32,
            "deletions": 48,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -32,28 +32,21 @@ The model can accept both images and videos as input. Here's an example code for\n \n ```python\n \n-from PIL import Image\n-import requests\n import torch\n-from torchvision import io\n-from typing import Dict\n-from transformers.image_utils import load_images, load_video\n from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n \n # Load the model in half-precision on the available device(s)\n model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n \n-# Image\n-url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n-image = Image.open(requests.get(url, stream=True).raw)\n \n conversation = [\n     {\n         \"role\":\"user\",\n         \"content\":[\n             {\n                 \"type\":\"image\",\n+                \"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n             },\n             {\n                 \"type\":\"text\",\n@@ -63,13 +56,14 @@ conversation = [\n     }\n ]\n \n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n \n-# Preprocess the inputs\n-text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n-\n-inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n-inputs = inputs.to('cuda')\n \n # Inference: Generation of the output\n output_ids = model.generate(**inputs, max_new_tokens=128)\n@@ -78,25 +72,24 @@ output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, cl\n print(output_text)\n \n # Video\n-video = load_video(video=\"/path/to/video.mp4\")\n conversation = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"video\"},\n+            {\"type\": \"video\", \"path\": \"/path/to/video.mp4\"},\n             {\"type\": \"text\", \"text\": \"What happened in the video?\"},\n         ],\n     }\n ]\n \n-# Preprocess the inputs\n-text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|video_pad|><|vision_end|>What happened in the video?<|im_end|>\\n<|im_start|>assistant\\n'\n-\n-# Qwen2.5VL modifies the time positional encoding (MRoPE) according to the video's frame rate (FPS).\n-# Therefore, the video's FPS information needs to be provided as input.\n-inputs = processor(text=[text_prompt], videos=[video], fps=[1.0], padding=True, return_tensors=\"pt\")\n-inputs = inputs.to('cuda')\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    video_fps=1,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n \n # Inference: Generation of the output\n output_ids = model.generate(**inputs, max_new_tokens=128)\n@@ -110,21 +103,12 @@ print(output_text)\n The model can batch inputs composed of mixed samples of various types such as images, videos, and text. Here is an example.\n \n ```python\n-images = load_images([\n-    \"/path/to/image1.jpg\",\n-    \"/path/to/image2.jpg\",\n-    \"/path/to/image3.jpg\",\n-    \"/path/to/image4.jpg\",\n-    \"/path/to/image5.jpg\",\n-])\n-video = load_video(video=\"/path/to/video.mp4\")\n-\n # Conversation for the first image\n conversation1 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image1.jpg\"},\n             {\"type\": \"text\", \"text\": \"Describe this image.\"}\n         ]\n     }\n@@ -135,8 +119,8 @@ conversation2 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image2.jpg\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n             {\"type\": \"text\", \"text\": \"What is written in the pictures?\"}\n         ]\n     }\n@@ -156,25 +140,25 @@ conversation4 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"image\"},\n-            {\"type\": \"video\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image4.jpg\"},\n+            {\"type\": \"video\", \"path\": \"/path/to/video.jpg\"},\n             {\"type\": \"text\", \"text\": \"What are the common elements in these medias?\"},\n         ],\n     }\n ]\n \n conversations = [conversation1, conversation2, conversation3, conversation4]\n # Preparation for batch inference\n-texts = [processor.apply_chat_template(msg, add_generation_prompt=True) for msg in conversations]\n-inputs = processor(\n-    text=texts,\n-    images=images,\n-    videos=[video],\n-    padding=True,\n-    return_tensors=\"pt\",\n-)\n-inputs = inputs.to('cuda')\n+ipnuts = processor.apply_chat_template(\n+    conversations,\n+    video_fps=1,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n+\n \n # Batch Inference\n output_ids = model.generate(**inputs, max_new_tokens=128)"
        },
        {
            "sha": "b0275ce94af558102407dffd9d1479c2317c544b",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 35,
            "deletions": 72,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=1931a351408dbd1d0e2c4d6d7ee0eb5e8807d7bf",
            "patch": "@@ -39,27 +39,21 @@ The model can accept both images and videos as input. Here's an example code for\n \n ```python\n \n-from PIL import Image\n-import requests\n import torch\n-from torchvision import io\n-from typing import Dict\n from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n \n # Load the model in half-precision on the available device(s)\n model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n \n-# Image\n-url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n-image = Image.open(requests.get(url, stream=True).raw)\n \n conversation = [\n     {\n         \"role\":\"user\",\n         \"content\":[\n             {\n                 \"type\":\"image\",\n+                \"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n             },\n             {\n                 \"type\":\"text\",\n@@ -69,64 +63,42 @@ conversation = [\n     }\n ]\n \n-\n-# Preprocess the inputs\n-text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\\n<|im_start|>assistant\\n'\n-\n-inputs = processor(text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\")\n-inputs = inputs.to('cuda')\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n \n # Inference: Generation of the output\n output_ids = model.generate(**inputs, max_new_tokens=128)\n generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n print(output_text)\n \n+\n+\n # Video\n-def fetch_video(ele: Dict, nframe_factor=2):\n-    if isinstance(ele['video'], str):\n-        def round_by_factor(number: int, factor: int) -> int:\n-            return round(number / factor) * factor\n-\n-        video = ele[\"video\"]\n-        if video.startswith(\"file://\"):\n-            video = video[7:]\n-\n-        video, _, info = io.read_video(\n-            video,\n-            start_pts=ele.get(\"video_start\", 0.0),\n-            end_pts=ele.get(\"video_end\", None),\n-            pts_unit=\"sec\",\n-            output_format=\"TCHW\",\n-        )\n-        assert not (\"fps\" in ele and \"nframes\" in ele), \"Only accept either `fps` or `nframes`\"\n-        if \"nframes\" in ele:\n-            nframes = round_by_factor(ele[\"nframes\"], nframe_factor)\n-        else:\n-            fps = ele.get(\"fps\", 1.0)\n-            nframes = round_by_factor(video.size(0) / info[\"video_fps\"] * fps, nframe_factor)\n-        idx = torch.linspace(0, video.size(0) - 1, nframes, dtype=torch.int64)\n-        return video[idx]\n-\n-video_info = {\"type\": \"video\", \"video\": \"/path/to/video.mp4\", \"fps\": 1.0}\n-video = fetch_video(video_info)\n conversation = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"video\"},\n+            {\"type\": \"video\", \"path\": \"/path/to/video.mp4\"},\n             {\"type\": \"text\", \"text\": \"What happened in the video?\"},\n         ],\n     }\n ]\n \n-# Preprocess the inputs\n-text_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|video_pad|><|vision_end|>What happened in the video?<|im_end|>\\n<|im_start|>assistant\\n'\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    video_fps=1,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n \n-inputs = processor(text=[text_prompt], videos=[video], padding=True, return_tensors=\"pt\")\n-inputs = inputs.to('cuda')\n \n # Inference: Generation of the output\n output_ids = model.generate(**inputs, max_new_tokens=128)\n@@ -140,23 +112,13 @@ print(output_text)\n The model can batch inputs composed of mixed samples of various types such as images, videos, and text. Here is an example.\n \n ```python\n-image1 = Image.open(\"/path/to/image1.jpg\")\n-image2 = Image.open(\"/path/to/image2.jpg\")\n-image3 = Image.open(\"/path/to/image3.jpg\")\n-image4 = Image.open(\"/path/to/image4.jpg\")\n-image5 = Image.open(\"/path/to/image5.jpg\")\n-video = fetch_video({\n-    \"type\": \"video\",\n-    \"video\": \"/path/to/video.mp4\",\n-    \"fps\": 1.0\n-})\n \n # Conversation for the first image\n conversation1 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image1.jpg\"},\n             {\"type\": \"text\", \"text\": \"Describe this image.\"}\n         ]\n     }\n@@ -167,8 +129,8 @@ conversation2 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"image\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image2.jpg\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n             {\"type\": \"text\", \"text\": \"What is written in the pictures?\"}\n         ]\n     }\n@@ -188,25 +150,25 @@ conversation4 = [\n     {\n         \"role\": \"user\",\n         \"content\": [\n-            {\"type\": \"image\"},\n-            {\"type\": \"image\"},\n-            {\"type\": \"video\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image3.jpg\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image4.jpg\"},\n+            {\"type\": \"video\", \"path\": \"/path/to/video.jpg\"},\n             {\"type\": \"text\", \"text\": \"What are the common elements in these medias?\"},\n         ],\n     }\n ]\n \n conversations = [conversation1, conversation2, conversation3, conversation4]\n # Preparation for batch inference\n-texts = [processor.apply_chat_template(msg, add_generation_prompt=True) for msg in conversations]\n-inputs = processor(\n-    text=texts,\n-    images=[image1, image2, image3, image4, image5],\n-    videos=[video],\n-    padding=True,\n-    return_tensors=\"pt\",\n-)\n-inputs = inputs.to('cuda')\n+ipnuts = processor.apply_chat_template(\n+    conversations,\n+    video_fps=1,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n+\n \n # Batch Inference\n output_ids = model.generate(**inputs, max_new_tokens=128)\n@@ -236,6 +198,7 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixel\n ```\n This ensures each image gets encoded using a number between 256-1024 tokens. The 28 comes from the fact that the model uses a patch size of 14 and a temporal patch size of 2 (14 x 2 = 28).\n \n+\n #### Multiple Image Inputs\n \n By default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:"
        }
    ],
    "stats": {
        "total": 3163,
        "additions": 1709,
        "deletions": 1454
    }
}