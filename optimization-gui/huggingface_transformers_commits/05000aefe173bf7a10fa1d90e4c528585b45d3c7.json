{
    "author": "danielhanchen",
    "message": "Fix GPT-OSS `swiglu_limit` not passed in for MXFP4 (#40197)\n\nAdd swiglu_limit = 7.0",
    "sha": "05000aefe173bf7a10fa1d90e4c528585b45d3c7",
    "files": [
        {
            "sha": "429b793b3499e757a156cedbd409bc263888c056",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05000aefe173bf7a10fa1d90e4c528585b45d3c7/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/05000aefe173bf7a10fa1d90e4c528585b45d3c7/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=05000aefe173bf7a10fa1d90e4c528585b45d3c7",
            "patch": "@@ -172,7 +172,7 @@ def __init__(self, config):\n             torch.zeros(self.num_experts, self.hidden_size, dtype=torch.float32), requires_grad=False\n         )\n         self.alpha = 1.702\n-\n+        self.limit = getattr(config, \"swiglu_limit\", 7.0)\n         self.gate_up_proj_precision_config = None\n         self.down_proj_precision_config = None\n \n@@ -185,7 +185,7 @@ def forward(self, hidden_states: torch.Tensor, routing_data, gather_idx, scatter\n         swiglu_fn = triton_kernels_hub.swiglu.swiglu_fn\n \n         with torch.cuda.device(hidden_states.device):\n-            act = FusedActivation(FnSpecs(\"swiglu\", swiglu_fn, (\"alpha\", \"limit\")), (self.alpha, None), 2)\n+            act = FusedActivation(FnSpecs(\"swiglu\", swiglu_fn, (\"alpha\", \"limit\")), (self.alpha, self.limit), 2)\n \n             intermediate_cache1 = matmul_ogs(\n                 hidden_states,"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}