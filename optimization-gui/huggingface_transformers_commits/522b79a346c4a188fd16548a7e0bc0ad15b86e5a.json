{
    "author": "MekkCyber",
    "message": "add more activation kernels, follow up  (#40944)\n\n* add more activation kernels\n\n* fixing style\n\n* fix version",
    "sha": "522b79a346c4a188fd16548a7e0bc0ad15b86e5a",
    "files": [
        {
            "sha": "7642e8aa238a6df701da95f0a58bef3156baf0e0",
            "filename": "src/transformers/activations.py",
            "status": "modified",
            "additions": 32,
            "deletions": 4,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/522b79a346c4a188fd16548a7e0bc0ad15b86e5a/src%2Ftransformers%2Factivations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/522b79a346c4a188fd16548a7e0bc0ad15b86e5a/src%2Ftransformers%2Factivations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Factivations.py?ref=522b79a346c4a188fd16548a7e0bc0ad15b86e5a",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import functools\n import math\n from collections import OrderedDict\n \n@@ -26,7 +27,8 @@\n logger = logging.get_logger(__name__)\n \n \n-class PytorchGELUTanh(nn.Module):\n+@use_kernel_forward_from_hub(\"GeluTanh\")\n+class GELUTanh(nn.Module):\n     \"\"\"\n     A fast C implementation of the tanh approximation of the GeLU activation function. See\n     https://huggingface.co/papers/1606.08415.\n@@ -35,8 +37,18 @@ class PytorchGELUTanh(nn.Module):\n     match due to rounding errors.\n     \"\"\"\n \n+    def __init__(self, use_gelu_tanh_python: bool = False):\n+        super().__init__()\n+        if use_gelu_tanh_python:\n+            self.act = self._gelu_tanh_python\n+        else:\n+            self.act = functools.partial(nn.functional.gelu, approximate=\"tanh\")\n+\n+    def _gelu_tanh_python(self, input: Tensor) -> Tensor:\n+        return input * 0.5 * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n+\n     def forward(self, input: Tensor) -> Tensor:\n-        return nn.functional.gelu(input, approximate=\"tanh\")\n+        return self.act(input)\n \n \n @use_kernel_forward_from_hub(\"NewGELU\")\n@@ -50,6 +62,7 @@ def forward(self, input: Tensor) -> Tensor:\n         return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n \n \n+@use_kernel_forward_from_hub(\"GeLU\")\n class GELUActivation(nn.Module):\n     \"\"\"\n     Original Implementation of the GELU activation function in Google BERT repo when initially created. For\n@@ -72,6 +85,20 @@ def forward(self, input: Tensor) -> Tensor:\n         return self.act(input)\n \n \n+@use_kernel_forward_from_hub(\"SiLU\")\n+class SiLUActivation(nn.Module):\n+    \"\"\"\n+    See Gaussian Error Linear Units (Hendrycks et al., https://arxiv.org/abs/1606.08415) where the SiLU (Sigmoid Linear\n+    Unit) was originally introduced and coined, and see Sigmoid-Weighted Linear Units for Neural Network Function\n+    Approximation in Reinforcement Learning (Elfwing et al., https://arxiv.org/abs/1702.03118) and Swish: a Self-Gated\n+    Activation Function (Ramachandran et al., https://arxiv.org/abs/1710.05941v1) where the SiLU was experimented with\n+    later.\n+    \"\"\"\n+\n+    def forward(self, input: Tensor) -> Tensor:\n+        return nn.functional.silu(input)\n+\n+\n @use_kernel_forward_from_hub(\"FastGELU\")\n class FastGELUActivation(nn.Module):\n     \"\"\"\n@@ -290,7 +317,8 @@ def forward(self, input: Tensor) -> Tensor:\n     \"gelu_fast\": FastGELUActivation,\n     \"gelu_new\": NewGELUActivation,\n     \"gelu_python\": (GELUActivation, {\"use_gelu_python\": True}),\n-    \"gelu_pytorch_tanh\": PytorchGELUTanh,\n+    \"gelu_pytorch_tanh\": GELUTanh,\n+    \"gelu_python_tanh\": (GELUTanh, {\"use_gelu_tanh_python\": True}),\n     \"gelu_accurate\": AccurateGELUActivation,\n     \"laplace\": LaplaceActivation,\n     \"leaky_relu\": nn.LeakyReLU,\n@@ -301,7 +329,7 @@ def forward(self, input: Tensor) -> Tensor:\n     \"relu2\": ReLUSquaredActivation,\n     \"relu6\": nn.ReLU6,\n     \"sigmoid\": nn.Sigmoid,\n-    \"silu\": nn.SiLU,\n+    \"silu\": SiLUActivation,\n     \"swish\": nn.SiLU,\n     \"tanh\": nn.Tanh,\n     \"prelu\": nn.PReLU,"
        },
        {
            "sha": "6bf8dbcc021962b7ef8049ee52557286836219c3",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/522b79a346c4a188fd16548a7e0bc0ad15b86e5a/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/522b79a346c4a188fd16548a7e0bc0ad15b86e5a/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=522b79a346c4a188fd16548a7e0bc0ad15b86e5a",
            "patch": "@@ -111,6 +111,27 @@\n                 )\n             }\n         },\n+        \"SiLU\": {\n+            \"cuda\": {\n+                Mode.INFERENCE | Mode.TORCH_COMPILE: LayerRepository(\n+                    repo_id=\"kernels-community/activation\", layer_name=\"Silu\", version=\">=0.1.0\"\n+                )\n+            }\n+        },\n+        \"GeLU\": {\n+            \"cuda\": {\n+                Mode.INFERENCE | Mode.TORCH_COMPILE: LayerRepository(\n+                    repo_id=\"kernels-community/activation\", layer_name=\"Gelu\", version=\">=0.1.0\"\n+                )\n+            }\n+        },\n+        \"GeluTanh\": {\n+            \"cuda\": {\n+                Mode.INFERENCE | Mode.TORCH_COMPILE: LayerRepository(\n+                    repo_id=\"kernels-community/activation\", layer_name=\"GeluTanh\", version=\">=0.1.0\"\n+                )\n+            }\n+        },\n     }\n \n     register_kernel_mapping(_KERNEL_MAPPING)"
        }
    ],
    "stats": {
        "total": 57,
        "additions": 53,
        "deletions": 4
    }
}