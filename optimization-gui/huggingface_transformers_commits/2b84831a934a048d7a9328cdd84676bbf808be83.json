{
    "author": "bimal-gajera",
    "message": "Update model card for Cohere (#37056)\n\n* Update Cohere model card to follow standard template\n\n* Update docs/source/en/model_doc/cohere.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/cohere.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/cohere.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/cohere.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/cohere.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/cohere.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update cohere.md\n\nUpdate code snippet for AutoModel, quantization, and transformers-cli\n\n* Update cohere.md\n\n* Update docs/source/en/model_doc/cohere.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "2b84831a934a048d7a9328cdd84676bbf808be83",
    "files": [
        {
            "sha": "48b924e1ff13c5f570712e65d26a932e93139dd6",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 71,
            "deletions": 82,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b84831a934a048d7a9328cdd84676bbf808be83/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b84831a934a048d7a9328cdd84676bbf808be83/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=2b84831a934a048d7a9328cdd84676bbf808be83",
            "patch": "@@ -1,124 +1,115 @@\n-# Cohere\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The Cohere Command-R model was proposed in the blogpost [Command-R: Retrieval Augmented Generation at Production Scale](https://txt.cohere.com/command-r/) by the Cohere Team.\n-\n-The abstract from the paper is the following:\n \n-*Command-R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprise. Today, we are introducing Command-R, a new LLM aimed at large-scale production workloads. Command-R targets the emerging ‚Äúscalable‚Äù category of models that balance high efficiency with strong accuracy, enabling companies to move beyond proof of concept, and into production.*\n+# Cohere\n \n-*Command-R is a generative model optimized for long context tasks such as retrieval augmented generation (RAG) and using external APIs and tools. It is designed to work in concert with our industry-leading Embed and Rerank models to provide best-in-class integration for RAG applications and excel at enterprise use cases. As a model built for companies to implement at scale, Command-R boasts:\n-- Strong accuracy on RAG and Tool Use\n-- Low latency, and high throughput\n-- Longer 128k context and lower pricing\n-- Strong capabilities across 10 key languages\n-- Model weights available on HuggingFace for research and evaluation\n+Cohere Command-R is a 35B parameter multilingual large language model designed for long context tasks like retrieval-augmented generation (RAG) and calling external APIs and tools. The model is specifically trained for grounded generation and supports both single-step and multi-step tool use. It supports a context length of 128K tokens.\n \n-Checkout model checkpoints [here](https://huggingface.co/CohereForAI/c4ai-command-r-v01).\n-This model was contributed by [Saurabh Dash](https://huggingface.co/saurabhdash) and [Ahmet √úst√ºn](https://huggingface.co/ahmetustun). The code of the implementation in Hugging Face is based on GPT-NeoX [here](https://github.com/EleutherAI/gpt-neox).\n+You can find all the original Command-R checkpoints under the [Command Models](https://huggingface.co/collections/CohereForAI/command-models-67652b401665205e17b192ad) collection.\n \n-## Usage tips\n \n-<Tip warning={true}>\n+> [!TIP]\n+> Click on the Cohere models in the right sidebar for more examples of how to apply Cohere to different language tasks.\n \n-The checkpoints uploaded on the Hub use `torch_dtype = 'float16'`, which will be\n-used by the `AutoModel` API to cast the checkpoints from `torch.float32` to `torch.float16`. \n+The example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`], and from the command line.\n \n-The `dtype` of the online weights is mostly irrelevant unless you are using `torch_dtype=\"auto\"` when initializing a model using `model = AutoModelForCausalLM.from_pretrained(\"path\", torch_dtype = \"auto\")`. The reason is that the model will first be downloaded ( using the `dtype` of the checkpoints online), then it will be casted to the default `dtype` of `torch` (becomes `torch.float32`), and finally, if there is a `torch_dtype` provided in the config, it will be used. \n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-Training the model in `float16` is not recommended and is known to produce `nan`; as such, the model should be trained in `bfloat16`.\n+```python\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"text-generation\",\n+    model=\"CohereForAI/c4ai-command-r-v01\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n+pipeline(\"Plants create energy through a process known as\")\n+```\n \n-</Tip>\n-The model and tokenizer can be loaded via:\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n ```python\n-# pip install transformers\n+import torch\n from transformers import AutoTokenizer, AutoModelForCausalLM\n \n-model_id = \"CohereForAI/c4ai-command-r-v01\"\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id)\n+tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n+model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n \n-# Format message with the command-r chat template\n-messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n-input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n-## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n-\n-gen_tokens = model.generate(\n+# format message with the Command-R chat template\n+messages = [{\"role\": \"user\", \"content\": \"How do plants make energy?\"}]\n+input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+output = model.generate(\n     input_ids, \n     max_new_tokens=100, \n     do_sample=True, \n     temperature=0.3,\n-    )\n-\n-gen_text = tokenizer.decode(gen_tokens[0])\n-print(gen_text)\n+    cache_implementation=\"static\",\n+)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n-- When using Flash Attention 2 via `attn_implementation=\"flash_attention_2\"`, don't pass `torch_dtype` to the `from_pretrained` class method and use Automatic Mixed-Precision training. When using `Trainer`, it is simply specifying either `fp16` or `bf16` to `True`. Otherwise, make sure you are using `torch.autocast`. This is required because the Flash Attention only support `fp16` and `bf16` data type.\n-\n+</hfoption>\n+<hfoption id=\"transformers-cli\">\n \n-## Resources\n+```bash\n+# pip install -U flash-attn --no-build-isolation\n+transformers-cli chat --model_name_or_path CohereForAI/c4ai-command-r-v01 --torch_dtype auto --attn_implementation flash_attention_2\n+```\n \n-A list of official Hugging Face and community (indicated by üåé) resources to help you get started with Command-R. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+</hfoption>\n+</hfoptions>\n \n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-<PipelineTag pipeline=\"text-generation\"/>\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to quantize the weights to 4-bits.\n \n-Loading FP16 model\n ```python\n-# pip install transformers\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-\n-model_id = \"CohereForAI/c4ai-command-r-v01\"\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id)\n+import torch\n+from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n \n-# Format message with the command-r chat template\n-messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n-input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n-## <BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Hello, how are you?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\n+bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n+tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n+model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", torch_dtype=torch.float16, device_map=\"auto\", quantization_config=bnb_config, attn_implementation=\"sdpa\")\n \n-gen_tokens = model.generate(\n+# format message with the Command-R chat template\n+messages = [{\"role\": \"user\", \"content\": \"How do plants make energy?\"}]\n+input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+output = model.generate(\n     input_ids, \n     max_new_tokens=100, \n     do_sample=True, \n     temperature=0.3,\n-    )\n-\n-gen_text = tokenizer.decode(gen_tokens[0])\n-print(gen_text)\n+    cache_implementation=\"static\",\n+)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n-Loading bitsnbytes 4bit quantized model\n-```python\n-# pip install transformers bitsandbytes accelerate\n-from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n+Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src/transformers/utils/attention_visualizer.py#L139) to better understand what tokens the model can and cannot attend to.\n \n-bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n+```py\n+from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n \n-model_id = \"CohereForAI/c4ai-command-r-v01\"\n-tokenizer = AutoTokenizer.from_pretrained(model_id)\n-model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)\n+visualizer = AttentionMaskVisualizer(\"CohereForAI/c4ai-command-r-v01\")\n+visualizer(\"Plants create energy through a process known as\")\n+```\n \n-gen_tokens = model.generate(\n-    input_ids, \n-    max_new_tokens=100, \n-    do_sample=True, \n-    temperature=0.3,\n-    )\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/cohere-attn-mask.png\"/>\n+</div>\n \n-gen_text = tokenizer.decode(gen_tokens[0])\n-print(gen_text)\n-```\n \n+## Notes\n+- Don‚Äôt use the torch_dtype parameter in [`~AutoModel.from_pretrained`] if you‚Äôre using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to True if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n \n ## CohereConfig\n \n@@ -143,5 +134,3 @@ print(gen_text)\n \n [[autodoc]] CohereForCausalLM\n     - forward\n-\n-"
        }
    ],
    "stats": {
        "total": 153,
        "additions": 71,
        "deletions": 82
    }
}