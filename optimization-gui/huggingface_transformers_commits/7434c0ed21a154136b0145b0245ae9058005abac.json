{
    "author": "vasqu",
    "message": "Mistral-related models for QnA (#34045)\n\n* mistral qna start\r\n\r\n* mixtral qna\r\n\r\n* oops\r\n\r\n* qwen2 qna\r\n\r\n* qwen2moe qna\r\n\r\n* add missing input embed methods\r\n\r\n* add copied to all methods, can't directly from llama due to the prefix\r\n\r\n* make top level copied from",
    "sha": "7434c0ed21a154136b0145b0245ae9058005abac",
    "files": [
        {
            "sha": "2be657109a8d464020b5f7f569119ac64f7085ff",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -208,6 +208,11 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n [[autodoc]] MistralForTokenClassification\n     - forward\n \n+## MistralForQuestionAnswering\n+\n+[[autodoc]] MistralForQuestionAnswering\n+- forward\n+\n ## FlaxMistralModel\n \n [[autodoc]] FlaxMistralModel"
        },
        {
            "sha": "7afcaa798ecac41f9557d9b014abdc064a221c56",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -209,3 +209,7 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n \n [[autodoc]] MixtralForTokenClassification\n     - forward\n+\n+## MixtralForQuestionAnswering\n+[[autodoc]] MixtralForQuestionAnswering\n+    - forward"
        },
        {
            "sha": "78138413c7fb3a012bfc46487a4ed1d517e60a78",
            "filename": "docs/source/en/model_doc/qwen2.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2.md?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -85,3 +85,8 @@ In the following, we demonstrate how to use `Qwen2-7B-Instruct` for the inferenc\n \n [[autodoc]] Qwen2ForTokenClassification\n     - forward\n+\n+## Qwen2ForQuestionAnswering\n+\n+[[autodoc]] Qwen2ForQuestionAnswering\n+    - forward"
        },
        {
            "sha": "3a7391ca194fc5bf809a49ad0d8532337ca0b342",
            "filename": "docs/source/en/model_doc/qwen2_moe.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -80,3 +80,8 @@ In the following, we demonstrate how to use `Qwen1.5-MoE-A2.7B-Chat` for the inf\n \n [[autodoc]] Qwen2MoeForTokenClassification\n     - forward\n+\n+## Qwen2MoeForQuestionAnswering\n+\n+[[autodoc]] Qwen2MoeForQuestionAnswering\n+    - forward"
        },
        {
            "sha": "daffe11987ef5d24fe042aa985cea77f4ea476d0",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -2709,6 +2709,7 @@\n     _import_structure[\"models.mistral\"].extend(\n         [\n             \"MistralForCausalLM\",\n+            \"MistralForQuestionAnswering\",\n             \"MistralForSequenceClassification\",\n             \"MistralForTokenClassification\",\n             \"MistralModel\",\n@@ -2718,6 +2719,7 @@\n     _import_structure[\"models.mixtral\"].extend(\n         [\n             \"MixtralForCausalLM\",\n+            \"MixtralForQuestionAnswering\",\n             \"MixtralForSequenceClassification\",\n             \"MixtralForTokenClassification\",\n             \"MixtralModel\",\n@@ -3094,6 +3096,7 @@\n     _import_structure[\"models.qwen2\"].extend(\n         [\n             \"Qwen2ForCausalLM\",\n+            \"Qwen2ForQuestionAnswering\",\n             \"Qwen2ForSequenceClassification\",\n             \"Qwen2ForTokenClassification\",\n             \"Qwen2Model\",\n@@ -3110,6 +3113,7 @@\n     _import_structure[\"models.qwen2_moe\"].extend(\n         [\n             \"Qwen2MoeForCausalLM\",\n+            \"Qwen2MoeForQuestionAnswering\",\n             \"Qwen2MoeForSequenceClassification\",\n             \"Qwen2MoeForTokenClassification\",\n             \"Qwen2MoeModel\",\n@@ -7323,13 +7327,15 @@\n         )\n         from .models.mistral import (\n             MistralForCausalLM,\n+            MistralForQuestionAnswering,\n             MistralForSequenceClassification,\n             MistralForTokenClassification,\n             MistralModel,\n             MistralPreTrainedModel,\n         )\n         from .models.mixtral import (\n             MixtralForCausalLM,\n+            MixtralForQuestionAnswering,\n             MixtralForSequenceClassification,\n             MixtralForTokenClassification,\n             MixtralModel,\n@@ -7625,6 +7631,7 @@\n         )\n         from .models.qwen2 import (\n             Qwen2ForCausalLM,\n+            Qwen2ForQuestionAnswering,\n             Qwen2ForSequenceClassification,\n             Qwen2ForTokenClassification,\n             Qwen2Model,\n@@ -7637,6 +7644,7 @@\n         )\n         from .models.qwen2_moe import (\n             Qwen2MoeForCausalLM,\n+            Qwen2MoeForQuestionAnswering,\n             Qwen2MoeForSequenceClassification,\n             Qwen2MoeForTokenClassification,\n             Qwen2MoeModel,"
        },
        {
            "sha": "dbfcccaa4684dc16f512edca187d2053532b2493",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -1046,6 +1046,8 @@\n         (\"mbart\", \"MBartForQuestionAnswering\"),\n         (\"mega\", \"MegaForQuestionAnswering\"),\n         (\"megatron-bert\", \"MegatronBertForQuestionAnswering\"),\n+        (\"mistral\", \"MistralForQuestionAnswering\"),\n+        (\"mixtral\", \"MixtralForQuestionAnswering\"),\n         (\"mobilebert\", \"MobileBertForQuestionAnswering\"),\n         (\"mpnet\", \"MPNetForQuestionAnswering\"),\n         (\"mpt\", \"MptForQuestionAnswering\"),\n@@ -1057,6 +1059,8 @@\n         (\"nystromformer\", \"NystromformerForQuestionAnswering\"),\n         (\"opt\", \"OPTForQuestionAnswering\"),\n         (\"qdqbert\", \"QDQBertForQuestionAnswering\"),\n+        (\"qwen2\", \"Qwen2ForQuestionAnswering\"),\n+        (\"qwen2_moe\", \"Qwen2MoeForQuestionAnswering\"),\n         (\"reformer\", \"ReformerForQuestionAnswering\"),\n         (\"rembert\", \"RemBertForQuestionAnswering\"),\n         (\"roberta\", \"RobertaForQuestionAnswering\"),"
        },
        {
            "sha": "31441efe6527d212529d47ed9a2d7d79b0efc305",
            "filename": "src/transformers/models/mistral/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fmistral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fmistral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2F__init__.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -35,6 +35,7 @@\n else:\n     _import_structure[\"modeling_mistral\"] = [\n         \"MistralForCausalLM\",\n+        \"MistralForQuestionAnswering\",\n         \"MistralModel\",\n         \"MistralPreTrainedModel\",\n         \"MistralForSequenceClassification\",\n@@ -78,6 +79,7 @@\n     else:\n         from .modeling_mistral import (\n             MistralForCausalLM,\n+            MistralForQuestionAnswering,\n             MistralForSequenceClassification,\n             MistralForTokenClassification,\n             MistralModel,"
        },
        {
            "sha": "1bb7a3f109ef9b6592339c8d096a62b73bf4988b",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -34,6 +34,7 @@\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n+    QuestionAnsweringModelOutput,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n@@ -1408,3 +1409,103 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+The Mistral Model transformer with a span classification head on top for extractive question-answering tasks like\n+SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n+    \"\"\",\n+    MISTRAL_START_DOCSTRING,\n+)\n+# Copied from transformers.models.llama.modeling_llama.LlamaForQuestionAnswering with Llama->Mistral,LLAMA->MISTRAL,transformer->model\n+class MistralForQuestionAnswering(MistralPreTrainedModel):\n+    base_model_prefix = \"model\"\n+\n+    # Copied from models.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Mistral\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = MistralModel(config)\n+        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+        r\"\"\"\n+        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        total_loss = None\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, split add a dimension\n+            if len(start_positions.size()) > 1:\n+                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n+            if len(end_positions.size()) > 1:\n+                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n+            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n+            ignored_index = start_logits.size(1)\n+            start_positions = start_positions.clamp(0, ignored_index)\n+            end_positions = end_positions.clamp(0, ignored_index)\n+\n+            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+        if not return_dict:\n+            output = (start_logits, end_logits) + outputs[2:]\n+            return ((total_loss,) + output) if total_loss is not None else output\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=total_loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )"
        },
        {
            "sha": "4ee4834dd24984fd864e730933b31ee6503b4a14",
            "filename": "src/transformers/models/mixtral/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fmixtral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fmixtral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2F__init__.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -33,6 +33,7 @@\n else:\n     _import_structure[\"modeling_mixtral\"] = [\n         \"MixtralForCausalLM\",\n+        \"MixtralForQuestionAnswering\",\n         \"MixtralModel\",\n         \"MixtralPreTrainedModel\",\n         \"MixtralForSequenceClassification\",\n@@ -51,6 +52,7 @@\n     else:\n         from .modeling_mixtral import (\n             MixtralForCausalLM,\n+            MixtralForQuestionAnswering,\n             MixtralForSequenceClassification,\n             MixtralForTokenClassification,\n             MixtralModel,"
        },
        {
            "sha": "9bb0654f030ba72c5da7bcb814f03a92defd3d7d",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n+    QuestionAnsweringModelOutput,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n@@ -1644,3 +1645,103 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+The Mixtral Model transformer with a span classification head on top for extractive question-answering tasks like\n+SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n+    \"\"\",\n+    MIXTRAL_START_DOCSTRING,\n+)\n+# Copied from transformers.models.mistral.modeling_mistral.MistralForQuestionAnswering with Mistral->Mixtral, MISTRAL->MIXTRAL\n+class MixtralForQuestionAnswering(MixtralPreTrainedModel):\n+    base_model_prefix = \"model\"\n+\n+    # Copied from models.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Mixtral\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = MixtralModel(config)\n+        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+        r\"\"\"\n+        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        total_loss = None\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, split add a dimension\n+            if len(start_positions.size()) > 1:\n+                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n+            if len(end_positions.size()) > 1:\n+                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n+            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n+            ignored_index = start_logits.size(1)\n+            start_positions = start_positions.clamp(0, ignored_index)\n+            end_positions = end_positions.clamp(0, ignored_index)\n+\n+            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+        if not return_dict:\n+            output = (start_logits, end_logits) + outputs[2:]\n+            return ((total_loss,) + output) if total_loss is not None else output\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=total_loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )"
        },
        {
            "sha": "301531655a1db53895ac59fea2289c5abd05ad49",
            "filename": "src/transformers/models/qwen2/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fqwen2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fqwen2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2F__init__.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -42,6 +42,7 @@\n else:\n     _import_structure[\"modeling_qwen2\"] = [\n         \"Qwen2ForCausalLM\",\n+        \"Qwen2ForQuestionAnswering\",\n         \"Qwen2Model\",\n         \"Qwen2PreTrainedModel\",\n         \"Qwen2ForSequenceClassification\",\n@@ -69,6 +70,7 @@\n     else:\n         from .modeling_qwen2 import (\n             Qwen2ForCausalLM,\n+            Qwen2ForQuestionAnswering,\n             Qwen2ForSequenceClassification,\n             Qwen2ForTokenClassification,\n             Qwen2Model,"
        },
        {
            "sha": "2585352fc9594dfbae83ddc4a0e1aafd4f77c90d",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -34,6 +34,7 @@\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n+    QuestionAnsweringModelOutput,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n@@ -1508,3 +1509,103 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+The Qwen2 Model transformer with a span classification head on top for extractive question-answering tasks like\n+SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n+    \"\"\",\n+    QWEN2_START_DOCSTRING,\n+)\n+# Copied from transformers.models.mistral.modeling_mistral.MistralForQuestionAnswering with Mistral->Qwen2, MISTRAL->QWEN2\n+class Qwen2ForQuestionAnswering(Qwen2PreTrainedModel):\n+    base_model_prefix = \"model\"\n+\n+    # Copied from models.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Qwen2\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Qwen2Model(config)\n+        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+        r\"\"\"\n+        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        total_loss = None\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, split add a dimension\n+            if len(start_positions.size()) > 1:\n+                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n+            if len(end_positions.size()) > 1:\n+                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n+            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n+            ignored_index = start_logits.size(1)\n+            start_positions = start_positions.clamp(0, ignored_index)\n+            end_positions = end_positions.clamp(0, ignored_index)\n+\n+            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+        if not return_dict:\n+            output = (start_logits, end_logits) + outputs[2:]\n+            return ((total_loss,) + output) if total_loss is not None else output\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=total_loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )"
        },
        {
            "sha": "9520141ea831fc3a4856b6bce107893448e7eec2",
            "filename": "src/transformers/models/qwen2_moe/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2F__init__.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -33,6 +33,7 @@\n else:\n     _import_structure[\"modeling_qwen2_moe\"] = [\n         \"Qwen2MoeForCausalLM\",\n+        \"Qwen2MoeForQuestionAnswering\",\n         \"Qwen2MoeModel\",\n         \"Qwen2MoePreTrainedModel\",\n         \"Qwen2MoeForSequenceClassification\",\n@@ -51,6 +52,7 @@\n     else:\n         from .modeling_qwen2_moe import (\n             Qwen2MoeForCausalLM,\n+            Qwen2MoeForQuestionAnswering,\n             Qwen2MoeForSequenceClassification,\n             Qwen2MoeForTokenClassification,\n             Qwen2MoeModel,"
        },
        {
            "sha": "1a5f6e2ff2fbdc37ae80078ac81158d620da8dff",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n+    QuestionAnsweringModelOutput,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n@@ -1713,3 +1714,103 @@ def forward(\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+The Qwen2MoE Model transformer with a span classification head on top for extractive question-answering tasks like\n+SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n+    \"\"\",\n+    QWEN2MOE_START_DOCSTRING,\n+)\n+# Copied from transformers.models.mistral.modeling_mistral.MistralForQuestionAnswering with Mistral->Qwen2Moe, MISTRAL->QWEN2MOE\n+class Qwen2MoeForQuestionAnswering(Qwen2MoePreTrainedModel):\n+    base_model_prefix = \"model\"\n+\n+    # Copied from models.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Qwen2Moe\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Qwen2MoeModel(config)\n+        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+        r\"\"\"\n+        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n+            are not taken into account for computing the loss.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        total_loss = None\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, split add a dimension\n+            if len(start_positions.size()) > 1:\n+                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n+            if len(end_positions.size()) > 1:\n+                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n+            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n+            ignored_index = start_logits.size(1)\n+            start_positions = start_positions.clamp(0, ignored_index)\n+            end_positions = end_positions.clamp(0, ignored_index)\n+\n+            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+        if not return_dict:\n+            output = (start_logits, end_logits) + outputs[2:]\n+            return ((total_loss,) + output) if total_loss is not None else output\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=total_loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )"
        },
        {
            "sha": "4ca25bc7914a1ccace84a96c32073a604eb3b426",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -5920,6 +5920,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class MistralForQuestionAnswering(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class MistralForSequenceClassification(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -5955,6 +5962,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class MixtralForQuestionAnswering(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class MixtralForSequenceClassification(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -7406,6 +7420,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Qwen2ForQuestionAnswering(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class Qwen2ForSequenceClassification(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -7462,6 +7483,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Qwen2MoeForQuestionAnswering(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class Qwen2MoeForSequenceClassification(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "ff7f1e87bc197266125ec7fc7253ceaa9986b4d2",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -47,6 +47,7 @@\n \n     from transformers import (\n         MistralForCausalLM,\n+        MistralForQuestionAnswering,\n         MistralForSequenceClassification,\n         MistralForTokenClassification,\n         MistralModel,\n@@ -291,7 +292,13 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MistralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n-        (MistralModel, MistralForCausalLM, MistralForSequenceClassification, MistralForTokenClassification)\n+        (\n+            MistralModel,\n+            MistralForCausalLM,\n+            MistralForSequenceClassification,\n+            MistralForTokenClassification,\n+            MistralForQuestionAnswering,\n+        )\n         if is_torch_available()\n         else ()\n     )\n@@ -303,6 +310,7 @@ class MistralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n             \"token-classification\": MistralForTokenClassification,\n             \"text-generation\": MistralForCausalLM,\n             \"zero-shot\": MistralForSequenceClassification,\n+            \"question-answering\": MistralForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "0e6b2a999e89a94b395e25fdca2c0575a5ee620f",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -41,6 +41,7 @@\n \n     from transformers import (\n         MixtralForCausalLM,\n+        MixtralForQuestionAnswering,\n         MixtralForSequenceClassification,\n         MixtralForTokenClassification,\n         MixtralModel,\n@@ -291,7 +292,13 @@ def prepare_config_and_inputs_for_common(self):\n # Copied from tests.models.mistral.test_modeling_mistral.MistralModelTest with Mistral->Mixtral\n class MixtralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n-        (MixtralModel, MixtralForCausalLM, MixtralForSequenceClassification, MixtralForTokenClassification)\n+        (\n+            MixtralModel,\n+            MixtralForCausalLM,\n+            MixtralForSequenceClassification,\n+            MixtralForTokenClassification,\n+            MixtralForQuestionAnswering,\n+        )\n         if is_torch_available()\n         else ()\n     )\n@@ -303,6 +310,7 @@ class MixtralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n             \"token-classification\": MixtralForTokenClassification,\n             \"text-generation\": MixtralForCausalLM,\n             \"zero-shot\": MixtralForSequenceClassification,\n+            \"question-answering\": MixtralForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "5e5c42d4c566301e10d5e41d7bc9c1f2c72df3cc",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -43,6 +43,7 @@\n \n     from transformers import (\n         Qwen2ForCausalLM,\n+        Qwen2ForQuestionAnswering,\n         Qwen2ForSequenceClassification,\n         Qwen2ForTokenClassification,\n         Qwen2Model,\n@@ -300,7 +301,13 @@ def prepare_config_and_inputs_for_common(self):\n # Copied from tests.models.mistral.test_modeling_mistral.MistralModelTest with Mistral->Qwen2\n class Qwen2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n-        (Qwen2Model, Qwen2ForCausalLM, Qwen2ForSequenceClassification, Qwen2ForTokenClassification)\n+        (\n+            Qwen2Model,\n+            Qwen2ForCausalLM,\n+            Qwen2ForSequenceClassification,\n+            Qwen2ForTokenClassification,\n+            Qwen2ForQuestionAnswering,\n+        )\n         if is_torch_available()\n         else ()\n     )\n@@ -312,6 +319,7 @@ class Qwen2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n             \"token-classification\": Qwen2ForTokenClassification,\n             \"text-generation\": Qwen2ForCausalLM,\n             \"zero-shot\": Qwen2ForSequenceClassification,\n+            \"question-answering\": Qwen2ForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "d7b17b740f9e852121adfaab3c69db58aead9e0c",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7434c0ed21a154136b0145b0245ae9058005abac/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7434c0ed21a154136b0145b0245ae9058005abac/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=7434c0ed21a154136b0145b0245ae9058005abac",
            "patch": "@@ -43,6 +43,7 @@\n \n     from transformers import (\n         Qwen2MoeForCausalLM,\n+        Qwen2MoeForQuestionAnswering,\n         Qwen2MoeForSequenceClassification,\n         Qwen2MoeForTokenClassification,\n         Qwen2MoeModel,\n@@ -327,7 +328,13 @@ def prepare_config_and_inputs_for_common(self):\n # Copied from tests.models.mistral.test_modeling_mistral.MistralModelTest with Mistral->Qwen2Moe\n class Qwen2MoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n-        (Qwen2MoeModel, Qwen2MoeForCausalLM, Qwen2MoeForSequenceClassification, Qwen2MoeForTokenClassification)\n+        (\n+            Qwen2MoeModel,\n+            Qwen2MoeForCausalLM,\n+            Qwen2MoeForSequenceClassification,\n+            Qwen2MoeForTokenClassification,\n+            Qwen2MoeForQuestionAnswering,\n+        )\n         if is_torch_available()\n         else ()\n     )\n@@ -339,6 +346,7 @@ class Qwen2MoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n             \"token-classification\": Qwen2MoeForTokenClassification,\n             \"text-generation\": Qwen2MoeForCausalLM,\n             \"zero-shot\": Qwen2MoeForSequenceClassification,\n+            \"question-answering\": Qwen2MoeForQuestionAnswering,\n         }\n         if is_torch_available()\n         else {}"
        }
    ],
    "stats": {
        "total": 511,
        "additions": 507,
        "deletions": 4
    }
}