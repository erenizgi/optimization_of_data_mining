{
    "author": "SunMarc",
    "message": "Remove SigOpt (#41479)\n\n* remove sigopt\n\n* style",
    "sha": "1a3a5f5289e13980642315277ea5d9986622ffc8",
    "files": [
        {
            "sha": "8382c46dd0e5bda8a552d9b13f225ab5ae0c4233",
            "filename": ".github/workflows/check_failed_tests.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fcheck_failed_tests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fcheck_failed_tests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fcheck_failed_tests.yml?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -35,7 +35,6 @@ env:\n   # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.\n   # This token is created under the bot `hf-transformers-bot`.\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n   CUDA_VISIBLE_DEVICES: 0,1\n "
        },
        {
            "sha": "c2389bdff9861badeca43104990758d579ba5f0a",
            "filename": ".github/workflows/doctest_job.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fdoctest_job.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fdoctest_job.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fdoctest_job.yml?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -16,7 +16,6 @@ env:\n   RUN_SLOW: yes\n   OMP_NUM_THREADS: 16\n   MKL_NUM_THREADS: 16\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n \n jobs:"
        },
        {
            "sha": "c9a00c9f26b1cb9094153e0ef3311a5a62f44e14",
            "filename": ".github/workflows/model_jobs.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fmodel_jobs.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fmodel_jobs.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs.yml?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -38,7 +38,6 @@ env:\n   # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.\n   # This token is created under the bot `hf-transformers-bot`.\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n   CUDA_VISIBLE_DEVICES: 0,1\n "
        },
        {
            "sha": "14c3855663944d8fd5dd41da3493a19a427200bc",
            "filename": ".github/workflows/model_jobs_intel_gaudi.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fmodel_jobs_intel_gaudi.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fmodel_jobs_intel_gaudi.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs_intel_gaudi.yml?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -26,7 +26,6 @@ env:\n   TRANSFORMERS_IS_CI: yes\n   PT_ENABLE_INT64_SUPPORT: 1\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   HF_HOME: /mnt/cache/.cache/huggingface\n \n jobs:"
        },
        {
            "sha": "2f81f4f0fe53f94d3a759e6dc53c97ce35ae3b32",
            "filename": ".github/workflows/self-comment-ci.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fself-comment-ci.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fself-comment-ci.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-comment-ci.yml?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -20,7 +20,6 @@ env:\n   # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.\n   # This token is created under the bot `hf-transformers-bot`.\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n   CUDA_VISIBLE_DEVICES: 0,1\n "
        },
        {
            "sha": "aa77876097659ddca4a5bbfd76e3ad0878b98b9f",
            "filename": ".github/workflows/self-scheduled-intel-gaudi.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fself-scheduled-intel-gaudi.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fself-scheduled-intel-gaudi.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-intel-gaudi.yml?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -26,7 +26,6 @@ env:\n   TRANSFORMERS_IS_CI: yes\n   PT_ENABLE_INT64_SUPPORT: 1\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   HF_HOME: /mnt/cache/.cache/huggingface\n \n jobs:"
        },
        {
            "sha": "0706622937705f851b004d93f663d2e6dab0ffc0",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -48,7 +48,6 @@ env:\n   # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.\n   # This token is created under the bot `hf-transformers-bot`.\n   HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n   CUDA_VISIBLE_DEVICES: 0,1\n   NUM_SLICES: 2"
        },
        {
            "sha": "4fc22b44edc3a9a344f35c924d77abb8707d5ebd",
            "filename": ".github/workflows/ssh-runner.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fssh-runner.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/.github%2Fworkflows%2Fssh-runner.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fssh-runner.yml?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -20,7 +20,6 @@ env:\n   OMP_NUM_THREADS: 8\n   MKL_NUM_THREADS: 8\n   RUN_SLOW: yes # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access. # This token is created under the bot `hf-transformers-bot`.\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n   TF_FORCE_GPU_ALLOW_GROWTH: true\n   CUDA_VISIBLE_DEVICES: 0,1\n "
        },
        {
            "sha": "73c76234fa173f52def39013e73e4b9873b6809e",
            "filename": "docs/source/en/hpo_train.md",
            "status": "modified",
            "additions": 3,
            "deletions": 30,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/docs%2Fsource%2Fen%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/docs%2Fsource%2Fen%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fhpo_train.md?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -15,15 +15,12 @@ rendered properly in your Markdown viewer.\n \n # Hyperparameter search\n \n-Hyperparameter search discovers an optimal set of hyperparameters that produces the best model performance. [`Trainer`] supports several hyperparameter search backends - [Optuna](https://optuna.readthedocs.io/en/stable/index.html), [SigOpt](https://docs.sigopt.com/), [Weights & Biases](https://docs.wandb.ai/), [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) - through  [`~Trainer.hyperparameter_search`] to optimize an objective or even multiple objectives.\n+Hyperparameter search discovers an optimal set of hyperparameters that produces the best model performance. [`Trainer`] supports several hyperparameter search backends - [Optuna](https://optuna.readthedocs.io/en/stable/index.html), [Weights & Biases](https://docs.wandb.ai/), [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) - through  [`~Trainer.hyperparameter_search`] to optimize an objective or even multiple objectives.\n \n This guide will go over how to set up a hyperparameter search for each of the backends.\n \n-> [!WARNING]\n-> [SigOpt](https://github.com/sigopt/sigopt-server) is in public archive mode and is no longer actively maintained. Try using Optuna, Weights & Biases or Ray Tune instead.\n-\n ```bash\n-pip install optuna/sigopt/wandb/ray[tune]\n+pip install optuna/wandb/ray[tune]\n ```\n \n To use [`~Trainer.hyperparameter_search`], you need to create a `model_init` function. This function includes basic model information (arguments and configuration) because it needs to be reinitialized for each search trial in the run.\n@@ -109,31 +106,7 @@ best_trials = trainer.hyperparameter_search(\n     n_trials=20,\n     compute_objective=compute_objective,\n )\n-```\n-\n-</hfoption>\n-<hfoption id=\"SigOpt\">\n-\n-[SigOpt](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter) optimizes double, integer, and categorical parameters.\n-\n-```py\n-def sigopt_hp_space(trial):\n-    return [\n-        {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n-        {\n-            \"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n-            \"name\": \"per_device_train_batch_size\",\n-            \"type\": \"categorical\",\n-        },\n-    ]\n \n-best_trials = trainer.hyperparameter_search(\n-    direction=[\"minimize\", \"maximize\"],\n-    backend=\"sigopt\",\n-    hp_space=sigopt_hp_space,\n-    n_trials=20,\n-    compute_objective=compute_objective,\n-)\n ```\n \n </hfoption>\n@@ -166,4 +139,4 @@ best_trials = trainer.hyperparameter_search(\n \n ## Distributed Data Parallel\n \n-[`Trainer`] only supports hyperparameter search for distributed data parallel (DDP) on the Optuna and SigOpt backends. Only the rank-zero process is used to generate the search trial, and the resulting parameters are passed along to the other ranks.\n+[`Trainer`] only supports hyperparameter search for distributed data parallel (DDP) on the Optuna backends. Only the rank-zero process is used to generate the search trial, and the resulting parameters are passed along to the other ranks."
        },
        {
            "sha": "f727382eccb8612baeb4c2274802654c0c88ee2c",
            "filename": "docs/source/ja/hpo_train.md",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/docs%2Fsource%2Fja%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/docs%2Fsource%2Fja%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fhpo_train.md?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -20,31 +20,17 @@ rendered properly in your Markdown viewer.\n ## Hyperparameter Search backend\n \n [`Trainer`]は現在、4つのハイパーパラメーター検索バックエンドをサポートしています：\n-[optuna](https://optuna.org/)、[sigopt](https://sigopt.com/)、[raytune](https://docs.ray.io/en/latest/tune/index.html)、および[wandb](https://wandb.ai/site/sweeps)。\n+[optuna](https://optuna.org/)、[raytune](https://docs.ray.io/en/latest/tune/index.html)、および[wandb](https://wandb.ai/site/sweeps)。\n \n これらを使用する前に、ハイパーパラメーター検索バックエンドをインストールする必要があります。\n ```bash\n-pip install optuna/sigopt/wandb/ray[tune]\n+pip install optuna/wandb/ray[tune]\n ```\n \n ## How to enable Hyperparameter search in example\n \n ハイパーパラメータの検索スペースを定義し、異なるバックエンドには異なるフォーマットが必要です。\n \n-Sigoptの場合、sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter) を参照してください。それは以下のようなものです：\n-```py\n->>> def sigopt_hp_space(trial):\n-...     return [\n-...         {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n-...         {\n-...             \"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n-...             \"name\": \"per_device_train_batch_size\",\n-...             \"type\": \"categorical\",\n-...         },\n-...     ]\n-```\n-\n-\n Optunaに関しては、[object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py)をご覧ください。以下のようになります：\n \n \n@@ -125,7 +111,7 @@ Wandbについては、[object_parameter](https://docs.wandb.ai/guides/sweeps/co\n ... )\n ```\n \n-ハイパーパラメーターの探索を呼び出し、最良のトライアル パラメーターを取得します。バックエンドは `\"optuna\"` / `\"sigopt\"` / `\"wandb\"` / `\"ray\"` である可能性があります。方向は `\"minimize\"` または `\"maximize\"` であり、目標をより大きくするか小さくするかを示します。\n+ハイパーパラメーターの探索を呼び出し、最良のトライアル パラメーターを取得します。バックエンドは `\"optuna\"` / `\"wandb\"` / `\"ray\"` である可能性があります。方向は `\"minimize\"` または `\"maximize\"` であり、目標をより大きくするか小さくするかを示します。\n \n `compute_objective` 関数を独自に定義することもできます。定義されていない場合、デフォルトの `compute_objective` が呼び出され、F1などの評価メトリックの合計が目標値として返されます。\n \n@@ -141,4 +127,4 @@ Wandbについては、[object_parameter](https://docs.wandb.ai/guides/sweeps/co\n ```\n \n ## Hyperparameter search For DDP finetune\n-現在、DDP（Distributed Data Parallel）のためのハイパーパラメーター検索は、Optuna と SigOpt に対して有効になっています。ランクゼロプロセスのみが検索トライアルを生成し、他のランクに引数を渡します。\n+現在、DDP（Distributed Data Parallel）のためのハイパーパラメーター検索は、Optuna に対して有効になっています。ランクゼロプロセスのみが検索トライアルを生成し、他のランクに引数を渡します。"
        },
        {
            "sha": "9ae8dbafc9d3ee32f7a6522e3ba2301c90883b69",
            "filename": "docs/source/ko/hpo_train.md",
            "status": "modified",
            "additions": 4,
            "deletions": 17,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/docs%2Fsource%2Fko%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/docs%2Fsource%2Fko%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fhpo_train.md?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -20,30 +20,17 @@ rendered properly in your Markdown viewer.\n ## 하이퍼파라미터 탐색 백엔드 [[hyperparameter-search-backend]]\n \n [`Trainer`]는 현재 아래 4가지 하이퍼파라미터 탐색 백엔드를 지원합니다:\n-[optuna](https://optuna.org/)와 [sigopt](https://sigopt.com/), [raytune](https://docs.ray.io/en/latest/tune/index.html), [wandb](https://wandb.ai/site/sweeps) 입니다.\n+[optuna](https://optuna.org/)와, [raytune](https://docs.ray.io/en/latest/tune/index.html), [wandb](https://wandb.ai/site/sweeps) 입니다.\n \n 하이퍼파라미터 탐색 백엔드로 사용하기 전에 아래의 명령어를 사용하여 라이브러리들을 설치하세요.\n ```bash\n-pip install optuna/sigopt/wandb/ray[tune]\n+pip install optuna/wandb/ray[tune]\n ```\n \n ## 예제에서 하이퍼파라미터 탐색을 활성화하는 방법 [[how-to-enable-hyperparameter-search-in-example]]\n \n 하이퍼파라미터 탐색 공간을 정의하세요. 하이퍼파라미터 탐색 백엔드마다 서로 다른 형식이 필요합니다.\n \n-sigopt의 경우, 해당 [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter) 문서를 참조하여 아래와 같이 작성하세요:\n-```py\n->>> def sigopt_hp_space(trial):\n-...     return [\n-...         {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n-...         {\n-...             \"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n-...             \"name\": \"per_device_train_batch_size\",\n-...             \"type\": \"categorical\",\n-...         },\n-...     ]\n-```\n-\n optuna의 경우, 해당 [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py) 문서를 참조하여 아래와 같이 작성하세요:\n \n ```py\n@@ -106,7 +93,7 @@ wandb의 경우, 해당 [object_parameter](https://docs.wandb.ai/guides/sweeps/c\n ... )\n ```\n \n-하이퍼파라미터 탐색을 호출하고, 최적의 시험 매개변수를 가져오세요. 백엔드는 `\"optuna\"`/`\"sigopt\"`/`\"wandb\"`/`\"ray\"` 중에서 선택할 수 있습니다. 방향은 `\"minimize\"` 또는 `\"maximize\"` 중 선택하며, 목표를 최소화할 것인지 최대화할 것인지를 결정합니다.\n+하이퍼파라미터 탐색을 호출하고, 최적의 시험 매개변수를 가져오세요. 백엔드는 `\"optuna\"`/`\"wandb\"`/`\"ray\"` 중에서 선택할 수 있습니다. 방향은 `\"minimize\"` 또는 `\"maximize\"` 중 선택하며, 목표를 최소화할 것인지 최대화할 것인지를 결정합니다.\n \n 자신만의 compute_objective 함수를 정의할 수 있습니다. 만약 이 함수를 정의하지 않으면, 기본 compute_objective가 호출되고, f1과 같은 평가 지표의 합이 목푯값으로 반환됩니다.\n \n@@ -121,4 +108,4 @@ wandb의 경우, 해당 [object_parameter](https://docs.wandb.ai/guides/sweeps/c\n ```\n \n ## DDP 미세 조정을 위한 하이퍼파라미터 탐색 [[hyperparameter-search-for-ddp-finetune]]\n-현재, DDP(Distributed Data Parallelism; 분산 데이터 병렬처리)를 위한 하이퍼파라미터 탐색은 optuna와 sigopt에서 가능합니다. 최상위 프로세스가 하이퍼파라미터 탐색 과정을 시작하고 그 결과를 다른 프로세스에 전달합니다.\n+현재, DDP(Distributed Data Parallelism; 분산 데이터 병렬처리)를 위한 하이퍼파라미터 탐색은 optuna 가능합니다. 최상위 프로세스가 하이퍼파라미터 탐색 과정을 시작하고 그 결과를 다른 프로세스에 전달합니다."
        },
        {
            "sha": "6246309d94435a5b364943b1072c38b65b7f619b",
            "filename": "docs/source/zh/hpo_train.md",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/docs%2Fsource%2Fzh%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/docs%2Fsource%2Fzh%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fhpo_train.md?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -20,32 +20,18 @@ rendered properly in your Markdown viewer.\n \n ## 超参数搜索后端\n \n-[`Trainer`] 目前支持四种超参数搜索后端：[optuna](https://optuna.org/)，[sigopt](https://sigopt.com/)，[raytune](https://docs.ray.io/en/latest/tune/index.html)，[wandb](https://wandb.ai/site/sweeps)\n+[`Trainer`] 目前支持四种超参数搜索后端：[optuna](https://optuna.org/)，[raytune](https://docs.ray.io/en/latest/tune/index.html)，[wandb](https://wandb.ai/site/sweeps)\n \n 在使用它们之前，您应该先安装它们作为超参数搜索后端。\n \n ```bash\n-pip install optuna/sigopt/wandb/ray[tune]\n+pip install optuna/wandb/ray[tune]\n ```\n \n ## 如何在示例中启用超参数搜索\n \n 定义超参数搜索空间，不同的后端需要不同的格式。\n \n-对于sigopt，请参阅sigopt [object_parameter](https://docs.sigopt.com/ai-module-api-references/api_reference/objects/object_parameter)，它类似于以下内容：\n-\n-```py\n->>> def sigopt_hp_space(trial):\n-...     return [\n-...         {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\"},\n-...         {\n-...             \"categorical_values\": [\"16\", \"32\", \"64\", \"128\"],\n-...             \"name\": \"per_device_train_batch_size\",\n-...             \"type\": \"categorical\",\n-...         },\n-...     ]\n-```\n-\n 对于optuna，请参阅optuna [object_parameter](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html#sphx-glr-tutorial-10-key-features-002-configurations-py)，它类似于以下内容：\n \n ```py\n@@ -120,7 +106,7 @@ Optuna提供了多目标HPO。您可以在`hyperparameter_search`中传递`direc\n ... )\n ```\n \n-调用超参数搜索，获取最佳试验参数，后端可以是`\"optuna\"`/`\"sigopt\"`/`\"wandb\"`/`\"ray\"`。方向可以是`\"minimize\"`或`\"maximize\"`，表示是否优化更大或更低的目标。\n+调用超参数搜索，获取最佳试验参数，后端可以是`\"optuna\"`/`\"wandb\"`/`\"ray\"`。方向可以是`\"minimize\"`或`\"maximize\"`，表示是否优化更大或更低的目标。\n \n 您可以定义自己的compute_objective函数，如果没有定义，将调用默认的compute_objective，并将评估指标（如f1）之和作为目标值返回。\n \n@@ -135,4 +121,4 @@ Optuna提供了多目标HPO。您可以在`hyperparameter_search`中传递`direc\n ```\n \n ## 针对DDP微调的超参数搜索\n-目前，Optuna和Sigopt已启用针对DDP的超参数搜索。只有rank-zero进程会进行超参数搜索并将参数传递给其他进程。\n+目前，Optuna已启用针对DDP的超参数搜索。只有rank-zero进程会进行超参数搜索并将参数传递给其他进程。"
        },
        {
            "sha": "9325850caf2d273a3ea24927aa1d14e9daf3a155",
            "filename": "setup.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -162,7 +162,6 @@\n     \"scikit-learn\",\n     \"scipy\",\n     \"sentencepiece>=0.1.91,!=0.1.92\",\n-    \"sigopt\",\n     \"starlette\",\n     \"sudachipy>=0.6.6\",\n     \"sudachidict_core>=20220729\",\n@@ -274,7 +273,6 @@ def run(self):\n extras[\"deepspeed\"] = deps_list(\"deepspeed\") + extras[\"accelerate\"]\n extras[\"optuna\"] = deps_list(\"optuna\")\n extras[\"ray\"] = deps_list(\"ray[tune]\")\n-extras[\"sigopt\"] = deps_list(\"sigopt\")\n extras[\"hub-kernels\"] = deps_list(\"kernels\")\n \n extras[\"integrations\"] = extras[\"hub-kernels\"] + extras[\"optuna\"] + extras[\"ray\"]"
        },
        {
            "sha": "2c7c6c671ece4da8c424128cb301ef35f1c9c2fa",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -123,7 +123,6 @@\n         \"is_optuna_available\",\n         \"is_ray_available\",\n         \"is_ray_tune_available\",\n-        \"is_sigopt_available\",\n         \"is_swanlab_available\",\n         \"is_tensorboard_available\",\n         \"is_trackio_available\",\n@@ -608,7 +607,6 @@\n     from .integrations import is_optuna_available as is_optuna_available\n     from .integrations import is_ray_available as is_ray_available\n     from .integrations import is_ray_tune_available as is_ray_tune_available\n-    from .integrations import is_sigopt_available as is_sigopt_available\n     from .integrations import is_swanlab_available as is_swanlab_available\n     from .integrations import is_tensorboard_available as is_tensorboard_available\n     from .integrations import is_trackio_available as is_trackio_available"
        },
        {
            "sha": "ead7fbec04d59b853e82b6e3f6f71fd71e570257",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -68,7 +68,6 @@\n     \"scikit-learn\": \"scikit-learn\",\n     \"scipy\": \"scipy\",\n     \"sentencepiece\": \"sentencepiece>=0.1.91,!=0.1.92\",\n-    \"sigopt\": \"sigopt\",\n     \"starlette\": \"starlette\",\n     \"sudachipy\": \"sudachipy>=0.6.6\",\n     \"sudachidict_core\": \"sudachidict_core>=20220729\","
        },
        {
            "sha": "0a4072a51788c27dadf9ada82147aff084ec3bb5",
            "filename": "src/transformers/hyperparameter_search.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Fhyperparameter_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Fhyperparameter_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fhyperparameter_search.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -16,18 +16,15 @@\n from .integrations import (\n     is_optuna_available,\n     is_ray_tune_available,\n-    is_sigopt_available,\n     is_wandb_available,\n     run_hp_search_optuna,\n     run_hp_search_ray,\n-    run_hp_search_sigopt,\n     run_hp_search_wandb,\n )\n from .trainer_utils import (\n     HPSearchBackend,\n     default_hp_space_optuna,\n     default_hp_space_ray,\n-    default_hp_space_sigopt,\n     default_hp_space_wandb,\n )\n from .utils import logging\n@@ -90,20 +87,6 @@ def default_hp_space(self, trial):\n         return default_hp_space_ray(trial)\n \n \n-class SigOptBackend(HyperParamSearchBackendBase):\n-    name = \"sigopt\"\n-\n-    @staticmethod\n-    def is_available():\n-        return is_sigopt_available()\n-\n-    def run(self, trainer, n_trials: int, direction: str, **kwargs):\n-        return run_hp_search_sigopt(trainer, n_trials, direction, **kwargs)\n-\n-    def default_hp_space(self, trial):\n-        return default_hp_space_sigopt(trial)\n-\n-\n class WandbBackend(HyperParamSearchBackendBase):\n     name = \"wandb\"\n \n@@ -119,7 +102,7 @@ def default_hp_space(self, trial):\n \n \n ALL_HYPERPARAMETER_SEARCH_BACKENDS = {\n-    HPSearchBackend(backend.name): backend for backend in [OptunaBackend, RayTuneBackend, SigOptBackend, WandbBackend]\n+    HPSearchBackend(backend.name): backend for backend in [OptunaBackend, RayTuneBackend, WandbBackend]\n }\n \n "
        },
        {
            "sha": "237d7420997fa3fc10438bd8d876c3cae52fd104",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -105,15 +105,13 @@\n         \"is_optuna_available\",\n         \"is_ray_available\",\n         \"is_ray_tune_available\",\n-        \"is_sigopt_available\",\n         \"is_swanlab_available\",\n         \"is_tensorboard_available\",\n         \"is_trackio_available\",\n         \"is_wandb_available\",\n         \"rewrite_logs\",\n         \"run_hp_search_optuna\",\n         \"run_hp_search_ray\",\n-        \"run_hp_search_sigopt\",\n         \"run_hp_search_wandb\",\n     ],\n     \"mxfp4\": [\n@@ -247,15 +245,13 @@\n         is_optuna_available,\n         is_ray_available,\n         is_ray_tune_available,\n-        is_sigopt_available,\n         is_swanlab_available,\n         is_tensorboard_available,\n         is_trackio_available,\n         is_wandb_available,\n         rewrite_logs,\n         run_hp_search_optuna,\n         run_hp_search_ray,\n-        run_hp_search_sigopt,\n         run_hp_search_wandb,\n     )\n     from .mxfp4 import ("
        },
        {
            "sha": "937adb230aa4cdc16ecbb0659b2d2e2774cbcb0c",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 108,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -22,12 +22,11 @@\n import json\n import numbers\n import os\n-import pickle\n import re\n import shutil\n import sys\n import tempfile\n-from dataclasses import asdict, fields\n+from dataclasses import fields\n from enum import Enum\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Literal, Optional, Union\n@@ -180,10 +179,6 @@ def is_ray_tune_available():\n     return importlib.util.find_spec(\"ray.tune\") is not None\n \n \n-def is_sigopt_available():\n-    return importlib.util.find_spec(\"sigopt\") is not None\n-\n-\n def is_azureml_available():\n     if importlib.util.find_spec(\"azureml\") is None:\n         return False\n@@ -234,11 +229,8 @@ def hp_params(trial):\n \n         if isinstance(trial, optuna.trial.BaseTrial):\n             return trial.params\n-    if is_ray_tune_available():\n-        if isinstance(trial, dict):\n-            return trial\n \n-    if is_sigopt_available():\n+    if is_ray_tune_available():\n         if isinstance(trial, dict):\n             return trial\n \n@@ -445,104 +437,6 @@ def dynamic_modules_import_trainable(*args, **kwargs):\n     return best_run\n \n \n-def run_hp_search_sigopt(trainer, n_trials: int, direction: str, **kwargs) -> BestRun:\n-    import sigopt\n-\n-    if trainer.args.process_index == 0:\n-        if importlib.metadata.version(\"sigopt\") >= \"8.0.0\":\n-            sigopt.set_project(\"huggingface\")\n-\n-            experiment = sigopt.create_experiment(\n-                name=\"huggingface-tune\",\n-                type=\"offline\",\n-                parameters=trainer.hp_space(None),\n-                metrics=[{\"name\": \"objective\", \"objective\": direction, \"strategy\": \"optimize\"}],\n-                parallel_bandwidth=1,\n-                budget=n_trials,\n-            )\n-\n-            logger.info(f\"created experiment: https://app.sigopt.com/experiment/{experiment.id}\")\n-\n-            for run in experiment.loop():\n-                with run:\n-                    trainer.objective = None\n-                    if trainer.args.world_size > 1:\n-                        if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:\n-                            raise RuntimeError(\"only support DDP Sigopt HPO for ParallelMode.DISTRIBUTED currently.\")\n-                        trainer._hp_search_setup(run.run)\n-                        torch.distributed.broadcast_object_list(pickle.dumps(trainer.args), src=0)\n-                        trainer.train(resume_from_checkpoint=None)\n-                    else:\n-                        trainer.train(resume_from_checkpoint=None, trial=run.run)\n-                    # If there hasn't been any evaluation during the training loop.\n-                    if getattr(trainer, \"objective\", None) is None:\n-                        metrics = trainer.evaluate()\n-                        trainer.objective = trainer.compute_objective(metrics)\n-                    run.log_metric(\"objective\", trainer.objective)\n-\n-            best = list(experiment.get_best_runs())[0]\n-            best_run = BestRun(best.id, best.values[\"objective\"].value, best.assignments)\n-        else:\n-            from sigopt import Connection\n-\n-            conn = Connection()\n-            proxies = kwargs.pop(\"proxies\", None)\n-            if proxies is not None:\n-                conn.set_proxies(proxies)\n-\n-            experiment = conn.experiments().create(\n-                name=\"huggingface-tune\",\n-                parameters=trainer.hp_space(None),\n-                metrics=[{\"name\": \"objective\", \"objective\": direction, \"strategy\": \"optimize\"}],\n-                parallel_bandwidth=1,\n-                observation_budget=n_trials,\n-                project=\"huggingface\",\n-            )\n-            logger.info(f\"created experiment: https://app.sigopt.com/experiment/{experiment.id}\")\n-\n-            while experiment.progress.observation_count < experiment.observation_budget:\n-                suggestion = conn.experiments(experiment.id).suggestions().create()\n-                trainer.objective = None\n-                if trainer.args.world_size > 1:\n-                    if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:\n-                        raise RuntimeError(\"only support DDP Sigopt HPO for ParallelMode.DISTRIBUTED currently.\")\n-                    trainer._hp_search_setup(suggestion)\n-                    torch.distributed.broadcast_object_list(pickle.dumps(trainer.args), src=0)\n-                    trainer.train(resume_from_checkpoint=None)\n-                else:\n-                    trainer.train(resume_from_checkpoint=None, trial=suggestion)\n-                # If there hasn't been any evaluation during the training loop.\n-                if getattr(trainer, \"objective\", None) is None:\n-                    metrics = trainer.evaluate()\n-                    trainer.objective = trainer.compute_objective(metrics)\n-\n-                values = [{\"name\": \"objective\", \"value\": trainer.objective}]\n-                obs = conn.experiments(experiment.id).observations().create(suggestion=suggestion.id, values=values)\n-                logger.info(f\"[suggestion_id, observation_id]: [{suggestion.id}, {obs.id}]\")\n-                experiment = conn.experiments(experiment.id).fetch()\n-\n-            best = list(conn.experiments(experiment.id).best_assignments().fetch().iterate_pages())[0]\n-            best_run = BestRun(best.id, best.value, best.assignments)\n-        return best_run\n-    else:\n-        for i in range(n_trials):\n-            trainer.objective = None\n-            args_main_rank = list(pickle.dumps(trainer.args))\n-            if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:\n-                raise RuntimeError(\"only support DDP Sigopt HPO for ParallelMode.DISTRIBUTED currently.\")\n-            torch.distributed.broadcast_object_list(args_main_rank, src=0)\n-            args = pickle.loads(bytes(args_main_rank))\n-            for key, value in asdict(args).items():\n-                if key != \"local_rank\":\n-                    setattr(trainer.args, key, value)\n-            trainer.train(resume_from_checkpoint=None)\n-            # If there hasn't been any evaluation during the training loop.\n-            if getattr(trainer, \"objective\", None) is None:\n-                metrics = trainer.evaluate()\n-                trainer.objective = trainer.compute_objective(metrics)\n-        return None\n-\n-\n def run_hp_search_wandb(trainer, n_trials: int, direction: str, **kwargs) -> BestRun:\n     if not is_wandb_available():\n         raise ImportError(\"This function needs wandb installed: `pip install wandb`\")"
        },
        {
            "sha": "89e5a9700739ba7ba9230a391694797114f6d717",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -57,7 +57,6 @@\n     is_clearml_available,\n     is_optuna_available,\n     is_ray_available,\n-    is_sigopt_available,\n     is_swanlab_available,\n     is_tensorboard_available,\n     is_trackio_available,\n@@ -1160,16 +1159,6 @@ def require_ray(test_case):\n     return unittest.skipUnless(is_ray_available(), \"test requires Ray/tune\")(test_case)\n \n \n-def require_sigopt(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires SigOpt.\n-\n-    These tests are skipped when SigOpt isn't installed.\n-\n-    \"\"\"\n-    return unittest.skipUnless(is_sigopt_available(), \"test requires SigOpt\")(test_case)\n-\n-\n def require_swanlab(test_case):\n     \"\"\"\n     Decorator marking a test that requires swanlab."
        },
        {
            "sha": "4473d67a6e68e5bda74856bf78ad22282286bc25",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -333,7 +333,7 @@ class Trainer:\n             A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n             from a new instance of the model as given by this function.\n \n-            The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n+            The function may have zero argument, or a single one containing the optuna/Ray Tune trial object, to\n             be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n             inner layers, dropout probabilities etc).\n         compute_loss_func (`Callable`, *optional*):\n@@ -1792,8 +1792,6 @@ def _hp_search_setup(self, trial: Union[\"optuna.Trial\", dict[str, Any]]):\n         elif self.hp_search_backend == HPSearchBackend.RAY:\n             params = trial\n             params.pop(\"wandb\", None)\n-        elif self.hp_search_backend == HPSearchBackend.SIGOPT:\n-            params = {k: int(v) if isinstance(v, str) else v for k, v in trial.assignments.items()}\n         elif self.hp_search_backend == HPSearchBackend.WANDB:\n             params = trial\n \n@@ -1812,8 +1810,6 @@ def _hp_search_setup(self, trial: Union[\"optuna.Trial\", dict[str, Any]]):\n             setattr(self.args, key, value)\n         if self.hp_search_backend == HPSearchBackend.OPTUNA:\n             logger.info(f\"Trial: {trial.params}\")\n-        if self.hp_search_backend == HPSearchBackend.SIGOPT:\n-            logger.info(f\"SigOpt Assignments: {trial.assignments}\")\n         if self.hp_search_backend == HPSearchBackend.WANDB:\n             logger.info(f\"W&B Sweep parameters: {trial}\")\n         if self.is_deepspeed_enabled:\n@@ -2709,8 +2705,6 @@ def _get_output_dir(self, trial):\n                 import ray.train\n \n                 run_id = ray.train.get_context().get_trial_id()\n-            elif self.hp_search_backend == HPSearchBackend.SIGOPT:\n-                run_id = trial.id\n             elif self.hp_search_backend == HPSearchBackend.WANDB:\n                 import wandb\n \n@@ -3500,7 +3494,7 @@ def hyperparameter_search(\n         **kwargs,\n     ) -> Union[BestRun, list[BestRun]]:\n         \"\"\"\n-        Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\n+        Launch an hyperparameter search using `optuna` or `Ray Tune`. The optimized quantity is determined\n         by `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\n         the sum of all metrics otherwise.\n \n@@ -3516,8 +3510,8 @@ def hyperparameter_search(\n         Args:\n             hp_space (`Callable[[\"optuna.Trial\"], dict[str, float]]`, *optional*):\n                 A function that defines the hyperparameter search space. Will default to\n-                [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n-                [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n+                [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`]\n+                depending on your backend.\n             compute_objective (`Callable[[dict[str, float]], float]`, *optional*):\n                 A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\n                 method. Will default to [`~trainer_utils.default_compute_objective`].\n@@ -3530,7 +3524,7 @@ def hyperparameter_search(\n                 `\"minimize\"` and `\"maximize\"`, you should pick `\"minimize\"` when optimizing the validation loss,\n                 `\"maximize\"` when optimizing one or several metrics.\n             backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\n-                The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\n+                The backend to use for hyperparameter search. Will default to optuna or Ray Tune, depending\n                 on which one is installed. If all are installed, will default to optuna.\n             hp_name (`Callable[[\"optuna.Trial\"], str]]`, *optional*):\n                 A function that defines the trial/run name. Will default to None.\n@@ -3545,9 +3539,6 @@ def hyperparameter_search(\n                   If `resources_per_trial` is not set in the `kwargs`, it defaults to 1 CPU core and 1 GPU (if available).\n                   If `progress_reporter` is not set in the `kwargs`,\n                   [ray.tune.CLIReporter](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.CLIReporter.html) is used.\n-                - `sigopt`: the parameter `proxies` from\n-                  [sigopt.Connection.set_proxies](https://docs.sigopt.com/support/faq#how-do-i-use-sigopt-with-a-proxy).\n-\n         Returns:\n             [`trainer_utils.BestRun` or `list[trainer_utils.BestRun]`]: All the information about the best run or best\n             runs for multi-objective optimization. Experiment summary can be found in `run_summary` attribute for Ray"
        },
        {
            "sha": "1ac5c5565b86d50a9fb7b04e423b4c3ecfe99365",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -24,7 +24,7 @@\n import numpy as np\n from tqdm.auto import tqdm\n \n-from .trainer_utils import HPSearchBackend, IntervalStrategy, SaveStrategy, has_length\n+from .trainer_utils import IntervalStrategy, SaveStrategy, has_length\n from .training_args import TrainingArguments\n from .utils import logging\n \n@@ -172,15 +172,14 @@ def init_training_references(self, trainer, max_steps, num_train_epochs, trial):\n         Stores the initial training references needed in `self`\n         \"\"\"\n         if trainer.hp_name is not None and trainer._trial is not None:\n-            # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n+            # use self._trial because the Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n             # parameter to Train when using DDP.\n             self.trial_name = trainer.hp_name(trainer._trial)\n         self.trial_params = None\n         if trial is not None:\n             from transformers.integrations import hp_params\n \n-            assignments = trial.assignments if trainer.hp_search_backend == HPSearchBackend.SIGOPT else trial\n-            self.trial_params = hp_params(assignments)\n+            self.trial_params = hp_params(trial)\n \n         self.max_steps = max_steps\n         self.num_train_epochs = num_train_epochs"
        },
        {
            "sha": "2945f49e04cbb2c0155d072676da2a6eb06fbcba",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -303,19 +303,6 @@ def default_hp_space_ray(trial) -> dict[str, Any]:\n     }\n \n \n-def default_hp_space_sigopt(trial):\n-    return [\n-        {\"bounds\": {\"min\": 1e-6, \"max\": 1e-4}, \"name\": \"learning_rate\", \"type\": \"double\", \"transformation\": \"log\"},\n-        {\"bounds\": {\"min\": 1, \"max\": 6}, \"name\": \"num_train_epochs\", \"type\": \"int\"},\n-        {\"bounds\": {\"min\": 1, \"max\": 40}, \"name\": \"seed\", \"type\": \"int\"},\n-        {\n-            \"categorical_values\": [\"4\", \"8\", \"16\", \"32\", \"64\"],\n-            \"name\": \"per_device_train_batch_size\",\n-            \"type\": \"categorical\",\n-        },\n-    ]\n-\n-\n def default_hp_space_wandb(trial) -> dict[str, Any]:\n     from .integrations import is_wandb_available\n \n@@ -337,7 +324,6 @@ def default_hp_space_wandb(trial) -> dict[str, Any]:\n class HPSearchBackend(ExplicitEnum):\n     OPTUNA = \"optuna\"\n     RAY = \"ray\"\n-    SIGOPT = \"sigopt\"\n     WANDB = \"wandb\"\n \n "
        },
        {
            "sha": "7d295cc56526a8733d3d76f521c8966f052e099c",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 53,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/1a3a5f5289e13980642315277ea5d9986622ffc8/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1a3a5f5289e13980642315277ea5d9986622ffc8/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=1a3a5f5289e13980642315277ea5d9986622ffc8",
            "patch": "@@ -88,7 +88,6 @@\n     require_ray,\n     require_schedulefree,\n     require_sentencepiece,\n-    require_sigopt,\n     require_tensorboard,\n     require_tokenizers,\n     require_torch,\n@@ -5690,58 +5689,6 @@ def test_hyperparameter_search_ray_client(self):\n             self.ray_hyperparameter_search()\n \n \n-@slow\n-@require_torch\n-@require_sigopt\n-class TrainerHyperParameterSigOptIntegrationTest(unittest.TestCase):\n-    def setUp(self):\n-        args = TrainingArguments(\"..\")\n-        self.n_epochs = args.num_train_epochs\n-        self.batch_size = args.train_batch_size\n-\n-    def test_hyperparameter_search(self):\n-        class MyTrialShortNamer(TrialShortNamer):\n-            DEFAULTS = {\"a\": 0, \"b\": 0}\n-\n-        def hp_space(trial):\n-            return [\n-                {\"bounds\": {\"min\": -4, \"max\": 4}, \"name\": \"a\", \"type\": \"int\"},\n-                {\"bounds\": {\"min\": -4, \"max\": 4}, \"name\": \"b\", \"type\": \"int\"},\n-            ]\n-\n-        def model_init(trial):\n-            if trial is not None:\n-                a = trial.assignments[\"a\"]\n-                b = trial.assignments[\"b\"]\n-            else:\n-                a = 0\n-                b = 0\n-            config = RegressionModelConfig(a=a, b=b, double_output=False)\n-\n-            return RegressionPreTrainedModel(config).to(torch_device)\n-\n-        def hp_name(trial):\n-            return MyTrialShortNamer.shortname(trial.assignments)\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            trainer = get_regression_trainer(\n-                output_dir=tmp_dir,\n-                learning_rate=0.1,\n-                logging_steps=1,\n-                eval_strategy=IntervalStrategy.EPOCH,\n-                save_strategy=IntervalStrategy.EPOCH,\n-                num_train_epochs=4,\n-                disable_tqdm=True,\n-                load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n-                run_name=\"test\",\n-                model_init=model_init,\n-            )\n-            trainer.hyperparameter_search(\n-                direction=\"minimize\", hp_space=hp_space, hp_name=hp_name, backend=\"sigopt\", n_trials=4\n-            )\n-\n-\n optim_test_params = []\n if is_torch_available():\n     default_adam_kwargs = {"
        }
    ],
    "stats": {
        "total": 348,
        "additions": 26,
        "deletions": 322
    }
}