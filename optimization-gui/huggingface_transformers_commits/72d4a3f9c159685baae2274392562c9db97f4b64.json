{
    "author": "gante",
    "message": "mps: add `isin_mps_friendly`, a wrapper function for `torch.isin` (#33099)",
    "sha": "72d4a3f9c159685baae2274392562c9db97f4b64",
    "files": [
        {
            "sha": "7e4096c0aa4c80926fb22aa6a0696bbf3592053e",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=72d4a3f9c159685baae2274392562c9db97f4b64",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n \n from ..cache_utils import DynamicCache\n+from ..pytorch_utils import isin_mps_friendly\n from .logits_process import LogitsProcessorList, MinLengthLogitsProcessor\n \n \n@@ -335,7 +336,7 @@ def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor,\n                     # remove remaining candidate ids if an \"eos\" token is found, otherwise the target model may\n                     # accept eos and the rest as valid, thus not stopping generation after \"eos\"\n                     # NOTE: below code is written based on the fact that assisted decoding supports only bs=1\n-                    mask = torch.isin(chosen_ids, self.eos_token_id)\n+                    mask = isin_mps_friendly(chosen_ids, self.eos_token_id)\n                     match_indices_eos = torch.nonzero(mask)\n                     if match_indices_eos.numel() > 0:\n                         first_eos_index = match_indices_eos[0].item()"
        },
        {
            "sha": "c586a97459ac6a306035cf00c10f34ea8b3a1da2",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=72d4a3f9c159685baae2274392562c9db97f4b64",
            "patch": "@@ -20,6 +20,7 @@\n import numpy as np\n import torch\n \n+from ..pytorch_utils import isin_mps_friendly\n from ..utils import add_start_docstrings\n from ..utils.logging import get_logger\n \n@@ -159,7 +160,7 @@ def __init__(self, min_length: int, eos_token_id: Union[int, List[int], torch.Te\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n-        eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id)\n+        eos_token_mask = isin_mps_friendly(vocab_tensor, self.eos_token_id)\n         scores_processed = scores.clone()\n         if input_ids.shape[-1] < self.min_length:\n             scores_processed = torch.where(eos_token_mask, -math.inf, scores)\n@@ -231,7 +232,7 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n         new_tokens_length = input_ids.shape[-1] - self.prompt_length_to_skip\n         scores_processed = scores.clone()\n         vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n-        eos_token_mask = torch.isin(vocab_tensor, self.eos_token_id)\n+        eos_token_mask = isin_mps_friendly(vocab_tensor, self.eos_token_id)\n         if new_tokens_length < self.min_new_tokens:\n             scores_processed = torch.where(eos_token_mask, -math.inf, scores)\n \n@@ -1795,7 +1796,7 @@ def set_begin_index(self, begin_index):\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n-        suppress_token_mask = torch.isin(vocab_tensor, self.begin_suppress_tokens)\n+        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.begin_suppress_tokens)\n         scores_processed = scores\n         if input_ids.shape[-1] == self.begin_index:\n             scores_processed = torch.where(suppress_token_mask, -float(\"inf\"), scores)\n@@ -1838,7 +1839,7 @@ def __init__(self, suppress_tokens, device: str = \"cpu\"):\n     @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         vocab_tensor = torch.arange(scores.shape[-1], device=scores.device)\n-        suppress_token_mask = torch.isin(vocab_tensor, self.suppress_tokens)\n+        suppress_token_mask = isin_mps_friendly(vocab_tensor, self.suppress_tokens)\n         scores = torch.where(suppress_token_mask, -float(\"inf\"), scores)\n         return scores\n "
        },
        {
            "sha": "b8d6540ca2f793f70fd5b7baeaf03afc68149680",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=72d4a3f9c159685baae2274392562c9db97f4b64",
            "patch": "@@ -9,8 +9,7 @@\n import torch\n from torch.nn import functional as F\n \n-from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n-\n+from ..pytorch_utils import isin_mps_friendly\n from ..tokenization_utils_base import PreTrainedTokenizerBase\n from ..utils import add_start_docstrings, logging\n \n@@ -457,19 +456,7 @@ def __init__(self, eos_token_id: Union[int, List[int], torch.Tensor]):\n     @add_start_docstrings(STOPPING_CRITERIA_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor:\n         self.eos_token_id = self.eos_token_id.to(input_ids.device)\n-        if input_ids.device.type == \"mps\" and not is_torch_greater_or_equal_than_2_4:\n-            # TODO: remove this workaround when we stop supporting torch<=2.3\n-            # https://github.com/pytorch/pytorch/issues/77764#issuecomment-2067838075\n-            is_done = (\n-                input_ids[:, -1]\n-                .tile(self.eos_token_id.shape[0], 1)\n-                .eq(self.eos_token_id.unsqueeze(1))\n-                .sum(dim=0)\n-                .bool()\n-                .squeeze()\n-            )\n-        else:\n-            is_done = torch.isin(input_ids[:, -1], self.eos_token_id)\n+        is_done = isin_mps_friendly(input_ids[:, -1], self.eos_token_id)\n         return is_done\n \n "
        },
        {
            "sha": "0d2baea6d85f1e87f716e309d213fbe7ff5e4ee5",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 11,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=72d4a3f9c159685baae2274392562c9db97f4b64",
            "patch": "@@ -47,7 +47,7 @@\n     MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,\n     MODEL_FOR_VISION_2_SEQ_MAPPING,\n )\n-from ..pytorch_utils import is_torch_greater_or_equal_than_2_4\n+from ..pytorch_utils import isin_mps_friendly\n from ..tokenization_utils import ExtensionsTrie\n from ..utils import (\n     ModelOutput,\n@@ -472,18 +472,11 @@ def _prepare_attention_mask_for_generation(\n         if not is_input_ids:\n             return default_attention_mask\n \n-        # Otherwise we have may have information -> try to infer the attention mask\n-        if inputs.device.type == \"mps\" and not is_torch_greater_or_equal_than_2_4:\n-            # mps does not support torch.isin for torch<2.4 (https://github.com/pytorch/pytorch/issues/77764)\n-            raise ValueError(\n-                \"Can't infer missing attention mask on `mps` device for torch<2.4. Please provide an `attention_mask` or upgrade to torch>=2.4\"\n-            )\n-\n         is_pad_token_in_inputs = (pad_token_id is not None) and (\n-            torch.isin(elements=inputs, test_elements=pad_token_id).any()\n+            isin_mps_friendly(elements=inputs, test_elements=pad_token_id).any()\n         )\n         is_pad_token_not_equal_to_eos_token_id = (eos_token_id is None) or ~(\n-            torch.isin(elements=eos_token_id, test_elements=pad_token_id).any()\n+            isin_mps_friendly(elements=eos_token_id, test_elements=pad_token_id).any()\n         )\n         can_infer_attention_mask = is_pad_token_in_inputs * is_pad_token_not_equal_to_eos_token_id\n         attention_mask_from_padding = inputs.ne(pad_token_id).long()\n@@ -1660,7 +1653,7 @@ def _tensor_or_none(token, device=None):\n         if not is_torchdynamo_compiling():  # Checks that depend on tensor-dependent control flow\n             if (\n                 eos_token_tensor is not None\n-                and torch.isin(elements=eos_token_tensor, test_elements=pad_token_tensor).any()\n+                and isin_mps_friendly(elements=eos_token_tensor, test_elements=pad_token_tensor).any()\n             ):\n                 if kwargs_has_attention_mask is not None and not kwargs_has_attention_mask:\n                     logger.warning_once("
        },
        {
            "sha": "b6d025a0b8e279dbaff0244511e960a1dcfdfbba",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=72d4a3f9c159685baae2274392562c9db97f4b64",
            "patch": "@@ -35,7 +35,7 @@\n     CausalLMOutputWithCrossAttentions,\n )\n from ...modeling_utils import PreTrainedModel, SequenceSummary\n-from ...pytorch_utils import Conv1D\n+from ...pytorch_utils import Conv1D, isin_mps_friendly\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n@@ -132,7 +132,7 @@ def _pad_extra_bos_eos_tokens(\n         )\n         for i, each_input_id in enumerate(input_ids):\n             # locate where the valid tokens end and then add the eos token\n-            if torch.isin(each_input_id, pad_token_id).sum():\n+            if isin_mps_friendly(each_input_id, pad_token_id).sum():\n                 pos = torch.where(each_input_id == pad_token_id)[0].min()\n                 modified_input_ids[i] = torch.concatenate(\n                     [each_input_id[:pos], torch.tensor([eos_token_id], device=input_ids.device), each_input_id[pos:]]"
        },
        {
            "sha": "8c1bd21fb2962ae4b8cac214a5413bf0800044fc",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d4a3f9c159685baae2274392562c9db97f4b64/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=72d4a3f9c159685baae2274392562c9db97f4b64",
            "patch": "@@ -303,3 +303,24 @@ def id_tensor_storage(tensor: torch.Tensor) -> Tuple[torch.device, int, int]:\n         unique_id = storage_ptr(tensor)\n \n     return tensor.device, unique_id, storage_size(tensor)\n+\n+\n+def isin_mps_friendly(elements: torch.Tensor, test_elements: torch.Tensor | int) -> torch.Tensor:\n+    \"\"\"\n+    Same as `torch.isin` without flags, but MPS-friendly. We can remove this function when we stop supporting\n+    torch <= 2.3. See https://github.com/pytorch/pytorch/issues/77764#issuecomment-2067838075\n+\n+    Args:\n+        elements (`torch.Tensor`): Input elements\n+        test_elements (`torch.Tensor`): The elements to check against.\n+\n+    Returns:\n+        `torch.Tensor`: A boolean tensor of the same shape as `elements` that is True for `elements` in `test_elements`\n+        and False otherwise\n+    \"\"\"\n+\n+    if elements.device.type == \"mps\" and not is_torch_greater_or_equal_than_2_4:\n+        return elements.tile(test_elements.shape[0], 1).eq(test_elements.unsqueeze(1)).sum(dim=0).bool().squeeze()\n+    else:\n+        # Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\n+        return torch.isin(elements, test_elements)"
        },
        {
            "sha": "f78285fdb90d903a1ee752f271438b8cecdc929e",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/72d4a3f9c159685baae2274392562c9db97f4b64/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72d4a3f9c159685baae2274392562c9db97f4b64/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=72d4a3f9c159685baae2274392562c9db97f4b64",
            "patch": "@@ -106,6 +106,7 @@\n         dtype_byte_size,\n         shard_checkpoint,\n     )\n+    from transformers.pytorch_utils import isin_mps_friendly\n \n     # Fake pretrained models for tests\n     class BaseModel(PreTrainedModel):\n@@ -1698,6 +1699,22 @@ def forward(self):\n         self.assertIn(\"beta_param\", missing_keys)\n         self.assertIn(\"bias_param\", unexpected_keys)\n \n+    def test_isin_mps_friendly(self):\n+        \"\"\"tests that our custom `isin_mps_friendly` matches `torch.isin`\"\"\"\n+        random_ids = torch.randint(0, 100, (100,))\n+        # We can match against an interger\n+        random_test_integer = torch.randint(0, 100, (1,)).item()\n+        self.assertTrue(\n+            torch.equal(\n+                torch.isin(random_ids, random_test_integer), isin_mps_friendly(random_ids, random_test_integer)\n+            )\n+        )\n+        # We can match against an tensor of integers\n+        random_test_tensor = torch.randint(0, 100, (10,))\n+        self.assertTrue(\n+            torch.equal(torch.isin(random_ids, random_test_tensor), isin_mps_friendly(random_ids, random_test_tensor))\n+        )\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 53,
        "deletions": 33
    }
}