{
    "author": "kwen2501",
    "message": "Simplify Tensor Parallel implementation with PyTorch TP (#34184)\n\n* Simplify Tensor Parallel implementation with PyTorch TP\r\n\r\n* Move tp_plan to config\r\n\r\n* Lint\r\n\r\n* Format and warning\r\n\r\n* Disable copy-from check\r\n\r\n* Conditionally get attr from config\r\n\r\n* make fix-copies\r\n\r\n* Move base_model_tp_plan to PretrainedConfig\r\n\r\n* Move TP into from_pretrained\r\n\r\n* Add device context for load\r\n\r\n* Do not serialize\r\n\r\n* Move _tp_plan setting to post_init\r\n\r\n* Add has_tp_plan\r\n\r\n* Add test_tp\r\n\r\n* Add 'Multi-gpu inference' doc\r\n\r\n* Add backward support for device type identification\r\n\r\n* Auto-detect accelerator\r\n\r\n* supports_tp_plan\r\n\r\n* copyright year\r\n\r\n* Fix copy",
    "sha": "20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
    "files": [
        {
            "sha": "ca7ee4557feec765adc9736a09e78133f1d936f3",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -218,6 +218,8 @@\n       title: CPU inference\n     - local: perf_infer_gpu_one\n       title: GPU inference\n+    - local: perf_infer_gpu_multi\n+      title: Multi-GPU inference\n     title: Optimizing inference\n   - local: big_models\n     title: Instantiate a big model"
        },
        {
            "sha": "9975094411527a99549c367e6ecc2ccfe814ae51",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "added",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -0,0 +1,68 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Multi-GPU inference\n+\n+Built-in Tensor Parallelism (TP) is now available with certain models using PyTorch. Tensor parallelism shards a model onto multiple GPUs, enabling larger model sizes, and parallelizes computations such as matrix multiplication.\n+\n+To enable tensor parallel, pass the argument `tp_plan=\"auto\"` to [`~AutoModelForCausalLM.from_pretrained`]:\n+\n+```python\n+import os\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n+\n+# Initialize distributed\n+rank = int(os.environ[\"RANK\"])\n+device = torch.device(f\"cuda:{rank}\")\n+torch.distributed.init_process_group(\"nccl\", device_id=device)\n+\n+# Retrieve tensor parallel model\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_id,\n+    tp_plan=\"auto\",\n+)\n+\n+# Prepare input tokens\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+prompt = \"Can I help\"\n+inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n+\n+# Distributed run\n+outputs = model(inputs)\n+```\n+\n+You can use `torchrun` to launch the above script with multiple processes, each mapping to a GPU:\n+\n+```\n+torchrun --nproc-per-node 4 demo.py\n+```\n+\n+PyTorch tensor parallel is currently supported for the following models:\n+* [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\n+\n+You can request to add tensor parallel support for another model by opening a GitHub Issue or Pull Request.\n+\n+### Expected speedups\n+\n+You can benefit from considerable speedups for inference, especially for inputs with large batch size or long sequences.\n+\n+For a single forward pass on [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel) with a sequence length of 512 and various batch sizes, the expected speedup is as follows:\n+\n+<div style=\"text-align: center\">\n+<img src=\"huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct, seqlen = 512, python, w_ compile.png\">\n+</div>"
        },
        {
            "sha": "b9176be04ec2063566e20827e91aba145a2b78f9",
            "filename": "docs/source/en/performance.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/docs%2Fsource%2Fen%2Fperformance.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/docs%2Fsource%2Fen%2Fperformance.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperformance.md?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -53,7 +53,7 @@ sections we go through the steps to run inference on CPU and single/multi-GPU se\n \n * [Inference on a single CPU](perf_infer_cpu)\n * [Inference on a single GPU](perf_infer_gpu_one)\n-* [Multi-GPU inference](perf_infer_gpu_one)\n+* [Multi-GPU inference](perf_infer_gpu_multi)\n * [XLA Integration for TensorFlow Models](tf_xla)\n \n "
        },
        {
            "sha": "e49eab86b4e12f4badb349a8256d62df389ca02e",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -71,6 +71,8 @@ class PretrainedConfig(PushToHubMixin):\n       outputs of the model during inference.\n     - **attribute_map** (`Dict[str, str]`) -- A dict that maps model specific attribute names to the standardized\n       naming of attributes.\n+    - **base_model_tp_plan** (`Dict[str, Any]`) -- A dict that maps sub-modules FQNs of a base model to a tensor\n+      parallel plan applied to the sub-module when `model.tensor_parallel` is called.\n \n     Common attributes (present in all subclasses):\n \n@@ -194,6 +196,7 @@ class PretrainedConfig(PushToHubMixin):\n     sub_configs: Dict[str, \"PretrainedConfig\"] = {}\n     is_composition: bool = False\n     attribute_map: Dict[str, str] = {}\n+    base_model_tp_plan: Optional[Dict[str, Any]] = None\n     _auto_class: Optional[str] = None\n \n     def __setattr__(self, key, value):\n@@ -848,6 +851,9 @@ def to_diff_dict(self) -> Dict[str, Any]:\n \n         if \"_attn_implementation_internal\" in serializable_config_dict:\n             del serializable_config_dict[\"_attn_implementation_internal\"]\n+        # Do not serialize `base_model_tp_plan` for now\n+        if \"base_model_tp_plan\" in serializable_config_dict:\n+            del serializable_config_dict[\"base_model_tp_plan\"]\n \n         return serializable_config_dict\n \n@@ -867,6 +873,9 @@ def to_dict(self) -> Dict[str, Any]:\n             del output[\"_commit_hash\"]\n         if \"_attn_implementation_internal\" in output:\n             del output[\"_attn_implementation_internal\"]\n+        # Do not serialize `base_model_tp_plan` for now\n+        if \"base_model_tp_plan\" in output:\n+            del output[\"base_model_tp_plan\"]\n \n         # Transformers version when serializing the model\n         output[\"transformers_version\"] = __version__"
        },
        {
            "sha": "57532c0c711b853340a17bafac1a1781d07c7d40",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 116,
            "deletions": 26,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -55,6 +55,7 @@\n     prune_conv1d_layer,\n     prune_layer,\n     prune_linear_layer,\n+    translate_to_torch_parallel_style,\n )\n from .quantizers import AutoHfQuantizer, HfQuantizer\n from .quantizers.quantizers_utils import get_module_from_name\n@@ -1326,6 +1327,12 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix\n     # Has support for a `QuantoQuantizedCache` instance as `past_key_values`\n     _supports_quantized_cache = False\n \n+    # A tensor parallel plan to be applied to the model when TP is enabled. For\n+    # top-level models, this attribute is currently defined in respective model\n+    # code. For base models, this attribute comes from\n+    # `config.base_model_tp_plan` during `post_init`.\n+    _tp_plan = None\n+\n     @property\n     def dummy_inputs(self) -> Dict[str, torch.Tensor]:\n         \"\"\"\n@@ -1370,6 +1377,9 @@ def post_init(self):\n         \"\"\"\n         self.init_weights()\n         self._backward_compatibility_gradient_checkpointing()\n+        # If current model is a base model, attach `base_model_tp_plan` from config\n+        if self.base_model is self:\n+            self._tp_plan = self.config.base_model_tp_plan\n \n     def dequantize(self):\n         \"\"\"\n@@ -3399,6 +3409,11 @@ def from_pretrained(\n         # Cache path to the GGUF file\n         gguf_path = None\n \n+        tp_plan = kwargs.pop(\"tp_plan\", None)\n+        if tp_plan is not None and tp_plan != \"auto\":\n+            # TODO: we can relax this check when we support taking tp_plan from a json file, for example.\n+            raise ValueError(f\"tp_plan supports 'auto' only for now but got {tp_plan}.\")\n+\n         if is_fsdp_enabled():\n             low_cpu_mem_usage = True\n \n@@ -4000,6 +4015,7 @@ def from_pretrained(\n \n         # Instantiate model.\n         init_contexts = [no_init_weights(_enable=_fast_init)]\n+        tp_device = None\n \n         if is_deepspeed_zero3_enabled() and not is_quantized:\n             import deepspeed\n@@ -4012,6 +4028,16 @@ def from_pretrained(\n                     f\"Using `low_cpu_mem_usage=True` or a `device_map` requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n                 )\n             init_contexts.append(init_empty_weights())\n+        elif tp_plan is not None:\n+            if not torch.distributed.is_initialized():\n+                raise ValueError(\"Tensor Parallel requires torch.distributed to be initialized first.\")\n+\n+            # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n+            device_type = torch._C._get_accelerator().type\n+            device_module = torch.get_device_module(device_type)\n+            # Get device with index assuming equal number of devices per host\n+            tp_device = torch.device(device_type, torch.distributed.get_rank() % device_module.device_count())\n+            init_contexts.append(tp_device)\n \n         if is_deepspeed_zero3_enabled() and is_quantized:\n             init_contexts.append(set_quantized_state())\n@@ -4145,32 +4171,38 @@ def from_pretrained(\n             if dtype_orig is not None:\n                 torch.set_default_dtype(dtype_orig)\n \n-            (\n-                model,\n-                missing_keys,\n-                unexpected_keys,\n-                mismatched_keys,\n-                offload_index,\n-                error_msgs,\n-            ) = cls._load_pretrained_model(\n-                model,\n-                state_dict,\n-                loaded_state_dict_keys,  # XXX: rename?\n-                resolved_archive_file,\n-                pretrained_model_name_or_path,\n-                ignore_mismatched_sizes=ignore_mismatched_sizes,\n-                sharded_metadata=sharded_metadata,\n-                _fast_init=_fast_init,\n-                low_cpu_mem_usage=low_cpu_mem_usage,\n-                device_map=device_map,\n-                offload_folder=offload_folder,\n-                offload_state_dict=offload_state_dict,\n-                dtype=torch_dtype,\n-                hf_quantizer=hf_quantizer,\n-                keep_in_fp32_modules=keep_in_fp32_modules,\n-                gguf_path=gguf_path,\n-                weights_only=weights_only,\n-            )\n+            load_contexts = []\n+            # Make sure we load onto targeted device\n+            if tp_device is not None:\n+                load_contexts.append(tp_device)\n+\n+            with ContextManagers(load_contexts):\n+                (\n+                    model,\n+                    missing_keys,\n+                    unexpected_keys,\n+                    mismatched_keys,\n+                    offload_index,\n+                    error_msgs,\n+                ) = cls._load_pretrained_model(\n+                    model,\n+                    state_dict,\n+                    loaded_state_dict_keys,  # XXX: rename?\n+                    resolved_archive_file,\n+                    pretrained_model_name_or_path,\n+                    ignore_mismatched_sizes=ignore_mismatched_sizes,\n+                    sharded_metadata=sharded_metadata,\n+                    _fast_init=_fast_init,\n+                    low_cpu_mem_usage=low_cpu_mem_usage,\n+                    device_map=device_map,\n+                    offload_folder=offload_folder,\n+                    offload_state_dict=offload_state_dict,\n+                    dtype=torch_dtype,\n+                    hf_quantizer=hf_quantizer,\n+                    keep_in_fp32_modules=keep_in_fp32_modules,\n+                    gguf_path=gguf_path,\n+                    weights_only=weights_only,\n+                )\n \n         # make sure token embedding weights are still tied if needed\n         model.tie_weights()\n@@ -4254,6 +4286,16 @@ def from_pretrained(\n                 }\n             return model, loading_info\n \n+        if tp_plan is not None:\n+            assert tp_device is not None, \"tp_device not set!\"\n+            if not model.supports_tp_plan:\n+                raise NotImplementedError(\"This model does not have a tensor parallel plan.\")\n+            # Assuming sharding the model onto the world\n+            world_size = torch.distributed.get_world_size()\n+            device_mesh = torch.distributed.init_device_mesh(tp_device.type, (world_size,))\n+            # Apply Tensor Parallelism\n+            model.tensor_parallel(device_mesh)\n+\n         return model\n \n     @classmethod\n@@ -4943,6 +4985,54 @@ def _is_quantized_training_enabled(self):\n \n         return self.hf_quantizer.is_trainable\n \n+    @property\n+    def supports_tp_plan(self):\n+        \"\"\"\n+        Returns whether the model has a tensor parallelism plan.\n+        \"\"\"\n+        if self._tp_plan is not None:\n+            return True\n+        # Check if base model has a TP plan\n+        if getattr(self.base_model, \"_tp_plan\", None) is not None:\n+            return True\n+        return False\n+\n+    def tensor_parallel(self, device_mesh):\n+        \"\"\"\n+        Tensor parallelize the model across the given device mesh.\n+\n+        Args:\n+            device_mesh (`torch.distributed.DeviceMesh`):\n+                The device mesh to use for tensor parallelism.\n+        \"\"\"\n+\n+        # Tensor parallelize a nn.Module based on the `_tp_plan` attribute of the module.\n+        # No op if `_tp_plan` attribute does not exist under the module.\n+        # This is a helper function to be used with `model.apply` to recursively\n+        # parallelize a model.\n+        def tplize(mod: torch.nn.Module) -> None:\n+            tp_plan = getattr(mod, \"_tp_plan\", None)\n+            if tp_plan is None:\n+                return\n+            logger.debug(f\"Applying tensor parallel to {mod.__class__.__name__}: {tp_plan}\")\n+            # In model configs, we use a neutral type (string) to specify\n+            # parallel styles, here we translate them into torch TP types.\n+            # Using tree_map because `tp_plan` is a dict.\n+            tp_plan = torch.utils._pytree.tree_map(\n+                translate_to_torch_parallel_style,\n+                tp_plan,\n+            )\n+            # Apply TP to current module.\n+            torch.distributed.tensor.parallel.parallelize_module(\n+                mod,\n+                device_mesh=device_mesh,\n+                parallelize_plan=tp_plan,\n+            )\n+\n+        # `apply` is a native method of `nn.Module` that recursively applies a\n+        # function to every submodule.\n+        self.apply(tplize)\n+\n     @property\n     def loss_function(self):\n         if getattr(self.config, \"loss_type\", None) is not None:"
        },
        {
            "sha": "0261f997da1110865f3a1fb2ef19f358abec0d30",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -1068,7 +1068,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with Llama->Cohere\n+# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with Llama->Cohere\n class CohereForCausalLM(CoherePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "6fead73eced704f4514ded0b673374856821534a",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -720,7 +720,10 @@ def __init__(self, config: GemmaConfig):\n             [GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n         self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -982,6 +985,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n class GemmaForCausalLM(GemmaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "6a3d8f27fb177db68babff7d923f96acc5fb2367",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -740,7 +740,10 @@ def __init__(self, config: Gemma2Config):\n             [Gemma2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n         self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -961,6 +964,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n class Gemma2ForCausalLM(Gemma2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "58a89d90b44ff5e6ef7563a8a4b880f6a17c2a25",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -708,6 +708,8 @@ def __init__(self, config: GlmConfig):\n             dim=config.head_dim // 2, max_position_embeddings=config.max_position_embeddings, base=config.rope_theta\n         )\n         self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -967,6 +969,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n class GlmForCausalLM(GlmPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config: GlmConfig):\n         super().__init__(config)"
        },
        {
            "sha": "98d5ecdd2a4fdb5d5caf3dd0781fd46ef19631ea",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -141,6 +141,16 @@ class LlamaConfig(PretrainedConfig):\n \n     model_type = \"llama\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `LlamaModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "679296648a91353415e92c982231638375dffad7",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 19,
            "deletions": 60,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -21,7 +21,6 @@\n from typing import List, Optional, Tuple, Union\n \n import torch\n-import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n \n@@ -240,25 +239,7 @@ def __init__(self, config):\n         self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(self, x):\n-        if self.config.pretraining_tp > 1:\n-            slice = self.intermediate_size // self.config.pretraining_tp\n-            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n-            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n-            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n-\n-            gate_proj = torch.cat(\n-                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n-            )\n-            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n-\n-            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n-            down_proj = [\n-                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n-            ]\n-            down_proj = sum(down_proj)\n-        else:\n-            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n         return down_proj\n \n \n@@ -320,31 +301,14 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n-        if self.config.pretraining_tp > 1:\n-            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n-            query_slices = self.q_proj.weight.split(\n-                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n-            )\n-            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n-            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n-\n-            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n-            query_states = torch.cat(query_states, dim=-1)\n-\n-            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n-            key_states = torch.cat(key_states, dim=-1)\n-\n-            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n-            value_states = torch.cat(value_states, dim=-1)\n-\n-        else:\n-            query_states = self.q_proj(hidden_states)\n-            key_states = self.k_proj(hidden_states)\n-            value_states = self.v_proj(hidden_states)\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -386,12 +350,7 @@ def forward(\n \n         attn_output = attn_output.reshape(bsz, q_len, -1)\n \n-        if self.config.pretraining_tp > 1:\n-            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n-            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n-            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n-        else:\n-            attn_output = self.o_proj(attn_output)\n+        attn_output = self.o_proj(attn_output)\n \n         if not output_attentions:\n             attn_weights = None\n@@ -564,9 +523,10 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -850,7 +810,10 @@ def __init__(self, config: LlamaConfig):\n         )\n         self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = LlamaRotaryEmbedding(config=config)\n+\n         self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1113,6 +1076,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n class LlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1211,13 +1175,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if self.config.pretraining_tp > 1:\n-            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n-            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n-            logits = torch.cat(logits, dim=-1)\n-        else:\n-            # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-            logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "8de6bc90ea3fec7c8bd46412ad24aa358b097eb3",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -980,7 +980,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n+# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n class NemotronForCausalLM(NemotronPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "d865c51e50578e50e7f5190c9b707779807fc9a5",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -1020,7 +1020,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->OLMO,Llama->Olmo\n+# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->OLMO,Llama->Olmo\n class OlmoForCausalLM(OlmoPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "5a9cca39b885701604db3b3921dead8766e0c327",
            "filename": "src/transformers/models/olmo_1124/modeling_olmo_1124.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Folmo_1124%2Fmodeling_olmo_1124.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Folmo_1124%2Fmodeling_olmo_1124.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo_1124%2Fmodeling_olmo_1124.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -971,6 +971,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->OLMO_1124,Llama->Olmo1124\n class Olmo1124ForCausalLM(Olmo1124PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "47cb0964eca8b60c88500173ac08191fcd41db2f",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -888,7 +888,7 @@ def _init_weights(self, module):\n     \"The bare Olmoe Model outputting raw hidden-states without any specific head on top.\",\n     OLMOE_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Olmoe\n+# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Olmoe\n class OlmoeModel(OlmoePreTrainedModel):\n     \"\"\"\n     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`OlmoeDecoderLayer`]"
        },
        {
            "sha": "2b3cf7eb0cb82e4458ae1820e8957d71e4f8e915",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -775,7 +775,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n         return causal_mask\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->RECURRENTGEMMA,Llama->RecurrentGemma,llama->gemma\n+# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->RECURRENTGEMMA,Llama->RecurrentGemma,llama->gemma\n class RecurrentGemmaForCausalLM(RecurrentGemmaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n "
        },
        {
            "sha": "a595f8bc9e1af6c3211ccc55df664bd97cf7fa0a",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -20,6 +20,11 @@\n from packaging import version\n from safetensors.torch import storage_ptr, storage_size\n from torch import nn\n+from torch.distributed.tensor import Replicate\n+from torch.distributed.tensor.parallel import (\n+    ColwiseParallel,\n+    RowwiseParallel,\n+)\n \n from .utils import is_torch_xla_available, logging\n \n@@ -329,3 +334,22 @@ def isin_mps_friendly(elements: torch.Tensor, test_elements: torch.Tensor | int)\n     else:\n         # Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\n         return torch.isin(elements, test_elements)\n+\n+\n+def translate_to_torch_parallel_style(style: str):\n+    \"\"\"\n+    In model configurations, we use a neutral type (string) to specify parallel\n+    styles, here we translate them into torch.distributed tensor-parallel\n+    types.\n+    \"\"\"\n+    if not isinstance(style, str):\n+        raise ValueError(f\"Unsupported parallel style type {type(style)}, expected str\")\n+\n+    if style == \"colwise\":\n+        return ColwiseParallel()\n+    elif style == \"rowwise\":\n+        return RowwiseParallel()\n+    elif style == \"colwise_rep\":\n+        return ColwiseParallel(output_layouts=Replicate())\n+    else:\n+        raise ValueError(f\"Unsupported parallel style value: {style}\")"
        },
        {
            "sha": "2139a648867b6198974e9c4d5abfc25ba476d15c",
            "filename": "tests/tp/test_tp.py",
            "status": "added",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/tests%2Ftp%2Ftest_tp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20142ab5422fbafcd10e221e37e95f8aaf1bde3c/tests%2Ftp%2Ftest_tp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftp%2Ftest_tp.py?ref=20142ab5422fbafcd10e221e37e95f8aaf1bde3c",
            "patch": "@@ -0,0 +1,91 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import os\n+\n+from transformers import is_torch_available\n+from transformers.models.llama.configuration_llama import LlamaConfig\n+from transformers.models.llama.modeling_llama import LlamaModel\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    execute_subprocess_async,\n+    get_torch_dist_unique_port,\n+    require_torch_multi_gpu,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class TestTensorParallel(TestCasePlus):\n+    @require_torch_multi_gpu\n+    def test_tp(self):\n+        distributed_args = f\"\"\"--nproc_per_node={torch.cuda.device_count()}\n+            --master_port={get_torch_dist_unique_port()}\n+            {self.test_file_dir}/test_tp.py\n+        \"\"\".split()\n+        output_dir = self.get_auto_remove_tmp_dir()\n+        args = f\"--output_dir {output_dir} --report_to none\".split()\n+        cmd = [\"torchrun\"] + distributed_args + args\n+        print(cmd)\n+        execute_subprocess_async(cmd, env=self.get_env())\n+        # successful return here == success - any errors would have caused an error in the sub-call\n+\n+\n+if __name__ == \"__main__\":\n+    # The script below is meant to be run under torch.distributed, on a machine with multiple GPUs:\n+    # CUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/tp/test_tp.py\n+    # or\n+    # PYTHONPATH=\"src\" python -m torch.distributed.run --nproc_per_node 2 ./tests/tp/test_tp.py\n+\n+    if not is_torch_available():\n+        exit(0)\n+\n+    # Test settings\n+    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n+    bs = 4\n+    seqlen = 64\n+\n+    # Get distributed settings\n+    rank = int(os.environ[\"RANK\"])\n+    world_size = int(os.environ[\"WORLD_SIZE\"])\n+\n+    # Initialize distributed\n+    device = torch.device(f\"cuda:{rank}\")\n+    torch.distributed.init_process_group(\"nccl\", device_id=device)\n+    device_mesh = torch.distributed.init_device_mesh(\"cuda\", (world_size,))\n+\n+    # Get model config\n+    config = LlamaConfig.from_pretrained(model_id)\n+    # Shrink model size\n+    config.num_hidden_layers //= 8\n+    config.vocab_size //= 8\n+\n+    # Instantiate model\n+    with device:\n+        model = LlamaModel(config)\n+\n+    model.eval()\n+\n+    # Tensor Parallel\n+    if world_size > 1:\n+        model.tensor_parallel(device_mesh)\n+\n+    # Run model\n+    inputs = torch.randint(config.vocab_size, (bs, seqlen), device=device)\n+    with torch.no_grad():\n+        out = model(inputs)\n+\n+    assert out.last_hidden_state.shape == torch.Size([bs, seqlen, config.hidden_size])"
        }
    ],
    "stats": {
        "total": 449,
        "additions": 357,
        "deletions": 92
    }
}