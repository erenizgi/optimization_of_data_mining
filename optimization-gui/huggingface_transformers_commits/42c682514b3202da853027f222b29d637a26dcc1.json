{
    "author": "tayo4christ",
    "message": "docs/examples(speech): pin CTC commands to Hub datasets; add Windows notes (#41027)\n\n* examples(speech): load Common Voice from Hub; remove deprecated dataset-script references (Windows-friendly notes)\n\n* docs/examples(speech): pin CTC streaming & other CTC commands to Hub datasets; add Windows notes\n\n* make style\n\n* examples(speech): align DataTrainingArguments help with datasets docs; minor wording fixes\n\n* docs/examples(speech): address review  remove Hub subsection & Whisper tip; align dataset help text\n\n* style: apply ruff/black/usort/codespell on examples/speech-recognition\n\n* Apply style fixes\n\n* Update examples/pytorch/speech-recognition/README.md\n\n* update doc to match load_dataset\n\n---------\n\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "42c682514b3202da853027f222b29d637a26dcc1",
    "files": [
        {
            "sha": "41df41880b5a650017a086c4688adf7792b6137e",
            "filename": "examples/pytorch/speech-recognition/README.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/42c682514b3202da853027f222b29d637a26dcc1/examples%2Fpytorch%2Fspeech-recognition%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/42c682514b3202da853027f222b29d637a26dcc1/examples%2Fpytorch%2Fspeech-recognition%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2FREADME.md?ref=42c682514b3202da853027f222b29d637a26dcc1",
            "patch": "@@ -66,7 +66,7 @@ The following command shows how to fine-tune [XLSR-Wav2Vec2](https://huggingface\n \n ```bash\n python run_speech_recognition_ctc.py \\\n-\t--dataset_name=\"common_voice\" \\\n+\t--dataset_name=\"mozilla-foundation/common_voice_17_0\" \\\n \t--model_name_or_path=\"facebook/wav2vec2-large-xlsr-53\" \\\n \t--dataset_config_name=\"tr\" \\\n \t--output_dir=\"./wav2vec2-common_voice-tr-demo\" \\\n@@ -102,7 +102,7 @@ The following command shows how to fine-tune [XLSR-Wav2Vec2](https://huggingface\n ```bash\n torchrun \\\n \t--nproc_per_node 8 run_speech_recognition_ctc.py \\\n-\t--dataset_name=\"common_voice\" \\\n+\t--dataset_name=\"mozilla-foundation/common_voice_17_0\" \\\n \t--model_name_or_path=\"facebook/wav2vec2-large-xlsr-53\" \\\n \t--dataset_config_name=\"tr\" \\\n \t--output_dir=\"./wav2vec2-common_voice-tr-demo-dist\" \\\n@@ -149,7 +149,7 @@ However, the `--shuffle_buffer_size` argument controls how many examples we can\n ```bash\n **torchrun \\\n \t--nproc_per_node 4 run_speech_recognition_ctc_streaming.py \\\n-\t--dataset_name=\"common_voice\" \\\n+\t--dataset_name=\"mozilla-foundation/common_voice_17_0\" \\\n \t--model_name_or_path=\"facebook/wav2vec2-xls-r-300m\" \\\n \t--tokenizer_name_or_path=\"anton-l/wav2vec2-tokenizer-turkish\" \\\n \t--dataset_config_name=\"tr\" \\\n@@ -314,7 +314,7 @@ below 27%.\n For an example run, you can have a look at [`patrickvonplaten/wav2vec2-common_voice-tr-mms-demo`](https://huggingface.co/patrickvonplaten/wav2vec2-common_voice-tr-mms-demo).\n \n \n-If you'd like to train another adapter model with the same base model, you can simply re-use the same `--output_dir`,\n+If you'd like to train another adapter model with the same base model, you can simply reuse the same `--output_dir`,\n but make sure to pass the `--output_dir` folder also to `--tokenizer_name_or_path` so that the vocabulary is not \n overwritten but **extended**. Assuming you would like to train adapter weights on Swedish in addition to Turkish and save \n the adapter weights in the same model repo, you can run:"
        },
        {
            "sha": "c756a6666187699652f14e683bd956cb5a33914f",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc.py",
            "status": "modified",
            "additions": 26,
            "deletions": 12,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/42c682514b3202da853027f222b29d637a26dcc1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42c682514b3202da853027f222b29d637a26dcc1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py?ref=42c682514b3202da853027f222b29d637a26dcc1",
            "patch": "@@ -63,7 +63,10 @@\n # Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n check_min_version(\"4.57.0.dev0\")\n \n-require_version(\"datasets>=1.18.0\", \"To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt\")\n+require_version(\n+    \"datasets>=1.18.0\",\n+    \"To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt\",\n+)\n \n \n logger = logging.getLogger(__name__)\n@@ -91,13 +94,16 @@ class ModelArguments:\n         metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n     )\n     freeze_feature_encoder: bool = field(\n-        default=True, metadata={\"help\": \"Whether to freeze the feature encoder layers of the model.\"}\n+        default=True,\n+        metadata={\"help\": \"Whether to freeze the feature encoder layers of the model.\"},\n     )\n     attention_dropout: float = field(\n-        default=0.0, metadata={\"help\": \"The dropout ratio for the attention probabilities.\"}\n+        default=0.0,\n+        metadata={\"help\": \"The dropout ratio for the attention probabilities.\"},\n     )\n     activation_dropout: float = field(\n-        default=0.0, metadata={\"help\": \"The dropout ratio for activations inside the fully connected layer.\"}\n+        default=0.0,\n+        metadata={\"help\": \"The dropout ratio for activations inside the fully connected layer.\"},\n     )\n     feat_proj_dropout: float = field(default=0.0, metadata={\"help\": \"The dropout ratio for the projected features.\"})\n     hidden_dropout: float = field(\n@@ -140,7 +146,8 @@ class ModelArguments:\n     )\n     layerdrop: float = field(default=0.0, metadata={\"help\": \"The LayerDrop probability.\"})\n     ctc_loss_reduction: Optional[str] = field(\n-        default=\"mean\", metadata={\"help\": \"The way the ctc loss should be reduced. Should be one of 'mean' or 'sum'.\"}\n+        default=\"mean\",\n+        metadata={\"help\": \"The way the ctc loss should be reduced. Should be one of 'mean' or 'sum'.\"},\n     )\n     ctc_zero_infinity: Optional[bool] = field(\n         default=False,\n@@ -169,10 +176,13 @@ class DataTrainingArguments:\n     \"\"\"\n \n     dataset_name: str = field(\n-        metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n+        metadata={\"help\": \"Path or name of the dataset (cf `load_dataset` method of the Datasets library).\"}\n     )\n     dataset_config_name: str = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n+        default=None,\n+        metadata={\n+            \"help\": \"The configuration name of the dataset to use (cf `load_dataset` method of the Datasets library).\"\n+        },\n     )\n     train_split_name: str = field(\n         default=\"train+validation\",\n@@ -198,7 +208,8 @@ class DataTrainingArguments:\n         metadata={\"help\": \"The name of the dataset column containing the text data. Defaults to 'text'\"},\n     )\n     overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n+        default=False,\n+        metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"},\n     )\n     preprocessing_num_workers: Optional[int] = field(\n         default=None,\n@@ -240,7 +251,8 @@ class DataTrainingArguments:\n         },\n     )\n     min_duration_in_seconds: float = field(\n-        default=0.0, metadata={\"help\": \"Filter audio files that are shorter than `min_duration_in_seconds` seconds\"}\n+        default=0.0,\n+        metadata={\"help\": \"Filter audio files that are shorter than `min_duration_in_seconds` seconds\"},\n     )\n     preprocessing_only: bool = field(\n         default=False,\n@@ -383,7 +395,8 @@ def extract_all_chars(batch):\n \n     # take union of all unique characters in each dataset\n     vocab_set = functools.reduce(\n-        lambda vocab_1, vocab_2: set(vocab_1[\"vocab\"][0]) | set(vocab_2[\"vocab\"][0]), vocabs.values()\n+        lambda vocab_1, vocab_2: set(vocab_1[\"vocab\"][0]) | set(vocab_2[\"vocab\"][0]),\n+        vocabs.values(),\n     )\n \n     vocab_dict = {v: k for k, v in enumerate(sorted(vocab_set))}\n@@ -571,7 +584,7 @@ def remove_special_characters(batch):\n         # it is defined by `tokenizer_class` if present in config else by `model_type`\n         tokenizer_kwargs = {\n             \"config\": config if config.tokenizer_class is not None else None,\n-            \"tokenizer_type\": config.model_type if config.tokenizer_class is None else None,\n+            \"tokenizer_type\": (config.model_type if config.tokenizer_class is None else None),\n             \"unk_token\": unk_token,\n             \"pad_token\": pad_token,\n             \"word_delimiter_token\": word_delimiter_token,\n@@ -639,7 +652,8 @@ def remove_special_characters(batch):\n     dataset_sampling_rate = next(iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate\n     if dataset_sampling_rate != feature_extractor.sampling_rate:\n         raw_datasets = raw_datasets.cast_column(\n-            data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n+            data_args.audio_column_name,\n+            datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate),\n         )\n \n     # derive max & min input length for sample rate & max duration"
        },
        {
            "sha": "aaebf59c866051da9348b0740f6cdc8d6043f740",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py",
            "status": "modified",
            "additions": 21,
            "deletions": 10,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/42c682514b3202da853027f222b29d637a26dcc1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42c682514b3202da853027f222b29d637a26dcc1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py?ref=42c682514b3202da853027f222b29d637a26dcc1",
            "patch": "@@ -66,7 +66,10 @@\n # Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n check_min_version(\"4.57.0.dev0\")\n \n-require_version(\"datasets>=1.18.0\", \"To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt\")\n+require_version(\n+    \"datasets>=1.18.0\",\n+    \"To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt\",\n+)\n \n \n logger = logging.getLogger(__name__)\n@@ -127,7 +130,8 @@ class ModelArguments:\n     )\n     layerdrop: float = field(default=0.0, metadata={\"help\": \"The LayerDrop probability.\"})\n     ctc_loss_reduction: Optional[str] = field(\n-        default=\"mean\", metadata={\"help\": \"The way the ctc loss should be reduced. Should be one of 'mean' or 'sum'.\"}\n+        default=\"mean\",\n+        metadata={\"help\": \"The way the ctc loss should be reduced. Should be one of 'mean' or 'sum'.\"},\n     )\n     adapter_attn_dim: int = field(\n         default=16,\n@@ -148,9 +152,9 @@ class DataTrainingArguments:\n     \"\"\"\n \n     dataset_name: str = field(\n-        metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n+        metadata={\"help\": \"Path or name of the dataset (cf `load_dataset` method of the Datasets library).\"}\n     )\n-    target_language: Optional[str] = field(\n+    target_language: str = field(\n         metadata={\n             \"help\": (\n                 \"The target language on which the adapter attention layers\"\n@@ -162,7 +166,10 @@ class DataTrainingArguments:\n         },\n     )\n     dataset_config_name: str = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n+        default=None,\n+        metadata={\n+            \"help\": \"The configuration name of the dataset to use (cf `load_dataset` method of the Datasets library).\"\n+        },\n     )\n     train_split_name: str = field(\n         default=\"train+validation\",\n@@ -188,7 +195,8 @@ class DataTrainingArguments:\n         metadata={\"help\": \"The name of the dataset column containing the text data. Defaults to 'text'\"},\n     )\n     overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n+        default=False,\n+        metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"},\n     )\n     preprocessing_num_workers: Optional[int] = field(\n         default=None,\n@@ -230,7 +238,8 @@ class DataTrainingArguments:\n         },\n     )\n     min_duration_in_seconds: float = field(\n-        default=0.0, metadata={\"help\": \"Filter audio files that are shorter than `min_duration_in_seconds` seconds\"}\n+        default=0.0,\n+        metadata={\"help\": \"Filter audio files that are shorter than `min_duration_in_seconds` seconds\"},\n     )\n     preprocessing_only: bool = field(\n         default=False,\n@@ -363,7 +372,8 @@ def extract_all_chars(batch):\n \n     # take union of all unique characters in each dataset\n     vocab_set = functools.reduce(\n-        lambda vocab_1, vocab_2: set(vocab_1[\"vocab\"][0]) | set(vocab_2[\"vocab\"][0]), vocabs.values()\n+        lambda vocab_1, vocab_2: set(vocab_1[\"vocab\"][0]) | set(vocab_2[\"vocab\"][0]),\n+        vocabs.values(),\n     )\n \n     vocab_dict = {v: k for k, v in enumerate(sorted(vocab_set))}\n@@ -578,7 +588,7 @@ def remove_special_characters(batch):\n         # it is defined by `tokenizer_class` if present in config else by `model_type`\n         tokenizer_kwargs = {\n             \"config\": config if config.tokenizer_class is not None else None,\n-            \"tokenizer_type\": config.model_type if config.tokenizer_class is None else None,\n+            \"tokenizer_type\": (config.model_type if config.tokenizer_class is None else None),\n             \"unk_token\": unk_token,\n             \"pad_token\": pad_token,\n             \"word_delimiter_token\": word_delimiter_token,\n@@ -650,7 +660,8 @@ def remove_special_characters(batch):\n     dataset_sampling_rate = next(iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate\n     if dataset_sampling_rate != feature_extractor.sampling_rate:\n         raw_datasets = raw_datasets.cast_column(\n-            data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n+            data_args.audio_column_name,\n+            datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate),\n         )\n \n     # derive max & min input length for sample rate & max duration"
        },
        {
            "sha": "4b6cda49925b1466382374c1f66892928b98120c",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 37,
            "deletions": 18,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/42c682514b3202da853027f222b29d637a26dcc1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42c682514b3202da853027f222b29d637a26dcc1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py?ref=42c682514b3202da853027f222b29d637a26dcc1",
            "patch": "@@ -62,7 +62,10 @@\n # Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n check_min_version(\"4.57.0.dev0\")\n \n-require_version(\"datasets>=1.18.0\", \"To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt\")\n+require_version(\n+    \"datasets>=1.18.0\",\n+    \"To fix: pip install -r examples/pytorch/speech-recognition/requirements.txt\",\n+)\n \n logger = logging.getLogger(__name__)\n \n@@ -77,13 +80,16 @@ class ModelArguments:\n         metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n     )\n     config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n+        default=None,\n+        metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"},\n     )\n     tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n+        default=None,\n+        metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"},\n     )\n     feature_extractor_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"feature extractor name or path if not the same as model_name\"}\n+        default=None,\n+        metadata={\"help\": \"feature extractor name or path if not the same as model_name\"},\n     )\n     cache_dir: Optional[str] = field(\n         default=None,\n@@ -117,10 +123,12 @@ class ModelArguments:\n         },\n     )\n     freeze_feature_encoder: bool = field(\n-        default=True, metadata={\"help\": \"Whether to freeze the feature encoder layers of the model.\"}\n+        default=True,\n+        metadata={\"help\": \"Whether to freeze the feature encoder layers of the model.\"},\n     )\n     freeze_encoder: bool = field(\n-        default=False, metadata={\"help\": \"Whether to freeze the entire encoder of the seq2seq model.\"}\n+        default=False,\n+        metadata={\"help\": \"Whether to freeze the entire encoder of the seq2seq model.\"},\n     )\n     forced_decoder_ids: list[list[int]] = field(\n         default=None,\n@@ -150,13 +158,17 @@ class DataTrainingArguments:\n     \"\"\"\n \n     dataset_name: str = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n+        metadata={\"help\": \"Path or name of the dataset (cf `load_dataset` method of the Datasets library).\"}\n     )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n+    dataset_config_name: str = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"The configuration name of the dataset to use (cf `load_dataset` method of the Datasets library).\"\n+        },\n     )\n     overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n+        default=False,\n+        metadata={\"help\": \"Overwrite the cached training and evaluation sets\"},\n     )\n     preprocessing_num_workers: Optional[int] = field(\n         default=None,\n@@ -198,7 +210,8 @@ class DataTrainingArguments:\n         },\n     )\n     min_duration_in_seconds: float = field(\n-        default=0.0, metadata={\"help\": \"Filter audio files that are shorter than `min_duration_in_seconds` seconds\"}\n+        default=0.0,\n+        metadata={\"help\": \"Filter audio files that are shorter than `min_duration_in_seconds` seconds\"},\n     )\n     preprocessing_only: bool = field(\n         default=False,\n@@ -387,7 +400,7 @@ def main():\n     # Distributed training:\n     # The .from_pretrained methods guarantee that only one local process can concurrently\n     config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n+        (model_args.config_name if model_args.config_name else model_args.model_name_or_path),\n         cache_dir=model_args.cache_dir,\n         revision=model_args.model_revision,\n         token=model_args.token,\n@@ -399,14 +412,14 @@ def main():\n         config.update({\"apply_spec_augment\": model_args.apply_spec_augment})\n \n     feature_extractor = AutoFeatureExtractor.from_pretrained(\n-        model_args.feature_extractor_name if model_args.feature_extractor_name else model_args.model_name_or_path,\n+        (model_args.feature_extractor_name if model_args.feature_extractor_name else model_args.model_name_or_path),\n         cache_dir=model_args.cache_dir,\n         revision=model_args.model_revision,\n         token=model_args.token,\n         trust_remote_code=model_args.trust_remote_code,\n     )\n     tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n+        (model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path),\n         cache_dir=model_args.cache_dir,\n         use_fast=model_args.use_fast_tokenizer,\n         revision=model_args.model_revision,\n@@ -465,7 +478,8 @@ def main():\n     dataset_sampling_rate = next(iter(raw_datasets.values())).features[data_args.audio_column_name].sampling_rate\n     if dataset_sampling_rate != feature_extractor.sampling_rate:\n         raw_datasets = raw_datasets.cast_column(\n-            data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n+            data_args.audio_column_name,\n+            datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate),\n         )\n \n     # 7. Preprocessing the datasets.\n@@ -494,7 +508,9 @@ def prepare_dataset(batch):\n         # process audio\n         sample = batch[audio_column_name]\n         inputs = feature_extractor(\n-            sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], return_attention_mask=forward_attention_mask\n+            sample[\"array\"],\n+            sampling_rate=sample[\"sampling_rate\"],\n+            return_attention_mask=forward_attention_mask,\n         )\n         # process audio length\n         batch[model_input_name] = inputs.get(model_input_name)[0]\n@@ -579,7 +595,7 @@ def compute_metrics(pred):\n         eval_dataset=vectorized_datasets[\"eval\"] if training_args.do_eval else None,\n         processing_class=feature_extractor,\n         data_collator=data_collator,\n-        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n+        compute_metrics=(compute_metrics if training_args.predict_with_generate else None),\n     )\n \n     # 12. Training\n@@ -621,7 +637,10 @@ def compute_metrics(pred):\n         trainer.save_metrics(\"eval\", metrics)\n \n     # 14. Write Training Stats\n-    kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"automatic-speech-recognition\"}\n+    kwargs = {\n+        \"finetuned_from\": model_args.model_name_or_path,\n+        \"tasks\": \"automatic-speech-recognition\",\n+    }\n     if data_args.dataset_name is not None:\n         kwargs[\"dataset_tags\"] = data_args.dataset_name\n         if data_args.dataset_config_name is not None:"
        }
    ],
    "stats": {
        "total": 132,
        "additions": 88,
        "deletions": 44
    }
}