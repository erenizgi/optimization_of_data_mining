{
    "author": "yonigozlan",
    "message": "Add DetrImageProcessorFast (#34063)\n\n* add fully functionning image_processing_detr_fast\r\n\r\n* Create tensors on the correct device\r\n\r\n* fix copies\r\n\r\n* fix doc\r\n\r\n* add tests equivalence cpu gpu\r\n\r\n* fix doc en\r\n\r\n* add relative imports and copied from\r\n\r\n* Fix copies and nit",
    "sha": "a4122813d1309385f303f60b59a1f9944fada761",
    "files": [
        {
            "sha": "43c6e6d17e2f70c784d861f6a42674e751951085",
            "filename": "docs/source/en/model_doc/detr.md",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -181,6 +181,15 @@ If you're interested in submitting a resource to be included here, please feel f\n     - post_process_instance_segmentation\n     - post_process_panoptic_segmentation\n \n+## DetrImageProcessorFast\n+\n+[[autodoc]] DetrImageProcessorFast\n+    - preprocess\n+    - post_process_object_detection\n+    - post_process_semantic_segmentation\n+    - post_process_instance_segmentation\n+    - post_process_panoptic_segmentation\n+\n ## DetrFeatureExtractor\n \n [[autodoc]] DetrFeatureExtractor"
        },
        {
            "sha": "3342b123a01b833795847c8b46c723ddb53f45f4",
            "filename": "docs/source/ja/model_doc/detr.md",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/docs%2Fsource%2Fja%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/docs%2Fsource%2Fja%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdetr.md?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -184,6 +184,15 @@ DETR の使用を開始するのに役立つ公式 Hugging Face およびコミ\n     - post_process_instance_segmentation\n     - post_process_panoptic_segmentation\n \n+## DetrImageProcessorFast\n+\n+[[autodoc]] DetrImageProcessorFast\n+    - preprocess\n+    - post_process_object_detection\n+    - post_process_semantic_segmentation\n+    - post_process_instance_segmentation\n+    - post_process_panoptic_segmentation\n+\n ## DetrFeatureExtractor\n \n [[autodoc]] DetrFeatureExtractor"
        },
        {
            "sha": "7f408859c539b0a40697fc83693d816000c496c3",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -1191,7 +1191,7 @@\n     _import_structure[\"models.deprecated.efficientformer\"].append(\"EfficientFormerImageProcessor\")\n     _import_structure[\"models.deprecated.tvlt\"].append(\"TvltImageProcessor\")\n     _import_structure[\"models.deprecated.vit_hybrid\"].extend([\"ViTHybridImageProcessor\"])\n-    _import_structure[\"models.detr\"].extend([\"DetrFeatureExtractor\", \"DetrImageProcessor\"])\n+    _import_structure[\"models.detr\"].extend([\"DetrFeatureExtractor\", \"DetrImageProcessor\", \"DetrImageProcessorFast\"])\n     _import_structure[\"models.donut\"].extend([\"DonutFeatureExtractor\", \"DonutImageProcessor\"])\n     _import_structure[\"models.dpt\"].extend([\"DPTFeatureExtractor\", \"DPTImageProcessor\"])\n     _import_structure[\"models.efficientnet\"].append(\"EfficientNetImageProcessor\")\n@@ -6090,7 +6090,7 @@\n         from .models.deprecated.efficientformer import EfficientFormerImageProcessor\n         from .models.deprecated.tvlt import TvltImageProcessor\n         from .models.deprecated.vit_hybrid import ViTHybridImageProcessor\n-        from .models.detr import DetrFeatureExtractor, DetrImageProcessor\n+        from .models.detr import DetrFeatureExtractor, DetrImageProcessor, DetrImageProcessorFast\n         from .models.donut import DonutFeatureExtractor, DonutImageProcessor\n         from .models.dpt import DPTFeatureExtractor, DPTImageProcessor\n         from .models.efficientnet import EfficientNetImageProcessor"
        },
        {
            "sha": "e7d3a5abb7a8db634ef1f1f19ea57219f14457b4",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -32,6 +32,7 @@\n     is_tf_available,\n     is_torch_available,\n     is_torchvision_available,\n+    is_torchvision_v2_available,\n     is_vision_available,\n     requires_backends,\n )\n@@ -51,7 +52,9 @@\n if is_flax_available():\n     import jax.numpy as jnp\n \n-if is_torchvision_available():\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n     from torchvision.transforms import functional as F\n \n "
        },
        {
            "sha": "d181afeb2d4d0d3bfed1332f771fb902f32a481d",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -72,7 +72,7 @@\n             (\"deit\", (\"DeiTImageProcessor\",)),\n             (\"depth_anything\", (\"DPTImageProcessor\",)),\n             (\"deta\", (\"DetaImageProcessor\",)),\n-            (\"detr\", (\"DetrImageProcessor\",)),\n+            (\"detr\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"dinat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"dinov2\", (\"BitImageProcessor\",)),\n             (\"donut-swin\", (\"DonutImageProcessor\",)),"
        },
        {
            "sha": "cc6687ff8bb4b49a50a098a27bd7a27a8a89eae9",
            "filename": "src/transformers/models/detr/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Fmodels%2Fdetr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Fmodels%2Fdetr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2F__init__.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -27,6 +27,7 @@\n else:\n     _import_structure[\"feature_extraction_detr\"] = [\"DetrFeatureExtractor\"]\n     _import_structure[\"image_processing_detr\"] = [\"DetrImageProcessor\"]\n+    _import_structure[\"image_processing_detr_fast\"] = [\"DetrImageProcessorFast\"]\n \n try:\n     if not is_torch_available():\n@@ -53,6 +54,7 @@\n     else:\n         from .feature_extraction_detr import DetrFeatureExtractor\n         from .image_processing_detr import DetrImageProcessor\n+        from .image_processing_detr_fast import DetrImageProcessorFast\n \n     try:\n         if not is_torch_available():"
        },
        {
            "sha": "97940ab3132dda4cf61af4ab48e708d1ae3ae86e",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "added",
            "additions": 1546,
            "deletions": 0,
            "changes": 1546,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -0,0 +1,1546 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for DETR.\"\"\"\n+\n+import functools\n+import io\n+import pathlib\n+from collections import defaultdict\n+from typing import Any, Dict, List, Optional, Set, Tuple, Union\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import BaseImageProcessorFast, SizeDict\n+from ...image_transforms import (\n+    center_to_corners_format,\n+    corners_to_center_format,\n+    id_to_rgb,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    AnnotationFormat,\n+    AnnotationType,\n+    ChannelDimension,\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    get_image_size,\n+    get_image_type,\n+    infer_channel_dimension_format,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    validate_annotations,\n+    validate_kwargs,\n+)\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    is_vision_available,\n+    logging,\n+)\n+from .image_processing_detr import (\n+    compute_segments,\n+    convert_segmentation_to_rle,\n+    get_size_with_aspect_ratio,\n+    max_across_indices,\n+    remove_low_and_no_objects,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+if is_torchvision_available():\n+    from torchvision.io import read_image\n+\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n+\n+\n+def get_image_size_for_max_height_width(\n+    image_size: Tuple[int, int],\n+    max_height: int,\n+    max_width: int,\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Computes the output image size given the input image and the maximum allowed height and width. Keep aspect ratio.\n+    Important, even if image_height < max_height and image_width < max_width, the image will be resized\n+    to at least one of the edges be equal to max_height or max_width.\n+\n+    For example:\n+        - input_size: (100, 200), max_height: 50, max_width: 50 -> output_size: (25, 50)\n+        - input_size: (100, 200), max_height: 200, max_width: 500 -> output_size: (200, 400)\n+\n+    Args:\n+        image_size (`Tuple[int, int]`):\n+            The image to resize.\n+        max_height (`int`):\n+            The maximum allowed height.\n+        max_width (`int`):\n+            The maximum allowed width.\n+    \"\"\"\n+    height, width = image_size\n+    height_scale = max_height / height\n+    width_scale = max_width / width\n+    min_scale = min(height_scale, width_scale)\n+    new_height = int(height * min_scale)\n+    new_width = int(width * min_scale)\n+    return new_height, new_width\n+\n+\n+def safe_squeeze(tensor: torch.Tensor, axis: Optional[int] = None) -> torch.Tensor:\n+    \"\"\"\n+    Squeezes a tensor, but only if the axis specified has dim 1.\n+    \"\"\"\n+    if axis is None:\n+        return tensor.squeeze()\n+\n+    try:\n+        return tensor.squeeze(axis=axis)\n+    except ValueError:\n+        return tensor\n+\n+\n+def get_max_height_width(images: List[torch.Tensor]) -> Tuple[int]:\n+    \"\"\"\n+    Get the maximum height and width across all images in a batch.\n+    \"\"\"\n+\n+    _, max_height, max_width = max_across_indices([img.shape for img in images])\n+\n+    return (max_height, max_width)\n+\n+\n+# inspired by https://github.com/facebookresearch/detr/blob/master/datasets/coco.py#L33\n+def convert_coco_poly_to_mask(segmentations, height: int, width: int, device: torch.device) -> torch.Tensor:\n+    \"\"\"\n+    Convert a COCO polygon annotation to a mask.\n+\n+    Args:\n+        segmentations (`List[List[float]]`):\n+            List of polygons, each polygon represented by a list of x-y coordinates.\n+        height (`int`):\n+            Height of the mask.\n+        width (`int`):\n+            Width of the mask.\n+    \"\"\"\n+    try:\n+        from pycocotools import mask as coco_mask\n+    except ImportError:\n+        raise ImportError(\"Pycocotools is not installed in your environment.\")\n+\n+    masks = []\n+    for polygons in segmentations:\n+        rles = coco_mask.frPyObjects(polygons, height, width)\n+        mask = coco_mask.decode(rles)\n+        if len(mask.shape) < 3:\n+            mask = mask[..., None]\n+        mask = torch.as_tensor(mask, dtype=torch.uint8, device=device)\n+        mask = torch.any(mask, axis=2)\n+        masks.append(mask)\n+    if masks:\n+        masks = torch.stack(masks, axis=0)\n+    else:\n+        masks = torch.zeros((0, height, width), dtype=torch.uint8, device=device)\n+\n+    return masks\n+\n+\n+# inspired by https://github.com/facebookresearch/detr/blob/master/datasets/coco.py#L50\n+def prepare_coco_detection_annotation(\n+    image,\n+    target,\n+    return_segmentation_masks: bool = False,\n+    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+):\n+    \"\"\"\n+    Convert the target in COCO format into the format expected by DETR.\n+    \"\"\"\n+    image_height, image_width = image.size()[-2:]\n+\n+    image_id = target[\"image_id\"]\n+    image_id = torch.as_tensor([image_id], dtype=torch.int64, device=image.device)\n+\n+    # Get all COCO annotations for the given image.\n+    annotations = target[\"annotations\"]\n+    annotations = [obj for obj in annotations if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0]\n+\n+    classes = [obj[\"category_id\"] for obj in annotations]\n+    classes = torch.as_tensor(classes, dtype=torch.int64, device=image.device)\n+\n+    # for conversion to coco api\n+    area = torch.as_tensor([obj[\"area\"] for obj in annotations], dtype=torch.float32, device=image.device)\n+    iscrowd = torch.as_tensor(\n+        [obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in annotations], dtype=torch.int64, device=image.device\n+    )\n+\n+    boxes = [obj[\"bbox\"] for obj in annotations]\n+    # guard against no boxes via resizing\n+    boxes = torch.as_tensor(boxes, dtype=torch.float32, device=image.device).reshape(-1, 4)\n+    boxes[:, 2:] += boxes[:, :2]\n+    boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=image_width)\n+    boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=image_height)\n+\n+    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n+\n+    new_target = {}\n+    new_target[\"image_id\"] = image_id\n+    new_target[\"class_labels\"] = classes[keep]\n+    new_target[\"boxes\"] = boxes[keep]\n+    new_target[\"area\"] = area[keep]\n+    new_target[\"iscrowd\"] = iscrowd[keep]\n+    new_target[\"orig_size\"] = torch.as_tensor(\n+        [int(image_height), int(image_width)], dtype=torch.int64, device=image.device\n+    )\n+\n+    if annotations and \"keypoints\" in annotations[0]:\n+        keypoints = [obj[\"keypoints\"] for obj in annotations]\n+        # Converting the filtered keypoints list to a numpy array\n+        keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=image.device)\n+        # Apply the keep mask here to filter the relevant annotations\n+        keypoints = keypoints[keep]\n+        num_keypoints = keypoints.shape[0]\n+        keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\n+        new_target[\"keypoints\"] = keypoints\n+\n+    if return_segmentation_masks:\n+        segmentation_masks = [obj[\"segmentation\"] for obj in annotations]\n+        masks = convert_coco_poly_to_mask(segmentation_masks, image_height, image_width, device=image.device)\n+        new_target[\"masks\"] = masks[keep]\n+\n+    return new_target\n+\n+\n+def masks_to_boxes(masks: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Compute the bounding boxes around the provided panoptic segmentation masks.\n+\n+    Args:\n+        masks: masks in format `[number_masks, height, width]` where N is the number of masks\n+\n+    Returns:\n+        boxes: bounding boxes in format `[number_masks, 4]` in xyxy format\n+    \"\"\"\n+    if masks.numel() == 0:\n+        return torch.zeros((0, 4), device=masks.device)\n+\n+    h, w = masks.shape[-2:]\n+    y = torch.arange(0, h, dtype=torch.float32, device=masks.device)\n+    x = torch.arange(0, w, dtype=torch.float32, device=masks.device)\n+    # see https://github.com/pytorch/pytorch/issues/50276\n+    y, x = torch.meshgrid(y, x, indexing=\"ij\")\n+\n+    x_mask = masks * torch.unsqueeze(x, 0)\n+    x_max = x_mask.view(x_mask.shape[0], -1).max(-1)[0]\n+    x_min = (\n+        torch.where(masks, x.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    y_mask = masks * torch.unsqueeze(y, 0)\n+    y_max = y_mask.view(y_mask.shape[0], -1).max(-1)[0]\n+    y_min = (\n+        torch.where(masks, y.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    return torch.stack([x_min, y_min, x_max, y_max], 1)\n+\n+\n+# 2 functions below adapted from https://github.com/cocodataset/panopticapi/blob/master/panopticapi/utils.py\n+# Copyright (c) 2018, Alexander Kirillov\n+# All rights reserved.\n+def rgb_to_id(color):\n+    \"\"\"\n+    Converts RGB color to unique ID.\n+    \"\"\"\n+    if isinstance(color, torch.Tensor) and len(color.shape) == 3:\n+        if color.dtype == torch.uint8:\n+            color = color.to(torch.int32)\n+        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n+    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n+\n+\n+def prepare_coco_panoptic_annotation(\n+    image: torch.Tensor,\n+    target: Dict,\n+    masks_path: Union[str, pathlib.Path],\n+    return_masks: bool = True,\n+    input_data_format: Union[ChannelDimension, str] = None,\n+) -> Dict:\n+    \"\"\"\n+    Prepare a coco panoptic annotation for DETR.\n+    \"\"\"\n+    image_height, image_width = get_image_size(image, channel_dim=input_data_format)\n+    annotation_path = pathlib.Path(masks_path) / target[\"file_name\"]\n+\n+    new_target = {}\n+    new_target[\"image_id\"] = torch.as_tensor(\n+        [target[\"image_id\"] if \"image_id\" in target else target[\"id\"]], dtype=torch.int64, device=image.device\n+    )\n+    new_target[\"size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+    new_target[\"orig_size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+\n+    if \"segments_info\" in target:\n+        masks = read_image(annotation_path).permute(1, 2, 0).to(torch.int32).to(image.device)\n+        masks = rgb_to_id(masks)\n+\n+        ids = torch.as_tensor([segment_info[\"id\"] for segment_info in target[\"segments_info\"]], device=image.device)\n+        masks = masks == ids[:, None, None]\n+        masks = masks.to(torch.bool)\n+        if return_masks:\n+            new_target[\"masks\"] = masks\n+        new_target[\"boxes\"] = masks_to_boxes(masks)\n+        new_target[\"class_labels\"] = torch.as_tensor(\n+            [segment_info[\"category_id\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"iscrowd\"] = torch.as_tensor(\n+            [segment_info[\"iscrowd\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"area\"] = torch.as_tensor(\n+            [segment_info[\"area\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.float32,\n+            device=image.device,\n+        )\n+\n+    return new_target\n+\n+\n+class DetrImageProcessorFast(BaseImageProcessorFast):\n+    r\"\"\"\n+    Constructs a fast Detr image processor.\n+\n+    Args:\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Controls whether to resize the image's `(height, width)` dimensions to the specified `size`. Can be\n+            overridden by the `do_resize` parameter in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 800, \"longest_edge\": 1333}`):\n+            Size of the image's `(height, width)` dimensions after resizing. Can be overridden by the `size` parameter\n+            in the `preprocess` method. Available options are:\n+                - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                    Do NOT keep the aspect ratio.\n+                - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                    the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                    less or equal to `longest_edge`.\n+                - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                    aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                    `max_width`.\n+        resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n+            Resampling filter to use if resizing the image.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Controls whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n+            `do_rescale` parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by the `rescale_factor` parameter in the\n+            `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Controls whether to normalize the image. Can be overridden by the `do_normalize` parameter in the\n+            `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_MEAN`):\n+            Mean values to use when normalizing the image. Can be a single value or a list of values, one for each\n+            channel. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_DEFAULT_STD`):\n+            Standard deviation values to use when normalizing the image. Can be a single value or a list of values, one\n+            for each channel. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+\n+    def __init__(\n+        self,\n+        format: Union[str, AnnotationFormat] = AnnotationFormat.COCO_DETECTION,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        resample: [Union[PILImageResampling, F.InterpolationMode]] = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Union[float, List[float]] = None,\n+        image_std: Union[float, List[float]] = None,\n+        do_convert_annotations: Optional[bool] = None,\n+        do_pad: bool = True,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        **kwargs,\n+    ) -> None:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n+\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n+                \"Please specify in `size['longest_edge'] instead`.\",\n+            )\n+            max_size = kwargs.pop(\"max_size\")\n+        else:\n+            max_size = None if size is None else 1333\n+\n+        size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+        size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+\n+        # Backwards compatibility\n+        if do_convert_annotations is None:\n+            do_convert_annotations = do_normalize\n+\n+        super().__init__(**kwargs)\n+        self.format = format\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.do_convert_annotations = do_convert_annotations\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n+        self.do_pad = do_pad\n+        self.pad_size = pad_size\n+        self._valid_processor_keys = [\n+            \"images\",\n+            \"annotations\",\n+            \"return_segmentation_masks\",\n+            \"masks_path\",\n+            \"do_resize\",\n+            \"size\",\n+            \"resample\",\n+            \"do_rescale\",\n+            \"rescale_factor\",\n+            \"do_normalize\",\n+            \"do_convert_annotations\",\n+            \"image_mean\",\n+            \"image_std\",\n+            \"do_pad\",\n+            \"pad_size\",\n+            \"format\",\n+            \"return_tensors\",\n+            \"data_format\",\n+            \"input_data_format\",\n+        ]\n+\n+    @classmethod\n+    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+        \"\"\"\n+        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n+        created using from_dict and kwargs e.g. `DetrImageProcessor.from_pretrained(checkpoint, size=600,\n+        max_size=800)`\n+        \"\"\"\n+        image_processor_dict = image_processor_dict.copy()\n+        if \"max_size\" in kwargs:\n+            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        return super().from_dict(image_processor_dict, **kwargs)\n+\n+    def prepare_annotation(\n+        self,\n+        image: torch.Tensor,\n+        target: Dict,\n+        format: Optional[AnnotationFormat] = None,\n+        return_segmentation_masks: bool = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> Dict:\n+        \"\"\"\n+        Prepare an annotation for feeding into DETR model.\n+        \"\"\"\n+        format = format if format is not None else self.format\n+\n+        if format == AnnotationFormat.COCO_DETECTION:\n+            return_segmentation_masks = False if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_detection_annotation(\n+                image, target, return_segmentation_masks, input_data_format=input_data_format\n+            )\n+        elif format == AnnotationFormat.COCO_PANOPTIC:\n+            return_segmentation_masks = True if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_panoptic_annotation(\n+                image,\n+                target,\n+                masks_path=masks_path,\n+                return_masks=return_segmentation_masks,\n+                input_data_format=input_data_format,\n+            )\n+        else:\n+            raise ValueError(f\"Format {format} is not supported.\")\n+        return target\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: F.InterpolationMode = F.InterpolationMode.BILINEAR,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize the image to the given size. Size can be `min_size` (scalar) or `(height, width)` tuple. If size is an\n+        int, smaller edge of the image will be matched to this number.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Resampling filter to use if resizing the image.\n+        \"\"\"\n+        if size.shortest_edge and size.longest_edge:\n+            # Resize the image so that the shortest edge or the longest edge is of the given size\n+            # while maintaining the aspect ratio of the original image.\n+            new_size = get_size_with_aspect_ratio(\n+                image.size()[-2:],\n+                size[\"shortest_edge\"],\n+                size[\"longest_edge\"],\n+            )\n+        elif size.max_height and size.max_width:\n+            new_size = get_image_size_for_max_height_width(image.size()[-2:], size[\"max_height\"], size[\"max_width\"])\n+        elif size.height and size.width:\n+            new_size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\n+                \"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys. Got\"\n+                f\" {size.keys()}.\"\n+            )\n+\n+        image = F.resize(\n+            image,\n+            size=new_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+        return image\n+\n+    def resize_annotation(\n+        self,\n+        annotation: Dict[str, Any],\n+        orig_size: Tuple[int, int],\n+        target_size: Tuple[int, int],\n+        threshold: float = 0.5,\n+        interpolation: F.InterpolationMode = F.InterpolationMode.NEAREST,\n+    ):\n+        \"\"\"\n+        Resizes an annotation to a target size.\n+\n+        Args:\n+            annotation (`Dict[str, Any]`):\n+                The annotation dictionary.\n+            orig_size (`Tuple[int, int]`):\n+                The original size of the input image.\n+            target_size (`Tuple[int, int]`):\n+                The target size of the image, as returned by the preprocessing `resize` step.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The threshold used to binarize the segmentation masks.\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+                The resampling filter to use when resizing the masks.\n+        \"\"\"\n+        ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n+\n+        new_annotation = {}\n+        new_annotation[\"size\"] = target_size\n+\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                scaled_boxes = boxes * torch.as_tensor(\n+                    [ratio_width, ratio_height, ratio_width, ratio_height], dtype=torch.float32, device=boxes.device\n+                )\n+                new_annotation[\"boxes\"] = scaled_boxes\n+            elif key == \"area\":\n+                area = value\n+                scaled_area = area * (ratio_width * ratio_height)\n+                new_annotation[\"area\"] = scaled_area\n+            elif key == \"masks\":\n+                masks = value[:, None]\n+                masks = [F.resize(mask, target_size, interpolation=interpolation) for mask in masks]\n+                masks = torch.stack(masks).to(torch.float32)\n+                masks = masks[:, 0] > threshold\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = target_size\n+            else:\n+                new_annotation[key] = value\n+\n+        return new_annotation\n+\n+    def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n+        image_height, image_width = image_size\n+        norm_annotation = {}\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                boxes = corners_to_center_format(boxes)\n+                boxes /= torch.as_tensor(\n+                    [image_width, image_height, image_width, image_height], dtype=torch.float32, device=boxes.device\n+                )\n+                norm_annotation[key] = boxes\n+            else:\n+                norm_annotation[key] = value\n+        return norm_annotation\n+\n+    def _update_annotation_for_padded_image(\n+        self,\n+        annotation: Dict,\n+        input_image_size: Tuple[int, int],\n+        output_image_size: Tuple[int, int],\n+        padding,\n+        update_bboxes,\n+    ) -> Dict:\n+        \"\"\"\n+        Update the annotation for a padded image.\n+        \"\"\"\n+        new_annotation = {}\n+        new_annotation[\"size\"] = output_image_size\n+        ratio_height, ratio_width = (input / output for output, input in zip(output_image_size, input_image_size))\n+\n+        for key, value in annotation.items():\n+            if key == \"masks\":\n+                masks = value\n+                masks = F.pad(\n+                    masks,\n+                    padding,\n+                    fill=0,\n+                )\n+                masks = safe_squeeze(masks, 1)\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"boxes\" and update_bboxes:\n+                boxes = value\n+                boxes *= torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height], device=boxes.device)\n+                new_annotation[\"boxes\"] = boxes\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = output_image_size\n+            else:\n+                new_annotation[key] = value\n+        return new_annotation\n+\n+    def pad(\n+        self,\n+        image: torch.Tensor,\n+        padded_size: Tuple[int, int],\n+        annotation: Optional[Dict[str, Any]] = None,\n+        update_bboxes: bool = True,\n+        fill: int = 0,\n+    ):\n+        original_size = image.size()[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+        if original_size != padded_size:\n+            padding = [0, 0, padding_right, padding_bottom]\n+            image = F.pad(image, padding, fill=fill)\n+            if annotation is not None:\n+                annotation = self._update_annotation_for_padded_image(\n+                    annotation, original_size, padded_size, padding, update_bboxes\n+                )\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = torch.zeros(padded_size, dtype=torch.int64, device=image.device)\n+        pixel_mask[: original_size[0], : original_size[1]] = 1\n+\n+        return image, pixel_mask, annotation\n+\n+    @functools.lru_cache(maxsize=1)\n+    def _validate_input_arguments(\n+        self,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Union[float, List[float]],\n+        image_std: Union[float, List[float]],\n+        do_resize: bool,\n+        size: Dict[str, int],\n+        resample: \"PILImageResampling\",\n+        data_format: Union[str, ChannelDimension],\n+        return_tensors: Union[TensorType, str],\n+    ):\n+        if return_tensors != \"pt\":\n+            raise ValueError(\"Only returning PyTorch tensors is currently supported.\")\n+\n+        if data_format != ChannelDimension.FIRST:\n+            raise ValueError(\"Only channel first data format is currently supported.\")\n+\n+        if do_resize and None in (size, resample):\n+            raise ValueError(\"Size and resample must be specified if do_resize is True.\")\n+\n+        if do_rescale and rescale_factor is None:\n+            raise ValueError(\"Rescale factor must be specified if do_rescale is True.\")\n+\n+        if do_normalize and None in (image_mean, image_std):\n+            raise ValueError(\"Image mean and standard deviation must be specified if do_normalize is True.\")\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        return_segmentation_masks: bool = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: Optional[Union[PILImageResampling, F.InterpolationMode]] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[Union[int, float]] = None,\n+        do_normalize: Optional[bool] = None,\n+        do_convert_annotations: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_pad: Optional[bool] = None,\n+        format: Optional[Union[str, AnnotationFormat]] = None,\n+        return_tensors: Optional[Union[TensorType, str]] = None,\n+        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        pad_size: Optional[Dict[str, int]] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or a batch of images so that it can be used by the model.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image or batch of images to preprocess. Expects a single or batch of images with pixel values ranging\n+                from 0 to 255. If passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n+                List of annotations associated with the image or batch of images. If annotation is for object\n+                detection, the annotations should be a dictionary with the following keys:\n+                - \"image_id\" (`int`): The image id.\n+                - \"annotations\" (`List[Dict]`): List of annotations for an image. Each annotation should be a\n+                  dictionary. An image can have no annotations, in which case the list should be empty.\n+                If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n+                - \"image_id\" (`int`): The image id.\n+                - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n+                  An image can have no segments, in which case the list should be empty.\n+                - \"file_name\" (`str`): The file name of the image.\n+            return_segmentation_masks (`bool`, *optional*, defaults to self.return_segmentation_masks):\n+                Whether to return segmentation masks.\n+            masks_path (`str` or `pathlib.Path`, *optional*):\n+                Path to the directory containing the segmentation masks.\n+            do_resize (`bool`, *optional*, defaults to self.do_resize):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to self.size):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            resample (`PILImageResampling` or `InterpolationMode`, *optional*, defaults to self.resample):\n+                Resampling filter to use when resizing the image.\n+            do_rescale (`bool`, *optional*, defaults to self.do_rescale):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to self.rescale_factor):\n+                Rescale factor to use when rescaling the image.\n+            do_normalize (`bool`, *optional*, defaults to self.do_normalize):\n+                Whether to normalize the image.\n+            do_convert_annotations (`bool`, *optional*, defaults to self.do_convert_annotations):\n+                Whether to convert the annotations to the format expected by the model. Converts the bounding\n+                boxes from the format `(top_left_x, top_left_y, width, height)` to `(center_x, center_y, width, height)`\n+                and in relative coordinates.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to self.image_mean):\n+                Mean to use when normalizing the image.\n+            image_std (`float` or `List[float]`, *optional*, defaults to self.image_std):\n+                Standard deviation to use when normalizing the image.\n+            do_pad (`bool`, *optional*, defaults to self.do_pad):\n+                Whether to pad the image. If `True`, padding will be applied to the bottom and right of\n+                the image with zeros. If `pad_size` is provided, the image will be padded to the specified\n+                dimensions. Otherwise, the image will be padded to the maximum height and width of the batch.\n+            format (`str` or `AnnotationFormat`, *optional*, defaults to self.format):\n+                Format of the annotations.\n+            return_tensors (`str` or `TensorType`, *optional*, defaults to self.return_tensors):\n+                Type of tensors to return. If `None`, will return the list of images.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            pad_size (`Dict[str, int]`, *optional*):\n+                The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+                provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+                height and width in the batch.\n+        \"\"\"\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            logger.warning_once(\n+                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n+                \"use `do_pad` instead.\"\n+            )\n+            do_pad = kwargs.pop(\"pad_and_return_pixel_mask\")\n+\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n+                \" `size['longest_edge']` instead.\"\n+            )\n+            size = kwargs.pop(\"max_size\")\n+        do_resize = self.do_resize if do_resize is None else do_resize\n+        size = self.size if size is None else size\n+        size = get_size_dict(size=size, default_to_square=False)\n+        resample = self.resample if resample is None else resample\n+        do_rescale = self.do_rescale if do_rescale is None else do_rescale\n+        rescale_factor = self.rescale_factor if rescale_factor is None else rescale_factor\n+        do_normalize = self.do_normalize if do_normalize is None else do_normalize\n+        image_mean = self.image_mean if image_mean is None else image_mean\n+        image_std = self.image_std if image_std is None else image_std\n+        do_convert_annotations = (\n+            self.do_convert_annotations if do_convert_annotations is None else do_convert_annotations\n+        )\n+        do_pad = self.do_pad if do_pad is None else do_pad\n+        pad_size = self.pad_size if pad_size is None else pad_size\n+        format = self.format if format is None else format\n+        device = kwargs.pop(\"device\", None)\n+\n+        # Make hashable for cache\n+        size = SizeDict(**size)\n+        image_mean = tuple(image_mean) if isinstance(image_mean, list) else image_mean\n+        image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n+\n+        images = make_list_of_images(images)\n+        image_type = get_image_type(images[0])\n+\n+        if image_type not in [ImageType.PIL, ImageType.TORCH, ImageType.NUMPY]:\n+            raise ValueError(f\"Unsupported input image type {image_type}\")\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n+\n+        self._validate_input_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            return_tensors=return_tensors,\n+            data_format=data_format,\n+        )\n+\n+        if annotations is not None and isinstance(annotations, dict):\n+            annotations = [annotations]\n+\n+        if annotations is not None and len(images) != len(annotations):\n+            raise ValueError(\n+                f\"The number of images ({len(images)}) and annotations ({len(annotations)}) do not match.\"\n+            )\n+\n+        format = AnnotationFormat(format)\n+        if annotations is not None:\n+            validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n+\n+        if (\n+            masks_path is not None\n+            and format == AnnotationFormat.COCO_PANOPTIC\n+            and not isinstance(masks_path, (pathlib.Path, str))\n+        ):\n+            raise ValueError(\n+                \"The path to the directory containing the mask PNG files should be provided as a\"\n+                f\" `pathlib.Path` or string object, but is {type(masks_path)} instead.\"\n+            )\n+\n+        data = {}\n+        if image_type == ImageType.PIL:\n+            images = [F.pil_to_tensor(image) for image in images]\n+        elif image_type == ImageType.NUMPY:\n+            # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n+            images = [torch.from_numpy(image).contiguous() for image in images]\n+\n+        if device is not None:\n+            images = [image.to(device) for image in images]\n+\n+        # We assume that all images have the same channel dimension format.\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(images[0])\n+        if input_data_format == ChannelDimension.LAST:\n+            images = [image.permute(2, 0, 1).contiguous() for image in images]\n+\n+        # prepare (COCO annotations as a list of Dict -> DETR target as a single Dict per image)\n+        if annotations is not None:\n+            prepared_images = []\n+            prepared_annotations = []\n+            for image, target in zip(images, annotations):\n+                target = self.prepare_annotation(\n+                    image,\n+                    target,\n+                    format,\n+                    return_segmentation_masks=return_segmentation_masks,\n+                    masks_path=masks_path,\n+                    input_data_format=input_data_format,\n+                )\n+                prepared_images.append(image)\n+                prepared_annotations.append(target)\n+            images = prepared_images\n+            annotations = prepared_annotations\n+            del prepared_images, prepared_annotations\n+\n+        if do_resize:\n+            if isinstance(resample, (PILImageResampling, int)):\n+                interpolation = pil_torch_interpolation_mapping[resample]\n+            else:\n+                interpolation = resample\n+            resized_images = [self.resize(image, size=size, interpolation=interpolation) for image in images]\n+            if annotations is not None:\n+                for i, (image, target) in enumerate(zip(resized_images, annotations)):\n+                    annotations[i] = self.resize_annotation(\n+                        target,\n+                        orig_size=images[i].size()[-2:],\n+                        target_size=image.size()[-2:],\n+                    )\n+            images = resized_images\n+            del resized_images\n+\n+        if do_rescale and do_normalize:\n+            # fused rescale and normalize\n+            new_mean = torch.tensor(image_mean, device=images[0].device) * (1.0 / rescale_factor)\n+            new_std = torch.tensor(image_std, device=images[0].device) * (1.0 / rescale_factor)\n+            images = [F.normalize(image.to(dtype=torch.float32), new_mean, new_std) for image in images]\n+        elif do_rescale:\n+            images = [image * rescale_factor for image in images]\n+        elif do_normalize:\n+            images = [F.normalize(image, image_mean, image_std) for image in images]\n+\n+        if do_convert_annotations and annotations is not None:\n+            annotations = [\n+                self.normalize_annotation(annotation, get_image_size(image, input_data_format))\n+                for annotation, image in zip(annotations, images)\n+            ]\n+\n+        if do_pad:\n+            # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n+            if pad_size is not None:\n+                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+            else:\n+                padded_size = get_max_height_width(images)\n+\n+            annotation_list = annotations if annotations is not None else [None] * len(images)\n+            padded_images = []\n+            pixel_masks = []\n+            padded_annotations = []\n+            for image, annotation in zip(images, annotation_list):\n+                if padded_size == image.size()[-2:]:\n+                    padded_images.append(image)\n+                    pixel_masks.append(torch.ones(padded_size, dtype=torch.int64, device=image.device))\n+                    padded_annotations.append(annotation)\n+                    continue\n+                padded_image, pixel_mask, padded_annotation = self.pad(\n+                    image, padded_size, annotation=annotation, update_bboxes=do_convert_annotations\n+                )\n+                padded_images.append(padded_image)\n+                pixel_masks.append(pixel_mask)\n+                padded_annotations.append(padded_annotation)\n+            images = padded_images\n+            if annotations is not None:\n+                annotations = padded_annotations\n+            del padded_images, padded_annotations\n+            data.update({\"pixel_mask\": torch.stack(pixel_masks, dim=0)})\n+\n+        data.update({\"pixel_values\": torch.stack(images, dim=0)})\n+        encoded_inputs = BatchFeature(data, tensor_type=return_tensors)\n+        if annotations is not None:\n+            encoded_inputs[\"labels\"] = [\n+                BatchFeature(annotation, tensor_type=return_tensors) for annotation in annotations\n+            ]\n+        return encoded_inputs\n+\n+    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process\n+    def post_process(self, outputs, target_sizes):\n+        \"\"\"\n+        Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the size (height, width) of each image of the batch. For evaluation, this must be the\n+                original image size (before any data augmentation). For visualization, this should be the image size\n+                after data augment, but before padding.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        logger.warning_once(\n+            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n+        )\n+\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if len(out_logits) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n+        if target_sizes.shape[1] != 2:\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        prob = nn.functional.softmax(out_logits, -1)\n+        scores, labels = prob[..., :-1].max(-1)\n+\n+        # convert to [x0, y0, x1, y1] format\n+        boxes = center_to_corners_format(out_bbox)\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        img_h, img_w = target_sizes.unbind(1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+        boxes = boxes * scale_fct[:, None, :]\n+\n+        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n+        return results\n+\n+    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_segmentation\n+    def post_process_segmentation(self, outputs, target_sizes, threshold=0.9, mask_threshold=0.5):\n+        \"\"\"\n+        Converts the output of [`DetrForSegmentation`] into image segmentation predictions. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrSegmentationOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`):\n+                Torch Tensor (or list) corresponding to the requested final size (h, w) of each prediction.\n+            threshold (`float`, *optional*, defaults to 0.9):\n+                Threshold to use to filter out queries.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels, and masks for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        logger.warning_once(\n+            \"`post_process_segmentation` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_semantic_segmentation`.\",\n+        )\n+        out_logits, raw_masks = outputs.logits, outputs.pred_masks\n+        empty_label = out_logits.shape[-1] - 1\n+        preds = []\n+\n+        def to_tuple(tup):\n+            if isinstance(tup, tuple):\n+                return tup\n+            return tuple(tup.cpu().tolist())\n+\n+        for cur_logits, cur_masks, size in zip(out_logits, raw_masks, target_sizes):\n+            # we filter empty queries and detection below threshold\n+            cur_scores, cur_labels = cur_logits.softmax(-1).max(-1)\n+            keep = cur_labels.ne(empty_label) & (cur_scores > threshold)\n+            cur_scores = cur_scores[keep]\n+            cur_labels = cur_labels[keep]\n+            cur_masks = cur_masks[keep]\n+            cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n+            cur_masks = (cur_masks.sigmoid() > mask_threshold) * 1\n+\n+            predictions = {\"scores\": cur_scores, \"labels\": cur_labels, \"masks\": cur_masks}\n+            preds.append(predictions)\n+        return preds\n+\n+    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_instance\n+    def post_process_instance(self, results, outputs, orig_target_sizes, max_target_sizes, threshold=0.5):\n+        \"\"\"\n+        Converts the output of [`DetrForSegmentation`] into actual instance segmentation predictions. Only supports\n+        PyTorch.\n+\n+        Args:\n+            results (`List[Dict]`):\n+                Results list obtained by [`~DetrImageProcessor.post_process`], to which \"masks\" results will be added.\n+            outputs ([`DetrSegmentationOutput`]):\n+                Raw outputs of the model.\n+            orig_target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n+                image size (before any data augmentation).\n+            max_target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the maximum size (h, w) of each image of the batch. For evaluation, this must be the\n+                original image size (before any data augmentation).\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels, boxes and masks for an\n+            image in the batch as predicted by the model.\n+        \"\"\"\n+        logger.warning_once(\n+            \"`post_process_instance` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_instance_segmentation`.\",\n+        )\n+\n+        if len(orig_target_sizes) != len(max_target_sizes):\n+            raise ValueError(\"Make sure to pass in as many orig_target_sizes as max_target_sizes\")\n+        max_h, max_w = max_target_sizes.max(0)[0].tolist()\n+        outputs_masks = outputs.pred_masks.squeeze(2)\n+        outputs_masks = nn.functional.interpolate(\n+            outputs_masks, size=(max_h, max_w), mode=\"bilinear\", align_corners=False\n+        )\n+        outputs_masks = (outputs_masks.sigmoid() > threshold).cpu()\n+\n+        for i, (cur_mask, t, tt) in enumerate(zip(outputs_masks, max_target_sizes, orig_target_sizes)):\n+            img_h, img_w = t[0], t[1]\n+            results[i][\"masks\"] = cur_mask[:, :img_h, :img_w].unsqueeze(1)\n+            results[i][\"masks\"] = nn.functional.interpolate(\n+                results[i][\"masks\"].float(), size=tuple(tt.tolist()), mode=\"nearest\"\n+            ).byte()\n+\n+        return results\n+\n+    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_panoptic\n+    def post_process_panoptic(self, outputs, processed_sizes, target_sizes=None, is_thing_map=None, threshold=0.85):\n+        \"\"\"\n+        Converts the output of [`DetrForSegmentation`] into actual panoptic predictions. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrSegmentationOutput`]):\n+                Raw outputs of the model.\n+            processed_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`):\n+                Torch Tensor (or list) containing the size (h, w) of each image of the batch, i.e. the size after data\n+                augmentation but before batching.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)` or `List[Tuple]` of length `batch_size`, *optional*):\n+                Torch Tensor (or list) corresponding to the requested final size `(height, width)` of each prediction.\n+                If left to None, it will default to the `processed_sizes`.\n+            is_thing_map (`torch.Tensor` of shape `(batch_size, 2)`, *optional*):\n+                Dictionary mapping class indices to either True or False, depending on whether or not they are a thing.\n+                If not set, defaults to the `is_thing_map` of COCO panoptic.\n+            threshold (`float`, *optional*, defaults to 0.85):\n+                Threshold to use to filter out queries.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing a PNG string and segments_info values for\n+            an image in the batch as predicted by the model.\n+        \"\"\"\n+        logger.warning_once(\n+            \"`post_process_panoptic is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_panoptic_segmentation`.\",\n+        )\n+        if target_sizes is None:\n+            target_sizes = processed_sizes\n+        if len(processed_sizes) != len(target_sizes):\n+            raise ValueError(\"Make sure to pass in as many processed_sizes as target_sizes\")\n+\n+        if is_thing_map is None:\n+            # default to is_thing_map of COCO panoptic\n+            is_thing_map = {i: i <= 90 for i in range(201)}\n+\n+        out_logits, raw_masks, raw_boxes = outputs.logits, outputs.pred_masks, outputs.pred_boxes\n+        if not len(out_logits) == len(raw_masks) == len(target_sizes):\n+            raise ValueError(\n+                \"Make sure that you pass in as many target sizes as the batch dimension of the logits and masks\"\n+            )\n+        empty_label = out_logits.shape[-1] - 1\n+        preds = []\n+\n+        def to_tuple(tup):\n+            if isinstance(tup, tuple):\n+                return tup\n+            return tuple(tup.cpu().tolist())\n+\n+        for cur_logits, cur_masks, cur_boxes, size, target_size in zip(\n+            out_logits, raw_masks, raw_boxes, processed_sizes, target_sizes\n+        ):\n+            # we filter empty queries and detection below threshold\n+            cur_scores, cur_labels = cur_logits.softmax(-1).max(-1)\n+            keep = cur_labels.ne(empty_label) & (cur_scores > threshold)\n+            cur_scores = cur_scores[keep]\n+            cur_labels = cur_labels[keep]\n+            cur_masks = cur_masks[keep]\n+            cur_masks = nn.functional.interpolate(cur_masks[:, None], to_tuple(size), mode=\"bilinear\").squeeze(1)\n+            cur_boxes = center_to_corners_format(cur_boxes[keep])\n+\n+            h, w = cur_masks.shape[-2:]\n+            if len(cur_boxes) != len(cur_labels):\n+                raise ValueError(\"Not as many boxes as there are classes\")\n+\n+            # It may be that we have several predicted masks for the same stuff class.\n+            # In the following, we track the list of masks ids for each stuff class (they are merged later on)\n+            cur_masks = cur_masks.flatten(1)\n+            stuff_equiv_classes = defaultdict(lambda: [])\n+            for k, label in enumerate(cur_labels):\n+                if not is_thing_map[label.item()]:\n+                    stuff_equiv_classes[label.item()].append(k)\n+\n+            def get_ids_area(masks, scores, dedup=False):\n+                # This helper function creates the final panoptic segmentation image\n+                # It also returns the area of the masks that appears on the image\n+\n+                m_id = masks.transpose(0, 1).softmax(-1)\n+\n+                if m_id.shape[-1] == 0:\n+                    # We didn't detect any mask :(\n+                    m_id = torch.zeros((h, w), dtype=torch.long, device=m_id.device)\n+                else:\n+                    m_id = m_id.argmax(-1).view(h, w)\n+\n+                if dedup:\n+                    # Merge the masks corresponding to the same stuff class\n+                    for equiv in stuff_equiv_classes.values():\n+                        if len(equiv) > 1:\n+                            for eq_id in equiv:\n+                                m_id.masked_fill_(m_id.eq(eq_id), equiv[0])\n+\n+                final_h, final_w = to_tuple(target_size)\n+\n+                seg_img = PIL.Image.fromarray(id_to_rgb(m_id.view(h, w).cpu().numpy()))\n+                seg_img = seg_img.resize(size=(final_w, final_h), resample=PILImageResampling.NEAREST)\n+\n+                np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes()))\n+                np_seg_img = np_seg_img.view(final_h, final_w, 3)\n+                np_seg_img = np_seg_img.numpy()\n+\n+                m_id = torch.from_numpy(rgb_to_id(np_seg_img))\n+\n+                area = []\n+                for i in range(len(scores)):\n+                    area.append(m_id.eq(i).sum().item())\n+                return area, seg_img\n+\n+            area, seg_img = get_ids_area(cur_masks, cur_scores, dedup=True)\n+            if cur_labels.numel() > 0:\n+                # We know filter empty masks as long as we find some\n+                while True:\n+                    filtered_small = torch.as_tensor(\n+                        [area[i] <= 4 for i, c in enumerate(cur_labels)], dtype=torch.bool, device=keep.device\n+                    )\n+                    if filtered_small.any().item():\n+                        cur_scores = cur_scores[~filtered_small]\n+                        cur_labels = cur_labels[~filtered_small]\n+                        cur_masks = cur_masks[~filtered_small]\n+                        area, seg_img = get_ids_area(cur_masks, cur_scores)\n+                    else:\n+                        break\n+\n+            else:\n+                cur_labels = torch.ones(1, dtype=torch.long, device=cur_labels.device)\n+\n+            segments_info = []\n+            for i, a in enumerate(area):\n+                cat = cur_labels[i].item()\n+                segments_info.append({\"id\": i, \"isthing\": is_thing_map[cat], \"category_id\": cat, \"area\": a})\n+            del cur_labels\n+\n+            with io.BytesIO() as out:\n+                seg_img.save(out, format=\"PNG\")\n+                predictions = {\"png_string\": out.getvalue(), \"segments_info\": segments_info}\n+            preds.append(predictions)\n+        return preds\n+\n+    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_object_detection\n+    def post_process_object_detection(\n+        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, List[Tuple]] = None\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`DetrForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n+        bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if target_sizes is not None:\n+            if len(out_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+        prob = nn.functional.softmax(out_logits, -1)\n+        scores, labels = prob[..., :-1].max(-1)\n+\n+        # Convert to [x0, y0, x1, y1] format\n+        boxes = center_to_corners_format(out_bbox)\n+\n+        # Convert from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            if isinstance(target_sizes, List):\n+                img_h = torch.Tensor([i[0] for i in target_sizes])\n+                img_w = torch.Tensor([i[1] for i in target_sizes])\n+            else:\n+                img_h, img_w = target_sizes.unbind(1)\n+\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+            boxes = boxes * scale_fct[:, None, :]\n+\n+        results = []\n+        for s, l, b in zip(scores, labels, boxes):\n+            score = s[s > threshold]\n+            label = l[s > threshold]\n+            box = b[s > threshold]\n+            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+\n+        return results\n+\n+    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_semantic_segmentation\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple[int, int]] = None):\n+        \"\"\"\n+        Converts the output of [`DetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrForSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`List[Tuple[int, int]]`, *optional*):\n+                A list of tuples (`Tuple[int, int]`) containing the target size (height, width) of each image in the\n+                batch. If unset, predictions will not be resized.\n+        Returns:\n+            `List[torch.Tensor]`:\n+                A list of length `batch_size`, where each item is a semantic segmentation map of shape (height, width)\n+                corresponding to the target_sizes entry (if `target_sizes` is specified). Each entry of each\n+                `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        class_queries_logits = outputs.logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.pred_masks  # [batch_size, num_queries, height, width]\n+\n+        # Remove the null class `[..., :-1]`\n+        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n+        masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n+        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n+        batch_size = class_queries_logits.shape[0]\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if batch_size != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            semantic_segmentation = []\n+            for idx in range(batch_size):\n+                resized_logits = nn.functional.interpolate(\n+                    segmentation[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = segmentation.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_instance_segmentation\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        target_sizes: Optional[List[Tuple[int, int]]] = None,\n+        return_coco_annotation: Optional[bool] = False,\n+    ) -> List[Dict]:\n+        \"\"\"\n+        Converts the output of [`DetrForSegmentation`] into instance segmentation predictions. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`DetrForSegmentation`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            target_sizes (`List[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction. If unset, predictions will not be resized.\n+            return_coco_annotation (`bool`, *optional*):\n+                Defaults to `False`. If set to `True`, segmentation maps are returned in COCO run-length encoding (RLE)\n+                format.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id` or\n+              `List[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to\n+              `True`. Set to `None` if no mask if found above `threshold`.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- An integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+        class_queries_logits = outputs.logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.pred_masks  # [batch_size, num_queries, height, width]\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        mask_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Predicted label and score of each query (batch_size, num_queries)\n+        pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n+\n+        # Loop over items in batch size\n+        results: List[Dict[str, TensorType]] = []\n+\n+        for i in range(batch_size):\n+            mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects(\n+                mask_probs[i], pred_scores[i], pred_labels[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs_item.shape[0] <= 0:\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs_item.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            # Get segmentation map and segment information of batch item\n+            target_size = target_sizes[i] if target_sizes is not None else None\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs_item,\n+                pred_scores=pred_scores_item,\n+                pred_labels=pred_labels_item,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                label_ids_to_fuse=[],\n+                target_size=target_size,\n+            )\n+\n+            # Return segmentation map in run-length encoding (RLE) format\n+            if return_coco_annotation:\n+                segmentation = convert_segmentation_to_rle(segmentation)\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+    # Copied from transformers.models.detr.image_processing_detr.DetrImageProcessor.post_process_panoptic_segmentation\n+    def post_process_panoptic_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        label_ids_to_fuse: Optional[Set[int]] = None,\n+        target_sizes: Optional[List[Tuple[int, int]]] = None,\n+    ) -> List[Dict]:\n+        \"\"\"\n+        Converts the output of [`DetrForSegmentation`] into image panoptic segmentation predictions. Only supports\n+        PyTorch.\n+\n+        Args:\n+            outputs ([`DetrForSegmentation`]):\n+                The outputs from [`DetrForSegmentation`].\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            label_ids_to_fuse (`Set[int]`, *optional*):\n+                The labels in this state will have all their instances be fused together. For instance we could say\n+                there can only be one sky in an image, but several persons, so the label ID for sky would be in that\n+                set, but not the one for person.\n+            target_sizes (`List[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction in batch. If unset, predictions will not be resized.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- a tensor of shape `(height, width)` where each pixel represents a `segment_id` or\n+              `None` if no mask if found above `threshold`. If `target_sizes` is specified, segmentation is resized to\n+              the corresponding `target_sizes` entry.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- an integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **was_fused** -- a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False` otherwise.\n+                  Multiple instances of the same class / label were fused and assigned a single `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+\n+        if label_ids_to_fuse is None:\n+            logger.warning_once(\"`label_ids_to_fuse` unset. No instance will be fused.\")\n+            label_ids_to_fuse = set()\n+\n+        class_queries_logits = outputs.logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.pred_masks  # [batch_size, num_queries, height, width]\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        mask_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Predicted label and score of each query (batch_size, num_queries)\n+        pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n+\n+        # Loop over items in batch size\n+        results: List[Dict[str, TensorType]] = []\n+\n+        for i in range(batch_size):\n+            mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects(\n+                mask_probs[i], pred_scores[i], pred_labels[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs_item.shape[0] <= 0:\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs_item.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            # Get segmentation map and segment information of batch item\n+            target_size = target_sizes[i] if target_sizes is not None else None\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs_item,\n+                pred_scores=pred_scores_item,\n+                pred_labels=pred_labels_item,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                label_ids_to_fuse=label_ids_to_fuse,\n+                target_size=target_size,\n+            )\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results"
        },
        {
            "sha": "a781389c2fbdc825286eaa1dff4561d473e232eb",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -225,6 +225,7 @@\n     is_torchdynamo_available,\n     is_torchdynamo_compiling,\n     is_torchvision_available,\n+    is_torchvision_v2_available,\n     is_training_run_on_sagemaker,\n     is_uroman_available,\n     is_vision_available,"
        },
        {
            "sha": "d7f87717ca834ab183b1f0a998cf51db93c5e62d",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -191,6 +191,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class DetrImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class DonutFeatureExtractor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "173aee9b1ac73935194448540a6e8843d7f1e16c",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -186,7 +186,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _torchaudio_available = _is_package_available(\"torchaudio\")\n _torchao_available = _is_package_available(\"torchao\")\n _torchdistx_available = _is_package_available(\"torchdistx\")\n-_torchvision_available = _is_package_available(\"torchvision\")\n+_torchvision_available, _torchvision_version = _is_package_available(\"torchvision\", return_version=True)\n _mlx_available = _is_package_available(\"mlx\")\n _hqq_available, _hqq_version = _is_package_available(\"hqq\", return_version=True)\n _tiktoken_available = _is_package_available(\"tiktoken\")\n@@ -362,6 +362,14 @@ def is_torchvision_available():\n     return _torchvision_available\n \n \n+def is_torchvision_v2_available():\n+    if not is_torchvision_available():\n+        return False\n+\n+    # NOTE: We require torchvision>=0.15 as v2 transforms are available from this version: https://pytorch.org/vision/stable/transforms.html#v1-or-v2-which-one-should-i-use\n+    return version.parse(_torchvision_version) >= version.parse(\"0.15\")\n+\n+\n def is_galore_torch_available():\n     return _galore_torch_available\n "
        },
        {
            "sha": "32b135bcd220bdaff3e43a61dfa61d0b15405887",
            "filename": "tests/models/conditional_detr/test_image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 230,
            "deletions": 227,
            "changes": 457,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -282,96 +282,97 @@ def test_batched_coco_detection_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotations_0, annotations_1]\n \n-        image_processing = ConditionalDetrImageProcessor()\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            return_tensors=\"pt\",  # do_convert_annotations=True\n-        )\n-\n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1066\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.6879, 0.4609, 0.0755, 0.3691],\n-                [0.2118, 0.3359, 0.2601, 0.1566],\n-                [0.5011, 0.5000, 0.9979, 1.0000],\n-                [0.5010, 0.5020, 0.9979, 0.9959],\n-                [0.3284, 0.5944, 0.5884, 0.8112],\n-                [0.8394, 0.5445, 0.3213, 0.9110],\n-            ]\n-        )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.4130, 0.2765, 0.0453, 0.2215],\n-                [0.1272, 0.2016, 0.1561, 0.0940],\n-                [0.3757, 0.4933, 0.7488, 0.9865],\n-                [0.3759, 0.5002, 0.7492, 0.9955],\n-                [0.1971, 0.5456, 0.3532, 0.8646],\n-                [0.5790, 0.4115, 0.3430, 0.7161],\n-            ]\n-        )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class()\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                return_tensors=\"pt\",  # do_convert_annotations=True\n+            )\n+\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1066\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.6879, 0.4609, 0.0755, 0.3691],\n+                    [0.2118, 0.3359, 0.2601, 0.1566],\n+                    [0.5011, 0.5000, 0.9979, 1.0000],\n+                    [0.5010, 0.5020, 0.9979, 0.9959],\n+                    [0.3284, 0.5944, 0.5884, 0.8112],\n+                    [0.8394, 0.5445, 0.3213, 0.9110],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.4130, 0.2765, 0.0453, 0.2215],\n+                    [0.1272, 0.2016, 0.1561, 0.0940],\n+                    [0.3757, 0.4933, 0.7488, 0.9865],\n+                    [0.3759, 0.5002, 0.7492, 0.9955],\n+                    [0.1971, 0.5456, 0.3532, 0.8646],\n+                    [0.5790, 0.4115, 0.3430, 0.7161],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_panoptic_annotations with Detr->ConditionalDetr\n     def test_batched_coco_panoptic_annotations(self):\n@@ -402,146 +403,148 @@ def test_batched_coco_panoptic_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotation_0, annotation_1]\n \n-        # encode them\n-        image_processing = ConditionalDetrImageProcessor(format=\"coco_panoptic\")\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_tensors=\"pt\",\n-            return_segmentation_masks=True,\n-        )\n-\n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1066\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.2625, 0.5437, 0.4688, 0.8625],\n-                [0.7719, 0.4104, 0.4531, 0.7125],\n-                [0.5000, 0.4927, 0.9969, 0.9854],\n-                [0.1688, 0.2000, 0.2063, 0.0917],\n-                [0.5492, 0.2760, 0.0578, 0.2187],\n-                [0.4992, 0.4990, 0.9984, 0.9979],\n-            ]\n-        )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.1576, 0.3262, 0.2814, 0.5175],\n-                [0.4634, 0.2463, 0.2720, 0.4275],\n-                [0.3002, 0.2956, 0.5985, 0.5913],\n-                [0.1013, 0.1200, 0.1238, 0.0550],\n-                [0.3297, 0.1656, 0.0347, 0.1312],\n-                [0.2997, 0.2994, 0.5994, 0.5987],\n-            ]\n-        )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class(format=\"coco_panoptic\")\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_tensors=\"pt\",\n+                return_segmentation_masks=True,\n+            )\n+\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1066\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.2625, 0.5437, 0.4688, 0.8625],\n+                    [0.7719, 0.4104, 0.4531, 0.7125],\n+                    [0.5000, 0.4927, 0.9969, 0.9854],\n+                    [0.1688, 0.2000, 0.2063, 0.0917],\n+                    [0.5492, 0.2760, 0.0578, 0.2187],\n+                    [0.4992, 0.4990, 0.9984, 0.9979],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.1576, 0.3262, 0.2814, 0.5175],\n+                    [0.4634, 0.2463, 0.2720, 0.4275],\n+                    [0.3002, 0.2956, 0.5985, 0.5913],\n+                    [0.1013, 0.1200, 0.1238, 0.0550],\n+                    [0.3297, 0.1656, 0.0347, 0.1312],\n+                    [0.2997, 0.2994, 0.5994, 0.5987],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_max_width_max_height_resizing_and_pad_strategy with Detr->ConditionalDetr\n     def test_max_width_max_height_resizing_and_pad_strategy(self):\n-        image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n-\n-        # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n-        image_processor = ConditionalDetrImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n-\n-        # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n-        image_processor = ConditionalDetrImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-\n-        # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n-        image_processor = ConditionalDetrImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n-\n-        # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n-        image_processor = ConditionalDetrImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 301, \"width\": 101},\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n-\n-        ### Check for batch\n-        image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n-\n-        # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n-        image_processor = ConditionalDetrImageProcessor(\n-            size={\"max_height\": 150, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 150, \"width\": 100},\n-        )\n-        inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n+        for image_processing_class in self.image_processor_list:\n+            image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n+\n+            # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n+\n+            # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+\n+            # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n+\n+            # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 301, \"width\": 101},\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n+\n+            ### Check for batch\n+            image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n+\n+            # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 150, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 150, \"width\": 100},\n+            )\n+            inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n \n     def test_longest_edge_shortest_edge_resizing_strategy(self):\n         image_1 = torch.ones([958, 653, 3], dtype=torch.uint8)"
        },
        {
            "sha": "29dd0556afcde1f8218c5fb78ad6d3f5efe7db20",
            "filename": "tests/models/deformable_detr/test_image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 230,
            "deletions": 227,
            "changes": 457,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -284,96 +284,97 @@ def test_batched_coco_detection_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotations_0, annotations_1]\n \n-        image_processing = DeformableDetrImageProcessor()\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            return_tensors=\"pt\",  # do_convert_annotations=True\n-        )\n-\n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1066\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.6879, 0.4609, 0.0755, 0.3691],\n-                [0.2118, 0.3359, 0.2601, 0.1566],\n-                [0.5011, 0.5000, 0.9979, 1.0000],\n-                [0.5010, 0.5020, 0.9979, 0.9959],\n-                [0.3284, 0.5944, 0.5884, 0.8112],\n-                [0.8394, 0.5445, 0.3213, 0.9110],\n-            ]\n-        )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.4130, 0.2765, 0.0453, 0.2215],\n-                [0.1272, 0.2016, 0.1561, 0.0940],\n-                [0.3757, 0.4933, 0.7488, 0.9865],\n-                [0.3759, 0.5002, 0.7492, 0.9955],\n-                [0.1971, 0.5456, 0.3532, 0.8646],\n-                [0.5790, 0.4115, 0.3430, 0.7161],\n-            ]\n-        )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class()\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                return_tensors=\"pt\",  # do_convert_annotations=True\n+            )\n+\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1066\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.6879, 0.4609, 0.0755, 0.3691],\n+                    [0.2118, 0.3359, 0.2601, 0.1566],\n+                    [0.5011, 0.5000, 0.9979, 1.0000],\n+                    [0.5010, 0.5020, 0.9979, 0.9959],\n+                    [0.3284, 0.5944, 0.5884, 0.8112],\n+                    [0.8394, 0.5445, 0.3213, 0.9110],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.4130, 0.2765, 0.0453, 0.2215],\n+                    [0.1272, 0.2016, 0.1561, 0.0940],\n+                    [0.3757, 0.4933, 0.7488, 0.9865],\n+                    [0.3759, 0.5002, 0.7492, 0.9955],\n+                    [0.1971, 0.5456, 0.3532, 0.8646],\n+                    [0.5790, 0.4115, 0.3430, 0.7161],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_panoptic_annotations with Detr->DeformableDetr\n     def test_batched_coco_panoptic_annotations(self):\n@@ -404,146 +405,148 @@ def test_batched_coco_panoptic_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotation_0, annotation_1]\n \n-        # encode them\n-        image_processing = DeformableDetrImageProcessor(format=\"coco_panoptic\")\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_tensors=\"pt\",\n-            return_segmentation_masks=True,\n-        )\n-\n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1066\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.2625, 0.5437, 0.4688, 0.8625],\n-                [0.7719, 0.4104, 0.4531, 0.7125],\n-                [0.5000, 0.4927, 0.9969, 0.9854],\n-                [0.1688, 0.2000, 0.2063, 0.0917],\n-                [0.5492, 0.2760, 0.0578, 0.2187],\n-                [0.4992, 0.4990, 0.9984, 0.9979],\n-            ]\n-        )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.1576, 0.3262, 0.2814, 0.5175],\n-                [0.4634, 0.2463, 0.2720, 0.4275],\n-                [0.3002, 0.2956, 0.5985, 0.5913],\n-                [0.1013, 0.1200, 0.1238, 0.0550],\n-                [0.3297, 0.1656, 0.0347, 0.1312],\n-                [0.2997, 0.2994, 0.5994, 0.5987],\n-            ]\n-        )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class(format=\"coco_panoptic\")\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_tensors=\"pt\",\n+                return_segmentation_masks=True,\n+            )\n+\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1066\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.2625, 0.5437, 0.4688, 0.8625],\n+                    [0.7719, 0.4104, 0.4531, 0.7125],\n+                    [0.5000, 0.4927, 0.9969, 0.9854],\n+                    [0.1688, 0.2000, 0.2063, 0.0917],\n+                    [0.5492, 0.2760, 0.0578, 0.2187],\n+                    [0.4992, 0.4990, 0.9984, 0.9979],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.1576, 0.3262, 0.2814, 0.5175],\n+                    [0.4634, 0.2463, 0.2720, 0.4275],\n+                    [0.3002, 0.2956, 0.5985, 0.5913],\n+                    [0.1013, 0.1200, 0.1238, 0.0550],\n+                    [0.3297, 0.1656, 0.0347, 0.1312],\n+                    [0.2997, 0.2994, 0.5994, 0.5987],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_max_width_max_height_resizing_and_pad_strategy with Detr->DeformableDetr\n     def test_max_width_max_height_resizing_and_pad_strategy(self):\n-        image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n-\n-        # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n-\n-        # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-\n-        # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n-\n-        # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 301, \"width\": 101},\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n-\n-        ### Check for batch\n-        image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n-\n-        # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n-        image_processor = DeformableDetrImageProcessor(\n-            size={\"max_height\": 150, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 150, \"width\": 100},\n-        )\n-        inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n+        for image_processing_class in self.image_processor_list:\n+            image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n+\n+            # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n+\n+            # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+\n+            # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n+\n+            # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 301, \"width\": 101},\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n+\n+            ### Check for batch\n+            image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n+\n+            # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 150, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 150, \"width\": 100},\n+            )\n+            inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n \n     def test_longest_edge_shortest_edge_resizing_strategy(self):\n         image_1 = torch.ones([958, 653, 3], dtype=torch.uint8)"
        },
        {
            "sha": "976b306115b68ad87fd9f36e56009b416d725016",
            "filename": "tests/models/detr/test_image_processing_detr.py",
            "status": "modified",
            "additions": 514,
            "deletions": 377,
            "changes": 891,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -19,8 +19,8 @@\n \n import numpy as np\n \n-from transformers.testing_utils import require_torch, require_vision, slow\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import AnnotationFormatTestMixin, ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -33,6 +33,9 @@\n \n     from transformers import DetrImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import DetrImageProcessorFast\n+\n \n class DetrImageProcessingTester(unittest.TestCase):\n     def __init__(\n@@ -51,6 +54,7 @@ def __init__(\n         image_std=[0.5, 0.5, 0.5],\n         do_pad=True,\n     ):\n+        super().__init__()\n         # by setting size[\"longest_edge\"] > max_resolution we're effectively not testing this :p\n         size = size if size is not None else {\"shortest_edge\": 18, \"longest_edge\": 1333}\n         self.parent = parent\n@@ -132,6 +136,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class DetrImageProcessingTest(AnnotationFormatTestMixin, ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = DetrImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = DetrImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -142,26 +147,28 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n-        self.assertEqual(image_processor.do_pad, True)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n+            self.assertEqual(image_processor.do_pad, True)\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-        )\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-        self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n+            )\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n+            self.assertEqual(image_processor.do_pad, False)\n \n     def test_should_raise_if_annotation_format_invalid(self):\n         image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n@@ -178,12 +185,13 @@ def test_should_raise_if_annotation_format_invalid(self):\n         }\n \n         image_processor_params = {**image_processor_dict, **{\"format\": \"_INVALID_FORMAT_\"}}\n-        image_processor = self.image_processing_class(**image_processor_params)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**image_processor_params)\n \n-        with self.assertRaises(ValueError) as e:\n-            image_processor(**params)\n+            with self.assertRaises(ValueError) as e:\n+                image_processor(**params)\n \n-        self.assertTrue(str(e.exception).startswith(\"_INVALID_FORMAT_ is not a valid AnnotationFormat\"))\n+                self.assertTrue(str(e.exception).startswith(\"_INVALID_FORMAT_ is not a valid AnnotationFormat\"))\n \n     def test_valid_coco_detection_annotations(self):\n         # prepare image and target\n@@ -193,32 +201,33 @@ def test_valid_coco_detection_annotations(self):\n \n         params = {\"image_id\": 39769, \"annotations\": target}\n \n-        # encode them\n-        image_processing = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class.from_pretrained(\"facebook/detr-resnet-50\")\n \n-        # legal encodings (single image)\n-        _ = image_processing(images=image, annotations=params, return_tensors=\"pt\")\n-        _ = image_processing(images=image, annotations=[params], return_tensors=\"pt\")\n+            # legal encodings (single image)\n+            _ = image_processing(images=image, annotations=params, return_tensors=\"pt\")\n+            _ = image_processing(images=image, annotations=[params], return_tensors=\"pt\")\n \n-        # legal encodings (batch of one image)\n-        _ = image_processing(images=[image], annotations=params, return_tensors=\"pt\")\n-        _ = image_processing(images=[image], annotations=[params], return_tensors=\"pt\")\n+            # legal encodings (batch of one image)\n+            _ = image_processing(images=[image], annotations=params, return_tensors=\"pt\")\n+            _ = image_processing(images=[image], annotations=[params], return_tensors=\"pt\")\n \n-        # legal encoding (batch of more than one image)\n-        n = 5\n-        _ = image_processing(images=[image] * n, annotations=[params] * n, return_tensors=\"pt\")\n+            # legal encoding (batch of more than one image)\n+            n = 5\n+            _ = image_processing(images=[image] * n, annotations=[params] * n, return_tensors=\"pt\")\n \n-        # example of an illegal encoding (missing the 'image_id' key)\n-        with self.assertRaises(ValueError) as e:\n-            image_processing(images=image, annotations={\"annotations\": target}, return_tensors=\"pt\")\n+            # example of an illegal encoding (missing the 'image_id' key)\n+            with self.assertRaises(ValueError) as e:\n+                image_processing(images=image, annotations={\"annotations\": target}, return_tensors=\"pt\")\n \n-        self.assertTrue(str(e.exception).startswith(\"Invalid COCO detection annotations\"))\n+            self.assertTrue(str(e.exception).startswith(\"Invalid COCO detection annotations\"))\n \n-        # example of an illegal encoding (unequal lengths of images and annotations)\n-        with self.assertRaises(ValueError) as e:\n-            image_processing(images=[image] * n, annotations=[params] * (n - 1), return_tensors=\"pt\")\n+            # example of an illegal encoding (unequal lengths of images and annotations)\n+            with self.assertRaises(ValueError) as e:\n+                image_processing(images=[image] * n, annotations=[params] * (n - 1), return_tensors=\"pt\")\n \n-        self.assertTrue(str(e.exception) == \"The number of images (5) and annotations (4) do not match.\")\n+            self.assertTrue(str(e.exception) == \"The number of images (5) and annotations (4) do not match.\")\n \n     @slow\n     def test_call_pytorch_with_coco_detection_annotations(self):\n@@ -229,40 +238,41 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n \n         target = {\"image_id\": 39769, \"annotations\": target}\n \n-        # encode them\n-        image_processing = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n-        encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1066])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n-\n-        # verify area\n-        expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n-        # verify size\n-        expected_size = torch.tensor([800, 1066])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class.from_pretrained(\"facebook/detr-resnet-50\")\n+            encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n+\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1066])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+\n+            # verify area\n+            expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            # verify size\n+            expected_size = torch.tensor([800, 1066])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n \n     @slow\n     def test_call_pytorch_with_coco_panoptic_annotations(self):\n@@ -275,43 +285,45 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n \n         masks_path = pathlib.Path(\"./tests/fixtures/tests_samples/COCO/coco_panoptic\")\n \n-        # encode them\n-        image_processing = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n-        encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1066])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n-\n-        # verify area\n-        expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n-        # verify masks\n-        expected_masks_sum = 822873\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].sum().item(), expected_masks_sum)\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n-        # verify size\n-        expected_size = torch.tensor([800, 1066])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n+            encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n+\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1066])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+\n+            # verify area\n+            expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            # verify masks\n+            expected_masks_sum = 822873\n+            relative_error = torch.abs(encoding[\"labels\"][0][\"masks\"].sum() - expected_masks_sum) / expected_masks_sum\n+            self.assertTrue(relative_error < 1e-3)\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            # verify size\n+            expected_size = torch.tensor([800, 1066])\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n \n     @slow\n     def test_batched_coco_detection_annotations(self):\n@@ -340,96 +352,97 @@ def test_batched_coco_detection_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotations_0, annotations_1]\n \n-        image_processing = DetrImageProcessor()\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            return_tensors=\"pt\",  # do_convert_annotations=True\n-        )\n-\n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1066\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.6879, 0.4609, 0.0755, 0.3691],\n-                [0.2118, 0.3359, 0.2601, 0.1566],\n-                [0.5011, 0.5000, 0.9979, 1.0000],\n-                [0.5010, 0.5020, 0.9979, 0.9959],\n-                [0.3284, 0.5944, 0.5884, 0.8112],\n-                [0.8394, 0.5445, 0.3213, 0.9110],\n-            ]\n-        )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.4130, 0.2765, 0.0453, 0.2215],\n-                [0.1272, 0.2016, 0.1561, 0.0940],\n-                [0.3757, 0.4933, 0.7488, 0.9865],\n-                [0.3759, 0.5002, 0.7492, 0.9955],\n-                [0.1971, 0.5456, 0.3532, 0.8646],\n-                [0.5790, 0.4115, 0.3430, 0.7161],\n-            ]\n-        )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class()\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                return_tensors=\"pt\",  # do_convert_annotations=True\n+            )\n+\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1066\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.6879, 0.4609, 0.0755, 0.3691],\n+                    [0.2118, 0.3359, 0.2601, 0.1566],\n+                    [0.5011, 0.5000, 0.9979, 1.0000],\n+                    [0.5010, 0.5020, 0.9979, 0.9959],\n+                    [0.3284, 0.5944, 0.5884, 0.8112],\n+                    [0.8394, 0.5445, 0.3213, 0.9110],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.4130, 0.2765, 0.0453, 0.2215],\n+                    [0.1272, 0.2016, 0.1561, 0.0940],\n+                    [0.3757, 0.4933, 0.7488, 0.9865],\n+                    [0.3759, 0.5002, 0.7492, 0.9955],\n+                    [0.1971, 0.5456, 0.3532, 0.8646],\n+                    [0.5790, 0.4115, 0.3430, 0.7161],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n \n     def test_batched_coco_panoptic_annotations(self):\n         # prepare image, target and masks_path\n@@ -459,194 +472,318 @@ def test_batched_coco_panoptic_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotation_0, annotation_1]\n \n-        # encode them\n-        image_processing = DetrImageProcessor(format=\"coco_panoptic\")\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_tensors=\"pt\",\n-            return_segmentation_masks=True,\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class(format=\"coco_panoptic\")\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_tensors=\"pt\",\n+                return_segmentation_masks=True,\n+            )\n+\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1066\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.2625, 0.5437, 0.4688, 0.8625],\n+                    [0.7719, 0.4104, 0.4531, 0.7125],\n+                    [0.5000, 0.4927, 0.9969, 0.9854],\n+                    [0.1688, 0.2000, 0.2063, 0.0917],\n+                    [0.5492, 0.2760, 0.0578, 0.2187],\n+                    [0.4992, 0.4990, 0.9984, 0.9979],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.1576, 0.3262, 0.2814, 0.5175],\n+                    [0.4634, 0.2463, 0.2720, 0.4275],\n+                    [0.3002, 0.2956, 0.5985, 0.5913],\n+                    [0.1013, 0.1200, 0.1238, 0.0550],\n+                    [0.3297, 0.1656, 0.0347, 0.1312],\n+                    [0.2997, 0.2994, 0.5994, 0.5987],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n \n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1066\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.2625, 0.5437, 0.4688, 0.8625],\n-                [0.7719, 0.4104, 0.4531, 0.7125],\n-                [0.5000, 0.4927, 0.9969, 0.9854],\n-                [0.1688, 0.2000, 0.2063, 0.0917],\n-                [0.5492, 0.2760, 0.0578, 0.2187],\n-                [0.4992, 0.4990, 0.9984, 0.9979],\n-            ]\n+    def test_max_width_max_height_resizing_and_pad_strategy(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n+\n+            # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n+\n+            # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+\n+            # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n+\n+            # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 301, \"width\": 101},\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n+\n+            ### Check for batch\n+            image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n+\n+            # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 150, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 150, \"width\": 100},\n+            )\n+            inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n+\n+    def test_longest_edge_shortest_edge_resizing_strategy(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_1 = torch.ones([958, 653, 3], dtype=torch.uint8)\n+\n+            # max size is set; width < height;\n+            # do_pad=False, longest_edge=640, shortest_edge=640, image=958x653 -> 640x436\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 640, \"shortest_edge\": 640},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 640, 436]))\n+\n+            image_2 = torch.ones([653, 958, 3], dtype=torch.uint8)\n+            # max size is set; height < width;\n+            # do_pad=False, longest_edge=640, shortest_edge=640, image=653x958 -> 436x640\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 640, \"shortest_edge\": 640},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_2], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 436, 640]))\n+\n+            image_3 = torch.ones([100, 120, 3], dtype=torch.uint8)\n+            # max size is set; width == size; height > max_size;\n+            # do_pad=False, longest_edge=118, shortest_edge=100, image=120x100 -> 118x98\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 118, \"shortest_edge\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_3], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 98, 118]))\n+\n+            image_4 = torch.ones([128, 50, 3], dtype=torch.uint8)\n+            # max size is set; height == size; width < max_size;\n+            # do_pad=False, longest_edge=256, shortest_edge=50, image=50x128 -> 50x128\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 256, \"shortest_edge\": 50},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_4], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 128, 50]))\n+\n+            image_5 = torch.ones([50, 50, 3], dtype=torch.uint8)\n+            # max size is set; height == width; width < max_size;\n+            # do_pad=False, longest_edge=117, shortest_edge=50, image=50x50 -> 50x50\n+            image_processor = image_processing_class(\n+                size={\"longest_edge\": 117, \"shortest_edge\": 50},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_5], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 50, 50]))\n+\n+    @slow\n+    @require_torch_gpu\n+    def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n+        # prepare image and target\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        with open(\"./tests/fixtures/tests_samples/COCO/coco_annotations.txt\", \"r\") as f:\n+            target = json.loads(f.read())\n+\n+        target = {\"image_id\": 39769, \"annotations\": target}\n+\n+        processor = self.image_processor_list[1].from_pretrained(\"facebook/detr-resnet-50\")\n+        # 1. run processor on CPU\n+        encoding_cpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cpu\")\n+        # 2. run processor on GPU\n+        encoding_gpu = processor(images=image, annotations=target, return_tensors=\"pt\", device=\"cuda\")\n+\n+        # verify pixel values\n+        self.assertEqual(encoding_cpu[\"pixel_values\"].shape, encoding_gpu[\"pixel_values\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"pixel_values\"][0, 0, 0, :3],\n+                encoding_gpu[\"pixel_values\"][0, 0, 0, :3].to(\"cpu\"),\n+                atol=1e-4,\n+            )\n         )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.1576, 0.3262, 0.2814, 0.5175],\n-                [0.4634, 0.2463, 0.2720, 0.4275],\n-                [0.3002, 0.2956, 0.5985, 0.5913],\n-                [0.1013, 0.1200, 0.1238, 0.0550],\n-                [0.3297, 0.1656, 0.0347, 0.1312],\n-                [0.2997, 0.2994, 0.5994, 0.5987],\n-            ]\n+        # verify area\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        # verify boxes\n+        self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"boxes\"][0], encoding_gpu[\"labels\"][0][\"boxes\"][0].to(\"cpu\"), atol=1e-3\n+            )\n         )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n+        # verify image_id\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n         )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        # verify is_crowd\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n+        )\n+        # verify class_labels\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"class_labels\"], encoding_gpu[\"labels\"][0][\"class_labels\"].to(\"cpu\")\n+            )\n+        )\n+        # verify orig_size\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n+        )\n+        # verify size\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))\n \n-    def test_max_width_max_height_resizing_and_pad_strategy(self):\n-        image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n+    @slow\n+    @require_torch_gpu\n+    def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n+        # prepare image, target and masks_path\n+        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+        with open(\"./tests/fixtures/tests_samples/COCO/coco_panoptic_annotations.txt\", \"r\") as f:\n+            target = json.loads(f.read())\n \n-        # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n-        image_processor = DetrImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n+        target = {\"file_name\": \"000000039769.png\", \"image_id\": 39769, \"segments_info\": target}\n \n-        # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n-        image_processor = DetrImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+        masks_path = pathlib.Path(\"./tests/fixtures/tests_samples/COCO/coco_panoptic\")\n \n-        # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n-        image_processor = DetrImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n+        processor = self.image_processor_list[1].from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n+        # 1. run processor on CPU\n+        encoding_cpu = processor(\n+            images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=\"cpu\"\n         )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n-\n-        # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n-        image_processor = DetrImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 301, \"width\": 101},\n+        # 2. run processor on GPU\n+        encoding_gpu = processor(\n+            images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\", device=\"cuda\"\n         )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n \n-        ### Check for batch\n-        image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n-\n-        # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n-        image_processor = DetrImageProcessor(\n-            size={\"max_height\": 150, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 150, \"width\": 100},\n+        # verify pixel values\n+        self.assertEqual(encoding_cpu[\"pixel_values\"].shape, encoding_gpu[\"pixel_values\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"pixel_values\"][0, 0, 0, :3],\n+                encoding_gpu[\"pixel_values\"][0, 0, 0, :3].to(\"cpu\"),\n+                atol=1e-4,\n+            )\n         )\n-        inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n-\n-    def test_longest_edge_shortest_edge_resizing_strategy(self):\n-        image_1 = torch.ones([958, 653, 3], dtype=torch.uint8)\n-\n-        # max size is set; width < height;\n-        # do_pad=False, longest_edge=640, shortest_edge=640, image=958x653 -> 640x436\n-        image_processor = DetrImageProcessor(\n-            size={\"longest_edge\": 640, \"shortest_edge\": 640},\n-            do_pad=False,\n+        # verify area\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        # verify boxes\n+        self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"boxes\"][0], encoding_gpu[\"labels\"][0][\"boxes\"][0].to(\"cpu\"), atol=1e-3\n+            )\n         )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 640, 436]))\n-\n-        image_2 = torch.ones([653, 958, 3], dtype=torch.uint8)\n-        # max size is set; height < width;\n-        # do_pad=False, longest_edge=640, shortest_edge=640, image=653x958 -> 436x640\n-        image_processor = DetrImageProcessor(\n-            size={\"longest_edge\": 640, \"shortest_edge\": 640},\n-            do_pad=False,\n+        # verify image_id\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n         )\n-        inputs = image_processor(images=[image_2], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 436, 640]))\n-\n-        image_3 = torch.ones([100, 120, 3], dtype=torch.uint8)\n-        # max size is set; width == size; height > max_size;\n-        # do_pad=False, longest_edge=118, shortest_edge=100, image=120x100 -> 118x98\n-        image_processor = DetrImageProcessor(\n-            size={\"longest_edge\": 118, \"shortest_edge\": 100},\n-            do_pad=False,\n+        # verify is_crowd\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n         )\n-        inputs = image_processor(images=[image_3], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 98, 118]))\n-\n-        image_4 = torch.ones([128, 50, 3], dtype=torch.uint8)\n-        # max size is set; height == size; width < max_size;\n-        # do_pad=False, longest_edge=256, shortest_edge=50, image=50x128 -> 50x128\n-        image_processor = DetrImageProcessor(\n-            size={\"longest_edge\": 256, \"shortest_edge\": 50},\n-            do_pad=False,\n+        # verify class_labels\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_cpu[\"labels\"][0][\"class_labels\"], encoding_gpu[\"labels\"][0][\"class_labels\"].to(\"cpu\")\n+            )\n         )\n-        inputs = image_processor(images=[image_4], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 128, 50]))\n-\n-        image_5 = torch.ones([50, 50, 3], dtype=torch.uint8)\n-        # max size is set; height == width; width < max_size;\n-        # do_pad=False, longest_edge=117, shortest_edge=50, image=50x50 -> 50x50\n-        image_processor = DetrImageProcessor(\n-            size={\"longest_edge\": 117, \"shortest_edge\": 50},\n-            do_pad=False,\n+        # verify masks\n+        masks_sum_cpu = encoding_cpu[\"labels\"][0][\"masks\"].sum()\n+        masks_sum_gpu = encoding_gpu[\"labels\"][0][\"masks\"].sum()\n+        relative_error = torch.abs(masks_sum_cpu - masks_sum_gpu) / masks_sum_cpu\n+        self.assertTrue(relative_error < 1e-3)\n+        # verify orig_size\n+        self.assertTrue(\n+            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n         )\n-        inputs = image_processor(images=[image_5], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 50, 50]))\n+        # verify size\n+        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))"
        },
        {
            "sha": "fc622ead7a711b1c6610c9724578f641abefec10",
            "filename": "tests/models/grounding_dino/test_image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 230,
            "deletions": 227,
            "changes": 457,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -269,96 +269,97 @@ def test_batched_coco_detection_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotations_0, annotations_1]\n \n-        image_processing = GroundingDinoImageProcessor()\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            return_tensors=\"pt\",  # do_convert_annotations=True\n-        )\n-\n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1066\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.6879, 0.4609, 0.0755, 0.3691],\n-                [0.2118, 0.3359, 0.2601, 0.1566],\n-                [0.5011, 0.5000, 0.9979, 1.0000],\n-                [0.5010, 0.5020, 0.9979, 0.9959],\n-                [0.3284, 0.5944, 0.5884, 0.8112],\n-                [0.8394, 0.5445, 0.3213, 0.9110],\n-            ]\n-        )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.4130, 0.2765, 0.0453, 0.2215],\n-                [0.1272, 0.2016, 0.1561, 0.0940],\n-                [0.3757, 0.4933, 0.7488, 0.9865],\n-                [0.3759, 0.5002, 0.7492, 0.9955],\n-                [0.1971, 0.5456, 0.3532, 0.8646],\n-                [0.5790, 0.4115, 0.3430, 0.7161],\n-            ]\n-        )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class()\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                return_tensors=\"pt\",  # do_convert_annotations=True\n+            )\n+\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1066\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.6879, 0.4609, 0.0755, 0.3691],\n+                    [0.2118, 0.3359, 0.2601, 0.1566],\n+                    [0.5011, 0.5000, 0.9979, 1.0000],\n+                    [0.5010, 0.5020, 0.9979, 0.9959],\n+                    [0.3284, 0.5944, 0.5884, 0.8112],\n+                    [0.8394, 0.5445, 0.3213, 0.9110],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.4130, 0.2765, 0.0453, 0.2215],\n+                    [0.1272, 0.2016, 0.1561, 0.0940],\n+                    [0.3757, 0.4933, 0.7488, 0.9865],\n+                    [0.3759, 0.5002, 0.7492, 0.9955],\n+                    [0.1971, 0.5456, 0.3532, 0.8646],\n+                    [0.5790, 0.4115, 0.3430, 0.7161],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n \n     @slow\n     # Copied from tests.models.deformable_detr.test_image_processing_deformable_detr.DeformableDetrImageProcessingTest.test_call_pytorch_with_coco_panoptic_annotations with DeformableDetr->GroundingDino\n@@ -440,146 +441,148 @@ def test_batched_coco_panoptic_annotations(self):\n         images = [image_0, image_1]\n         annotations = [annotation_0, annotation_1]\n \n-        # encode them\n-        image_processing = GroundingDinoImageProcessor(format=\"coco_panoptic\")\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_tensors=\"pt\",\n-            return_segmentation_masks=True,\n-        )\n-\n-        # Check the pixel values have been padded\n-        postprocessed_height, postprocessed_width = 800, 1066\n-        expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        # Check the bounding boxes have been adjusted for padded images\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        expected_boxes_0 = torch.tensor(\n-            [\n-                [0.2625, 0.5437, 0.4688, 0.8625],\n-                [0.7719, 0.4104, 0.4531, 0.7125],\n-                [0.5000, 0.4927, 0.9969, 0.9854],\n-                [0.1688, 0.2000, 0.2063, 0.0917],\n-                [0.5492, 0.2760, 0.0578, 0.2187],\n-                [0.4992, 0.4990, 0.9984, 0.9979],\n-            ]\n-        )\n-        expected_boxes_1 = torch.tensor(\n-            [\n-                [0.1576, 0.3262, 0.2814, 0.5175],\n-                [0.4634, 0.2463, 0.2720, 0.4275],\n-                [0.3002, 0.2956, 0.5985, 0.5913],\n-                [0.1013, 0.1200, 0.1238, 0.0550],\n-                [0.3297, 0.1656, 0.0347, 0.1312],\n-                [0.2997, 0.2994, 0.5994, 0.5987],\n-            ]\n-        )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n-\n-        # Check the masks have also been padded\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-        self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n-\n-        # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n-        # format and not in the range [0, 1]\n-        encoding = image_processing(\n-            images=images,\n-            annotations=annotations,\n-            masks_path=masks_path,\n-            return_segmentation_masks=True,\n-            do_convert_annotations=False,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n-        self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n-        # Convert to absolute coordinates\n-        unnormalized_boxes_0 = torch.vstack(\n-            [\n-                expected_boxes_0[:, 0] * postprocessed_width,\n-                expected_boxes_0[:, 1] * postprocessed_height,\n-                expected_boxes_0[:, 2] * postprocessed_width,\n-                expected_boxes_0[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        unnormalized_boxes_1 = torch.vstack(\n-            [\n-                expected_boxes_1[:, 0] * postprocessed_width,\n-                expected_boxes_1[:, 1] * postprocessed_height,\n-                expected_boxes_1[:, 2] * postprocessed_width,\n-                expected_boxes_1[:, 3] * postprocessed_height,\n-            ]\n-        ).T\n-        # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n-        expected_boxes_0 = torch.vstack(\n-            [\n-                unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n-                unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n-                unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n-            ]\n-        ).T\n-        expected_boxes_1 = torch.vstack(\n-            [\n-                unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n-                unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n-                unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n-            ]\n-        ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class(format=\"coco_panoptic\")\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_tensors=\"pt\",\n+                return_segmentation_masks=True,\n+            )\n+\n+            # Check the pixel values have been padded\n+            postprocessed_height, postprocessed_width = 800, 1066\n+            expected_shape = torch.Size([2, 3, postprocessed_height, postprocessed_width])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            # Check the bounding boxes have been adjusted for padded images\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            expected_boxes_0 = torch.tensor(\n+                [\n+                    [0.2625, 0.5437, 0.4688, 0.8625],\n+                    [0.7719, 0.4104, 0.4531, 0.7125],\n+                    [0.5000, 0.4927, 0.9969, 0.9854],\n+                    [0.1688, 0.2000, 0.2063, 0.0917],\n+                    [0.5492, 0.2760, 0.0578, 0.2187],\n+                    [0.4992, 0.4990, 0.9984, 0.9979],\n+                ]\n+            )\n+            expected_boxes_1 = torch.tensor(\n+                [\n+                    [0.1576, 0.3262, 0.2814, 0.5175],\n+                    [0.4634, 0.2463, 0.2720, 0.4275],\n+                    [0.3002, 0.2956, 0.5985, 0.5913],\n+                    [0.1013, 0.1200, 0.1238, 0.0550],\n+                    [0.3297, 0.1656, 0.0347, 0.1312],\n+                    [0.2997, 0.2994, 0.5994, 0.5987],\n+                ]\n+            )\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+\n+            # Check the masks have also been padded\n+            self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+            self.assertEqual(encoding[\"labels\"][1][\"masks\"].shape, torch.Size([6, 800, 1066]))\n+\n+            # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n+            # format and not in the range [0, 1]\n+            encoding = image_processing(\n+                images=images,\n+                annotations=annotations,\n+                masks_path=masks_path,\n+                return_segmentation_masks=True,\n+                do_convert_annotations=False,\n+                return_tensors=\"pt\",\n+            )\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, torch.Size([6, 4]))\n+            self.assertEqual(encoding[\"labels\"][1][\"boxes\"].shape, torch.Size([6, 4]))\n+            # Convert to absolute coordinates\n+            unnormalized_boxes_0 = torch.vstack(\n+                [\n+                    expected_boxes_0[:, 0] * postprocessed_width,\n+                    expected_boxes_0[:, 1] * postprocessed_height,\n+                    expected_boxes_0[:, 2] * postprocessed_width,\n+                    expected_boxes_0[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            unnormalized_boxes_1 = torch.vstack(\n+                [\n+                    expected_boxes_1[:, 0] * postprocessed_width,\n+                    expected_boxes_1[:, 1] * postprocessed_height,\n+                    expected_boxes_1[:, 2] * postprocessed_width,\n+                    expected_boxes_1[:, 3] * postprocessed_height,\n+                ]\n+            ).T\n+            # Convert from centre_x, centre_y, width, height to x_min, y_min, x_max, y_max\n+            expected_boxes_0 = torch.vstack(\n+                [\n+                    unnormalized_boxes_0[:, 0] - unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] - unnormalized_boxes_0[:, 3] / 2,\n+                    unnormalized_boxes_0[:, 0] + unnormalized_boxes_0[:, 2] / 2,\n+                    unnormalized_boxes_0[:, 1] + unnormalized_boxes_0[:, 3] / 2,\n+                ]\n+            ).T\n+            expected_boxes_1 = torch.vstack(\n+                [\n+                    unnormalized_boxes_1[:, 0] - unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] - unnormalized_boxes_1[:, 3] / 2,\n+                    unnormalized_boxes_1[:, 0] + unnormalized_boxes_1[:, 2] / 2,\n+                    unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n+                ]\n+            ).T\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n+            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_max_width_max_height_resizing_and_pad_strategy with Detr->GroundingDino\n     def test_max_width_max_height_resizing_and_pad_strategy(self):\n-        image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n-\n-        # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n-        image_processor = GroundingDinoImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n-\n-        # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n-        image_processor = GroundingDinoImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-\n-        # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n-        image_processor = GroundingDinoImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n-\n-        # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n-        image_processor = GroundingDinoImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 301, \"width\": 101},\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n-\n-        ### Check for batch\n-        image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n-\n-        # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n-        image_processor = GroundingDinoImageProcessor(\n-            size={\"max_height\": 150, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 150, \"width\": 100},\n-        )\n-        inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n+        for image_processing_class in self.image_processor_list:\n+            image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n+\n+            # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n+\n+            # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+\n+            # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n+\n+            # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 301, \"width\": 101},\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n+\n+            ### Check for batch\n+            image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n+\n+            # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 150, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 150, \"width\": 100},\n+            )\n+            inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n \n     def test_longest_edge_shortest_edge_resizing_strategy(self):\n         image_1 = torch.ones([958, 653, 3], dtype=torch.uint8)"
        },
        {
            "sha": "67508532e9c8290f56612df0beafdde99953bf91",
            "filename": "tests/models/yolos/test_image_processing_yolos.py",
            "status": "modified",
            "additions": 45,
            "deletions": 44,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -553,47 +553,48 @@ def test_batched_coco_panoptic_annotations(self):\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_max_width_max_height_resizing_and_pad_strategy with Detr->Yolos\n     def test_max_width_max_height_resizing_and_pad_strategy(self):\n-        image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n-\n-        # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n-        image_processor = YolosImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n-\n-        # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n-        image_processor = YolosImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=False,\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-\n-        # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n-        image_processor = YolosImageProcessor(\n-            size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n-\n-        # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n-        image_processor = YolosImageProcessor(\n-            size={\"max_height\": 300, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 301, \"width\": 101},\n-        )\n-        inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n-\n-        ### Check for batch\n-        image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n-\n-        # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n-        image_processor = YolosImageProcessor(\n-            size={\"max_height\": 150, \"max_width\": 100},\n-            do_pad=True,\n-            pad_size={\"height\": 150, \"width\": 100},\n-        )\n-        inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n-        self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))\n+        for image_processing_class in self.image_processor_list:\n+            image_1 = torch.ones([200, 100, 3], dtype=torch.uint8)\n+\n+            # do_pad=False, max_height=100, max_width=100, image=200x100 -> 100x50\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 50]))\n+\n+            # do_pad=False, max_height=300, max_width=100, image=200x100 -> 200x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=False,\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+\n+            # do_pad=True, max_height=100, max_width=100, image=200x100 -> 100x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 100, \"max_width\": 100}, do_pad=True, pad_size={\"height\": 100, \"width\": 100}\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 100, 100]))\n+\n+            # do_pad=True, max_height=300, max_width=100, image=200x100 -> 300x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 300, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 301, \"width\": 101},\n+            )\n+            inputs = image_processor(images=[image_1], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([1, 3, 301, 101]))\n+\n+            ### Check for batch\n+            image_2 = torch.ones([100, 150, 3], dtype=torch.uint8)\n+\n+            # do_pad=True, max_height=150, max_width=100, images=[200x100, 100x150] -> 150x100\n+            image_processor = image_processing_class(\n+                size={\"max_height\": 150, \"max_width\": 100},\n+                do_pad=True,\n+                pad_size={\"height\": 150, \"width\": 100},\n+            )\n+            inputs = image_processor(images=[image_1, image_2], return_tensors=\"pt\")\n+            self.assertEqual(inputs[\"pixel_values\"].shape, torch.Size([2, 3, 150, 100]))"
        },
        {
            "sha": "7d89b43ce35ba4b59b76203aec6cf04a1b5aef29",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4122813d1309385f303f60b59a1f9944fada761/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4122813d1309385f303f60b59a1f9944fada761/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=a4122813d1309385f303f60b59a1f9944fada761",
            "patch": "@@ -191,7 +191,7 @@ def measure_time(image_processor, image):\n \n         dummy_images = torch.randint(0, 255, (4, 3, 224, 224), dtype=torch.uint8)\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class()\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n \n         fast_time = measure_time(image_processor_fast, dummy_images)\n         slow_time = measure_time(image_processor_slow, dummy_images)"
        }
    ],
    "stats": {
        "total": 3948,
        "additions": 2840,
        "deletions": 1108
    }
}