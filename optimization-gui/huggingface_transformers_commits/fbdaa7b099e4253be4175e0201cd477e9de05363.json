{
    "author": "yaswanth19",
    "message": "Add Aimv2 model (#36625)\n\n* Model skelton\n\n* changes\n\n* temp push\n\n* changes\n\n* Added support for aimv2-native\n\n* More changes\n\n* More changes\n\n* Stupid mistake correction\n\n* Added config and refactor\n\n* Added vison model\n\n* update\n\n* Refactor for lit variant\n\n* Added Text Model\n\n* Minor fixes\n\n* nits\n\n* update\n\n* Preliminary tests\n\n* More fixes\n\n* Updated tests ðŸ¤—\n\n* Refactor\n\n* Updated testcase\n\n* Updated config\n\n* make fixup\n\n* more fixes\n\n* Bug fix and updates\n\n* deadcode\n\n* Fixes\n\n* nit\n\n* up\n\n* Happy CI âœ…\n\n* Reduce LOC\n\n* nit\n\n* nit\n\n* make style\n\n* return_dict refactor\n\n* bug fix\n\n* fix\n\n* doc update\n\n* nit\n\n* make fixup\n\n* Minor update\n\n* _init_weigths modifcation\n\n* update tests\n\n* Minor fixes post review\n\n* Update w.r.t GradientCheckpointingLayer\n\n* docs update\n\n* update\n\n* nit\n\n* Use more Modular ðŸ˜‰\n\n* Change name from AIMv2 to Aimv2\n\n* Nit\n\n* make style\n\n* Add model doc pointer\n\n* make style\n\n* Update model doc section\n\n* updates\n\n* Modify attn mask and interface\n\n* update test\n\n* Final change\n\n* Utilize flash and flex attn\n\n* keep attn mask\n\n* camelcase model name in test file\n\n* Fix docstring\n\n* Fix config warning finally and create_causal_mask\n\n* disable torchscript\n\n* remove unused arg\n\n* remove from tests\n\n* balance model size for tests\n\n* fix device\n\n* tests\n\n* tests\n\n* flaky test\n\n* fix import\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "fbdaa7b099e4253be4175e0201cd477e9de05363",
    "files": [
        {
            "sha": "2675a722232365bf5f72c7900e02329e5dec33c2",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -691,6 +691,8 @@\n         title: Zamba2\n       title: Text models\n     - sections:\n+      - local: model_doc/aimv2\n+        title: Aimv2\n       - local: model_doc/beit\n         title: BEiT\n       - local: model_doc/bit"
        },
        {
            "sha": "1c05c5068d75a13927d8e6bcf6246d5b4114ed0c",
            "filename": "docs/source/en/model_doc/aimv2.md",
            "status": "added",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/docs%2Fsource%2Fen%2Fmodel_doc%2Faimv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/docs%2Fsource%2Fen%2Fmodel_doc%2Faimv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faimv2.md?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -0,0 +1,104 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# AIMv2\n+\n+## Overview\n+\n+The AIMv2 model was proposed in [Multimodal Autoregressive Pre-training of Large Vision Encoders](https://arxiv.org/abs/2411.14402) by Enrico Fini, Mustafa Shukor, Xiujun Li, Philipp Dufter, Michal Klein, David Haldimann, Sai Aitharaju, Victor Guilherme Turrisi da Costa, Louis BÃ©thune, Zhe Gan, Alexander T Toshev, Marcin Eichner, Moin Nabi, Yinfei Yang, Joshua M. Susskind, Alaaeldin El-Nouby.\n+\n+The abstract from the paper is the following:\n+\n+*We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Our encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Furthermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal image understanding across diverse settings.*\n+\n+\n+This model was contributed by [Yaswanth Gali](https://huggingface.co/yaswanthgali).\n+The original code can be found [here](https://github.com/apple/ml-aim).\n+\n+## Usage Example\n+\n+Here is an example of Image Feature Extraction using specific checkpoints on resized images and native resolution images:\n+\n+```python\n+import requests\n+from PIL import Image\n+from transformers import AutoImageProcessor, AutoModel\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+processor = AutoImageProcessor.from_pretrained(\"apple/aimv2-large-patch14-native\")\n+model = AutoModel.from_pretrained(\"apple/aimv2-large-patch14-native\")\n+\n+inputs = processor(images=image, return_tensors=\"pt\")\n+outputs = model(**inputs)\n+```\n+\n+Here is an example of a checkpoint performing zero-shot classification:\n+\n+```python\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModel\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+text = [\"Picture of a dog.\", \"Picture of a cat.\", \"Picture of a horse.\"]\n+\n+processor = AutoProcessor.from_pretrained(\"apple/aimv2-large-patch14-224-lit\")\n+model = AutoModel.from_pretrained(\"apple/aimv2-large-patch14-224-lit\")\n+\n+inputs = processor(\n+    images=image,\n+    text=text,\n+    add_special_tokens=True,\n+    truncation=True,\n+    padding=True,\n+    return_tensors=\"pt\",\n+)\n+outputs = model(**inputs)\n+probs = outputs.logits_per_image.softmax(dim=-1)\n+```\n+\n+## Aimv2Config\n+\n+[[autodoc]] Aimv2Config\n+\n+## Aimv2TextConfig\n+\n+[[autodoc]] Aimv2TextConfig\n+\n+## Aimv2VisionConfig\n+\n+[[autodoc]] Aimv2VisionConfig\n+\n+## Aimv2Model\n+\n+[[autodoc]] Aimv2Model\n+    - forward\n+\n+## Aimv2VisionModel\n+\n+[[autodoc]] Aimv2VisionModel\n+    - forward\n+\n+## Aimv2TextModel\n+\n+[[autodoc]] Aimv2TextModel\n+    - forward\n+\n+</pt>\n+<tf>"
        },
        {
            "sha": "98b5a002e684a3ae3c77777ae9d330db9a2e60eb",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -18,6 +18,7 @@\n \n \n if TYPE_CHECKING:\n+    from .aimv2 import *\n     from .albert import *\n     from .align import *\n     from .altclip import *"
        },
        {
            "sha": "c013363996040eb995d7924b20e8f86d6b318a83",
            "filename": "src/transformers/models/aimv2/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2F__init__.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_aimv2 import *\n+    from .modeling_aimv2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "bf0064ad9f6186215a265c4d356072ccaf970057",
            "filename": "src/transformers/models/aimv2/configuration_aimv2.py",
            "status": "added",
            "additions": 296,
            "deletions": 0,
            "changes": 296,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -0,0 +1,296 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/aimv2/modular_aimv2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_aimv2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Apple Inc. and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Aimv2VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Aimv2VisionModel`]. It is used to instantiate a\n+    AIMv2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the vision encoder of the AIMv2\n+    [apple/aimv2-large-patch14-224](https://huggingface.co/apple/aimv2-large-patch14-224) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 2816):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input images.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        qkv_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the queries, keys and values.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the Linear layers or Not.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the for initializing all weight matrices.\n+        use_head (`str`, *optional*, defaults to `True`):\n+            Whether to use Attention Pooling Head or Not.\n+        is_native (`str`, *optional*, defaults to `False`):\n+            Whether to use ckpt trained for image native resolution or not.\n+    Example:\n+\n+    ```python\n+    >>> from transformers import SiglipVisionConfig, SiglipVisionModel\n+\n+    >>> # Initializing a Aimv2VisionConfig with apple/aimv2-large-patch14-224 style configuration\n+    >>> configuration = Aimv2VisionConfig()\n+\n+    >>> # Initializing a Aimv2VisionModel (with random weights) from the apple/aimv2-large-patch14-224 style configuration\n+    >>> model = Aimv2VisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"aimv2_vision_model\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size: int = 1024,\n+        intermediate_size: int = 2816,\n+        num_hidden_layers: int = 24,\n+        num_attention_heads: int = 8,\n+        num_channels: int = 3,\n+        image_size: int = 224,\n+        patch_size: int = 14,\n+        rms_norm_eps: float = 1e-5,\n+        attention_dropout: float = 0.0,\n+        qkv_bias: bool = False,\n+        mlp_bias: bool = False,\n+        hidden_act: str = \"silu\",\n+        initializer_range: float = 0.02,\n+        use_head: bool = True,\n+        is_native: bool = False,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.image_size = image_size\n+        self.attention_dropout = attention_dropout\n+        self.hidden_act = hidden_act\n+\n+        self.use_head = use_head\n+        self.initializer_range = initializer_range\n+        self.mlp_bias = mlp_bias\n+        self.qkv_bias = qkv_bias\n+        self.rms_norm_eps = rms_norm_eps\n+        self.is_native = is_native\n+\n+\n+class Aimv2TextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Aimv2TextModel`]. It is used to instantiate a\n+    AIMv2 text encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the text encoder of the AIMv2\n+    [apple/aimv2-large-patch14-224-lit](https://huggingface.co/apple/aimv2-large-patch14-224-lit) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 49408):\n+            Vocabulary size of the AIMv2 text model. Defines the number of different tokens that can be represented by\n+            the `inputs_ids` passed when calling [`Aimv2Model`].\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 6):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        qkv_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the queries, keys and values.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the Linear layers or Not.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            The id of the padding token in the vocabulary.\n+        bos_token_id (`int`, *optional*, defaults to 49406):\n+            The id of the beginning-of-sequence token in the vocabulary.\n+        eos_token_id (`int`, *optional*, defaults to 49407):\n+            The id of the end-of-sequence token in the vocabulary.\n+        max_position_embeddings (`int`, *optional*, defaults to 77):\n+            The maximum sequence length that this model might ever be used with. Typically set this to something large\n+            just in case (e.g., 512 or 1024 or 2048).\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the for initializing all weight matrices.\n+    \"\"\"\n+\n+    model_type = \"aimv2_text_model\"\n+    base_config_key = \"text_config\"\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 49408,\n+        hidden_size: int = 768,\n+        intermediate_size: int = 2048,\n+        num_hidden_layers: int = 12,\n+        num_attention_heads: int = 6,\n+        rms_norm_eps: float = 1e-5,\n+        attention_dropout: float = 0.0,\n+        qkv_bias: bool = False,\n+        mlp_bias: bool = False,\n+        hidden_act: str = \"silu\",\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = None,\n+        eos_token_id: int = 49407,\n+        max_position_embeddings: int = 77,\n+        initializer_range: bool = 0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n+\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_act = hidden_act\n+        self.attention_dropout = attention_dropout\n+\n+        self.initializer_range = initializer_range\n+        self.mlp_bias = mlp_bias\n+        self.qkv_bias = qkv_bias\n+        self.rms_norm_eps = rms_norm_eps\n+\n+\n+class Aimv2Config(PretrainedConfig):\n+    r\"\"\"\n+    [`Aimv2Config`] is the configuration class to store the configuration of a [`Aimv2Model`]. It is used to\n+    instantiate a AIMv2 model according to the specified arguments, defining the text model and vision model configs.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the AIMv2\n+    [apple/aimv2-large-patch14-224-lit](https://huggingface.co/apple/aimv2-large-patch14-224-lit) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`Aimv2TextConfig`].\n+        vision_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`Aimv2VisionConfig`].\n+        projection_dim (`int`, *optional*, defaults to 512):\n+            Dimensionality of text and vision projection layers.\n+        logit_scale_init_value (`float`, *optional*, defaults to 2.6592):\n+            The initial value of the *logit_scale* parameter.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Aimv2Config, Aimv2Model\n+\n+    >>> # Initializing a Aimv2Config with apple/aimv2-large-patch14-224-lit style configuration\n+    >>> configuration = Aimv2Config()\n+\n+    >>> # Initializing a Aimv2Model (with random weights) from the apple/aimv2-large-patch14-224-lit style configuration\n+    >>> model = Aimv2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a Aimv2Config from a Aimv2TextConfig and a Aimv2VisionConfig\n+    >>> from transformers import Aimv2TextConfig, Aimv2VisionConfig\n+\n+    >>> # Initializing a AIMv2Text and AIMv2Vision configuration\n+    >>> config_text = Aimv2TextConfig()\n+    >>> config_vision = Aimv2VisionConfig()\n+\n+    >>> config = Aimv2Config(text_config=config_text, vision_config=config_vision)\n+    ```\"\"\"\n+\n+    model_type = \"aimv2\"\n+    sub_configs = {\"text_config\": Aimv2TextConfig, \"vision_config\": Aimv2VisionConfig}\n+\n+    def __init__(\n+        self, text_config=None, vision_config=None, projection_dim=512, logit_scale_init_value=2.6592, **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if text_config is None:\n+            text_config = {}\n+            logger.info(\"`text_config` is `None`. Initializing the `Aimv2TextConfig` with default values.\")\n+\n+        if vision_config is None:\n+            vision_config = {}\n+            logger.info(\"`vision_config` is `None`. initializing the `Aimv2VisionConfig` with default values.\")\n+\n+        self.text_config = Aimv2TextConfig(**text_config)\n+        self.vision_config = Aimv2VisionConfig(**vision_config)\n+        self.projection_dim = projection_dim\n+        self.logit_scale_init_value = logit_scale_init_value\n+        self.max_logit_scale = 100.0\n+\n+    @classmethod\n+    def from_text_vision_configs(cls, text_config: Aimv2TextConfig, vision_config: Aimv2VisionConfig, **kwargs):\n+        r\"\"\"\n+        Instantiate a [`Aimv2Config`] (or a derived class) from aimv2 text model configuration and aimv2 vision\n+        model configuration.\n+\n+        Returns:\n+            [`Aimv2Config`]: An instance of a configuration object\n+        \"\"\"\n+\n+        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n+\n+\n+__all__ = [\"Aimv2Config\", \"Aimv2VisionConfig\", \"Aimv2TextConfig\"]"
        },
        {
            "sha": "824d6b5138f744e34bdb9500c92f3909b4df1f03",
            "filename": "src/transformers/models/aimv2/convert_aimv2_original_pytorch_to_hf.py",
            "status": "added",
            "additions": 269,
            "deletions": 0,
            "changes": 269,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2Fconvert_aimv2_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2Fconvert_aimv2_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fconvert_aimv2_original_pytorch_to_hf.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -0,0 +1,269 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import gc\n+import os\n+import re\n+from typing import Optional\n+\n+import torch\n+from huggingface_hub import snapshot_download\n+from safetensors import safe_open\n+\n+from transformers import (\n+    Aimv2Config,\n+    Aimv2Model,\n+    Aimv2VisionConfig,\n+    Aimv2VisionModel,\n+    AutoImageProcessor,\n+    AutoProcessor,\n+)\n+\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING_VISION_MODEL = {\n+    # Embeddings\n+    r\"preprocessor.patchifier.proj\": r\"embeddings.patch_embed\",\n+    r\"preprocessor.pos_embed\": r\"embeddings.position_embedding.weight\",\n+    r\"preprocessor.patchifier.norm.weight\": r\"embeddings.rms_norm.weight\",\n+    # Encoder Layers\n+    r\"trunk.blocks.(\\d+).attn.qkv\": r\"encoder.layers.\\1.attention.qkv\",\n+    r\"trunk.blocks.(\\d+).attn.proj\": r\"encoder.layers.\\1.attention.out_proj\",\n+    r\"trunk.blocks.(\\d+).mlp.fc1\": r\"encoder.layers.\\1.ffn.gate_proj\",\n+    r\"trunk.blocks.(\\d+).mlp.fc2\": r\"encoder.layers.\\1.ffn.down_proj\",\n+    r\"trunk.blocks.(\\d+).mlp.fc3\": r\"encoder.layers.\\1.ffn.up_proj\",\n+    # Normalization Layers\n+    r\"trunk.blocks.(\\d+).norm_1\": r\"encoder.layers.\\1.rms_norm1\",\n+    r\"trunk.blocks.(\\d+).norm_2\": r\"encoder.layers.\\1.rms_norm2\",\n+    # Final Norm\n+    r\"trunk.post_trunk_norm\": r\"rms_norm\",\n+}\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # Vision Embeddings\n+    r\"image_encoder.preprocessor.patchifier.proj\": r\"vision_model.embeddings.patch_embed\",\n+    r\"image_encoder.preprocessor.pos_embed\": r\"vision_model.embeddings.position_embedding.weight\",\n+    r\"image_encoder.preprocessor.patchifier.norm.weight\": r\"vision_model.embeddings.rms_norm.weight\",\n+    # Vision Encoder Layers\n+    r\"image_encoder.trunk.blocks.(\\d+).attn.qkv\": r\"vision_model.encoder.layers.\\1.attention.qkv\",\n+    r\"image_encoder.trunk.blocks.(\\d+).attn.proj\": r\"vision_model.encoder.layers.\\1.attention.out_proj\",\n+    r\"image_encoder.trunk.blocks.(\\d+).mlp.fc1\": r\"vision_model.encoder.layers.\\1.ffn.gate_proj\",\n+    r\"image_encoder.trunk.blocks.(\\d+).mlp.fc2\": r\"vision_model.encoder.layers.\\1.ffn.down_proj\",\n+    r\"image_encoder.trunk.blocks.(\\d+).mlp.fc3\": r\"vision_model.encoder.layers.\\1.ffn.up_proj\",\n+    # Normalization Layers\n+    r\"image_encoder.trunk.blocks.(\\d+).norm_1\": r\"vision_model.encoder.layers.\\1.rms_norm1\",\n+    r\"image_encoder.trunk.blocks.(\\d+).norm_2\": r\"vision_model.encoder.layers.\\1.rms_norm2\",\n+    r\"image_encoder.trunk.post_trunk_norm\": r\"vision_model.rms_norm\",\n+    r\"image_projector\": r\"visual_projection\",\n+    # Vision Head\n+    r\"image_encoder.head.cls_token\": r\"vision_model.head.cls_token\",\n+    r\"image_encoder.head.k\": r\"vision_model.head.k_proj\",\n+    r\"image_encoder.head.v\": r\"vision_model.head.v_proj\",\n+    r\"image_encoder.head.linear\": r\"vision_model.head.output_proj\",\n+    # Text Embeddings\n+    r\"text_encoder.preprocessor.text_embedding.weight\": r\"text_model.embeddings.token_embedding.weight\",\n+    r\"text_encoder.preprocessor.positional_embedding\": r\"text_model.embeddings.position_embedding.weight\",\n+    # Text Encoder Layers\n+    r\"text_encoder.trunk.blocks.(\\d+).attn.qkv\": r\"text_model.encoder.layers.\\1.attention.qkv\",\n+    r\"text_encoder.trunk.blocks.(\\d+).attn.proj\": r\"text_model.encoder.layers.\\1.attention.out_proj\",\n+    r\"text_encoder.trunk.blocks.(\\d+).mlp.fc1\": r\"text_model.encoder.layers.\\1.ffn.gate_proj\",\n+    r\"text_encoder.trunk.blocks.(\\d+).mlp.fc2\": r\"text_model.encoder.layers.\\1.ffn.down_proj\",\n+    r\"text_encoder.trunk.blocks.(\\d+).mlp.fc3\": r\"text_model.encoder.layers.\\1.ffn.up_proj\",\n+    # Text Normalization Layers\n+    r\"text_encoder.trunk.blocks.(\\d+).norm_1\": r\"text_model.encoder.layers.\\1.rms_norm1\",\n+    r\"text_encoder.trunk.blocks.(\\d+).norm_2\": r\"text_model.encoder.layers.\\1.rms_norm2\",\n+    r\"text_encoder.trunk.post_trunk_norm\": r\"text_model.rms_norm\",\n+    r\"text_projector\": r\"text_projection\",\n+    r\"log_logit_scale\": r\"logit_scale\",\n+}\n+\n+\n+def load_original_state_dict(model_id: str, revision: Optional[str] = None) -> dict[str, torch.Tensor]:\n+    # Download only the model.safetensors file\n+    directory_path = snapshot_download(\n+        repo_id=model_id,\n+        revision=revision,\n+        allow_patterns=[\"model.safetensors\"],\n+    )\n+\n+    original_state_dict = {}\n+    safetensor_path = f\"{directory_path}/model.safetensors\"\n+\n+    with safe_open(safetensor_path, framework=\"pt\", device=\"cpu\") as f:\n+        for key in f.keys():\n+            original_state_dict[key] = f.get_tensor(key)\n+\n+    return original_state_dict\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict, ORIGINAL_TO_CONVERTED_KEY_MAPPING: dict):\n+    \"\"\"Converts state dict keys from the old format to the new format.\"\"\"\n+\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def split_qkv_tensor(key, tensor):\n+    \"\"\"Splits a qkv tensor into separate q, k, v tensors and updates the key accordingly.\"\"\"\n+\n+    new_keys = [\"q_proj\", \"k_proj\", \"v_proj\"]\n+    split_size = tensor.shape[0] // 3\n+    split_tensors = torch.split(tensor, split_size, dim=0)\n+\n+    return {key.replace(\"qkv\", new_key): split_tensors[i] for i, new_key in enumerate(new_keys)}\n+\n+\n+def get_model_config_mapping(model_id: str):\n+    \"\"\"Determines the correct model, config, and key mappings based on the checkpoint name.\"\"\"\n+\n+    if model_id == \"apple/aimv2-large-patch14-224-lit\":\n+        return Aimv2Model, Aimv2Config, ORIGINAL_TO_CONVERTED_KEY_MAPPING\n+    else:\n+        return Aimv2VisionModel, Aimv2VisionConfig, ORIGINAL_TO_CONVERTED_KEY_MAPPING_VISION_MODEL\n+\n+\n+def write_model(\n+    hf_repo_id: str,\n+    output_dir: str,\n+    safe_serialization: bool = True,\n+):\n+    \"\"\"\n+    Converts a model checkpoint to Hugging Face format and saves it.\n+\n+    Args:\n+        hf_repo_id (str): The Hugging Face repo ID to load from.\n+        output_dir (str): The directory to save the converted model.\n+        safe_serialization (bool): Whether to use safe serialization.\n+\n+    Returns:\n+        model: The reloaded Hugging Face model.\n+    \"\"\"\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    # Get the appropriate model, config, and key mapping\n+    model_class, config_class, key_mapping = get_model_config_mapping(hf_repo_id)\n+\n+    # Load config and original state dict\n+    config = config_class.from_pretrained(hf_repo_id)\n+\n+    # Checkpoint `apple/aimv2-large-patch14-224-lit` uses AttentionPoolingHead hence set the required attr in config.\n+    if hf_repo_id != \"apple/aimv2-large-patch14-224-lit\":\n+        config.use_head = False\n+\n+    if hf_repo_id == \"apple/aimv2-large-patch14-native\":\n+        config.is_native = True\n+\n+    original_state_dict = load_original_state_dict(hf_repo_id)\n+\n+    print(\"Converting model...\")\n+\n+    state_dict = {}\n+    result = convert_old_keys_to_new_keys(original_state_dict, key_mapping)\n+    all_keys = list(original_state_dict.keys())\n+\n+    for key in all_keys:\n+        value = original_state_dict[key]\n+        new_key = result.pop(key)\n+\n+        if \"qkv\" in new_key:\n+            qkv_state_dict = split_qkv_tensor(new_key, value)\n+            state_dict.update(qkv_state_dict)\n+        else:\n+            state_dict[new_key] = value\n+\n+        # Check if position embeddings exist before squeezing\n+        if new_key.endswith(\"position_embedding.weight\"):\n+            state_dict[new_key] = value.squeeze(0)\n+\n+    print(f\"Loading the checkpoint in a {model_class.__name__}.\")\n+    model = model_class(config)\n+    model.load_state_dict(state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    del state_dict, model\n+    gc.collect()\n+\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    model = model_class.from_pretrained(output_dir, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n+    return model\n+\n+\n+def write_image_processor(hf_repo_id: str, output_dir: str):\n+    if hf_repo_id == \"apple/aimv2-large-patch14-224-lit\":\n+        image_processor = AutoProcessor.from_pretrained(hf_repo_id, use_fast=True)\n+    else:\n+        image_processor = AutoImageProcessor.from_pretrained(hf_repo_id, use_fast=True)\n+    image_processor.save_pretrained(output_dir)\n+    return image_processor\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--hf_repo_id\",\n+        default=\"apple/aimv2-large-patch14-224\",\n+        help=\"Location of official weights from apple on HF\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=\"aimv2_model\",\n+        help=\"Location to write the converted model and processor\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=argparse.BooleanOptionalAction,\n+        help=\"Whether or not to push the converted model to the huggingface hub.\",\n+    )\n+    parser.add_argument(\n+        \"--hub_repo_id\",\n+        default=None,\n+        help=\"Huggingface hub repo to write the converted model and processor\",\n+    )\n+    args = parser.parse_args()\n+\n+    model = write_model(\n+        hf_repo_id=args.hf_repo_id,\n+        output_dir=args.output_dir,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+    image_processor = write_image_processor(\n+        hf_repo_id=args.hf_repo_id,\n+        output_dir=args.output_dir,\n+    )\n+\n+    if args.push_to_hub:\n+        print(\"Pushing to hub...\")\n+        model.push_to_hub(args.hub_repo_id)\n+        image_processor.push_to_hub(args.hub_repo_id)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "868b57a48ff1648fe75dab0a3868fa4f79c2dc28",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "added",
            "additions": 833,
            "deletions": 0,
            "changes": 833,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -0,0 +1,833 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/aimv2/modular_aimv2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_aimv2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Apple Inc. and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Any, Callable, Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple\n+from .configuration_aimv2 import Aimv2Config, Aimv2TextConfig, Aimv2VisionConfig\n+\n+\n+@dataclass\n+@auto_docstring\n+class Aimv2Output(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`Aimv2TextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of [`Aimv2VisionModel`].\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`Aimv2TextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`Aimv2VisionModel`].\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    text_model_output: BaseModelOutputWithPooling = None\n+    vision_model_output: BaseModelOutputWithPooling = None\n+\n+    def to_tuple(self) -> tuple[Any]:\n+        return tuple(\n+            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n+            for k in self.keys()\n+        )\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Aimv2RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Aimv2RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Aimv2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Aimv2VisionEmbeddings(nn.Module):\n+    def __init__(self, config: Aimv2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.patch_size = config.patch_size\n+        self.patch_embed = nn.Conv2d(\n+            config.num_channels, config.hidden_size, kernel_size=config.patch_size, stride=config.patch_size\n+        )\n+        self.rms_norm = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+        num_patches = (config.image_size // config.patch_size) ** 2\n+        if not self.config.is_native:\n+            self.position_embedding = nn.Embedding(num_patches, config.hidden_size)\n+        self.register_buffer(\"position_ids\", torch.arange(num_patches).expand((1, -1)), persistent=False)\n+\n+    @staticmethod\n+    def build_2d_sincos_position_embedding(\n+        height, width, embed_dim=256, temperature=10000.0, device=\"cpu\", dtype=torch.float32\n+    ) -> torch.Tensor:\n+        grid_w = torch.arange(int(width), dtype=dtype, device=device)\n+        grid_h = torch.arange(int(height), dtype=dtype, device=device)\n+        grid_h, grid_w = torch.meshgrid(grid_w, grid_h, indexing=\"xy\")\n+\n+        pos_dim = embed_dim // 4\n+        omega = torch.arange(pos_dim, dtype=dtype, device=device) / pos_dim\n+        omega = 1.0 / (temperature**omega)\n+\n+        out_h = grid_h.flatten()[..., None] @ omega[None, :]\n+        out_w = grid_w.flatten()[..., None] @ omega[None, :]\n+\n+        return torch.concat([out_h.sin(), out_h.cos(), out_w.sin(), out_w.cos()], dim=1)[None, :, :]\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        _, _, height, width = pixel_values.size()\n+        hidden_states = self.patch_embed(pixel_values).flatten(2).transpose(1, 2)\n+        hidden_states = self.rms_norm(hidden_states)\n+\n+        if self.config.is_native:\n+            pos_embed = self.build_2d_sincos_position_embedding(\n+                height // self.patch_size,\n+                width // self.patch_size,\n+                embed_dim=self.config.hidden_size,\n+                device=hidden_states.device,\n+                dtype=hidden_states.dtype,\n+            )\n+        else:\n+            pos_embed = self.position_embedding(self.position_ids)\n+\n+        hidden_states = hidden_states + pos_embed\n+        return hidden_states\n+\n+\n+class Aimv2TextEmbeddings(nn.Module):\n+    def __init__(self, config: Aimv2TextConfig):\n+        super().__init__()\n+        embed_dim = config.hidden_size\n+\n+        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n+        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n+\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+    ) -> torch.Tensor:\n+        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, :seq_length]\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.token_embedding(input_ids)\n+\n+        position_embeddings = self.position_embedding(position_ids)\n+        embeddings = inputs_embeds + position_embeddings\n+\n+        return embeddings\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Aimv2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+        self.is_causal = False\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class Aimv2EncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Aimv2VisionConfig):\n+        super().__init__()\n+        self.attention = Aimv2Attention(config)\n+        self.ffn = Aimv2MLP(config)\n+        self.rms_norm1 = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.rms_norm2 = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        norm_hidden_states = self.rms_norm1(hidden_states)\n+        attn_output, attn_weights = self.attention(hidden_states=norm_hidden_states, attention_mask=attention_mask)\n+\n+        hidden_states = hidden_states + attn_output\n+        norm_hidden_states = self.rms_norm2(hidden_states)\n+        mlp_output = self.ffn(norm_hidden_states)\n+\n+        hidden_states = hidden_states + mlp_output\n+        return (hidden_states, attn_weights) if output_attentions else (hidden_states, None)\n+\n+\n+class Aimv2Encoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`Aimv2EncoderLayer`].\n+\n+    Args:\n+        config: Aimv2Config\n+    \"\"\"\n+\n+    def __init__(self, config: Aimv2Config):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([Aimv2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n+        )\n+\n+\n+class Aimv2AttentionPoolingHead(nn.Module):\n+    def __init__(self, config: Aimv2VisionConfig):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+\n+        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.qkv_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.qkv_bias)\n+\n+        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_size))\n+        self.output_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, seq_len, hidden_dim = hidden_states.shape\n+\n+        cls_token = self.cls_token.expand(batch_size, -1, -1)\n+\n+        key = self.k_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, hidden_dim // self.num_heads)\n+        value = self.v_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, hidden_dim // self.num_heads)\n+        query = cls_token.reshape(batch_size, 1, self.num_heads, hidden_dim // self.num_heads)\n+\n+        key = key.permute(0, 2, 1, 3)\n+        value = value.permute(0, 2, 1, 3)\n+        query = query.permute(0, 2, 1, 3)\n+\n+        attn_output = F.scaled_dot_product_attention(query, key, value)\n+\n+        attn_output = attn_output.transpose(1, 2).reshape(batch_size, 1, hidden_dim)\n+        attn_output = attn_output.mean(dim=1)\n+\n+        output = self.output_proj(attn_output)\n+        return output\n+\n+\n+@auto_docstring\n+class Aimv2PreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models. The model is only intended for inference and doesn't support finetuning.\n+    \"\"\"\n+\n+    config_class = Aimv2Config\n+    base_model_prefix = \"aimv2\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\n+        \"Aimv2EncoderLayer\",\n+        \"Aimv2AttentionPoolingHead\",\n+        \"Aimv2VisionEmbeddings\",\n+        \"Aimv2TextEmbeddings\",\n+    ]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+\n+    def _init_weights(self, module):\n+        std = (\n+            self.config.vision_config.initializer_range\n+            if hasattr(self.config, \"vision_config\")\n+            else self.config.initializer_range\n+        )\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, Aimv2RMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+        elif hasattr(module, \"logit_scale\"):\n+            if isinstance(module.logit_scale, nn.Parameter):\n+                module.logit_scale.data.fill_(math.log(1 / 0.07))\n+        elif isinstance(module, Aimv2AttentionPoolingHead):\n+            module.cls_token.data.normal_(mean=0.0, std=std)\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Vision model from AIMv2 without any head or projection on top.\n+    \"\"\"\n+)\n+class Aimv2VisionModel(Aimv2PreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    config_class = Aimv2VisionConfig\n+\n+    def __init__(self, config: Aimv2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = Aimv2VisionEmbeddings(config)\n+        self.encoder = Aimv2Encoder(config)\n+        # The only change from SiglipVisionTransformer is, layernorm -> rms_norm.\n+        self.rms_norm = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+        self.use_head = config.use_head\n+        if self.use_head:\n+            self.head = Aimv2AttentionPoolingHead(config)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.embeddings.patch_embed\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Siglip2VisionModel\n+\n+        >>> model = Aimv2VisionModel.from_pretrained(\"apple/aimv2-large-patch14-native\")\n+        >>> processor = AutoProcessor.from_pretrained(\"apple/aimv2-large-patch14-native\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        >>> pooled_output = outputs.pooler_output  # pooled features\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(pixel_values)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.rms_norm(last_hidden_state)\n+\n+        pooler_output = self.head(last_hidden_state) if self.use_head else None\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooler_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The text model from AIMv2 without any head or projection on top.\n+    \"\"\"\n+)\n+class Aimv2TextModel(Aimv2PreTrainedModel):\n+    main_input_name = \"input_ids\"\n+\n+    def __init__(self, config: Aimv2TextConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = Aimv2TextEmbeddings(config)\n+        self.encoder = Aimv2Encoder(config)\n+        self.rms_norm = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+        self.eos_token_id = config.eos_token_id\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.embeddings.token_embedding\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.token_embedding = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutputWithPooling:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(input_ids)\n+        batch_size, seq_len, _ = hidden_states.shape\n+\n+        cache_position = torch.arange(seq_len, dtype=torch.long, device=hidden_states.device)\n+        position_ids = cache_position.unsqueeze(0).expand(batch_size, -1)\n+        if attention_mask is not None:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=hidden_states,\n+                position_ids=position_ids,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=None,\n+            )\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.rms_norm(last_hidden_state)\n+\n+        # Get pooled output\n+        pooled_output = last_hidden_state[\n+            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n+            (input_ids.to(dtype=torch.int, device=last_hidden_state.device) == self.eos_token_id).int().argmax(dim=-1),\n+        ]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+def _get_vector_norm(tensor: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    This method is equivalent to tensor.norm(p=2, dim=-1, keepdim=True) and used to make\n+    model `executorch` exportable. See issue https://github.com/pytorch/executorch/issues/3566\n+    \"\"\"\n+    square_tensor = torch.pow(tensor, 2)\n+    sum_tensor = torch.sum(square_tensor, dim=-1, keepdim=True)\n+    normed_tensor = torch.pow(sum_tensor, 0.5)\n+    return normed_tensor\n+\n+\n+@auto_docstring\n+class Aimv2Model(Aimv2PreTrainedModel):\n+    config_class = Aimv2Config\n+    _no_split_modules = [\"Aimv2TextEmbeddings\", \"Aimv2EncoderLayer\", \"Aimv2VisionEmbeddings\"]\n+\n+    def __init__(self, config: Aimv2Config):\n+        super().__init__(config)\n+\n+        self.projection_dim = config.projection_dim\n+        self.vision_embed_dim = config.vision_config.hidden_size\n+        self.text_embed_dim = config.text_config.hidden_size\n+\n+        self.vision_model = Aimv2VisionModel._from_config(config.vision_config)\n+        self.text_model = Aimv2TextModel._from_config(config.text_config)\n+\n+        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)\n+        self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)\n+\n+        self.logit_scale = nn.Parameter(torch.tensor(self.config.logit_scale_init_value))\n+        self.max_log_logit_scale = math.log(config.max_logit_scale)\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    def get_text_features(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> torch.FloatTensor:\n+        r\"\"\"\n+        Returns:\n+            text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n+            applying the projection layer to the pooled output of [`Aimv2TextModel`].\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Aimv2Model\n+\n+        >>> model = Aimv2Model.from_pretrained(\"openai/aimv2-vit-base-patch32\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/aimv2-vit-base-patch32\")\n+\n+        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+        >>> text_features = model.get_text_features(**inputs)\n+        ```\"\"\"\n+        # Use AIMV2 model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        pooled_output = text_outputs.pooler_output\n+        text_features = self.text_projection(pooled_output)\n+\n+        return text_features\n+\n+    @auto_docstring\n+    def get_image_features(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> torch.FloatTensor:\n+        r\"\"\"\n+        Returns:\n+            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n+            applying the projection layer to the pooled output of [`Aimv2VisionModel`].\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Aimv2Model\n+\n+        >>> model = Aimv2Model.from_pretrained(\"openai/aimv2-vit-base-patch32\")\n+        >>> processor = AutoProcessor.from_pretrained(\"openai/aimv2-vit-base-patch32\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> image_features = model.get_image_features(**inputs)\n+        ```\"\"\"\n+        # Use AIMV2 model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+\n+        pooled_output = vision_outputs.pooler_output\n+        image_features = self.visual_projection(pooled_output)\n+\n+        return image_features\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Aimv2Output:\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Aimv2Model\n+\n+        >>> model = Aimv2Model.from_pretrained(\"apple/aimv2-large-patch14-224-lit\")\n+        >>> processor = AutoProcessor.from_pretrained(\"apple/aimv2-large-patch14-224-lit\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(\n+        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n+        ... )\n+\n+        >>> outputs = model(**inputs)\n+        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n+        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        image_embeds = vision_outputs.pooler_output\n+        image_embeds = self.visual_projection(image_embeds)\n+\n+        text_embeds = text_outputs.pooler_output\n+        text_embeds = self.text_projection(text_embeds)\n+\n+        # normalized features\n+        image_embeds = image_embeds / _get_vector_norm(image_embeds)\n+        text_embeds = text_embeds / _get_vector_norm(text_embeds)\n+\n+        logit_scale = self.logit_scale.clamp(0.0, self.max_log_logit_scale).exp().to(text_embeds.device)\n+        logits_per_text = (logit_scale * text_embeds) @ image_embeds.t()\n+        logits_per_image = logits_per_text.t()\n+\n+        return Aimv2Output(\n+            logits_per_image=logits_per_image,\n+            logits_per_text=logits_per_text,\n+            text_embeds=text_embeds,\n+            image_embeds=image_embeds,\n+            text_model_output=text_outputs,\n+            vision_model_output=vision_outputs,\n+        )\n+\n+\n+__all__ = [\"Aimv2VisionModel\", \"Aimv2Model\", \"Aimv2PreTrainedModel\", \"Aimv2TextModel\"]"
        },
        {
            "sha": "371bf59f88d23b487ac4a621f9cd108af39eaa4d",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "added",
            "additions": 732,
            "deletions": 0,
            "changes": 732,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -0,0 +1,732 @@\n+# coding=utf-8\n+# Copyright 2025 Apple Inc. and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"Pytorch implementation of AIMv2 Model\"\"\"\n+\n+import math\n+from typing import Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPooling\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    auto_docstring,\n+    can_return_tuple,\n+)\n+from ..clip.modeling_clip import CLIPModel, CLIPTextEmbeddings, _get_vector_norm\n+from ..llama.modeling_llama import LlamaMLP, LlamaRMSNorm\n+from ..siglip.configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n+from ..siglip.modeling_siglip import SiglipAttention, SiglipEncoder, SiglipOutput\n+\n+\n+class Aimv2VisionConfig(SiglipVisionConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Aimv2VisionModel`]. It is used to instantiate a\n+    AIMv2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the vision encoder of the AIMv2\n+    [apple/aimv2-large-patch14-224](https://huggingface.co/apple/aimv2-large-patch14-224) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 2816):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input images.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The size (resolution) of each patch.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        qkv_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the queries, keys and values.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the Linear layers or Not.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the for initializing all weight matrices.\n+        use_head (`str`, *optional*, defaults to `True`):\n+            Whether to use Attention Pooling Head or Not.\n+        is_native (`str`, *optional*, defaults to `False`):\n+            Whether to use ckpt trained for image native resolution or not.\n+    Example:\n+\n+    ```python\n+    >>> from transformers import SiglipVisionConfig, SiglipVisionModel\n+\n+    >>> # Initializing a Aimv2VisionConfig with apple/aimv2-large-patch14-224 style configuration\n+    >>> configuration = Aimv2VisionConfig()\n+\n+    >>> # Initializing a Aimv2VisionModel (with random weights) from the apple/aimv2-large-patch14-224 style configuration\n+    >>> model = Aimv2VisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    def __init__(\n+        self,\n+        hidden_size: int = 1024,\n+        intermediate_size: int = 2816,\n+        num_hidden_layers: int = 24,\n+        num_attention_heads: int = 8,\n+        num_channels: int = 3,\n+        image_size: int = 224,\n+        patch_size: int = 14,\n+        rms_norm_eps: float = 1e-5,\n+        attention_dropout: float = 0.0,\n+        qkv_bias: bool = False,\n+        mlp_bias: bool = False,\n+        hidden_act: str = \"silu\",\n+        initializer_range: float = 0.02,\n+        use_head: bool = True,\n+        is_native: bool = False,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            hidden_act=hidden_act,\n+            num_channels=num_channels,\n+            image_size=image_size,\n+            patch_size=patch_size,\n+            qkv_bias=qkv_bias,\n+            **kwargs,\n+        )\n+\n+        self.use_head = use_head\n+        self.initializer_range = initializer_range\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.qkv_bias = qkv_bias\n+        self.rms_norm_eps = rms_norm_eps\n+        self.is_native = is_native\n+\n+        del self.layer_norm_eps\n+\n+\n+class Aimv2TextConfig(SiglipTextConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Aimv2TextModel`]. It is used to instantiate a\n+    AIMv2 text encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the text encoder of the AIMv2\n+    [apple/aimv2-large-patch14-224-lit](https://huggingface.co/apple/aimv2-large-patch14-224-lit) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 49408):\n+            Vocabulary size of the AIMv2 text model. Defines the number of different tokens that can be represented by\n+            the `inputs_ids` passed when calling [`Aimv2Model`].\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 6):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        qkv_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the queries, keys and values.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the Linear layers or Not.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            The id of the padding token in the vocabulary.\n+        bos_token_id (`int`, *optional*, defaults to 49406):\n+            The id of the beginning-of-sequence token in the vocabulary.\n+        eos_token_id (`int`, *optional*, defaults to 49407):\n+            The id of the end-of-sequence token in the vocabulary.\n+        max_position_embeddings (`int`, *optional*, defaults to 77):\n+            The maximum sequence length that this model might ever be used with. Typically set this to something large\n+            just in case (e.g., 512 or 1024 or 2048).\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the for initializing all weight matrices.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 49408,\n+        hidden_size: int = 768,\n+        intermediate_size: int = 2048,\n+        num_hidden_layers: int = 12,\n+        num_attention_heads: int = 6,\n+        rms_norm_eps: float = 1e-5,\n+        attention_dropout: float = 0.0,\n+        qkv_bias: bool = False,\n+        mlp_bias: bool = False,\n+        hidden_act: str = \"silu\",\n+        pad_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = None,\n+        eos_token_id: int = 49407,\n+        max_position_embeddings: int = 77,\n+        initializer_range: bool = 0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            hidden_act=hidden_act,\n+            max_position_embeddings=max_position_embeddings,\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            **kwargs,\n+        )\n+\n+        self.initializer_range = initializer_range\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.qkv_bias = qkv_bias\n+        self.rms_norm_eps = rms_norm_eps\n+\n+        del self.bos_token_id\n+        del self.pad_token_id\n+        del self.projection_size\n+        del self.layer_norm_eps\n+\n+\n+class Aimv2Config(SiglipConfig):\n+    r\"\"\"\n+    [`Aimv2Config`] is the configuration class to store the configuration of a [`Aimv2Model`]. It is used to\n+    instantiate a AIMv2 model according to the specified arguments, defining the text model and vision model configs.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the AIMv2\n+    [apple/aimv2-large-patch14-224-lit](https://huggingface.co/apple/aimv2-large-patch14-224-lit) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`Aimv2TextConfig`].\n+        vision_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`Aimv2VisionConfig`].\n+        projection_dim (`int`, *optional*, defaults to 512):\n+            Dimensionality of text and vision projection layers.\n+        logit_scale_init_value (`float`, *optional*, defaults to 2.6592):\n+            The initial value of the *logit_scale* parameter.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Aimv2Config, Aimv2Model\n+\n+    >>> # Initializing a Aimv2Config with apple/aimv2-large-patch14-224-lit style configuration\n+    >>> configuration = Aimv2Config()\n+\n+    >>> # Initializing a Aimv2Model (with random weights) from the apple/aimv2-large-patch14-224-lit style configuration\n+    >>> model = Aimv2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a Aimv2Config from a Aimv2TextConfig and a Aimv2VisionConfig\n+    >>> from transformers import Aimv2TextConfig, Aimv2VisionConfig\n+\n+    >>> # Initializing a AIMv2Text and AIMv2Vision configuration\n+    >>> config_text = Aimv2TextConfig()\n+    >>> config_vision = Aimv2VisionConfig()\n+\n+    >>> config = Aimv2Config(text_config=config_text, vision_config=config_vision)\n+    ```\"\"\"\n+\n+    def __init__(\n+        self, text_config=None, vision_config=None, projection_dim=512, logit_scale_init_value=2.6592, **kwargs\n+    ):\n+        super().__init__(text_config, vision_config, **kwargs)\n+        self.projection_dim = projection_dim\n+        self.logit_scale_init_value = logit_scale_init_value\n+        self.max_logit_scale = 100.0\n+\n+        del self.initializer_factor\n+\n+\n+class Aimv2Output(SiglipOutput):\n+    pass\n+\n+\n+class Aimv2RMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class Aimv2MLP(LlamaMLP):\n+    pass\n+\n+\n+class Aimv2VisionEmbeddings(nn.Module):\n+    def __init__(self, config: Aimv2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.patch_size = config.patch_size\n+        self.patch_embed = nn.Conv2d(\n+            config.num_channels, config.hidden_size, kernel_size=config.patch_size, stride=config.patch_size\n+        )\n+        self.rms_norm = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+        num_patches = (config.image_size // config.patch_size) ** 2\n+        if not self.config.is_native:\n+            self.position_embedding = nn.Embedding(num_patches, config.hidden_size)\n+        self.register_buffer(\"position_ids\", torch.arange(num_patches).expand((1, -1)), persistent=False)\n+\n+    @staticmethod\n+    def build_2d_sincos_position_embedding(\n+        height, width, embed_dim=256, temperature=10000.0, device=\"cpu\", dtype=torch.float32\n+    ) -> torch.Tensor:\n+        grid_w = torch.arange(int(width), dtype=dtype, device=device)\n+        grid_h = torch.arange(int(height), dtype=dtype, device=device)\n+        grid_h, grid_w = torch.meshgrid(grid_w, grid_h, indexing=\"xy\")\n+\n+        pos_dim = embed_dim // 4\n+        omega = torch.arange(pos_dim, dtype=dtype, device=device) / pos_dim\n+        omega = 1.0 / (temperature**omega)\n+\n+        out_h = grid_h.flatten()[..., None] @ omega[None, :]\n+        out_w = grid_w.flatten()[..., None] @ omega[None, :]\n+\n+        return torch.concat([out_h.sin(), out_h.cos(), out_w.sin(), out_w.cos()], dim=1)[None, :, :]\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        _, _, height, width = pixel_values.size()\n+        hidden_states = self.patch_embed(pixel_values).flatten(2).transpose(1, 2)\n+        hidden_states = self.rms_norm(hidden_states)\n+\n+        if self.config.is_native:\n+            pos_embed = self.build_2d_sincos_position_embedding(\n+                height // self.patch_size,\n+                width // self.patch_size,\n+                embed_dim=self.config.hidden_size,\n+                device=hidden_states.device,\n+                dtype=hidden_states.dtype,\n+            )\n+        else:\n+            pos_embed = self.position_embedding(self.position_ids)\n+\n+        hidden_states = hidden_states + pos_embed\n+        return hidden_states\n+\n+\n+class Aimv2TextEmbeddings(CLIPTextEmbeddings):\n+    pass\n+\n+\n+class Aimv2Attention(SiglipAttention):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=config.qkv_bias)\n+\n+\n+class Aimv2EncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Aimv2VisionConfig):\n+        super().__init__()\n+        self.attention = Aimv2Attention(config)\n+        self.ffn = Aimv2MLP(config)\n+        self.rms_norm1 = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.rms_norm2 = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        norm_hidden_states = self.rms_norm1(hidden_states)\n+        attn_output, attn_weights = self.attention(hidden_states=norm_hidden_states, attention_mask=attention_mask)\n+\n+        hidden_states = hidden_states + attn_output\n+        norm_hidden_states = self.rms_norm2(hidden_states)\n+        mlp_output = self.ffn(norm_hidden_states)\n+\n+        hidden_states = hidden_states + mlp_output\n+        return (hidden_states, attn_weights) if output_attentions else (hidden_states, None)\n+\n+\n+class Aimv2Encoder(SiglipEncoder):\n+    pass\n+\n+\n+class Aimv2AttentionPoolingHead(nn.Module):\n+    def __init__(self, config: Aimv2VisionConfig):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+\n+        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.qkv_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.qkv_bias)\n+\n+        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_size))\n+        self.output_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, seq_len, hidden_dim = hidden_states.shape\n+\n+        cls_token = self.cls_token.expand(batch_size, -1, -1)\n+\n+        key = self.k_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, hidden_dim // self.num_heads)\n+        value = self.v_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, hidden_dim // self.num_heads)\n+        query = cls_token.reshape(batch_size, 1, self.num_heads, hidden_dim // self.num_heads)\n+\n+        key = key.permute(0, 2, 1, 3)\n+        value = value.permute(0, 2, 1, 3)\n+        query = query.permute(0, 2, 1, 3)\n+\n+        attn_output = F.scaled_dot_product_attention(query, key, value)\n+\n+        attn_output = attn_output.transpose(1, 2).reshape(batch_size, 1, hidden_dim)\n+        attn_output = attn_output.mean(dim=1)\n+\n+        output = self.output_proj(attn_output)\n+        return output\n+\n+\n+@auto_docstring\n+class Aimv2PreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models. The model is only intended for inference and doesn't support finetuning.\n+    \"\"\"\n+\n+    config_class = Aimv2Config\n+    base_model_prefix = \"aimv2\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\n+        \"Aimv2EncoderLayer\",\n+        \"Aimv2AttentionPoolingHead\",\n+        \"Aimv2VisionEmbeddings\",\n+        \"Aimv2TextEmbeddings\",\n+    ]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+\n+    def _init_weights(self, module):\n+        std = (\n+            self.config.vision_config.initializer_range\n+            if hasattr(self.config, \"vision_config\")\n+            else self.config.initializer_range\n+        )\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, Aimv2RMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+        elif hasattr(module, \"logit_scale\"):\n+            if isinstance(module.logit_scale, nn.Parameter):\n+                module.logit_scale.data.fill_(math.log(1 / 0.07))\n+        elif isinstance(module, Aimv2AttentionPoolingHead):\n+            module.cls_token.data.normal_(mean=0.0, std=std)\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Vision model from AIMv2 without any head or projection on top.\n+    \"\"\"\n+)\n+class Aimv2VisionModel(Aimv2PreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    config_class = Aimv2VisionConfig\n+\n+    def __init__(self, config: Aimv2VisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = Aimv2VisionEmbeddings(config)\n+        self.encoder = Aimv2Encoder(config)\n+        # The only change from SiglipVisionTransformer is, layernorm -> rms_norm.\n+        self.rms_norm = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+        self.use_head = config.use_head\n+        if self.use_head:\n+            self.head = Aimv2AttentionPoolingHead(config)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.embeddings.patch_embed\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Siglip2VisionModel\n+\n+        >>> model = Aimv2VisionModel.from_pretrained(\"apple/aimv2-large-patch14-native\")\n+        >>> processor = AutoProcessor.from_pretrained(\"apple/aimv2-large-patch14-native\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        >>> pooled_output = outputs.pooler_output  # pooled features\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(pixel_values)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.rms_norm(last_hidden_state)\n+\n+        pooler_output = self.head(last_hidden_state) if self.use_head else None\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooler_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The text model from AIMv2 without any head or projection on top.\n+    \"\"\"\n+)\n+class Aimv2TextModel(Aimv2PreTrainedModel):\n+    main_input_name = \"input_ids\"\n+\n+    def __init__(self, config: Aimv2TextConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.embeddings = Aimv2TextEmbeddings(config)\n+        self.encoder = Aimv2Encoder(config)\n+        self.rms_norm = Aimv2RMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+        self.eos_token_id = config.eos_token_id\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.embeddings.token_embedding\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.token_embedding = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutputWithPooling:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_states = self.embeddings(input_ids)\n+        batch_size, seq_len, _ = hidden_states.shape\n+\n+        cache_position = torch.arange(seq_len, dtype=torch.long, device=hidden_states.device)\n+        position_ids = cache_position.unsqueeze(0).expand(batch_size, -1)\n+        if attention_mask is not None:\n+            attention_mask = create_causal_mask(\n+                config=self.config,\n+                input_embeds=hidden_states,\n+                position_ids=position_ids,\n+                attention_mask=attention_mask,\n+                cache_position=cache_position,\n+                past_key_values=None,\n+            )\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.rms_norm(last_hidden_state)\n+\n+        # Get pooled output\n+        pooled_output = last_hidden_state[\n+            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n+            (input_ids.to(dtype=torch.int, device=last_hidden_state.device) == self.eos_token_id).int().argmax(dim=-1),\n+        ]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class Aimv2Model(CLIPModel, nn.Module):\n+    def __init__(self, config: Aimv2Config):\n+        nn.Module().__init__(config)\n+\n+        self.projection_dim = config.projection_dim\n+        self.vision_embed_dim = config.vision_config.hidden_size\n+        self.text_embed_dim = config.text_config.hidden_size\n+\n+        self.vision_model = Aimv2VisionModel._from_config(config.vision_config)\n+        self.text_model = Aimv2TextModel._from_config(config.text_config)\n+\n+        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)\n+        self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)\n+\n+        self.logit_scale = nn.Parameter(torch.tensor(self.config.logit_scale_init_value))\n+        self.max_log_logit_scale = math.log(config.max_logit_scale)\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Aimv2Output:\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Aimv2Model\n+\n+        >>> model = Aimv2Model.from_pretrained(\"apple/aimv2-large-patch14-224-lit\")\n+        >>> processor = AutoProcessor.from_pretrained(\"apple/aimv2-large-patch14-224-lit\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(\n+        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n+        ... )\n+\n+        >>> outputs = model(**inputs)\n+        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n+        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        image_embeds = vision_outputs.pooler_output\n+        image_embeds = self.visual_projection(image_embeds)\n+\n+        text_embeds = text_outputs.pooler_output\n+        text_embeds = self.text_projection(text_embeds)\n+\n+        # normalized features\n+        image_embeds = image_embeds / _get_vector_norm(image_embeds)\n+        text_embeds = text_embeds / _get_vector_norm(text_embeds)\n+\n+        logit_scale = self.logit_scale.clamp(0.0, self.max_log_logit_scale).exp().to(text_embeds.device)\n+        logits_per_text = (logit_scale * text_embeds) @ image_embeds.t()\n+        logits_per_image = logits_per_text.t()\n+\n+        return Aimv2Output(\n+            logits_per_image=logits_per_image,\n+            logits_per_text=logits_per_text,\n+            text_embeds=text_embeds,\n+            image_embeds=image_embeds,\n+            text_model_output=text_outputs,\n+            vision_model_output=vision_outputs,\n+        )\n+\n+\n+__all__ = [\n+    \"Aimv2Config\",\n+    \"Aimv2VisionConfig\",\n+    \"Aimv2TextConfig\",\n+    \"Aimv2VisionModel\",\n+    \"Aimv2Model\",\n+    \"Aimv2PreTrainedModel\",\n+    \"Aimv2TextModel\",\n+]"
        },
        {
            "sha": "a3d4cf196fc0805ef12ac151a9ae3cd7645cc9f0",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -36,6 +36,8 @@\n CONFIG_MAPPING_NAMES = OrderedDict[str, str](\n     [\n         # Add configs here\n+        (\"aimv2\", \"Aimv2Config\"),\n+        (\"aimv2_vision_model\", \"Aimv2VisionConfig\"),\n         (\"albert\", \"AlbertConfig\"),\n         (\"align\", \"AlignConfig\"),\n         (\"altclip\", \"AltCLIPConfig\"),\n@@ -407,6 +409,8 @@\n MODEL_NAMES_MAPPING = OrderedDict[str, str](\n     [\n         # Add full (and cased) model names here\n+        (\"aimv2\", \"AIMv2\"),\n+        (\"aimv2_vision_model\", \"Aimv2VisionModel\"),\n         (\"albert\", \"ALBERT\"),\n         (\"align\", \"ALIGN\"),\n         (\"altclip\", \"AltCLIP\"),\n@@ -859,6 +863,7 @@\n         (\"glm4v_text\", \"glm4v\"),\n         (\"idefics3_vision\", \"idefics3\"),\n         (\"siglip_vision_model\", \"siglip\"),\n+        (\"aimv2_vision_model\", \"aimv2\"),\n         (\"smolvlm_vision\", \"smolvlm\"),\n         (\"chinese_clip_vision_model\", \"chinese_clip\"),\n         (\"rt_detr_resnet\", \"rt_detr\"),"
        },
        {
            "sha": "70982f2e498a259f4c66dca554815a6a6004ad10",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -56,6 +56,8 @@\n else:\n     IMAGE_PROCESSOR_MAPPING_NAMES = OrderedDict(\n         [\n+            (\"aimv2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n+            (\"aimv2_vision_model\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"align\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n             (\"aria\", (\"AriaImageProcessor\")),\n             (\"beit\", (\"BeitImageProcessor\", \"BeitImageProcessorFast\")),"
        },
        {
            "sha": "d59e2497408142f57e523fbd76c8b0573dd8ac70",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -32,6 +32,8 @@\n MODEL_MAPPING_NAMES = OrderedDict(\n     [\n         # Base model mapping\n+        (\"aimv2\", \"Aimv2Model\"),\n+        (\"aimv2_vision_model\", \"Aimv2VisionModel\"),\n         (\"albert\", \"AlbertModel\"),\n         (\"align\", \"AlignModel\"),\n         (\"altclip\", \"AltCLIPModel\"),\n@@ -678,6 +680,7 @@\n MODEL_FOR_IMAGE_MAPPING_NAMES = OrderedDict(\n     [\n         # Model for Image mapping\n+        (\"aimv2_vision_model\", \"Aimv2VisionModel\"),\n         (\"beit\", \"BeitModel\"),\n         (\"bit\", \"BitModel\"),\n         (\"conditional_detr\", \"ConditionalDetrModel\"),"
        },
        {
            "sha": "bb04231d7bd8bc6b1f6cf4469a3122f44b75e695",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -45,6 +45,7 @@\n \n PROCESSOR_MAPPING_NAMES = OrderedDict(\n     [\n+        (\"aimv2\", \"CLIPProcessor\"),\n         (\"align\", \"AlignProcessor\"),\n         (\"altclip\", \"AltCLIPProcessor\"),\n         (\"aria\", \"AriaProcessor\"),"
        },
        {
            "sha": "85e2d8c460d0c1fac202b1b5f596a5dc76c3a6a6",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -56,6 +56,13 @@\n # Explicit rather than inferred generics to significantly improves completion suggestion performance for language servers.\n TOKENIZER_MAPPING_NAMES = OrderedDict[str, tuple[Optional[str], Optional[str]]](\n     [\n+        (\n+            \"aimv2\",\n+            (\n+                \"CLIPTokenizer\",\n+                \"CLIPTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n         (\n             \"albert\",\n             ("
        },
        {
            "sha": "2b442057208672ea17686040a55e47b7d7de3633",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -475,7 +475,6 @@ class SiglipPreTrainedModel(PreTrainedModel):\n \n     _no_split_modules = [\n         \"SiglipTextEmbeddings\",\n-        \"SiglipEncoderLayer\",\n         \"SiglipVisionEmbeddings\",\n         \"SiglipEncoderLayer\",\n         \"SiglipMultiheadAttentionPoolingHead\","
        },
        {
            "sha": "153d27115c3bf645759214a6bce04d8bcd33ddee",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -708,7 +708,6 @@ class Siglip2PreTrainedModel(PreTrainedModel):\n \n     _no_split_modules = [\n         \"Siglip2TextEmbeddings\",\n-        \"Siglip2EncoderLayer\",\n         \"Siglip2VisionEmbeddings\",\n         \"Siglip2EncoderLayer\",\n         \"Siglip2MultiheadAttentionPoolingHead\","
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/aimv2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/tests%2Fmodels%2Faimv2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/tests%2Fmodels%2Faimv2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2F__init__.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363"
        },
        {
            "sha": "bcebbc8ffa3512b61f06a5fad97a84ab70e5e818",
            "filename": "tests/models/aimv2/test_modeling_aimv2.py",
            "status": "added",
            "additions": 694,
            "deletions": 0,
            "changes": 694,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -0,0 +1,694 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch AIMv2 model.\"\"\"\n+\n+import inspect\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import requests\n+from parameterized import parameterized\n+from pytest import mark\n+\n+from transformers import Aimv2Config, Aimv2TextConfig, Aimv2VisionConfig\n+from transformers.testing_utils import (\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    require_torch_sdpa,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    is_torch_available,\n+    is_vision_available,\n+)\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    ModelTesterMixin,\n+    _config_zero_init,\n+    _test_eager_matches_sdpa_inference,\n+    floats_tensor,\n+    ids_tensor,\n+    random_attention_mask,\n+)\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import (\n+        Aimv2Model,\n+        Aimv2TextModel,\n+        Aimv2VisionModel,\n+    )\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoImageProcessor, AutoProcessor\n+\n+\n+class Aimv2VisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=12,\n+        image_size=30,\n+        patch_size=2,\n+        num_channels=3,\n+        is_training=False,\n+        hidden_size=32,\n+        projection_dim=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        dropout=0.1,\n+        attention_dropout=0.1,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.hidden_size = hidden_size\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        return Aimv2VisionConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            projection_dim=self.projection_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = Aimv2VisionModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+class Aimv2ModelTesterMixin(ModelTesterMixin):\n+    \"\"\"\n+    Subclass of ModelTesterMixin with methods specific to testing Aimv2 models.\n+    The SDPA equivalence test is overridden here because Aimv2 models may have test/vision/text+vision inputs,\n+    different output logits, and are not supposed to be used or tested with padding_side=\"left\".\n+    \"\"\"\n+\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Load the model with SDPA\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+\n+                # Load model with eager attention\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    attn_implementation=\"eager\",\n+                )\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+            if hasattr(model_sdpa, \"vision_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+\n+            if hasattr(model_sdpa, \"text_model\"):\n+                self.assertTrue(model_sdpa.text_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_eager.text_model.config._attn_implementation == \"eager\")\n+\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+\n+@require_torch\n+class Aimv2VisionModelTest(Aimv2ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as Aimv2 does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (Aimv2VisionModel,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torchscript = False\n+\n+    def setUp(self):\n+        self.model_tester = Aimv2VisionModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self, config_class=Aimv2VisionConfig, has_text_modality=False, hidden_size=37\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"Aimv2 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+\n+class Aimv2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=12,\n+        seq_length=7,\n+        is_training=False,\n+        use_input_mask=True,\n+        use_labels=True,\n+        vocab_size=99,\n+        hidden_size=32,\n+        projection_dim=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        dropout=0.1,\n+        attention_dropout=0.1,\n+        max_position_embeddings=25,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.max_position_embeddings = max_position_embeddings\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        input_mask = None\n+        if self.use_input_mask:\n+            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n+\n+        if input_mask is not None:\n+            batch_size, seq_length = input_mask.shape\n+            rnd_start_indices = np.random.randint(1, seq_length - 1, size=(batch_size,))\n+            for batch_idx, start_index in enumerate(rnd_start_indices):\n+                input_mask[batch_idx, :start_index] = 1\n+                input_mask[batch_idx, start_index:] = 0\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, input_mask\n+\n+    def get_config(self):\n+        return Aimv2TextConfig(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            projection_dim=self.projection_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            max_position_embeddings=self.max_position_embeddings,\n+        )\n+\n+    def create_and_check_model(self, config, input_ids, input_mask):\n+        model = Aimv2TextModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(input_ids, attention_mask=input_mask)\n+            result = model(input_ids)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, input_mask = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Aimv2TextModelTest(Aimv2ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (Aimv2TextModel,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_head_masking = False\n+    test_resize_embeddings = False\n+    test_torchscript = False\n+\n+    def setUp(self):\n+        self.model_tester = Aimv2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Aimv2TextConfig, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"Aimv2 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+\n+class Aimv2ModelTester:\n+    def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=False):\n+        if text_kwargs is None:\n+            text_kwargs = {}\n+        if vision_kwargs is None:\n+            vision_kwargs = {}\n+\n+        self.parent = parent\n+        self.text_model_tester = Aimv2TextModelTester(parent, **text_kwargs)\n+        self.vision_model_tester = Aimv2VisionModelTester(parent, **vision_kwargs)\n+        self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n+        self.is_training = is_training\n+\n+    def prepare_config_and_inputs(self):\n+        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n+        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, attention_mask, pixel_values\n+\n+    def get_config(self):\n+        return Aimv2Config.from_text_vision_configs(\n+            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        )\n+\n+    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n+        model = Aimv2Model(config).to(torch_device).eval()\n+        with torch.no_grad():\n+            result = model(input_ids, pixel_values, attention_mask)\n+        self.parent.assertEqual(\n+            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n+        )\n+        self.parent.assertEqual(\n+            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, attention_mask, pixel_values = config_and_inputs\n+\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"pixel_values\": pixel_values,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Aimv2ModelTest(Aimv2ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    additional_model_inputs = [\"pixel_values\"]\n+    all_model_classes = (Aimv2Model,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": Aimv2Model, \"image-feature-extraction\": Aimv2VisionModel}\n+        if is_torch_available()\n+        else {}\n+    )\n+    fx_compatible = False\n+    test_head_masking = False\n+    test_pruning = False\n+    test_torchscript = False\n+    test_resize_embeddings = False\n+    test_attention_outputs = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Aimv2ModelTester(self)\n+        common_properties = [\"projection_dim\", \"logit_scale_init_value\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=Aimv2Config, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        print(config_and_inputs)\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Aimv2Model does not have input/output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(\"Size mismatch on CUDA\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+    # Override as the `logit_scale` parameter initialization is different for Aimv2\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    # check if `logit_scale` is initialized as per the original implementation\n+                    if name == \"logit_scale\":\n+                        self.assertAlmostEqual(\n+                            param.data.item(),\n+                            np.log(1 / 0.07),\n+                            delta=1e-3,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    else:\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+\n+    def test_load_vision_text_config(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # Save Aimv2Config and check if we can load Aimv2VisionConfig from it\n+        with tempfile.TemporaryDirectory() as tmp_dir_name:\n+            config.save_pretrained(tmp_dir_name)\n+            vision_config = Aimv2VisionConfig.from_pretrained(tmp_dir_name)\n+            self.assertDictEqual(config.vision_config.to_dict(), vision_config.to_dict())\n+\n+        # Save Aimv2Config and check if we can load Aimv2TextConfig from it\n+        with tempfile.TemporaryDirectory() as tmp_dir_name:\n+            config.save_pretrained(tmp_dir_name)\n+            text_config = Aimv2TextConfig.from_pretrained(tmp_dir_name)\n+            self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_inference_equivalence(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n+                model.to(torch_device)\n+\n+                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n+                dummy_input_ids = inputs_dict[\"input_ids\"]\n+\n+                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n+                outputs_fa = model_fa(\n+                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n+                )\n+\n+                self.assertTrue(\n+                    torch.allclose(outputs.logits_per_image, outputs_fa.logits_per_image, atol=4e-2, rtol=4e-2),\n+                    f\"Image logits max diff: {torch.max(torch.abs(outputs.logits_per_image - outputs_fa.logits_per_image))}\",\n+                )\n+                self.assertTrue(\n+                    torch.allclose(outputs.logits_per_text, outputs_fa.logits_per_text, atol=4e-2, rtol=4e-2),\n+                    f\"Text logits max diff: {torch.max(torch.abs(outputs.logits_per_text - outputs_fa.logits_per_text))}\",\n+                )\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n+                )\n+                model.to(torch_device)\n+\n+                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n+                dummy_input_ids = inputs_dict[\"input_ids\"]\n+                dummy_pixel_mask = inputs_dict[\"attention_mask\"]\n+\n+                # right padding\n+                dummy_pixel_mask[:] = 1\n+                dummy_pixel_mask[:, -1:] = 0\n+\n+                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n+                outputs_fa = model_fa(\n+                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n+                )\n+\n+                logits_per_image_eager = outputs.logits_per_image[:, :-1]\n+                logits_per_text_eager = outputs.logits_per_text[:, :-1]\n+\n+                logits_per_image_sdpa = outputs_fa.logits_per_image[:, :-1]\n+                logits_per_text_sdpa = outputs_fa.logits_per_text[:, :-1]\n+\n+                self.assertTrue(\n+                    torch.allclose(logits_per_image_eager, logits_per_image_sdpa, atol=4e-2, rtol=4e-2),\n+                    f\"Image logits max diff: {torch.max(torch.abs(logits_per_image_eager - logits_per_image_sdpa))}\",\n+                )\n+                self.assertTrue(\n+                    torch.allclose(logits_per_text_eager, logits_per_text_sdpa, atol=4e-2, rtol=4e-2),\n+                    f\"Text logits max diff: {torch.max(torch.abs(logits_per_text_eager - logits_per_text_sdpa))}\",\n+                )\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    def test_eager_matches_sdpa_inference(\n+        self,\n+        name,\n+        torch_dtype,\n+        padding_side,\n+        use_attention_mask,\n+        output_attentions,\n+        enable_kernels,\n+    ):\n+        \"We need to relax a bit the `atols` for fp32 here due to the altup projections\"\n+        atols = {\n+            (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 3e-2,  # this was relaxed\n+            (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 3e-2,  # this was relaxed\n+            (\"cuda\", False, torch.float32): 1e-6,\n+            (\"cuda\", False, torch.bfloat16): 3e-2,  # this was relaxed\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-6,\n+            (\"cuda\", True, torch.bfloat16): 3e-2,  # this was relaxed\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+        _test_eager_matches_sdpa_inference(\n+            self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels, atols=atols\n+        )\n+\n+\n+@require_vision\n+@require_torch\n+class Aimv2ModelIntegrationTest(unittest.TestCase):\n+    @slow\n+    def test_inference(self):\n+        model_name = \"yaswanthgali/aimv2-large-patch14-224-lit-HF\"\n+        model = Aimv2Model.from_pretrained(model_name, device_map=torch_device)\n+        processor = AutoProcessor.from_pretrained(model_name)\n+\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+        inputs = processor(\n+            text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, padding=True, return_tensors=\"pt\"\n+        ).to(model.device)\n+\n+        # Forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # Verify the logits\n+        self.assertEqual(\n+            outputs.logits_per_image.shape,\n+            torch.Size((inputs.pixel_values.shape[0], inputs.input_ids.shape[0])),\n+        )\n+        self.assertEqual(\n+            outputs.logits_per_text.shape,\n+            torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n+        )\n+\n+        # handle device\n+        expected_logits = torch.tensor([[34.2415, 24.6724]]).to(model.device)\n+        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+\n+\n+@require_vision\n+@require_torch\n+class Aimv2VisionModelIntegrationTests(unittest.TestCase):\n+    @slow\n+    def test_inference(self):\n+        model_name = \"yaswanthgali/aimv2-large-patch14-224-HF\"\n+\n+        model = Aimv2VisionModel.from_pretrained(model_name, device_map=torch_device)\n+        processor = AutoImageProcessor.from_pretrained(model_name)\n+\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+        inputs = processor(image, return_tensors=\"pt\").to(model.device)\n+\n+        with torch.no_grad():\n+            output = model(**inputs)\n+\n+        # Verify logits shape\n+        self.assertEqual(output.last_hidden_state.shape, torch.Size([1, 256, 1024]))\n+\n+        # Verify logits slice\n+        # fmt: off\n+        expected_logits = torch.tensor(\n+        [[ 0.0510,  0.0806, -0.0990, -0.0154],\n+        [ 2.7850, -2.5143, -0.3320,  2.4196],\n+        [ 2.8179, -2.4089, -0.2770,  2.3218],\n+        [ 2.7641, -2.4114, -0.3684,  2.2998],\n+        [ 2.7972, -2.3180, -0.4490,  2.2302],\n+        [ 2.8584, -2.5322, -0.2302,  2.4936],\n+        [-2.7849,  2.4121,  1.3670, -1.5514]]).to(model.device)\n+        # fmt: on\n+\n+        output_slice = output.last_hidden_state.squeeze(0)[0:7, 0:4]\n+        self.assertTrue(torch.allclose(output_slice, expected_logits, atol=1e-3))\n+\n+    @slow\n+    def test_inference_for_native_resolution(self):\n+        model_name = \"yaswanthgali/aimv2-large-patch14-native-HF\"\n+\n+        model = Aimv2VisionModel.from_pretrained(model_name, device_map=\"auto\")\n+        processor = AutoImageProcessor.from_pretrained(model_name)\n+\n+        image = image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        inputs = processor(image, return_tensors=\"pt\").to(model.device)\n+\n+        with torch.no_grad():\n+            output = model(**inputs)\n+\n+        # Verify logits shape\n+        self.assertEqual(output.last_hidden_state.shape, torch.Size([1, 1530, 1024]))\n+\n+        # Verify logits slice\n+        # fmt: off\n+        expected_logits = torch.tensor(\n+        [[-1.3342,  0.3720,  0.0963,  0.4159],\n+        [-1.5328,  0.4677,  0.0936,  0.4321],\n+        [-0.3775, -0.2758, -0.0803, -0.5367],\n+        [-1.3877,  0.5561, -1.9064, -1.1766],\n+        [-0.5148,  0.0108, -0.4515, -0.6402],\n+        [-0.3400, -0.1711, -0.1855, -0.4219],\n+        [-1.2877, -0.0585, -0.1646,  0.7420]]).to(model.device)\n+        # fmt: on\n+\n+        output_slice = output.last_hidden_state.squeeze(0)[0:7, 0:4]\n+        self.assertTrue(torch.allclose(output_slice, expected_logits, atol=1e-3))"
        },
        {
            "sha": "9fcf14babb9a36ec2edff3e405975967f6873a52",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbdaa7b099e4253be4175e0201cd477e9de05363/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbdaa7b099e4253be4175e0201cd477e9de05363/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=fbdaa7b099e4253be4175e0201cd477e9de05363",
            "patch": "@@ -196,6 +196,7 @@\n # should **not** be the rule.\n IGNORE_NON_AUTO_CONFIGURED = PRIVATE_MODELS.copy() + [\n     # models to ignore for model xxx mapping\n+    \"Aimv2TextModel\",\n     \"AlignTextModel\",\n     \"AlignVisionModel\",\n     \"ClapTextModel\","
        }
    ],
    "stats": {
        "total": 2979,
        "additions": 2977,
        "deletions": 2
    }
}