{
    "author": "minostauros",
    "message": "fix: Qwen2-VL generate with inputs_embeds (#35466)\n\n* fix: Qwen2-VL generate with inputs_embeds\n\n* change: optional input_ids in get_rope_index",
    "sha": "4349a0e4012330b83116a3dc671292a8e46df923",
    "files": [
        {
            "sha": "ea169987b94c838daf60b2cb5dd17986244c4a22",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/4349a0e4012330b83116a3dc671292a8e46df923/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4349a0e4012330b83116a3dc671292a8e46df923/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=4349a0e4012330b83116a3dc671292a8e46df923",
            "patch": "@@ -32,13 +32,8 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-)\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    ModelOutput,\n-)\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -1420,7 +1415,7 @@ def get_decoder(self):\n \n     def get_rope_index(\n         self,\n-        input_ids: torch.LongTensor,\n+        input_ids: Optional[torch.LongTensor] = None,\n         image_grid_thw: Optional[torch.LongTensor] = None,\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n@@ -1550,7 +1545,7 @@ def get_rope_index(\n             if attention_mask is not None:\n                 position_ids = attention_mask.long().cumsum(-1) - 1\n                 position_ids.masked_fill_(attention_mask == 0, 1)\n-                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(input_ids.device)\n+                position_ids = position_ids.unsqueeze(0).expand(3, -1, -1).to(attention_mask.device)\n                 max_position_ids = position_ids.max(0, keepdim=False)[0].max(-1, keepdim=True)[0]\n                 mrope_position_deltas = max_position_ids + 1 - attention_mask.shape[-1]\n             else:\n@@ -1676,7 +1671,7 @@ def forward(\n                 attention_mask = attention_mask.to(inputs_embeds.device)\n \n         # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n-        if position_ids is None and input_ids is not None and (attention_mask is None or attention_mask.ndim == 2):\n+        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n             # calculate RoPE index once per generation in the pre-fill stage only\n             if (cache_position is not None and cache_position[0] == 0) or self.rope_deltas is None:\n                 position_ids, rope_deltas = self.get_rope_index("
        },
        {
            "sha": "c19e0cc4fbd795e70014849c8ee2d3e1d71a0757",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4349a0e4012330b83116a3dc671292a8e46df923/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4349a0e4012330b83116a3dc671292a8e46df923/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=4349a0e4012330b83116a3dc671292a8e46df923",
            "patch": "@@ -1615,9 +1615,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n \n             # There are a few exception patterns in this test:\n             # 1 - Some models can't generate without `input_ids`, when `inputs_embeds` are passed\n-            requires_inputs_ids = any(\n-                model_name in model_class.__name__.lower() for model_name in [\"idefics\", \"qwen2vl\"]\n-            )\n+            requires_inputs_ids = any(model_name in model_class.__name__.lower() for model_name in [\"idefics\"])\n             # 2 - Complex `inputs_embeds` computation, i.e. the correct computation of inputs embeds is more complex\n             # than calling the embedding layer with `input_ids`. Subcases of this exception:\n             #   2.A - Ignore `scale_embedding`, if the model supports it (it is controlled by a model-dependent flag)"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 6,
        "deletions": 13
    }
}