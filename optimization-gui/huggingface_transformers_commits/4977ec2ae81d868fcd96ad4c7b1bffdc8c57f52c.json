{
    "author": "ArthurZucker",
    "message": "[`GPT OSS`] Refactor the tests as it was not properly checking the outputs (#40288)\n\n* it was long due!\n\n* use the official kernel\n\n* more permissive\n\n* update the kernel as well\n\n* mmm should it be this?\n\n* up pu\n\n* fixup\n\n* Update test_modeling_gpt_oss.py\n\n* style\n\n* start with 20b",
    "sha": "4977ec2ae81d868fcd96ad4c7b1bffdc8c57f52c",
    "files": [
        {
            "sha": "62adcf47c48b508127d5e241dc7c0209b22e89b4",
            "filename": "tests/fixtures/gpt_oss/integration_tests.json",
            "status": "modified",
            "additions": 122,
            "deletions": 346,
            "changes": 468,
            "blob_url": "https://github.com/huggingface/transformers/blob/4977ec2ae81d868fcd96ad4c7b1bffdc8c57f52c/tests%2Ffixtures%2Fgpt_oss%2Fintegration_tests.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/4977ec2ae81d868fcd96ad4c7b1bffdc8c57f52c/tests%2Ffixtures%2Fgpt_oss%2Fintegration_tests.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffixtures%2Fgpt_oss%2Fintegration_tests.json?ref=4977ec2ae81d868fcd96ad4c7b1bffdc8c57f52c",
            "patch": "@@ -1,346 +1,122 @@\n-[\n-  {\n-    \"quantized\": true,\n-    \"model\": \"120b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I am a language model, and I can help you too!\\n\\nSure! Here\",\n-      \"How are you? Tell me the name of the president of the United\\n\\nHello! As of my last update in November 2023, the President of the\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"120b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I am a language model, and I can help you too!\\n\\nSure! Here\",\n-      \"How are you? Tell me the name of the president of the United\\n\\nHello! As of my last update in November 2023, the President of the\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"120b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \"Did not work\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"120b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \"Did not work\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"120b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I am a language model, and I can help you too!\\n\\nSure! Here\",\n-      \"How are you? Tell me the name of the president of the United\\n\\nHello! As of my last update in November 2023, the President of the\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"120b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I am a language model, and I can help you too!\\n\\nSure! Here\",\n-      \"How are you? Tell me the name of the president of the United\\n\\nHello! As of my last update in November 2023, the President of the\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"120b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \"Did not work\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"120b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \"Did not work\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"20b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I love you, and I love you too.\\n\\nIt sounds like you're looking for\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"20b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I love you, and I love you too.\\n\\nIt sounds like you're looking for\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"20b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \"Did not work\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"20b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \"Did not work\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"20b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I love you, and I love you too.\\n\\nIt sounds like you're expressing a\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"20b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I love you, and I love you too.\\n\\nIt sounds like you're expressing a\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"20b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \"Did not work\"\n-    ]\n-  },\n-  {\n-    \"quantized\": true,\n-    \"model\": \"20b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \"Did not work\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"120b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue,\\nI am a language model, not a human being.\\n```\\n\\nThis poem is a\",\n-      \"How are you? Tell me the name of the president of the United Kingdom?\\n\\nThe United Kingdom does not have a president. The head of state is the\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"120b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I am a language model trained by OpenAI.\\n\\nI am a large language model\",\n-      \"How are you? Tell me the name of the president of the United\\n\\nHello! I'm an AI language model, so I don't have feelings, but I'm here\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"120b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue,\\nI am a language model, not a human being.\\n```\\n\\nThis poem is a\",\n-      \"How are you? Tell me the name of the president of the United Kingdom?\\n\\nThe United Kingdom does not have a president. The head of state is the\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"120b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I am a language model trained by OpenAI.\\n\\nI am a large language model\",\n-      \"How are you? Tell me the name of the president of the United\\n\\nHello! I'm an AI language model, so I don't have feelings, but I'm here\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"120b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue,\\nI am a language model, not a human being.\\n```\\n\\nThis poem is a\",\n-      \"How are you? Tell me the name of the president of the United States?\\n\\nAs an AI language model, I do not have personal feelings or emotions,\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"120b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I am a language model, and I can help you with your request.\\n\\nSure\",\n-      \"How are you? Tell me the name of the president of the United\\n\\nHello! I'm an AI language model, so I don't have feelings, but I'm here\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"120b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue,\\nI am a language model, not a human being.\\n```\\n\\nThis poem is a\",\n-      \"How are you? Tell me the name of the president of the United States?\\n\\nAs an AI language model, I do not have personal feelings or emotions,\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"120b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I am a language model, and I can help you with your request.\\n\\nSure\",\n-      \"How are you? Tell me the name of the president of the United\\n\\nHello! I'm an AI language model, so I don't have feelings, but I'm here\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"20b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I love you, and I love you too!\\n\\nRoses are red, vio\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"20b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue\\\" (makes sense). But the phrase \\\"the answer is 3\\\" is not a\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The answer to that is \\\"Joe Biden.\\\" The user is asking for the name\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"20b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I love you, and I love you too!\\n\\nRoses are red, vio\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"20b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"ft-hf-o-c/vllm-flash-attn3\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue\\\" (makes sense). But the phrase \\\"the answer is 3\\\" is not a\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The answer to that is \\\"Joe Biden.\\\" The user is asking for the name\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"20b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I love you, and I love you too!\\n\\nRoses are red, vio\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"20b\",\n-    \"kernels\": false,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue.\\\" -> from which we can derive a rule: if we have a red object that is\",\n-      \"How are you? Tell me the name of the president of the United States.\\n\\nI am an AI language model and I do not have a personal life or\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"20b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"eval\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue, I love you, and I love you too!\\n\\nRoses are red, vio\",\n-      \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n-    ]\n-  },\n-  {\n-    \"quantized\": false,\n-    \"model\": \"20b\",\n-    \"kernels\": true,\n-    \"attn_impl\": \"eager\",\n-    \"mode\": \"train\",\n-    \"outputs\": [\n-      \".....Roses are red, violets are blue.\\\" -> from which we can derive a rule: if we have a red object that is\",\n-      \"How are you? Tell me the name of the president of the United States.\\n\\nI am an AI language model and I do not have a personal life or\"\n-    ]\n-  }\n-]\n+{\n+  \"quantized=true|model=120b|kernels=false|attn_impl=kernels-community/vllm-flash-attn3|mode=eval\": [\n+    \"Roses are red, violets are blue, I am a language model, and I can help you too!\\n\\nSure! Here\",\n+    \"How are you? Tell me the name of the president of the United\\n\\nHello! As of my last update in November 2023, the President of the\"\n+  ],\n+  \"quantized=true|model=120b|kernels=false|attn_impl=kernels-community/vllm-flash-attn3|mode=train\": [\n+    \"Roses are red, violets are blue, I am a language model, and I can help you too!\\n\\nSure! Here\",\n+    \"How are you? Tell me the name of the president of the United\\n\\nHello! As of my last update in November 2023, the President of the\"\n+  ],\n+  \"quantized=true|model=120b|kernels=true|attn_impl=kernels-community/vllm-flash-attn3|mode=eval\": [\n+    \"Did not work\"\n+  ],\n+  \"quantized=true|model=120b|kernels=true|attn_impl=kernels-community/vllm-flash-attn3|mode=train\": [\n+    \"Did not work\"\n+  ],\n+  \"quantized=true|model=120b|kernels=false|attn_impl=eager|mode=eval\": [\n+    \"Roses are red, violets are blue, I am a language model, and I can help you too!\\n\\nSure! Here\",\n+    \"How are you? Tell me the name of the president of the United\\n\\nHello! As of my last update in November 2023, the President of the\"\n+  ],\n+  \"quantized=true|model=120b|kernels=false|attn_impl=eager|mode=train\": [\n+    \"Roses are red, violets are blue, I am a language model, and I can help you too!\\n\\nSure! Here\",\n+    \"How are you? Tell me the name of the president of the United\\n\\nHello! As of my last update in November 2023, the President of the\"\n+  ],\n+  \"quantized=true|model=120b|kernels=true|attn_impl=eager|mode=eval\": [\n+    \"Did not work\"\n+  ],\n+  \"quantized=true|model=120b|kernels=true|attn_impl=eager|mode=train\": [\n+    \"Did not work\"\n+  ],\n+  \"quantized=true|model=20b|kernels=false|attn_impl=kernels-community/vllm-flash-attn3|mode=eval\": [\n+    \"Roses are red, violets are blue, I love you, and I love you too.\\nIt sounds like you're looking for\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n+  ],\n+  \"quantized=true|model=20b|kernels=false|attn_impl=kernels-community/vllm-flash-attn3|mode=train\": [\n+    \"Roses are red, violets are blue, I love you, and I love you too.\\n\\nIt sounds like you're looking for\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n+  ],\n+  \"quantized=true|model=20b|kernels=true|attn_impl=kernels-community/vllm-flash-attn3|mode=eval\": [\n+    \"Did not work\"\n+  ],\n+  \"quantized=true|model=20b|kernels=true|attn_impl=kernels-community/vllm-flash-attn3|mode=train\": [\n+    \"Did not work\"\n+  ],\n+  \"quantized=true|model=20b|kernels=false|attn_impl=eager|mode=eval\": [\n+    \"Roses are red, violets are blue, I love you, and I love you too.\\n\\nIt sounds like you're expressing a\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n+  ],\n+  \"quantized=true|model=20b|kernels=false|attn_impl=eager|mode=train\": [\n+    \"Roses are red, violets are blue, I love you, and I love you too.\\n\\nIt sounds like you're expressing a\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n+  ],\n+  \"quantized=true|model=20b|kernels=true|attn_impl=eager|mode=eval\": [\n+    \"Did not work\"\n+  ],\n+  \"quantized=true|model=20b|kernels=true|attn_impl=eager|mode=train\": [\n+    \"Did not work\"\n+  ],\n+  \"quantized=false|model=120b|kernels=false|attn_impl=kernels-community/vllm-flash-attn3|mode=eval\": [\n+    \"Roses are red, violets are blue,\\nI am a language model, not a human being.\\n```\\n\\nThis poem is a\",\n+    \"How are you? Tell me the name of the president of the United Kingdom?\\n\\nThe United Kingdom does not have a president. The head of state is the\"\n+  ],\n+  \"quantized=false|model=120b|kernels=false|attn_impl=kernels-community/vllm-flash-attn3|mode=train\": [\n+    \"Roses are red, violets are blue, I am a language model trained by OpenAI.\\n\\nI am a large language model\",\n+    \"How are you? Tell me the name of the president of the United\\n\\nHello! I'm an AI language model, so I don't have feelings, but I'm here\"\n+  ],\n+  \"quantized=false|model=120b|kernels=true|attn_impl=kernels-community/vllm-flash-attn3|mode=eval\": [\n+    \"Roses are red, violets are blue,\\nI am a language model, not a human being.\\n```\\n\\nThis poem is a\",\n+    \"How are you? Tell me the name of the president of the United Kingdom?\\n\\nThe United Kingdom does not have a president. The head of state is the\"\n+  ],\n+  \"quantized=false|model=120b|kernels=true|attn_impl=kernels-community/vllm-flash-attn3|mode=train\": [\n+    \"Roses are red, violets are blue, I am a language model trained by OpenAI.\\n\\nI am a large language model\",\n+    \"How are you? Tell me the name of the president of the United\\n\\nHello! I'm an AI language model, so I don't have feelings, but I'm here\"\n+  ],\n+  \"quantized=false|model=120b|kernels=false|attn_impl=eager|mode=eval\": [\n+    \"Roses are red, violets are blue,\\nI am a language model, not a human being.\\n```\\n\\nThis poem is a\",\n+    \"How are you? Tell me the name of the president of the United States?\\n\\nAs an AI language model, I do not have personal feelings or emotions,\"\n+  ],\n+  \"quantized=false|model=120b|kernels=false|attn_impl=eager|mode=train\": [\n+    \"Roses are red, violets are blue, I am a language model, and I can help you with your request.\\n\\nSure\",\n+    \"How are you? Tell me the name of the president of the United\\n\\nHello! I'm an AI language model, so I don't have feelings, but I'm here\"\n+  ],\n+  \"quantized=false|model=120b|kernels=true|attn_impl=eager|mode=eval\": [\n+    \"Roses are red, violets are blue,\\nI am a language model, not a human being.\\n```\\n\\nThis poem is a\",\n+    \"How are you? Tell me the name of the president of the United States?\\n\\nAs an AI language model, I do not have personal feelings or emotions,\"\n+  ],\n+  \"quantized=false|model=120b|kernels=true|attn_impl=eager|mode=train\": [\n+    \"Roses are red, violets are blue, I am a language model, and I can help you with your request.\\n\\nSure\",\n+    \"How are you? Tell me the name of the president of the United\\n\\nHello! I'm an AI language model, so I don't have feelings, but I'm here\"\n+  ],\n+  \"quantized=false|model=20b|kernels=false|attn_impl=kernels-community/vllm-flash-attn3|mode=eval\": [\n+    \"Roses are red, violets are blue, I love you, and I love you too!\\n\\nRoses are red, vio\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n+  ],\n+  \"quantized=false|model=20b|kernels=false|attn_impl=kernels-community/vllm-flash-attn3|mode=train\": [\n+    \"Roses are red, violets are blue\\\" (makes sense). But the phrase \\\"the answer is 3\\\" is not a\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The answer to that is \\\"Joe Biden.\\\" The user is asking for the name\"\n+  ],\n+  \"quantized=false|model=20b|kernels=true|attn_impl=kernels-community/vllm-flash-attn3|mode=eval\": [\n+    \"Roses are red, violets are blue, I love you, and I love you too!\\n\\nRoses are red, vio\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n+  ],\n+  \"quantized=false|model=20b|kernels=true|attn_impl=kernels-community/vllm-flash-attn3|mode=train\": [\n+    \"Roses are red, violets are blue\\\" (makes sense). But the phrase \\\"the answer is 3\\\" is not a\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The answer to that is \\\"Joe Biden.\\\" The user is asking for the name\"\n+  ],\n+  \"quantized=false|model=20b|kernels=false|attn_impl=eager|mode=eval\": [\n+    \"Roses are red, violets are blue, I love you, and I love you too!\\n\\nRoses are red, vio\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n+  ],\n+  \"quantized=false|model=20b|kernels=false|attn_impl=eager|mode=train\": [\n+    \"Roses are red, violets are blue.\\\" -> from which we can derive a rule: if we have a red object that is\",\n+    \"How are you? Tell me the name of the president of the United States.\\n\\nI am an AI language model and I do not have a personal life or\"\n+  ],\n+  \"quantized=false|model=20b|kernels=true|attn_impl=eager|mode=eval\": [\n+    \"Roses are red, violets are blue, I love you, and I love you too!\\n\\nRoses are red, vio\",\n+    \"How are you? Tell me the name of the president of the United States.\\\" The assistant should respond with the name of the president. The user is asking for\"\n+  ],\n+  \"quantized=false|model=20b|kernels=true|attn_impl=eager|mode=train\": [\n+    \"Roses are red, violets are blue.\\\" -> from which we can derive a rule: if we have a red object that is\",\n+    \"How are you? Tell me the name of the president of the United States.\\n\\nI am an AI language model and I do not have a personal life or\"\n+  ]\n+}"
        },
        {
            "sha": "c439f68d4a46fb4997fb9db32da0638c27f5f939",
            "filename": "tests/models/gpt_oss/test_modeling_gpt_oss.py",
            "status": "modified",
            "additions": 124,
            "deletions": 59,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/4977ec2ae81d868fcd96ad4c7b1bffdc8c57f52c/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4977ec2ae81d868fcd96ad4c7b1bffdc8c57f52c/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py?ref=4977ec2ae81d868fcd96ad4c7b1bffdc8c57f52c",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch GptOss model.\"\"\"\n \n+import difflib\n import inspect\n import json\n import os\n@@ -194,6 +195,10 @@ def distributed_worker(quantized, model_size, kernels, attn_impl, mode):\n     from transformers import AutoModelForCausalLM, AutoTokenizer\n     from transformers.testing_utils import torch_device\n \n+    def generate_config_key(quantized, model, kernels, attn_impl, mode):\n+        \"\"\"Generate a key for the restructured integration test results.\"\"\"\n+        return f\"quantized={str(quantized).lower()}|model={model}|kernels={str(kernels).lower()}|attn_impl={attn_impl}|mode={mode}\"\n+\n     input_text = [\n         \"Roses are red, violets\",\n         \"How are you? Tell me the name of the president of\",\n@@ -204,7 +209,7 @@ def distributed_worker(quantized, model_size, kernels, attn_impl, mode):\n     kernels = kernels.lower() == \"true\"\n \n     # Distributed model loading\n-    model_id = f\"/fsx/vb/new-oai/gpt-oss-{model_size}-trfs\"\n+    model_id = f\"openai/gpt-oss-{model_size}\"\n     model = AutoModelForCausalLM.from_pretrained(\n         model_id,\n         torch_dtype=\"auto\",\n@@ -219,26 +224,54 @@ def distributed_worker(quantized, model_size, kernels, attn_impl, mode):\n     output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n     output_texts = tokenizer.batch_decode(output, skip_special_tokens=False)\n \n-    # Only rank 0 writes results\n+    # Only rank 0 writes results and validates against expected outputs\n     if int(os.environ.get(\"RANK\", \"0\")) == 0:\n-        result_entry = {\n-            \"quantized\": quantized,\n-            \"model\": model_size,\n-            \"kernels\": kernels,\n-            \"attn_impl\": attn_impl,\n-            \"mode\": mode,\n-            \"outputs\": output_texts,\n-        }\n+        # Generate key to look up expected outputs\n+        key = generate_config_key(quantized, model_size, kernels, attn_impl, mode)\n \n+        # Load expected outputs from restructured JSON\n         if os.path.exists(RESULTS_PATH):\n             with open(RESULTS_PATH, \"r\") as f:\n-                results = json.load(f)\n+                expected_results = json.load(f)\n+\n+            # Check if we have expected results for this configuration\n+            if key in expected_results:\n+                expected_outputs = expected_results[key]\n+\n+                # Compare actual outputs with expected outputs\n+                assert len(output_texts) == len(expected_outputs), f\"Output length mismatch for {key}\"\n+\n+                for i, (actual, expected) in enumerate(zip(output_texts, expected_outputs)):\n+                    actual_stripped = actual.strip()\n+                    expected_stripped = expected.strip()\n+\n+                    # Make lengths match by taking minimum length to be resilient to generation differences\n+                    min_length = min(len(actual_stripped), len(expected_stripped))\n+                    actual_truncated = actual_stripped[:min_length]\n+                    expected_truncated = expected_stripped[:min_length]\n+\n+                    if actual_truncated != expected_truncated:\n+                        diff = \"\\n\".join(\n+                            difflib.unified_diff(\n+                                expected_truncated.splitlines(keepends=True),\n+                                actual_truncated.splitlines(keepends=True),\n+                                fromfile=f\"expected[{i}]\",\n+                                tofile=f\"actual[{i}]\",\n+                                lineterm=\"\",\n+                            )\n+                        )\n+                        raise AssertionError(\n+                            f\"Output mismatch at index {i} for {key}:\\n\"\n+                            f\"Expected: '{expected_stripped}'\\n\"\n+                            f\"Actual:   '{actual_stripped}'\\n\"\n+                            f\"Diff (truncated to min length {min_length}):\\n{diff}\"\n+                        )\n+\n+                print(f\"âœ“ Outputs match expected results for {key}\")\n+            else:\n+                print(f\"Warning: No expected results found for configuration: {key}\")\n         else:\n-            results = []\n-        results.append(result_entry)\n-\n-        with open(RESULTS_PATH, \"w\") as f:\n-            json.dump(results, f, indent=2)\n+            print(f\"Warning: Results file {RESULTS_PATH} not found\")\n \n \n @slow\n@@ -249,6 +282,11 @@ class GptOssIntegrationTest(unittest.TestCase):\n         \"How are you? Tell me the name of the president of\",\n     ]\n \n+    @staticmethod\n+    def generate_config_key(quantized, model, kernels, attn_impl, mode):\n+        \"\"\"Generate a key for the restructured integration test results.\"\"\"\n+        return f\"quantized={str(quantized).lower()}|model={model}|kernels={str(kernels).lower()}|attn_impl={attn_impl}|mode={mode}\"\n+\n     def setUp(self):\n         cleanup(torch_device, gc_collect=True)\n \n@@ -271,7 +309,7 @@ def load_and_forward(model_id, attn_implementation, input_text, **pretrained_kwa\n \n         inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(model.device)\n         output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n-        output_text = tokenizer.batch_decode(output, skip_special_tokens=False)\n+        output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n         return output_text\n \n     # ------------------------\n@@ -320,38 +358,38 @@ def run_distributed_test(quantized, model, kernels, attn_impl, mode):\n     # Shared parameterization\n     # ------------------------\n     PARAMETERS = [\n-        (False, \"120b\", False, \"eager\", \"eval\"),\n-        (False, \"120b\", False, \"eager\", \"train\"),\n-        (False, \"120b\", False, \"ft-hf-o-c/vllm-flash-attn3\", \"eval\"),\n-        (False, \"120b\", False, \"ft-hf-o-c/vllm-flash-attn3\", \"train\"),\n-        (False, \"120b\", True, \"eager\", \"eval\"),\n-        (False, \"120b\", True, \"eager\", \"train\"),\n-        (False, \"120b\", True, \"ft-hf-o-c/vllm-flash-attn3\", \"eval\"),\n-        (False, \"120b\", True, \"ft-hf-o-c/vllm-flash-attn3\", \"train\"),\n-        (True, \"120b\", False, \"eager\", \"eval\"),\n-        (True, \"120b\", False, \"eager\", \"train\"),\n-        (True, \"120b\", False, \"ft-hf-o-c/vllm-flash-attn3\", \"eval\"),\n-        (True, \"120b\", False, \"ft-hf-o-c/vllm-flash-attn3\", \"train\"),\n-        (True, \"120b\", True, \"eager\", \"eval\"),\n-        (True, \"120b\", True, \"eager\", \"train\"),\n-        (True, \"120b\", True, \"ft-hf-o-c/vllm-flash-attn3\", \"eval\"),\n-        (True, \"120b\", True, \"ft-hf-o-c/vllm-flash-attn3\", \"train\"),\n         (False, \"20b\", False, \"eager\", \"eval\"),\n         (False, \"20b\", False, \"eager\", \"train\"),\n-        (False, \"20b\", False, \"ft-hf-o-c/vllm-flash-attn3\", \"eval\"),\n-        (False, \"20b\", False, \"ft-hf-o-c/vllm-flash-attn3\", \"train\"),\n+        (False, \"20b\", False, \"kernels-community/vllm-flash-attn3\", \"eval\"),\n+        (False, \"20b\", False, \"kernels-community/vllm-flash-attn3\", \"train\"),\n         (False, \"20b\", True, \"eager\", \"eval\"),\n         (False, \"20b\", True, \"eager\", \"train\"),\n-        (False, \"20b\", True, \"ft-hf-o-c/vllm-flash-attn3\", \"eval\"),\n-        (False, \"20b\", True, \"ft-hf-o-c/vllm-flash-attn3\", \"train\"),\n+        (False, \"20b\", True, \"kernels-community/vllm-flash-attn3\", \"eval\"),\n+        (False, \"20b\", True, \"kernels-community/vllm-flash-attn3\", \"train\"),\n         (True, \"20b\", False, \"eager\", \"eval\"),\n         (True, \"20b\", False, \"eager\", \"train\"),\n-        (True, \"20b\", False, \"ft-hf-o-c/vllm-flash-attn3\", \"eval\"),\n-        (True, \"20b\", False, \"ft-hf-o-c/vllm-flash-attn3\", \"train\"),\n+        (True, \"20b\", False, \"kernels-community/vllm-flash-attn3\", \"eval\"),\n+        (True, \"20b\", False, \"kernels-community/vllm-flash-attn3\", \"train\"),\n         (True, \"20b\", True, \"eager\", \"eval\"),\n         (True, \"20b\", True, \"eager\", \"train\"),\n-        (True, \"20b\", True, \"ft-hf-o-c/vllm-flash-attn3\", \"eval\"),\n-        (True, \"20b\", True, \"ft-hf-o-c/vllm-flash-attn3\", \"train\"),\n+        (True, \"20b\", True, \"kernels-community/vllm-flash-attn3\", \"eval\"),\n+        (True, \"20b\", True, \"kernels-community/vllm-flash-attn3\", \"train\"),\n+        (False, \"120b\", False, \"eager\", \"eval\"),\n+        (False, \"120b\", False, \"eager\", \"train\"),\n+        (False, \"120b\", False, \"kernels-community/vllm-flash-attn3\", \"eval\"),\n+        (False, \"120b\", False, \"kernels-community/vllm-flash-attn3\", \"train\"),\n+        (False, \"120b\", True, \"eager\", \"eval\"),\n+        (False, \"120b\", True, \"eager\", \"train\"),\n+        (False, \"120b\", True, \"kernels-community/vllm-flash-attn3\", \"eval\"),\n+        (False, \"120b\", True, \"kernels-community/vllm-flash-attn3\", \"train\"),\n+        (True, \"120b\", False, \"eager\", \"eval\"),\n+        (True, \"120b\", False, \"eager\", \"train\"),\n+        (True, \"120b\", False, \"kernels-community/vllm-flash-attn3\", \"eval\"),\n+        (True, \"120b\", False, \"kernels-community/vllm-flash-attn3\", \"train\"),\n+        (True, \"120b\", True, \"eager\", \"eval\"),\n+        (True, \"120b\", True, \"eager\", \"train\"),\n+        (True, \"120b\", True, \"kernels-community/vllm-flash-attn3\", \"eval\"),\n+        (True, \"120b\", True, \"kernels-community/vllm-flash-attn3\", \"train\"),\n     ]\n \n     # ------------------------\n@@ -360,31 +398,58 @@ def run_distributed_test(quantized, model, kernels, attn_impl, mode):\n     @parameterized.expand(PARAMETERS)\n     @require_read_token\n     def test_model_outputs(self, quantized, model, kernels, attn_impl, mode):\n-        model_id = f\"/fsx/vb/new-oai/gpt-oss-{model}-trfs\"\n+        model_id = f\"openai/gpt-oss-{model}\"\n         output_texts = self.load_and_forward(\n             model_id,\n             attn_impl,\n             self.input_text,\n             use_kernels=kernels,\n         )\n \n-        result_entry = {\n-            \"quantized\": quantized,\n-            \"model\": model,\n-            \"kernels\": kernels,\n-            \"attn_impl\": attn_impl,\n-            \"mode\": mode,\n-            \"outputs\": output_texts,\n-        }\n+        # Generate key to look up expected outputs\n+        key = self.generate_config_key(quantized, model, kernels, attn_impl, mode)\n \n+        # Load expected outputs from restructured JSON\n         if os.path.exists(RESULTS_PATH):\n             with open(RESULTS_PATH, \"r\") as f:\n-                results = json.load(f)\n-        else:\n-            results = []\n-        results.append(result_entry)\n-        with open(RESULTS_PATH, \"w\") as f:\n-            json.dump(results, f, indent=2)\n+                expected_results = json.load(f)\n+\n+            # Check if we have expected results for this configuration\n+            if key in expected_results:\n+                expected_outputs = expected_results[key]\n+\n+                # Compare actual outputs with expected outputs\n+                self.assertEqual(len(output_texts), len(expected_outputs), f\"Output length mismatch for {key}\")\n+\n+                for i, (actual, expected) in enumerate(zip(output_texts, expected_outputs)):\n+                    actual_stripped = actual.strip()\n+                    expected_stripped = expected.strip()\n+\n+                    # Make lengths match by taking minimum length to be resilient to generation differences\n+                    min_length = min(len(actual_stripped), len(expected_stripped))\n+                    actual_truncated = actual_stripped[:min_length]\n+                    expected_truncated = expected_stripped[:min_length]\n+\n+                    if actual_truncated != expected_truncated:\n+                        diff = \"\\n\".join(\n+                            difflib.unified_diff(\n+                                expected_truncated.splitlines(keepends=True),\n+                                actual_truncated.splitlines(keepends=True),\n+                                fromfile=f\"expected[{i}]\",\n+                                tofile=f\"actual[{i}]\",\n+                                lineterm=\"\",\n+                            )\n+                        )\n+                        self.fail(\n+                            f\"Output mismatch at index {i} for {key}:\\n\"\n+                            f\"Expected: '{expected_stripped}'\\n\"\n+                            f\"Actual:   '{actual_stripped}'\\n\"\n+                            f\"Diff (truncated to min length {min_length}):\\n{diff}\"\n+                        )\n+            else:\n+                # If no expected results exist, this is a new configuration\n+                # We could optionally add it to the results file here\n+                print(f\"Warning: No expected results found for configuration: {key}\")\n \n         self.assertIsInstance(output_texts, list)\n         self.assertTrue(all(isinstance(x, str) for x in output_texts))\n@@ -409,7 +474,7 @@ def test_training_step(self, quantized, model, kernels, attn_impl, mode):\n         if quantized:\n             self.skipTest(\"Training test for quantized models is not supported.\")\n \n-        model_id = f\"/fsx/vb/new-oai/gpt-oss-{model}-trfs\"\n+        model_id = f\"openai/gpt-oss-{model}\"\n \n         model_obj = AutoModelForCausalLM.from_pretrained(\n             model_id,\n@@ -471,7 +536,7 @@ def test_model_matches_original_20b(self):\n             ]\n         )\n \n-        model_id = \"/fsx/vb/new-oai/gpt-oss-20b-trfs\"\n+        model_id = \"openai/gpt-oss-20b\"\n \n         model = AutoModelForCausalLM.from_pretrained(\n             model_id,\n@@ -537,7 +602,7 @@ def test_model_matches_original_120b(self):\n             ]\n         )\n \n-        model_id = \"/fsx/vb/new-oai/gpt-oss-120b-trfs\"\n+        model_id = \"openai/gpt-oss-120b\"\n \n         model = AutoModelForCausalLM.from_pretrained(\n             model_id,"
        }
    ],
    "stats": {
        "total": 651,
        "additions": 246,
        "deletions": 405
    }
}