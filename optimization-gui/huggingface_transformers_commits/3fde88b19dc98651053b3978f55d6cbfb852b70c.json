{
    "author": "lhoestq",
    "message": "support chat generator as input of TextGenerationPipeline (#35551)\n\n* support chat generator as input of TextGenerationPipeline\r\n\r\n* missing import\r\n\r\n* fix tests\r\n\r\n* again\r\n\r\n* simpler\r\n\r\n* add test",
    "sha": "3fde88b19dc98651053b3978f55d6cbfb852b70c",
    "files": [
        {
            "sha": "e15228fbe0aa6e9c03dd288d026ee2c191f3236f",
            "filename": "src/transformers/pipelines/text_generation.py",
            "status": "modified",
            "additions": 22,
            "deletions": 9,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/3fde88b19dc98651053b3978f55d6cbfb852b70c/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3fde88b19dc98651053b3978f55d6cbfb852b70c/src%2Ftransformers%2Fpipelines%2Ftext_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_generation.py?ref=3fde88b19dc98651053b3978f55d6cbfb852b70c",
            "patch": "@@ -1,4 +1,6 @@\n import enum\n+import itertools\n+import types\n import warnings\n from typing import Dict\n \n@@ -260,16 +262,27 @@ def __call__(self, text_inputs, **kwargs):\n               ids of the generated text.\n         \"\"\"\n         if isinstance(\n-            text_inputs, (list, tuple, KeyDataset) if is_torch_available() else (list, tuple)\n-        ) and isinstance(text_inputs[0], (list, tuple, dict)):\n-            # We have one or more prompts in list-of-dicts format, so this is chat mode\n-            if isinstance(text_inputs[0], dict):\n-                return super().__call__(Chat(text_inputs), **kwargs)\n+            text_inputs,\n+            (list, tuple, types.GeneratorType, KeyDataset)\n+            if is_torch_available()\n+            else (list, tuple, types.GeneratorType),\n+        ):\n+            if isinstance(text_inputs, types.GeneratorType):\n+                text_inputs, _ = itertools.tee(text_inputs)\n+                text_inputs, first_item = (x for x in text_inputs), next(_)\n             else:\n-                chats = [Chat(chat) for chat in text_inputs]  # üêà üêà üêà\n-                return super().__call__(chats, **kwargs)\n-        else:\n-            return super().__call__(text_inputs, **kwargs)\n+                first_item = text_inputs[0]\n+            if isinstance(first_item, (list, tuple, dict)):\n+                # We have one or more prompts in list-of-dicts format, so this is chat mode\n+                if isinstance(first_item, dict):\n+                    return super().__call__(Chat(text_inputs), **kwargs)\n+                else:\n+                    chats = (Chat(chat) for chat in text_inputs)  # üêà üêà üêà\n+                    if isinstance(text_inputs, types.GeneratorType):\n+                        return super().__call__(chats, **kwargs)\n+                    else:\n+                        return super().__call__(list(chats), **kwargs)\n+        return super().__call__(text_inputs, **kwargs)\n \n     def preprocess(\n         self,"
        },
        {
            "sha": "7de84e646e192d95661b5c49d5a6a16606faae8e",
            "filename": "tests/pipelines/test_pipelines_text_generation.py",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/3fde88b19dc98651053b3978f55d6cbfb852b70c/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3fde88b19dc98651053b3978f55d6cbfb852b70c/tests%2Fpipelines%2Ftest_pipelines_text_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_generation.py?ref=3fde88b19dc98651053b3978f55d6cbfb852b70c",
            "patch": "@@ -292,6 +292,50 @@ def __getitem__(self, i):\n                 ],\n             )\n \n+    @require_torch\n+    def test_small_chat_model_with_iterator_pt(self):\n+        from transformers.pipelines.pt_utils import PipelineIterator\n+\n+        text_generator = pipeline(\n+            task=\"text-generation\", model=\"hf-internal-testing/tiny-gpt2-with-chatml-template\", framework=\"pt\"\n+        )\n+\n+        # Using `do_sample=False` to force deterministic output\n+        chat1 = [\n+            {\"role\": \"system\", \"content\": \"This is a system message.\"},\n+            {\"role\": \"user\", \"content\": \"This is a test\"},\n+        ]\n+        chat2 = [\n+            {\"role\": \"system\", \"content\": \"This is a system message.\"},\n+            {\"role\": \"user\", \"content\": \"This is a second test\"},\n+        ]\n+        expected_chat1 = chat1 + [\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": \" factors factors factors factors factors factors factors factors factors factors\",\n+            }\n+        ]\n+        expected_chat2 = chat2 + [\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": \" stairs stairs stairs stairs stairs stairs stairs stairs stairs stairs\",\n+            }\n+        ]\n+\n+        def data():\n+            yield from [chat1, chat2]\n+\n+        outputs = text_generator(data(), do_sample=False, max_new_tokens=10)\n+        assert isinstance(outputs, PipelineIterator)\n+        outputs = list(outputs)\n+        self.assertEqual(\n+            outputs,\n+            [\n+                [{\"generated_text\": expected_chat1}],\n+                [{\"generated_text\": expected_chat2}],\n+            ],\n+        )\n+\n     @require_tf\n     def test_small_model_tf(self):\n         text_generator = pipeline(task=\"text-generation\", model=\"sshleifer/tiny-ctrl\", framework=\"tf\")"
        }
    ],
    "stats": {
        "total": 75,
        "additions": 66,
        "deletions": 9
    }
}