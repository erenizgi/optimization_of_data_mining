{
    "author": "SunMarc",
    "message": "Fix serving continuous batching (#41624)\n\n* udpate-serving-cb\n\n* style\n\n* style\n\n* check none\n\n* Apply suggestions from code review\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "9839d57a0244f86125c89981f6301b48309b4913",
    "files": [
        {
            "sha": "a23869b196182ee5ff7f5cf6f68c1dc0c2e4bfb4",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9839d57a0244f86125c89981f6301b48309b4913/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9839d57a0244f86125c89981f6301b48309b4913/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=9839d57a0244f86125c89981f6301b48309b4913",
            "patch": "@@ -380,7 +380,7 @@ CB is opt-in and currently applies to chat completions.\n ```sh\n transformers serve \\\n   --continuous-batching\n-  --attn_implementation sdpa_paged\n+  --attn_implementation \"sdpa\"\n ```\n \n ### Performance tips\n@@ -390,11 +390,10 @@ transformers serve \\\n ```sh\n transformers serve \\\n   --continuous_batching \\\n-  --attn_implementation paged_attention\n+  --attn_implementation \"flash_attention_2\"\n ```\n \n > [!TIP]\n-> If you choose `paged_attention`, you must install `flash-attn` separately: `pip install flash-attn --no-build-isolation`\n \n - `--dtype {bfloat16|float16}` typically improve throughput and memory use vs. `float32`\n "
        },
        {
            "sha": "21fc006e2f40327c76ac17e57e24967207e4aac7",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9839d57a0244f86125c89981f6301b48309b4913/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9839d57a0244f86125c89981f6301b48309b4913/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=9839d57a0244f86125c89981f6301b48309b4913",
            "patch": "@@ -929,14 +929,6 @@ def request_id_iter(self, request_id: str) -> Generator[GenerationOutput]:\n             if self.batch_processor is not None:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n-    @staticmethod\n-    def supported_attention_implementations() -> set[str]:\n-        return {\"eager_paged\", \"sdpa_paged\", \"flash_attention_2\"}\n-\n-    @staticmethod\n-    def default_attention_implementation() -> str:\n-        return \"sdpa_paged\"\n-\n     @traced\n     def warmup(self, batch_processor: ContinuousBatchProcessor) -> None:\n         stream = torch.cuda.Stream(device=self.model.device)"
        },
        {
            "sha": "38f5161453b710a01ae23e01f8315b7def62ef66",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/9839d57a0244f86125c89981f6301b48309b4913/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9839d57a0244f86125c89981f6301b48309b4913/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9839d57a0244f86125c89981f6301b48309b4913",
            "patch": "@@ -2419,30 +2419,30 @@ def get_correct_attn_implementation(self, requested_attention: Optional[str], is\n         if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n             message = (\n                 f'Specified `attn_implementation=\"{applicable_attention}\"` is not supported. The only possible arguments are '\n-                '`attn_implementation=\"eager\"`'\n+                '`attn_implementation=\"eager\"`, `\"paged|eager\"`'\n             )\n             # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n             if self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False):\n-                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`'\n+                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`, `\"attn_implementation=paged|flash_attention_2\"`'\n             if self._supports_sdpa:\n-                message += ', `\"attn_implementation=sdpa\"'\n+                message += ', `\"attn_implementation=sdpa\"`, `\"attn_implementation=paged|spda\"`'\n             if self._supports_flex_attn:\n                 message += ', `\"attn_implementation=flex_attention\"`'\n             raise ValueError(message + \".\")\n \n         # Perform relevant checks\n-        if applicable_attention == \"flash_attention_2\":\n+        if \"flash_attention_2\" in applicable_attention:\n             self._flash_attn_2_can_dispatch(is_init_check)\n-        elif applicable_attention == \"flash_attention_3\":\n+        elif \"flash_attention_3\" in applicable_attention:\n             self._flash_attn_3_can_dispatch(is_init_check)\n-        elif applicable_attention == \"flex_attention\":\n+        elif \"flex_attention\" in applicable_attention:\n             self._flex_attn_can_dispatch(is_init_check)\n-        elif applicable_attention == \"sdpa\":\n+        elif \"sdpa\" in applicable_attention:\n             # Sdpa is the default, so we try it and fallback to eager otherwise when not possible\n             try:\n                 self._sdpa_can_dispatch(is_init_check)\n             except (ValueError, ImportError) as e:\n-                if requested_attention == \"sdpa\":\n+                if requested_attention is not None and \"sdpa\" in requested_attention:\n                     raise e\n                 applicable_attention = \"eager\"\n "
        },
        {
            "sha": "55cdab28ae610197d7a3ee7ef969373ee7d21908",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9839d57a0244f86125c89981f6301b48309b4913/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9839d57a0244f86125c89981f6301b48309b4913/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=9839d57a0244f86125c89981f6301b48309b4913",
            "patch": "@@ -1167,6 +1167,13 @@ def is_mistral_common_available() -> bool:\n     return _is_package_available(\"mistral_common\")\n \n \n+@lru_cache\n+def is_opentelemetry_available() -> bool:\n+    return _is_package_available(\"opentelemetry\") and version.parse(\n+        importlib.metadata.version(\"opentelemetry-api\")\n+    ) >= version.parse(\"1.30.0\")\n+\n+\n def check_torch_load_is_safe() -> None:\n     if not is_torch_greater_or_equal(\"2.6\"):\n         raise ValueError("
        },
        {
            "sha": "a85a3a0fa28b1df454b55152187a8215f9a5fbd9",
            "filename": "src/transformers/utils/metrics.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9839d57a0244f86125c89981f6301b48309b4913/src%2Ftransformers%2Futils%2Fmetrics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9839d57a0244f86125c89981f6301b48309b4913/src%2Ftransformers%2Futils%2Fmetrics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fmetrics.py?ref=9839d57a0244f86125c89981f6301b48309b4913",
            "patch": "@@ -5,6 +5,8 @@\n from enum import Enum\n from typing import Any, Optional, Union\n \n+from .import_utils import is_opentelemetry_available\n+\n \n class RequestStatus(Enum):\n     \"\"\"Status of a generation request through its lifecycle.\"\"\"\n@@ -18,12 +20,12 @@ class RequestStatus(Enum):\n     FAILED = \"failed\"\n \n \n-try:\n+if is_opentelemetry_available():\n     from opentelemetry import metrics\n     from opentelemetry.trace import Status, StatusCode, get_tracer\n \n     _has_opentelemetry = True\n-except ImportError:\n+else:\n     _has_opentelemetry = False\n \n \n@@ -183,7 +185,10 @@ def _setup_metrics(self):\n         \"\"\"Initialize OpenTelemetry metrics and tracing if the library is available.\"\"\"\n \n         if not _has_opentelemetry:\n-            logger.info(\"OpenTelemetry is not installed. Metrics and tracing will not be recorded.\")\n+            logger.info(\n+                \"OpenTelemetry is not installed. Metrics and tracing will not be recorded.\"\n+                \"You can install it with `pip install opentelemetry-api>=1.30.0`\"\n+            )\n             return\n \n         self.meter = metrics.get_meter(\"transformers.generation.continuous_batch_processor\")"
        }
    ],
    "stats": {
        "total": 47,
        "additions": 25,
        "deletions": 22
    }
}