{
    "author": "nikosanto13",
    "message": "Introduce modular files for speech models (#35902)\n\n* WAV_2_VEC_2 to WAV2VEC2\n\n* added modular files for hubert, wavlm, wav2vec2_bert, data2vec_audio\n\n* remove unnessary definitions in modulars\n\n* added modular files for UniSpeech, UniSpeechSat, Wav2Vec2Conformer\n\n* docstring fix for UniSpeechForCTC\n\n* removed unneccessary re-definition of modular classes\n\n* reverted lazy imports change on modular_model_converter, type-alias for Wav2Vec2BaseModelOutput\n\n* top-level import of deepspeed in seamless_m4t, speecht5\n\n* avoid tracking imports inside classes, relocate lazy deepspeed, peft imports in their original locations\n\n* convert modular\n\n* tiny modular typing fixes\n\n* some more modular fixes\n\n* make style\n\n---------\n\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>",
    "sha": "f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
    "files": [
        {
            "sha": "d3271b5e54a3f60e4196333500ab4e92f7624acb",
            "filename": "setup.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -307,7 +307,12 @@ def run(self):\n extras[\"integrations\"] = extras[\"hub-kernels\"] + extras[\"optuna\"] + extras[\"ray\"] + extras[\"sigopt\"]\n \n extras[\"serving\"] = deps_list(\"pydantic\", \"uvicorn\", \"fastapi\", \"starlette\")\n-extras[\"audio\"] = deps_list(\"librosa\", \"pyctcdecode\", \"phonemizer\", \"kenlm@git+https://github.com/ydshieh/kenlm@78f664fb3dafe1468d868d71faf19534530698d5\")\n+extras[\"audio\"] = deps_list(\n+    \"librosa\",\n+    \"pyctcdecode\",\n+    \"phonemizer\",\n+    \"kenlm@git+https://github.com/ydshieh/kenlm@78f664fb3dafe1468d868d71faf19534530698d5\",\n+)\n # `pip install \".[speech]\"` is deprecated and `pip install \".[torch-speech]\"` should be used instead\n extras[\"speech\"] = deps_list(\"torchaudio\") + extras[\"audio\"]\n extras[\"torch-speech\"] = deps_list(\"torchaudio\") + extras[\"audio\"]"
        },
        {
            "sha": "3d1bdaeca944f32551013fc4d3c65b86b78b6782",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -14,7 +14,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from functools import partial\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple\n \n import torch\n import torch.nn as nn\n@@ -465,7 +465,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "7f799e812970335de33110a9be8a648db8303b04",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 171,
            "deletions": 199,
            "changes": 370,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1,26 +1,15 @@\n-# coding=utf-8\n-# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Data2VecAudio model.\"\"\"\n-\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/data2vec/modular_data2vec_audio.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_data2vec_audio.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n import warnings\n from typing import Optional, Tuple, Union\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n@@ -50,141 +39,14 @@\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n-logger = logging.get_logger(__name__)\n-\n-\n-_HIDDEN_STATES_START_POSITION = 2\n \n-# General docstring\n-_CONFIG_FOR_DOC = \"Data2VecAudioConfig\"\n+logger = logging.get_logger(__name__)\n \n # Base docstring\n _CHECKPOINT_FOR_DOC = \"facebook/data2vec-audio-base-960h\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n-\n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n-_CTC_EXPECTED_LOSS = 66.95\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices\n-def _compute_mask_indices(\n-    shape: Tuple[int, int],\n-    mask_prob: float,\n-    mask_length: int,\n-    attention_mask: Optional[torch.LongTensor] = None,\n-    min_masks: int = 0,\n-) -> np.ndarray:\n-    \"\"\"\n-    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n-    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n-    CPU as part of the preprocessing during training.\n-\n-    Args:\n-        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n-               the first element is the batch size and the second element is the length of the axis to span.\n-        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n-                    independently generated mask spans of length `mask_length` is computed by\n-                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n-                    actual percentage will be smaller.\n-        mask_length: size of the mask\n-        min_masks: minimum number of masked spans\n-        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n-                        each batch dimension.\n-    \"\"\"\n-    batch_size, sequence_length = shape\n \n-    if mask_length < 1:\n-        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n-\n-    if mask_length > sequence_length:\n-        raise ValueError(\n-            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n-            f\" and `sequence_length`: {sequence_length}`\"\n-        )\n-\n-    # epsilon is used for probabilistic rounding\n-    epsilon = np.random.rand(1).item()\n-\n-    def compute_num_masked_span(input_length):\n-        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n-        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n-        num_masked_span = max(num_masked_span, min_masks)\n-\n-        # make sure num masked span <= sequence_length\n-        if num_masked_span * mask_length > sequence_length:\n-            num_masked_span = sequence_length // mask_length\n-\n-        # make sure num_masked span is also <= input_length - (mask_length - 1)\n-        if input_length - (mask_length - 1) < num_masked_span:\n-            num_masked_span = max(input_length - (mask_length - 1), 0)\n-\n-        return num_masked_span\n-\n-    # compute number of masked spans in batch\n-    input_lengths = (\n-        attention_mask.detach().sum(-1).tolist()\n-        if attention_mask is not None\n-        else [sequence_length for _ in range(batch_size)]\n-    )\n-\n-    # SpecAugment mask to fill\n-    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n-    spec_aug_mask_idxs = []\n-\n-    max_num_masked_span = compute_num_masked_span(sequence_length)\n-\n-    if max_num_masked_span == 0:\n-        return spec_aug_mask\n-\n-    for input_length in input_lengths:\n-        # compute num of masked spans for this input\n-        num_masked_span = compute_num_masked_span(input_length)\n-\n-        # get random indices to mask\n-        spec_aug_mask_idx = np.random.choice(\n-            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n-        )\n-\n-        # pick first sampled index that will serve as a dummy index to pad vector\n-        # to ensure same dimension for all batches due to probabilistic rounding\n-        # Picking first sample just pads those vectors twice.\n-        if len(spec_aug_mask_idx) == 0:\n-            # this case can only happen if `input_length` is strictly smaller then\n-            # `sequence_length` in which case the last token has to be a padding\n-            # token which we can use as a dummy mask id\n-            dummy_mask_idx = sequence_length - 1\n-        else:\n-            dummy_mask_idx = spec_aug_mask_idx[0]\n-\n-        spec_aug_mask_idx = np.concatenate(\n-            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n-        )\n-        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n-\n-    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n-\n-    # expand masked indices to masked spans\n-    spec_aug_mask_idxs = np.broadcast_to(\n-        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n-\n-    # add offset to the starting indexes so that indexes now create a span\n-    offsets = np.arange(mask_length)[None, None, :]\n-    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n-        batch_size, max_num_masked_span * mask_length\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n-\n-    # ensure that we cannot have indices larger than sequence_length\n-    if spec_aug_mask_idxs.max() > sequence_length - 1:\n-        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n-\n-    # scatter indices to mask\n-    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n-\n-    return spec_aug_mask\n+# General docstring\n+_CONFIG_FOR_DOC = \"Data2VecAudioConfig\"\n \n \n class Data2VecAudioConvLayer(nn.Module):\n@@ -214,7 +76,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->Data2VecAudio\n class Data2VecAudioPadLayer(nn.Module):\n     def __init__(self, num_conv_pos_embeddings):\n         super().__init__()\n@@ -279,13 +140,11 @@ def __init__(self, config):\n         self.gradient_checkpointing = False\n         self._requires_grad = True\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder._freeze_parameters\n     def _freeze_parameters(self):\n         for param in self.parameters():\n             param.requires_grad = False\n         self._requires_grad = False\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder.forward\n     def forward(self, input_values):\n         hidden_states = input_values[:, None]\n \n@@ -305,7 +164,6 @@ def forward(self, input_values):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection with Wav2Vec2->Data2VecAudio\n class Data2VecAudioFeatureProjection(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -321,7 +179,6 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Data2VecAudio\n class Data2VecAudioAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -480,7 +337,6 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->Data2VecAudio\n class Data2VecAudioFlashAttention2(Data2VecAudioAttention):\n     \"\"\"\n     Data2VecAudio flash attention module. This module inherits from `Data2VecAudioAttention` as the weights of the module stays\n@@ -608,7 +464,6 @@ def forward(\n \n \n class Data2VecAudioSdpaAttention(Data2VecAudioAttention):\n-    # Copied from transformers.models.bart.modeling_bart.BartSdpaAttention.forward with Bart->Data2VecAudio\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -714,14 +569,6 @@ def forward(\n         return attn_output, None, past_key_value\n \n \n-DATA2VEC2AUDIO_ATTENTION_CLASSES = {\n-    \"eager\": Data2VecAudioAttention,\n-    \"sdpa\": Data2VecAudioSdpaAttention,\n-    \"flash_attention_2\": Data2VecAudioFlashAttention2,\n-}\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->Data2VecAudio\n class Data2VecAudioFeedForward(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -746,11 +593,17 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayer with Wav2Vec2->Data2VecAudio, WAV2VEC2->DATA2VEC2AUDIO\n+DATA2VEC_AUDIO_ATTENTION_CLASSES = {\n+    \"eager\": Data2VecAudioAttention,\n+    \"sdpa\": Data2VecAudioSdpaAttention,\n+    \"flash_attention_2\": Data2VecAudioFlashAttention2,\n+}\n+\n+\n class Data2VecAudioEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = DATA2VEC2AUDIO_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = DATA2VEC_AUDIO_ATTENTION_CLASSES[config._attn_implementation](\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n@@ -782,7 +635,6 @@ def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         return outputs\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Encoder with Wav2Vec2->Data2VecAudio\n class Data2VecAudioEncoder(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -868,7 +720,24 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Adapter with Wav2Vec2->Data2VecAudio\n+class Data2VecAudioAdapterLayer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.conv = nn.Conv1d(\n+            config.output_hidden_size,\n+            2 * config.output_hidden_size,\n+            config.adapter_kernel_size,\n+            stride=config.adapter_stride,\n+            padding=1,\n+        )\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = nn.functional.glu(hidden_states, dim=1)\n+\n+        return hidden_states\n+\n+\n class Data2VecAudioAdapter(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -900,25 +769,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2AdapterLayer with Wav2Vec2->Data2VecAudio\n-class Data2VecAudioAdapterLayer(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.conv = nn.Conv1d(\n-            config.output_hidden_size,\n-            2 * config.output_hidden_size,\n-            config.adapter_kernel_size,\n-            stride=config.adapter_stride,\n-            padding=1,\n-        )\n-\n-    def forward(self, hidden_states):\n-        hidden_states = self.conv(hidden_states)\n-        hidden_states = nn.functional.glu(hidden_states, dim=1)\n-\n-        return hidden_states\n-\n-\n class Data2VecAudioPreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n@@ -957,7 +807,6 @@ def _init_weights(self, module):\n                 k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 nn.init.uniform_(module.bias, a=-k, b=k)\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PreTrainedModel._get_feat_extract_output_lengths with\n     def _get_feat_extract_output_lengths(\n         self, input_lengths: Union[torch.LongTensor, int], add_adapter: Optional[bool] = None\n     ):\n@@ -981,7 +830,6 @@ def _conv_out_length(input_length, kernel_size, stride):\n \n         return input_lengths\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PreTrainedModel._get_feature_vector_attention_mask\n     def _get_feature_vector_attention_mask(\n         self, feature_vector_length: int, attention_mask: torch.LongTensor, add_adapter=None\n     ):\n@@ -1003,6 +851,128 @@ def _get_feature_vector_attention_mask(\n         return attention_mask\n \n \n+def _compute_mask_indices(\n+    shape: Tuple[int, int],\n+    mask_prob: float,\n+    mask_length: int,\n+    attention_mask: Optional[torch.LongTensor] = None,\n+    min_masks: int = 0,\n+) -> np.ndarray:\n+    \"\"\"\n+    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n+    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n+    CPU as part of the preprocessing during training.\n+\n+    Args:\n+        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n+               the first element is the batch size and the second element is the length of the axis to span.\n+        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n+                    independently generated mask spans of length `mask_length` is computed by\n+                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n+                    actual percentage will be smaller.\n+        mask_length: size of the mask\n+        min_masks: minimum number of masked spans\n+        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n+                        each batch dimension.\n+    \"\"\"\n+    batch_size, sequence_length = shape\n+\n+    if mask_length < 1:\n+        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n+\n+    if mask_length > sequence_length:\n+        raise ValueError(\n+            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n+            f\" and `sequence_length`: {sequence_length}`\"\n+        )\n+\n+    # epsilon is used for probabilistic rounding\n+    epsilon = np.random.rand(1).item()\n+\n+    def compute_num_masked_span(input_length):\n+        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n+        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n+        num_masked_span = max(num_masked_span, min_masks)\n+\n+        # make sure num masked span <= sequence_length\n+        if num_masked_span * mask_length > sequence_length:\n+            num_masked_span = sequence_length // mask_length\n+\n+        # make sure num_masked span is also <= input_length - (mask_length - 1)\n+        if input_length - (mask_length - 1) < num_masked_span:\n+            num_masked_span = max(input_length - (mask_length - 1), 0)\n+\n+        return num_masked_span\n+\n+    # compute number of masked spans in batch\n+    input_lengths = (\n+        attention_mask.detach().sum(-1).tolist()\n+        if attention_mask is not None\n+        else [sequence_length for _ in range(batch_size)]\n+    )\n+\n+    # SpecAugment mask to fill\n+    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n+    spec_aug_mask_idxs = []\n+\n+    max_num_masked_span = compute_num_masked_span(sequence_length)\n+\n+    if max_num_masked_span == 0:\n+        return spec_aug_mask\n+\n+    for input_length in input_lengths:\n+        # compute num of masked spans for this input\n+        num_masked_span = compute_num_masked_span(input_length)\n+\n+        # get random indices to mask\n+        spec_aug_mask_idx = np.random.choice(\n+            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+        )\n+\n+        # pick first sampled index that will serve as a dummy index to pad vector\n+        # to ensure same dimension for all batches due to probabilistic rounding\n+        # Picking first sample just pads those vectors twice.\n+        if len(spec_aug_mask_idx) == 0:\n+            # this case can only happen if `input_length` is strictly smaller then\n+            # `sequence_length` in which case the last token has to be a padding\n+            # token which we can use as a dummy mask id\n+            dummy_mask_idx = sequence_length - 1\n+        else:\n+            dummy_mask_idx = spec_aug_mask_idx[0]\n+\n+        spec_aug_mask_idx = np.concatenate(\n+            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n+        )\n+        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n+\n+    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n+\n+    # expand masked indices to masked spans\n+    spec_aug_mask_idxs = np.broadcast_to(\n+        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+\n+    # add offset to the starting indexes so that indexes now create a span\n+    offsets = np.arange(mask_length)[None, None, :]\n+    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n+        batch_size, max_num_masked_span * mask_length\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+\n+    # ensure that we cannot have indices larger than sequence_length\n+    if spec_aug_mask_idxs.max() > sequence_length - 1:\n+        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+\n+    # scatter indices to mask\n+    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+\n+    return spec_aug_mask\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n+\n+\n DATA2VEC_AUDIO_START_DOCSTRING = r\"\"\"\n     Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and\n     Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and\n@@ -1021,7 +991,6 @@ def _get_feature_vector_attention_mask(\n             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n-\n DATA2VEC_AUDIO_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1058,6 +1027,8 @@ def _get_feature_vector_attention_mask(\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n \n+Data2VecAudioBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n \n @add_start_docstrings(\n     \"The bare Data2VecAudio Model transformer outputting raw hidden-states without any specific head on top.\",\n@@ -1137,7 +1108,7 @@ def _mask_hidden_states(\n     @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Wav2Vec2BaseModelOutput,\n+        output_type=Data2VecAudioBaseModelOutput,\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n         expected_output=_EXPECTED_OUTPUT_SHAPE,\n@@ -1150,7 +1121,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Wav2Vec2BaseModelOutput]:\n+    ) -> Union[Tuple, Data2VecAudioBaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1187,14 +1158,21 @@ def forward(\n         if not return_dict:\n             return (hidden_states, extract_features) + encoder_outputs[1:]\n \n-        return Wav2Vec2BaseModelOutput(\n+        return Data2VecAudioBaseModelOutput(\n             last_hidden_state=hidden_states,\n             extract_features=extract_features,\n             hidden_states=encoder_outputs.hidden_states,\n             attentions=encoder_outputs.attentions,\n         )\n \n \n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n+_CTC_EXPECTED_LOSS = 66.95\n+\n+\n @add_start_docstrings(\n     \"\"\"Data2VecAudio Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n     DATA2VEC_AUDIO_START_DOCSTRING,\n@@ -1248,7 +1226,6 @@ def freeze_feature_encoder(self):\n         expected_output=_CTC_EXPECTED_OUTPUT,\n         expected_loss=_CTC_EXPECTED_LOSS,\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.forward with wav2vec2->data2vec_audio\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1379,7 +1356,6 @@ def freeze_base_model(self):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.forward with wav2vec2->data2vec_audio\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1455,8 +1431,7 @@ def __init__(self, config):\n \n         if hasattr(config, \"add_adapter\") and config.add_adapter:\n             raise ValueError(\n-                \"Audio frame classification does not support the use of Data2VecAudio adapters\"\n-                \" (config.add_adapter=True)\"\n+                \"Audio frame classification does not support the use of Data2VecAudio adapters (config.add_adapter=True)\"\n             )\n         self.data2vec_audio = Data2VecAudioModel(config)\n         num_layers = config.num_hidden_layers + 1  # transformer layers + input embeddings\n@@ -1501,7 +1476,6 @@ def freeze_base_model(self):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification.forward with wav2vec2->data2vec_audio\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1556,7 +1530,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.AMSoftmaxLoss\n class AMSoftmaxLoss(nn.Module):\n     def __init__(self, input_dim, num_labels, scale=30.0, margin=0.4):\n         super(AMSoftmaxLoss, self).__init__()\n@@ -1580,7 +1553,6 @@ def forward(self, hidden_states, labels):\n         return loss\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.TDNNLayer\n class TDNNLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -1596,6 +1568,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if is_peft_available():\n             from peft.tuners.lora import LoraLayer\n \n+        if is_peft_available():\n             if isinstance(self.kernel, LoraLayer):\n                 warnings.warn(\n                     \"Detected LoRA on TDNNLayer. LoRA weights won't be applied due to optimization. \"\n@@ -1687,7 +1660,6 @@ def _conv_out_length(input_length, kernel_size, stride):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector.forward with wav2vec2->data2vec_audio\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],"
        },
        {
            "sha": "052f22a960fb42e1ecc379c2fb7881329785e8f4",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "added",
            "additions": 400,
            "deletions": 0,
            "changes": 400,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -0,0 +1,400 @@\n+import math\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_outputs import (\n+    CausalLMOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+    Wav2Vec2BaseModelOutput,\n+    XVectorOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+)\n+from ..wav2vec2.modeling_wav2vec2 import (\n+    Wav2Vec2Adapter,\n+    Wav2Vec2Encoder,\n+    Wav2Vec2FeatureEncoder,\n+    Wav2Vec2FeatureProjection,\n+    Wav2Vec2ForAudioFrameClassification,\n+    Wav2Vec2ForCTC,\n+    Wav2Vec2ForSequenceClassification,\n+    Wav2Vec2ForXVector,\n+    Wav2Vec2Model,\n+    Wav2Vec2PreTrainedModel,\n+    Wav2Vec2SamePadLayer,\n+)\n+from .configuration_data2vec_audio import Data2VecAudioConfig\n+\n+\n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"Data2VecAudioConfig\"\n+\n+# Base docstring\n+_CHECKPOINT_FOR_DOC = \"facebook/data2vec-audio-base-960h\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n+_CTC_EXPECTED_LOSS = 66.95\n+\n+\n+class Data2VecAudioConvLayer(nn.Module):\n+    def __init__(self, config, layer_id=0):\n+        super().__init__()\n+        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n+        self.out_conv_dim = config.conv_dim[layer_id]\n+\n+        self.conv = nn.Conv1d(\n+            self.in_conv_dim,\n+            self.out_conv_dim,\n+            kernel_size=config.conv_kernel[layer_id],\n+            stride=config.conv_stride[layer_id],\n+            bias=config.conv_bias,\n+        )\n+        self.layer_norm = nn.LayerNorm(self.out_conv_dim, elementwise_affine=True)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+\n+        hidden_states = hidden_states.transpose(-2, -1)\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = hidden_states.transpose(-2, -1)\n+\n+        hidden_states = self.activation(hidden_states)\n+        return hidden_states\n+\n+\n+class Data2VecAudioPadLayer(Wav2Vec2SamePadLayer):\n+    pass\n+\n+\n+class Data2VecAudioPositionalConvLayer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.conv = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=config.conv_pos_kernel_size,\n+            padding=config.conv_pos_kernel_size // 2,\n+            groups=config.num_conv_pos_embedding_groups,\n+        )\n+\n+        self.padding = Data2VecAudioPadLayer(config.conv_pos_kernel_size)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+        # no learnable parameters\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, elementwise_affine=False)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.padding(hidden_states)\n+\n+        hidden_states = hidden_states.transpose(1, 2)\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2)\n+        hidden_states = self.activation(hidden_states)\n+        return hidden_states\n+\n+\n+class Data2VecAudioPositionalConvEmbedding(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.layers = nn.ModuleList(\n+            [Data2VecAudioPositionalConvLayer(config) for _ in range(config.num_conv_pos_embeddings)]\n+        )\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.transpose(1, 2)\n+        for layer in self.layers:\n+            hidden_states = layer(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return hidden_states\n+\n+\n+class Data2VecAudioFeatureEncoder(Wav2Vec2FeatureEncoder, nn.Module):\n+    def __init__(self, config):\n+        nn.Module.__init__()\n+        self.conv_layers = nn.ModuleList(\n+            [Data2VecAudioConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)]\n+        )\n+        self.gradient_checkpointing = False\n+        self._requires_grad = True\n+\n+\n+class Data2VecAudioFeatureProjection(Wav2Vec2FeatureProjection):\n+    pass\n+\n+\n+class Data2VecAudioEncoder(Wav2Vec2Encoder):\n+    pass\n+\n+\n+class Data2VecAudioAdapter(Wav2Vec2Adapter):\n+    pass\n+\n+\n+class Data2VecAudioPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = Data2VecAudioConfig\n+    base_model_prefix = \"data2vec_audio\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, Data2VecAudioFeatureProjection):\n+            k = math.sqrt(1 / module.projection.in_features)\n+            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n+            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n+        elif isinstance(module, Data2VecAudioPositionalConvLayer):\n+            nn.init.constant_(module.conv.bias, 0)\n+        elif isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+            if module.weight is not None:\n+                module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            nn.init.kaiming_normal_(module.weight)\n+\n+            if module.bias is not None:\n+                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n+                nn.init.uniform_(module.bias, a=-k, b=k)\n+\n+    def _get_adapters(self):\n+        raise AttributeError(\"Not needed for Data2VecAudio\")\n+\n+    def init_adapter_layers(self):\n+        raise AttributeError(\"Not needed for Data2VecAudio\")\n+\n+    def load_adapter(self):\n+        raise AttributeError(\"Not needed for Data2VecAudio\")\n+\n+\n+DATA2VEC_AUDIO_START_DOCSTRING = r\"\"\"\n+    Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and\n+    Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and\n+    Michael Auli.\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving etc.).\n+\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n+    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`Data2VecAudioConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+DATA2VEC_AUDIO_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a *.flac* or *.wav* audio file\n+            into an array of type *List[float]* or a *numpy.ndarray*, *e.g.* via the soundfile library (*pip install\n+            soundfile*). To prepare the array into *input_values*, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type *torch.FloatTensor*. See [`Wav2Vec2Processor.__call__`] for details.\n+        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            <Tip warning={true}>\n+\n+            `attention_mask` should be passed if the corresponding processor has `config.return_attention_mask ==\n+            True`, which is the case for all pre-trained Data2Vec Audio models. Be aware that that even with\n+            `attention_mask`, zero-padded inputs will have slightly different outputs compared to non-padded inputs\n+            because there are more than one convolutional layer in the positional encodings. For a more detailed\n+            explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).\n+\n+            </Tip>\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+Data2VecAudioBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n+\n+@add_start_docstrings(\n+    \"The bare Data2VecAudio Model transformer outputting raw hidden-states without any specific head on top.\",\n+    DATA2VEC_AUDIO_START_DOCSTRING,\n+)\n+class Data2VecAudioModel(Data2VecAudioPreTrainedModel, Wav2Vec2Model):\n+    def __init__(self, config: Data2VecAudioConfig):\n+        Data2VecAudioPreTrainedModel.__init__(config)\n+        self.config = config\n+        self.feature_extractor = Data2VecAudioFeatureEncoder(config)\n+        self.feature_projection = Data2VecAudioFeatureProjection(config)\n+\n+        # model only needs masking vector if mask prob is > 0.0\n+        if config.mask_time_prob > 0.0 or config.mask_feature_prob > 0.0:\n+            self.masked_spec_embed = nn.Parameter(torch.Tensor(config.hidden_size).uniform_())\n+\n+        self.encoder = Data2VecAudioEncoder(config)\n+\n+        self.adapter = Data2VecAudioAdapter(config) if config.add_adapter else None\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Data2VecAudio\")\n+\n+    def freeze_feature_encoder(self):\n+        \"\"\"\n+        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n+        not be updated during training.\n+        \"\"\"\n+        self.feature_extractor._freeze_parameters()\n+\n+    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=Data2VecAudioBaseModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Data2VecAudio Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n+    DATA2VEC_AUDIO_START_DOCSTRING,\n+)\n+class Data2VecAudioForCTC(Data2VecAudioPreTrainedModel, Wav2Vec2ForCTC):\n+    def __init__(self, config):\n+        Data2VecAudioPreTrainedModel.__init__(config)\n+\n+        self.data2vec_audio = Data2VecAudioModel(config)\n+        self.dropout = nn.Dropout(config.final_dropout)\n+\n+        if config.vocab_size is None:\n+            raise ValueError(\n+                f\"You are trying to instantiate {self.__class__} with a configuration that \"\n+                \"does not define the vocabulary size of the language model head. Please \"\n+                \"instantiate the model as follows: `Data2VecAudioForCTC.from_pretrained(..., vocab_size=vocab_size)`. \"\n+                \"or define `vocab_size` of your model's configuration.\"\n+            )\n+        output_hidden_size = (\n+            config.output_hidden_size if hasattr(config, \"add_adapter\") and config.add_adapter else config.hidden_size\n+        )\n+        self.lm_head = nn.Linear(output_hidden_size, config.vocab_size)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def freeze_base_model(self):\n+        raise AttributeError(\"Not needed for Data2VecAudio\")\n+\n+    def tie_weights(self):\n+        raise AttributeError(\"Not needed for Data2VecAudio\")\n+\n+    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=CausalLMOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_CTC_EXPECTED_OUTPUT,\n+        expected_loss=_CTC_EXPECTED_LOSS,\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Data2VecAudio Model with a sequence classification head on top (a linear layer over the pooled output) for tasks\n+    like SUPERB Keyword Spotting.\n+    \"\"\",\n+    DATA2VEC_AUDIO_START_DOCSTRING,\n+)\n+class Data2VecAudioForSequenceClassification(Wav2Vec2ForSequenceClassification):\n+    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=SequenceClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Data2VecAudio Model with a frame classification head on top for tasks like Speaker Diarization.\n+    \"\"\",\n+    DATA2VEC_AUDIO_START_DOCSTRING,\n+)\n+class Data2VecAudioForAudioFrameClassification(Wav2Vec2ForAudioFrameClassification):\n+    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Data2VecAudio Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n+    \"\"\",\n+    DATA2VEC_AUDIO_START_DOCSTRING,\n+)\n+class Data2VecAudioForXVector(Wav2Vec2ForXVector):\n+    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=XVectorOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"Data2VecAudioForAudioFrameClassification\",\n+    \"Data2VecAudioForCTC\",\n+    \"Data2VecAudioForSequenceClassification\",\n+    \"Data2VecAudioForXVector\",\n+    \"Data2VecAudioModel\",\n+    \"Data2VecAudioPreTrainedModel\",\n+]"
        },
        {
            "sha": "e372817bf7136f85417fb3194cbc8f9dbb27df2b",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from ...configuration_utils import PretrainedConfig\n \n "
        },
        {
            "sha": "fa6af70ecfd4137bdab4310b5dd85a40d9d3e0c7",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -13,7 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n \n import sentencepiece as spm\n import torch\n@@ -379,7 +379,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwarg for now\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "384f3e08023d1509ef69fd29e64bd66044d28588",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -415,7 +415,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -588,7 +588,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **loss_kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "6364f8902190c313af4abb3a46da88a33d758bb2",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -326,7 +326,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]\n         hidden_shape = (*input_shape, -1, self.head_dim)\n \n@@ -1203,7 +1203,7 @@ def _update_causal_mask(\n \n         return causal_mask\n \n-    def get_image_features(self, pixel_values: torch.Tensor):\n+    def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Projects the last hidden state from the vision model into language model space.\n "
        },
        {
            "sha": "3f7292f13a0739c9ac479d374b07904f6de9afc9",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -597,7 +597,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         last_cache_position: Optional[int] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "c474ef36900fee4c3af6b45a59e86c0949d60618",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -753,7 +753,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-    ) -> Union[Tuple, GotOcr2CausalLMOutputWithPast]:\n+    ) -> GotOcr2CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "4b7e7f1adb5e1393d4edf978e0d68d2bbadf7fe2",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -397,7 +397,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-    ) -> LlavaCausalLMOutputWithPast:\n+    ) -> GotOcr2CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "58b697c3f1059b87deb7db8486e14cbb3f920b9e",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -343,7 +343,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "25929dbb33765d9e86d99f4bd1b4036f44814ee0",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -132,7 +132,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -244,7 +244,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "ae03cea1c1309ab58452d3272dadae63c48e2c3e",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 268,
            "deletions": 307,
            "changes": 575,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1,26 +1,15 @@\n-# coding=utf-8\n-# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Hubert model.\"\"\"\n-\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/hubert/modular_hubert.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_hubert.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import warnings\n from typing import Optional, Tuple, Union\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n-from torch import nn\n+import torch.nn as nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n@@ -45,146 +34,74 @@\n \n logger = logging.get_logger(__name__)\n \n-_HIDDEN_STATES_START_POSITION = 1\n-\n-# General docstring\n-_CONFIG_FOR_DOC = \"HubertConfig\"\n-\n # Base docstring\n _CHECKPOINT_FOR_DOC = \"facebook/hubert-large-ls960-ft\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n-\n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n-_CTC_EXPECTED_LOSS = 22.68\n-\n-# Audio class docstring\n-_SEQ_CLASS_CHECKPOINT = \"superb/hubert-base-superb-ks\"\n-_SEQ_CLASS_EXPECTED_OUTPUT = \"'_unknown_'\"\n-_SEQ_CLASS_EXPECTED_LOSS = 8.53\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices\n-def _compute_mask_indices(\n-    shape: Tuple[int, int],\n-    mask_prob: float,\n-    mask_length: int,\n-    attention_mask: Optional[torch.LongTensor] = None,\n-    min_masks: int = 0,\n-) -> np.ndarray:\n-    \"\"\"\n-    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n-    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n-    CPU as part of the preprocessing during training.\n-\n-    Args:\n-        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n-               the first element is the batch size and the second element is the length of the axis to span.\n-        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n-                    independently generated mask spans of length `mask_length` is computed by\n-                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n-                    actual percentage will be smaller.\n-        mask_length: size of the mask\n-        min_masks: minimum number of masked spans\n-        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n-                        each batch dimension.\n-    \"\"\"\n-    batch_size, sequence_length = shape\n-\n-    if mask_length < 1:\n-        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n-\n-    if mask_length > sequence_length:\n-        raise ValueError(\n-            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n-            f\" and `sequence_length`: {sequence_length}`\"\n-        )\n-\n-    # epsilon is used for probabilistic rounding\n-    epsilon = np.random.rand(1).item()\n-\n-    def compute_num_masked_span(input_length):\n-        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n-        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n-        num_masked_span = max(num_masked_span, min_masks)\n-\n-        # make sure num masked span <= sequence_length\n-        if num_masked_span * mask_length > sequence_length:\n-            num_masked_span = sequence_length // mask_length\n-\n-        # make sure num_masked span is also <= input_length - (mask_length - 1)\n-        if input_length - (mask_length - 1) < num_masked_span:\n-            num_masked_span = max(input_length - (mask_length - 1), 0)\n-\n-        return num_masked_span\n-\n-    # compute number of masked spans in batch\n-    input_lengths = (\n-        attention_mask.detach().sum(-1).tolist()\n-        if attention_mask is not None\n-        else [sequence_length for _ in range(batch_size)]\n-    )\n-\n-    # SpecAugment mask to fill\n-    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n-    spec_aug_mask_idxs = []\n-\n-    max_num_masked_span = compute_num_masked_span(sequence_length)\n \n-    if max_num_masked_span == 0:\n-        return spec_aug_mask\n+# General docstring\n+_CONFIG_FOR_DOC = \"HubertConfig\"\n \n-    for input_length in input_lengths:\n-        # compute num of masked spans for this input\n-        num_masked_span = compute_num_masked_span(input_length)\n \n-        # get random indices to mask\n-        spec_aug_mask_idx = np.random.choice(\n-            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+class HubertPositionalConvEmbedding(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.conv = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=config.num_conv_pos_embeddings,\n+            padding=config.num_conv_pos_embeddings // 2,\n+            groups=config.num_conv_pos_embedding_groups,\n         )\n \n-        # pick first sampled index that will serve as a dummy index to pad vector\n-        # to ensure same dimension for all batches due to probabilistic rounding\n-        # Picking first sample just pads those vectors twice.\n-        if len(spec_aug_mask_idx) == 0:\n-            # this case can only happen if `input_length` is strictly smaller then\n-            # `sequence_length` in which case the last token has to be a padding\n-            # token which we can use as a dummy mask id\n-            dummy_mask_idx = sequence_length - 1\n+        self.batch_norm = None\n+        if config.conv_pos_batch_norm:\n+            self.batch_norm = nn.BatchNorm1d(config.hidden_size)\n         else:\n-            dummy_mask_idx = spec_aug_mask_idx[0]\n+            weight_norm = nn.utils.weight_norm\n+            if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+                weight_norm = nn.utils.parametrizations.weight_norm\n \n-        spec_aug_mask_idx = np.concatenate(\n-            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n-        )\n-        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n+            if is_deepspeed_zero3_enabled():\n+                import deepspeed\n \n-    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n+                with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n+                    self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n+                if hasattr(self.conv, \"parametrizations\"):\n+                    weight_g = self.conv.parametrizations.weight.original0\n+                    weight_v = self.conv.parametrizations.weight.original1\n+                else:\n+                    weight_g = self.conv.weight_g\n+                    weight_v = self.conv.weight_v\n+                deepspeed.zero.register_external_parameter(self, weight_v)\n+                deepspeed.zero.register_external_parameter(self, weight_g)\n+            else:\n+                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n \n-    # expand masked indices to masked spans\n-    spec_aug_mask_idxs = np.broadcast_to(\n-        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+        self.padding = HubertSamePadLayer(config.num_conv_pos_embeddings)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n \n-    # add offset to the starting indexes so that indexes now create a span\n-    offsets = np.arange(mask_length)[None, None, :]\n-    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n-        batch_size, max_num_masked_span * mask_length\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.transpose(1, 2)\n+        if self.batch_norm is not None:\n+            hidden_states = self.batch_norm(hidden_states)\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.padding(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n \n-    # ensure that we cannot have indices larger than sequence_length\n-    if spec_aug_mask_idxs.max() > sequence_length - 1:\n-        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return hidden_states\n \n-    # scatter indices to mask\n-    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n \n-    return spec_aug_mask\n+class HubertSamePadLayer(nn.Module):\n+    def __init__(self, num_conv_pos_embeddings):\n+        super().__init__()\n+        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n+\n+    def forward(self, hidden_states):\n+        if self.num_pad_remove > 0:\n+            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n+        return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->Hubert\n class HubertNoLayerNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -206,7 +123,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->Hubert\n class HubertLayerNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -234,7 +150,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->Hubert\n class HubertGroupNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -259,69 +174,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class HubertPositionalConvEmbedding(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.conv = nn.Conv1d(\n-            config.hidden_size,\n-            config.hidden_size,\n-            kernel_size=config.num_conv_pos_embeddings,\n-            padding=config.num_conv_pos_embeddings // 2,\n-            groups=config.num_conv_pos_embedding_groups,\n-        )\n-\n-        self.batch_norm = None\n-        if config.conv_pos_batch_norm:\n-            self.batch_norm = nn.BatchNorm1d(config.hidden_size)\n-        else:\n-            weight_norm = nn.utils.weight_norm\n-            if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n-                weight_norm = nn.utils.parametrizations.weight_norm\n-\n-            if is_deepspeed_zero3_enabled():\n-                import deepspeed\n-\n-                with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n-                    self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n-                if hasattr(self.conv, \"parametrizations\"):\n-                    weight_g = self.conv.parametrizations.weight.original0\n-                    weight_v = self.conv.parametrizations.weight.original1\n-                else:\n-                    weight_g = self.conv.weight_g\n-                    weight_v = self.conv.weight_v\n-                deepspeed.zero.register_external_parameter(self, weight_v)\n-                deepspeed.zero.register_external_parameter(self, weight_g)\n-            else:\n-                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n-\n-        self.padding = HubertSamePadLayer(config.num_conv_pos_embeddings)\n-        self.activation = ACT2FN[config.feat_extract_activation]\n-\n-    def forward(self, hidden_states):\n-        hidden_states = hidden_states.transpose(1, 2)\n-        if self.batch_norm is not None:\n-            hidden_states = self.batch_norm(hidden_states)\n-        hidden_states = self.conv(hidden_states)\n-        hidden_states = self.padding(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n-\n-        hidden_states = hidden_states.transpose(1, 2)\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->Hubert\n-class HubertSamePadLayer(nn.Module):\n-    def __init__(self, num_conv_pos_embeddings):\n-        super().__init__()\n-        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n-\n-    def forward(self, hidden_states):\n-        if self.num_pad_remove > 0:\n-            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->Hubert\n class HubertFeatureEncoder(nn.Module):\n     \"\"\"Construct the features from raw audio waveform\"\"\"\n \n@@ -366,17 +218,6 @@ def forward(self, input_values):\n         return hidden_states\n \n \n-class HubertFeatureExtractor(HubertFeatureEncoder):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        warnings.warn(\n-            f\"The class `{self.__class__.__name__}` has been depreciated \"\n-            \"and will be removed in Transformers v5. \"\n-            f\"Use `{self.__class__.__bases__[0].__name__}` instead.\",\n-            FutureWarning,\n-        )\n-\n-\n class HubertFeatureProjection(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -395,7 +236,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->Hubert\n class HubertAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -554,7 +394,6 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->Hubert\n class HubertFlashAttention2(HubertAttention):\n     \"\"\"\n     Hubert flash attention module. This module inherits from `HubertAttention` as the weights of the module stays\n@@ -682,7 +521,6 @@ def forward(\n \n \n class HubertSdpaAttention(HubertAttention):\n-    # Copied from transformers.models.bart.modeling_bart.BartSdpaAttention.forward with Bart->Hubert\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -788,14 +626,6 @@ def forward(\n         return attn_output, None, past_key_value\n \n \n-HUBERT_ATTENTION_CLASSES = {\n-    \"eager\": HubertAttention,\n-    \"sdpa\": HubertSdpaAttention,\n-    \"flash_attention_2\": HubertFlashAttention2,\n-}\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->Hubert\n class HubertFeedForward(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -820,7 +650,13 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayer with Wav2Vec2->Hubert, WAV2VEC2->HUBERT\n+HUBERT_ATTENTION_CLASSES = {\n+    \"eager\": HubertAttention,\n+    \"sdpa\": HubertSdpaAttention,\n+    \"flash_attention_2\": HubertFlashAttention2,\n+}\n+\n+\n class HubertEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -856,80 +692,7 @@ def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         return outputs\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2AttnAdapterLayer with Wav2Vec2->Hubert\n-class HubertAttnAdapterLayer(nn.Module):\n-    def __init__(self, config):\n-        \"\"\"\n-        Implements adapter modules directly with 3D tensor weight as parameters and without using ModuleList to speed\n-        up training throughput.\n-        \"\"\"\n-        super().__init__()\n-        self.input_dim = config.adapter_attn_dim\n-        self.hidden_dim = config.hidden_size\n-\n-        self.norm = nn.LayerNorm(self.hidden_dim)\n-        self.linear_1 = nn.Linear(self.hidden_dim, self.input_dim)\n-        self.act_fn = nn.ReLU()\n-        self.linear_2 = nn.Linear(self.input_dim, self.hidden_dim)\n-\n-    def forward(self, hidden_states: torch.FloatTensor):\n-        hidden_states = self.norm(hidden_states)\n-\n-        hidden_states = self.linear_1(hidden_states)\n-        hidden_states = self.act_fn(hidden_states)\n-        hidden_states = self.linear_2(hidden_states)\n-\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayerStableLayerNorm with Wav2Vec2->Hubert, WAV2VEC2->HUBERT\n-class HubertEncoderLayerStableLayerNorm(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.attention = HUBERT_ATTENTION_CLASSES[config._attn_implementation](\n-            embed_dim=config.hidden_size,\n-            num_heads=config.num_attention_heads,\n-            dropout=config.attention_dropout,\n-            is_decoder=False,\n-        )\n-        self.dropout = nn.Dropout(config.hidden_dropout)\n-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.feed_forward = HubertFeedForward(config)\n-        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-\n-        if getattr(config, \"adapter_attn_dim\", None) is not None:\n-            self.adapter_layer = HubertAttnAdapterLayer(config)\n-        else:\n-            self.adapter_layer = None\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ):\n-        attn_residual = hidden_states\n-        hidden_states = self.layer_norm(hidden_states)\n-        hidden_states, attn_weights, _ = self.attention(\n-            hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-        )\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = attn_residual + hidden_states\n-        hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))\n-\n-        if self.adapter_layer is not None:\n-            hidden_states = hidden_states + self.adapter_layer(hidden_states)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Encoder with Wav2Vec2->Hubert\n-class HubertEncoder(nn.Module):\n+class HubertEncoder(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n@@ -1014,7 +777,76 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderStableLayerNorm with Wav2Vec2->Hubert\n+class HubertAttnAdapterLayer(nn.Module):\n+    def __init__(self, config):\n+        \"\"\"\n+        Implements adapter modules directly with 3D tensor weight as parameters and without using ModuleList to speed\n+        up training throughput.\n+        \"\"\"\n+        super().__init__()\n+        self.input_dim = config.adapter_attn_dim\n+        self.hidden_dim = config.hidden_size\n+\n+        self.norm = nn.LayerNorm(self.hidden_dim)\n+        self.linear_1 = nn.Linear(self.hidden_dim, self.input_dim)\n+        self.act_fn = nn.ReLU()\n+        self.linear_2 = nn.Linear(self.input_dim, self.hidden_dim)\n+\n+    def forward(self, hidden_states: torch.FloatTensor):\n+        hidden_states = self.norm(hidden_states)\n+\n+        hidden_states = self.linear_1(hidden_states)\n+        hidden_states = self.act_fn(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class HubertEncoderLayerStableLayerNorm(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.attention = HUBERT_ATTENTION_CLASSES[config._attn_implementation](\n+            embed_dim=config.hidden_size,\n+            num_heads=config.num_attention_heads,\n+            dropout=config.attention_dropout,\n+            is_decoder=False,\n+        )\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.feed_forward = HubertFeedForward(config)\n+        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        if getattr(config, \"adapter_attn_dim\", None) is not None:\n+            self.adapter_layer = HubertAttnAdapterLayer(config)\n+        else:\n+            self.adapter_layer = None\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ):\n+        attn_residual = hidden_states\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states, attn_weights, _ = self.attention(\n+            hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+        )\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = attn_residual + hidden_states\n+        hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))\n+\n+        if self.adapter_layer is not None:\n+            hidden_states = hidden_states + self.adapter_layer(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n class HubertEncoderStableLayerNorm(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1170,6 +1002,125 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n         return attention_mask\n \n \n+def _compute_mask_indices(\n+    shape: Tuple[int, int],\n+    mask_prob: float,\n+    mask_length: int,\n+    attention_mask: Optional[torch.LongTensor] = None,\n+    min_masks: int = 0,\n+) -> np.ndarray:\n+    \"\"\"\n+    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n+    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n+    CPU as part of the preprocessing during training.\n+\n+    Args:\n+        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n+               the first element is the batch size and the second element is the length of the axis to span.\n+        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n+                    independently generated mask spans of length `mask_length` is computed by\n+                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n+                    actual percentage will be smaller.\n+        mask_length: size of the mask\n+        min_masks: minimum number of masked spans\n+        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n+                        each batch dimension.\n+    \"\"\"\n+    batch_size, sequence_length = shape\n+\n+    if mask_length < 1:\n+        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n+\n+    if mask_length > sequence_length:\n+        raise ValueError(\n+            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n+            f\" and `sequence_length`: {sequence_length}`\"\n+        )\n+\n+    # epsilon is used for probabilistic rounding\n+    epsilon = np.random.rand(1).item()\n+\n+    def compute_num_masked_span(input_length):\n+        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n+        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n+        num_masked_span = max(num_masked_span, min_masks)\n+\n+        # make sure num masked span <= sequence_length\n+        if num_masked_span * mask_length > sequence_length:\n+            num_masked_span = sequence_length // mask_length\n+\n+        # make sure num_masked span is also <= input_length - (mask_length - 1)\n+        if input_length - (mask_length - 1) < num_masked_span:\n+            num_masked_span = max(input_length - (mask_length - 1), 0)\n+\n+        return num_masked_span\n+\n+    # compute number of masked spans in batch\n+    input_lengths = (\n+        attention_mask.detach().sum(-1).tolist()\n+        if attention_mask is not None\n+        else [sequence_length for _ in range(batch_size)]\n+    )\n+\n+    # SpecAugment mask to fill\n+    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n+    spec_aug_mask_idxs = []\n+\n+    max_num_masked_span = compute_num_masked_span(sequence_length)\n+\n+    if max_num_masked_span == 0:\n+        return spec_aug_mask\n+\n+    for input_length in input_lengths:\n+        # compute num of masked spans for this input\n+        num_masked_span = compute_num_masked_span(input_length)\n+\n+        # get random indices to mask\n+        spec_aug_mask_idx = np.random.choice(\n+            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+        )\n+\n+        # pick first sampled index that will serve as a dummy index to pad vector\n+        # to ensure same dimension for all batches due to probabilistic rounding\n+        # Picking first sample just pads those vectors twice.\n+        if len(spec_aug_mask_idx) == 0:\n+            # this case can only happen if `input_length` is strictly smaller then\n+            # `sequence_length` in which case the last token has to be a padding\n+            # token which we can use as a dummy mask id\n+            dummy_mask_idx = sequence_length - 1\n+        else:\n+            dummy_mask_idx = spec_aug_mask_idx[0]\n+\n+        spec_aug_mask_idx = np.concatenate(\n+            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n+        )\n+        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n+\n+    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n+\n+    # expand masked indices to masked spans\n+    spec_aug_mask_idxs = np.broadcast_to(\n+        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+\n+    # add offset to the starting indexes so that indexes now create a span\n+    offsets = np.arange(mask_length)[None, None, :]\n+    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n+        batch_size, max_num_masked_span * mask_length\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+\n+    # ensure that we cannot have indices larger than sequence_length\n+    if spec_aug_mask_idxs.max() > sequence_length - 1:\n+        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+\n+    # scatter indices to mask\n+    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+\n+    return spec_aug_mask\n+\n+\n HUBERT_START_DOCSTRING = r\"\"\"\n     Hubert was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden\n     Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia,\n@@ -1238,6 +1189,7 @@ def __init__(self, config: HubertConfig):\n         self.feature_extractor = HubertFeatureEncoder(config)\n         self.feature_projection = HubertFeatureProjection(config)\n \n+        # model only needs masking vector if mask prob is > 0.0\n         if config.mask_time_prob > 0.0 or config.mask_feature_prob > 0.0:\n             self.masked_spec_embed = nn.Parameter(torch.Tensor(config.hidden_size).uniform_())\n \n@@ -1249,7 +1201,6 @@ def __init__(self, config: HubertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states\n     def _mask_hidden_states(\n         self,\n         hidden_states: torch.FloatTensor,\n@@ -1370,11 +1321,17 @@ def forward(\n         )\n \n \n+_HIDDEN_STATES_START_POSITION = 1\n+\n+\n+_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n+_CTC_EXPECTED_LOSS = 22.68\n+\n+\n @add_start_docstrings(\n     \"\"\"Hubert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n     HUBERT_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->Hubert, wav2vec2->hubert, WAV_2_VEC_2->HUBERT\n class HubertForCTC(HubertPreTrainedModel):\n     def __init__(self, config, target_lang: Optional[str] = None):\n         super().__init__(config)\n@@ -1526,14 +1483,18 @@ def forward(\n         )\n \n \n+_SEQ_CLASS_CHECKPOINT = \"superb/hubert-base-superb-ks\"\n+_SEQ_CLASS_EXPECTED_OUTPUT = \"'_unknown_'\"\n+_SEQ_CLASS_EXPECTED_LOSS = 8.53\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     Hubert Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like\n     SUPERB Keyword Spotting.\n     \"\"\",\n     HUBERT_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification with Wav2Vec2->Hubert, wav2vec2->hubert, WAV_2_VEC_2->HUBERT\n class HubertForSequenceClassification(HubertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "a42bb5e598b58fc8238075190c6534fbd178292c",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "added",
            "additions": 400,
            "deletions": 0,
            "changes": 400,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -0,0 +1,400 @@\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    replace_return_docstrings,\n+)\n+from ..wav2vec2.modeling_wav2vec2 import (\n+    Wav2Vec2Encoder,\n+    Wav2Vec2EncoderStableLayerNorm,\n+    Wav2Vec2FeatureEncoder,\n+    Wav2Vec2ForCTC,\n+    Wav2Vec2ForSequenceClassification,\n+    Wav2Vec2Model,\n+    Wav2Vec2SamePadLayer,\n+)\n+from .configuration_hubert import HubertConfig\n+\n+\n+_HIDDEN_STATES_START_POSITION = 1\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"HubertConfig\"\n+\n+# Base docstring\n+_CHECKPOINT_FOR_DOC = \"facebook/hubert-large-ls960-ft\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n+\n+\n+_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n+_CTC_EXPECTED_LOSS = 22.68\n+\n+\n+_SEQ_CLASS_CHECKPOINT = \"superb/hubert-base-superb-ks\"\n+_SEQ_CLASS_EXPECTED_OUTPUT = \"'_unknown_'\"\n+_SEQ_CLASS_EXPECTED_LOSS = 8.53\n+\n+\n+class HubertPositionalConvEmbedding(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.conv = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=config.num_conv_pos_embeddings,\n+            padding=config.num_conv_pos_embeddings // 2,\n+            groups=config.num_conv_pos_embedding_groups,\n+        )\n+\n+        self.batch_norm = None\n+        if config.conv_pos_batch_norm:\n+            self.batch_norm = nn.BatchNorm1d(config.hidden_size)\n+        else:\n+            weight_norm = nn.utils.weight_norm\n+            if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+                weight_norm = nn.utils.parametrizations.weight_norm\n+\n+            if is_deepspeed_zero3_enabled():\n+                import deepspeed\n+\n+                with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n+                    self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n+                if hasattr(self.conv, \"parametrizations\"):\n+                    weight_g = self.conv.parametrizations.weight.original0\n+                    weight_v = self.conv.parametrizations.weight.original1\n+                else:\n+                    weight_g = self.conv.weight_g\n+                    weight_v = self.conv.weight_v\n+                deepspeed.zero.register_external_parameter(self, weight_v)\n+                deepspeed.zero.register_external_parameter(self, weight_g)\n+            else:\n+                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n+\n+        self.padding = HubertSamePadLayer(config.num_conv_pos_embeddings)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.transpose(1, 2)\n+        if self.batch_norm is not None:\n+            hidden_states = self.batch_norm(hidden_states)\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.padding(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return hidden_states\n+\n+\n+class HubertSamePadLayer(Wav2Vec2SamePadLayer):\n+    pass\n+\n+\n+class HubertFeatureEncoder(Wav2Vec2FeatureEncoder):\n+    pass\n+\n+\n+class HubertFeatureProjection(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.feat_proj_layer_norm = config.feat_proj_layer_norm\n+        if self.feat_proj_layer_norm:\n+            self.layer_norm = nn.LayerNorm(config.conv_dim[-1], eps=config.layer_norm_eps)\n+        self.projection = nn.Linear(config.conv_dim[-1], config.hidden_size)\n+        self.dropout = nn.Dropout(config.feat_proj_dropout)\n+\n+    def forward(self, hidden_states):\n+        # non-projected hidden states are needed for quantization\n+        if self.feat_proj_layer_norm:\n+            hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = self.projection(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        return hidden_states\n+\n+\n+class HubertEncoder(Wav2Vec2Encoder):\n+    pass\n+\n+\n+class HubertEncoderStableLayerNorm(Wav2Vec2EncoderStableLayerNorm):\n+    pass\n+\n+\n+class HubertPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = HubertConfig\n+    base_model_prefix = \"hubert\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            if is_deepspeed_zero3_enabled():\n+                import deepspeed\n+\n+                if hasattr(module, \"weight_v\") and hasattr(module, \"weight_g\"):\n+                    with deepspeed.zero.GatheredParameters([module.weight_v, module.weight_g], modifier_rank=0):\n+                        nn.init.kaiming_normal_(module.weight.data)\n+                else:\n+                    with deepspeed.zero.GatheredParameters(module.weight, modifier_rank=0):\n+                        nn.init.kaiming_normal_(module.weight.data)\n+            else:\n+                nn.init.kaiming_normal_(module.weight.data)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv1d)) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+    def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+\n+        def _conv_out_length(input_length, kernel_size, stride):\n+            # 1D convolutional layer output length formula taken\n+            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n+            return torch.div(input_length - kernel_size, stride, rounding_mode=\"floor\") + 1\n+\n+        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):\n+            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n+\n+        return input_lengths\n+\n+    def _get_feature_vector_attention_mask(self, feature_vector_length: int, attention_mask: torch.LongTensor):\n+        output_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(-1)).to(torch.long)\n+        batch_size = attention_mask.shape[0]\n+\n+        attention_mask = torch.zeros(\n+            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n+        )\n+        # these two operations makes sure that all values before the output lengths idxs are attended to\n+        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n+        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n+        return attention_mask\n+\n+\n+HUBERT_START_DOCSTRING = r\"\"\"\n+    Hubert was proposed in [HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden\n+    Units](https://arxiv.org/abs/2106.07447) by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia,\n+    Ruslan Salakhutdinov, Abdelrahman Mohamed.\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving etc.).\n+\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n+    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`HubertConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+HUBERT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n+            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n+            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n+        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            <Tip warning={true}>\n+\n+            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==\n+            True`. For all models whose processor has `config.return_attention_mask == False`, such as\n+            [hubert-base](https://huggingface.co/facebook/hubert-base-ls960), `attention_mask` should **not** be passed\n+            to avoid degraded performance when doing batched inference. For such models `input_values` should simply be\n+            padded with 0 and passed without `attention_mask`. Be aware that these models also yield slightly different\n+            results depending on whether `input_values` is padded or not.\n+\n+            </Tip>\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Hubert Model transformer outputting raw hidden-states without any specific head on top.\",\n+    HUBERT_START_DOCSTRING,\n+)\n+class HubertModel(Wav2Vec2Model, HubertPreTrainedModel):\n+    def __init__(self, config: HubertConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.feature_extractor = HubertFeatureEncoder(config)\n+        self.feature_projection = HubertFeatureProjection(config)\n+\n+        if config.mask_time_prob > 0.0 or config.mask_feature_prob > 0.0:\n+            self.masked_spec_embed = nn.Parameter(torch.Tensor(config.hidden_size).uniform_())\n+\n+        if config.do_stable_layer_norm:\n+            self.encoder = HubertEncoderStableLayerNorm(config)\n+        else:\n+            self.encoder = HubertEncoder(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+        del self.adapter\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Hubert\")\n+\n+    def freeze_feature_encoder(self):\n+        raise AttributeError(\"Not needed for Hubert\")\n+\n+    @add_start_docstrings_to_model_forward(HUBERT_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_values: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        mask_time_indices: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        \"\"\"\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, HubertModel\n+        >>> from datasets import load_dataset\n+        >>> import soundfile as sf\n+\n+        >>> processor = AutoProcessor.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n+        >>> model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n+\n+\n+        >>> def map_to_array(batch):\n+        ...     speech, _ = sf.read(batch[\"file\"])\n+        ...     batch[\"speech\"] = speech\n+        ...     return batch\n+\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.map(map_to_array)\n+\n+        >>> input_values = processor(ds[\"speech\"][0], return_tensors=\"pt\").input_values  # Batch size 1\n+        >>> hidden_states = model(input_values).last_hidden_state\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        extract_features = self.feature_extractor(input_values)\n+        extract_features = extract_features.transpose(1, 2)\n+\n+        if attention_mask is not None:\n+            # compute reduced attention_mask corresponding to feature vectors\n+            attention_mask = self._get_feature_vector_attention_mask(extract_features.shape[1], attention_mask)\n+\n+        hidden_states = self.feature_projection(extract_features)\n+        hidden_states = self._mask_hidden_states(hidden_states, mask_time_indices=mask_time_indices)\n+\n+        encoder_outputs = self.encoder(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        hidden_states = encoder_outputs[0]\n+\n+        if not return_dict:\n+            return (hidden_states,) + encoder_outputs[1:]\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Hubert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n+    HUBERT_START_DOCSTRING,\n+)\n+class HubertForCTC(Wav2Vec2ForCTC):\n+    pass\n+\n+    @add_start_docstrings_to_model_forward(HUBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=CausalLMOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_CTC_EXPECTED_OUTPUT,\n+        expected_loss=_CTC_EXPECTED_LOSS,\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Hubert Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like\n+    SUPERB Keyword Spotting.\n+    \"\"\",\n+    HUBERT_START_DOCSTRING,\n+)\n+class HubertForSequenceClassification(Wav2Vec2ForSequenceClassification):\n+    pass\n+\n+    @add_start_docstrings_to_model_forward(HUBERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_SEQ_CLASS_CHECKPOINT,\n+        output_type=SequenceClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n+        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\"HubertForCTC\", \"HubertForSequenceClassification\", \"HubertModel\", \"HubertPreTrainedModel\"]"
        },
        {
            "sha": "6026f5a7e07bacaa4134ebe821d34f48a0729d2c",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -303,7 +303,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n+    ) -> QuestionAnsweringModelOutput:\n         r\"\"\"\n         start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for position (index) of the start of the labelled span for computing the token classification loss."
        },
        {
            "sha": "bc3f09a778b15f8c51c0e24b4f2deb6526b20fd8",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -620,7 +620,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> BaseModelOutputWithPast:\n+    ) -> MoeModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n@@ -1013,7 +1013,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> CausalLMOutputWithPast:\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "bc46c9ab0c104d7c1a0d50ce3a77c40c3df304af",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -853,7 +853,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> BaseModelOutputWithPast:\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n         \"\"\"\n         Args:\n             encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n@@ -1415,7 +1415,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n+    ) -> Seq2SeqModelOutput:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "f0eb31058c905adc807c1847bec3eb44d77af73c",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1,5 +1,5 @@\n from functools import partial\n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple\n \n import torch\n import torch.nn as nn\n@@ -191,7 +191,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+    ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "8e0b86da40793b6a11ee8a4f5ebce0a077ddd4eb",
            "filename": "src/transformers/models/qwen3/modular_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch Qwen3 model.\"\"\"\n \n-from typing import Callable, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple\n \n import torch\n import torch.utils.checkpoint\n@@ -146,7 +146,7 @@ class Qwen3ForCausalLM(LlamaForCausalLM):\n     def forward(\n         self,\n         **super_kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+    ) -> CausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "d239f79ec7d99bcfa7a2a352e4bacc9080e9867b",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -633,7 +633,7 @@ def forward(\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> BaseModelOutputWithPast:\n+    ) -> MoeModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n@@ -1026,7 +1026,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> CausalLMOutputWithPast:\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "385f338c78e4521a6d18be8e32ffdf7466b96b5c",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -257,7 +257,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[KwargsForCausalLM],\n-    ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "572c07e3c9d9d573ab61280ced5fe7fdd2e8a87a",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1218,7 +1218,7 @@ def forward(\n     \"\"\"SEW Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n     SEW_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->SEW, wav2vec2->sew, WAV_2_VEC_2->SEW\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->SEW, wav2vec2->sew, WAV2VEC2->SEW\n class SEWForCTC(SEWPreTrainedModel):\n     def __init__(self, config, target_lang: Optional[str] = None):\n         super().__init__(config)\n@@ -1377,7 +1377,7 @@ def forward(\n     \"\"\",\n     SEW_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification with Wav2Vec2->SEW, wav2vec2->sew, WAV_2_VEC_2->SEW\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification with Wav2Vec2->SEW, wav2vec2->sew, WAV2VEC2->SEW\n class SEWForSequenceClassification(SEWPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "96c587fbb2eb4254c238cc7ee88a3b1126381c5a",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1468,7 +1468,7 @@ def forward(\n     \"\"\"SEW-D Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n     SEWD_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->SEWD, wav2vec2->sew_d, WAV_2_VEC_2->SEWD\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->SEWD, wav2vec2->sew_d, WAV2VEC2->SEWD\n class SEWDForCTC(SEWDPreTrainedModel):\n     def __init__(self, config, target_lang: Optional[str] = None):\n         super().__init__(config)\n@@ -1627,7 +1627,7 @@ def forward(\n     \"\"\",\n     SEWD_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification with Wav2Vec2->SEWD, wav2vec2->sew_d, WAV_2_VEC_2->SEWD\n+# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification with Wav2Vec2->SEWD, wav2vec2->sew_d, WAV2VEC2->SEWD\n class SEWDForSequenceClassification(SEWDPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "612e149d5447f97453961d5de453a91064be054d",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -999,7 +999,7 @@ def __init__(self, config: Siglip2VisionConfig):\n         self.mlp = Siglip2MLP(config)\n         self.num_heads = config.num_attention_heads\n \n-    def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n+    def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n         batch_size = hidden_state.shape[0]\n         probe = self.probe.repeat(batch_size, 1, 1)\n "
        },
        {
            "sha": "23df3b0413d9c2719925a2cb93418ddc70d11f5a",
            "filename": "src/transformers/models/siglip2/modular_siglip2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -12,7 +12,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Tuple, Union\n+from typing import Optional\n \n import torch\n import torch.nn as nn\n@@ -243,7 +243,7 @@ def forward(\n         spatial_shapes: torch.LongTensor,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+    ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Returns:\n "
        },
        {
            "sha": "cd8544156832ece63f5eeac3dc815e33d9ed8c20",
            "filename": "src/transformers/models/smolvlm/configuration_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fconfiguration_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fconfiguration_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fconfiguration_smolvlm.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig"
        },
        {
            "sha": "74608797ab68238824760df0953eb09dbf52138f",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 273,
            "deletions": 309,
            "changes": 582,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1,38 +1,32 @@\n-# coding=utf-8\n-# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch UniSpeech model.\"\"\"\n-\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/unispeech/modular_unispeech.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_unispeech.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n import warnings\n from dataclasses import dataclass\n from typing import Optional, Tuple, Union\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n-from torch import nn\n+import torch.nn as nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n-from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput, Wav2Vec2BaseModelOutput\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    CausalLMOutput,\n+    ModelOutput,\n+    SequenceClassifierOutput,\n+    Wav2Vec2BaseModelOutput,\n+)\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n-    ModelOutput,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -48,19 +42,11 @@\n \n logger = logging.get_logger(__name__)\n \n-\n-_HIDDEN_STATES_START_POSITION = 2\n-\n-# General docstring\n-_CONFIG_FOR_DOC = \"UniSpeechConfig\"\n-\n # Base docstring\n _CHECKPOINT_FOR_DOC = \"patrickvonplaten/unispeech-large-1500h-cv-timit\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 292, 1024]\n \n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = \"'mister quilter is the apposl of the midle classes and weare glad to welcom his gosepl'\"\n-_CTC_EXPECTED_LOSS = 17.17\n+# General docstring\n+_CONFIG_FOR_DOC = \"UniSpeechConfig\"\n \n \n @dataclass\n@@ -99,127 +85,62 @@ class UniSpeechForPreTrainingOutput(ModelOutput):\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices\n-def _compute_mask_indices(\n-    shape: Tuple[int, int],\n-    mask_prob: float,\n-    mask_length: int,\n-    attention_mask: Optional[torch.LongTensor] = None,\n-    min_masks: int = 0,\n-) -> np.ndarray:\n-    \"\"\"\n-    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n-    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n-    CPU as part of the preprocessing during training.\n+class UniSpeechSamePadLayer(nn.Module):\n+    def __init__(self, num_conv_pos_embeddings):\n+        super().__init__()\n+        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n \n-    Args:\n-        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n-               the first element is the batch size and the second element is the length of the axis to span.\n-        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n-                    independently generated mask spans of length `mask_length` is computed by\n-                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n-                    actual percentage will be smaller.\n-        mask_length: size of the mask\n-        min_masks: minimum number of masked spans\n-        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n-                        each batch dimension.\n-    \"\"\"\n-    batch_size, sequence_length = shape\n+    def forward(self, hidden_states):\n+        if self.num_pad_remove > 0:\n+            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n+        return hidden_states\n \n-    if mask_length < 1:\n-        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n \n-    if mask_length > sequence_length:\n-        raise ValueError(\n-            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n-            f\" and `sequence_length`: {sequence_length}`\"\n+class UniSpeechPositionalConvEmbedding(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.conv = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=config.num_conv_pos_embeddings,\n+            padding=config.num_conv_pos_embeddings // 2,\n+            groups=config.num_conv_pos_embedding_groups,\n         )\n \n-    # epsilon is used for probabilistic rounding\n-    epsilon = np.random.rand(1).item()\n-\n-    def compute_num_masked_span(input_length):\n-        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n-        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n-        num_masked_span = max(num_masked_span, min_masks)\n-\n-        # make sure num masked span <= sequence_length\n-        if num_masked_span * mask_length > sequence_length:\n-            num_masked_span = sequence_length // mask_length\n-\n-        # make sure num_masked span is also <= input_length - (mask_length - 1)\n-        if input_length - (mask_length - 1) < num_masked_span:\n-            num_masked_span = max(input_length - (mask_length - 1), 0)\n-\n-        return num_masked_span\n-\n-    # compute number of masked spans in batch\n-    input_lengths = (\n-        attention_mask.detach().sum(-1).tolist()\n-        if attention_mask is not None\n-        else [sequence_length for _ in range(batch_size)]\n-    )\n-\n-    # SpecAugment mask to fill\n-    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n-    spec_aug_mask_idxs = []\n-\n-    max_num_masked_span = compute_num_masked_span(sequence_length)\n-\n-    if max_num_masked_span == 0:\n-        return spec_aug_mask\n-\n-    for input_length in input_lengths:\n-        # compute num of masked spans for this input\n-        num_masked_span = compute_num_masked_span(input_length)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n \n-        # get random indices to mask\n-        spec_aug_mask_idx = np.random.choice(\n-            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n-        )\n+        if is_deepspeed_zero3_enabled():\n+            import deepspeed\n \n-        # pick first sampled index that will serve as a dummy index to pad vector\n-        # to ensure same dimension for all batches due to probabilistic rounding\n-        # Picking first sample just pads those vectors twice.\n-        if len(spec_aug_mask_idx) == 0:\n-            # this case can only happen if `input_length` is strictly smaller then\n-            # `sequence_length` in which case the last token has to be a padding\n-            # token which we can use as a dummy mask id\n-            dummy_mask_idx = sequence_length - 1\n+            with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n+                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n+            if hasattr(self.conv, \"parametrizations\"):\n+                weight_g = self.conv.parametrizations.weight.original0\n+                weight_v = self.conv.parametrizations.weight.original1\n+            else:\n+                weight_g = self.conv.weight_g\n+                weight_v = self.conv.weight_v\n+            deepspeed.zero.register_external_parameter(self, weight_v)\n+            deepspeed.zero.register_external_parameter(self, weight_g)\n         else:\n-            dummy_mask_idx = spec_aug_mask_idx[0]\n-\n-        spec_aug_mask_idx = np.concatenate(\n-            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n-        )\n-        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n-\n-    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n-\n-    # expand masked indices to masked spans\n-    spec_aug_mask_idxs = np.broadcast_to(\n-        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+            self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n \n-    # add offset to the starting indexes so that indexes now create a span\n-    offsets = np.arange(mask_length)[None, None, :]\n-    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n-        batch_size, max_num_masked_span * mask_length\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+        self.padding = UniSpeechSamePadLayer(config.num_conv_pos_embeddings)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n \n-    # ensure that we cannot have indices larger than sequence_length\n-    if spec_aug_mask_idxs.max() > sequence_length - 1:\n-        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.transpose(1, 2)\n \n-    # scatter indices to mask\n-    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.padding(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n \n-    return spec_aug_mask\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->UniSpeech\n class UniSpeechNoLayerNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -241,7 +162,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->UniSpeech\n class UniSpeechLayerNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -269,7 +189,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->UniSpeech\n class UniSpeechGroupNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -294,65 +213,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->UniSpeech\n-class UniSpeechPositionalConvEmbedding(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.conv = nn.Conv1d(\n-            config.hidden_size,\n-            config.hidden_size,\n-            kernel_size=config.num_conv_pos_embeddings,\n-            padding=config.num_conv_pos_embeddings // 2,\n-            groups=config.num_conv_pos_embedding_groups,\n-        )\n-\n-        weight_norm = nn.utils.weight_norm\n-        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n-            weight_norm = nn.utils.parametrizations.weight_norm\n-\n-        if is_deepspeed_zero3_enabled():\n-            import deepspeed\n-\n-            with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n-                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n-            if hasattr(self.conv, \"parametrizations\"):\n-                weight_g = self.conv.parametrizations.weight.original0\n-                weight_v = self.conv.parametrizations.weight.original1\n-            else:\n-                weight_g = self.conv.weight_g\n-                weight_v = self.conv.weight_v\n-            deepspeed.zero.register_external_parameter(self, weight_v)\n-            deepspeed.zero.register_external_parameter(self, weight_g)\n-        else:\n-            self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n-\n-        self.padding = UniSpeechSamePadLayer(config.num_conv_pos_embeddings)\n-        self.activation = ACT2FN[config.feat_extract_activation]\n-\n-    def forward(self, hidden_states):\n-        hidden_states = hidden_states.transpose(1, 2)\n-\n-        hidden_states = self.conv(hidden_states)\n-        hidden_states = self.padding(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n-\n-        hidden_states = hidden_states.transpose(1, 2)\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->UniSpeech\n-class UniSpeechSamePadLayer(nn.Module):\n-    def __init__(self, num_conv_pos_embeddings):\n-        super().__init__()\n-        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n-\n-    def forward(self, hidden_states):\n-        if self.num_pad_remove > 0:\n-            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->UniSpeech\n class UniSpeechFeatureEncoder(nn.Module):\n     \"\"\"Construct the features from raw audio waveform\"\"\"\n \n@@ -400,18 +260,6 @@ def forward(self, input_values):\n         return hidden_states\n \n \n-class UniSpeechFeatureExtractor(UniSpeechFeatureEncoder):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        warnings.warn(\n-            f\"The class `{self.__class__.__name__}` has been depreciated \"\n-            \"and will be removed in Transformers v5. \"\n-            f\"Use `{self.__class__.__bases__[0].__name__}` instead.\",\n-            FutureWarning,\n-        )\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection with Wav2Vec2->UniSpeech\n class UniSpeechFeatureProjection(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -427,7 +275,6 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->UniSpeech\n class UniSpeechAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -586,7 +433,6 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->UniSpeech\n class UniSpeechFlashAttention2(UniSpeechAttention):\n     \"\"\"\n     UniSpeech flash attention module. This module inherits from `UniSpeechAttention` as the weights of the module stays\n@@ -714,7 +560,6 @@ def forward(\n \n \n class UniSpeechSdpaAttention(UniSpeechAttention):\n-    # Copied from transformers.models.bart.modeling_bart.BartSdpaAttention.forward with Bart->UniSpeech\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -820,14 +665,6 @@ def forward(\n         return attn_output, None, past_key_value\n \n \n-UNISPEECH_ATTENTION_CLASSES = {\n-    \"eager\": UniSpeechAttention,\n-    \"sdpa\": UniSpeechSdpaAttention,\n-    \"flash_attention_2\": UniSpeechFlashAttention2,\n-}\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->UniSpeech\n class UniSpeechFeedForward(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -852,7 +689,13 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayer with Wav2Vec2->UniSpeech, WAV2VEC2->UNISPEECH\n+UNISPEECH_ATTENTION_CLASSES = {\n+    \"eager\": UniSpeechAttention,\n+    \"sdpa\": UniSpeechSdpaAttention,\n+    \"flash_attention_2\": UniSpeechFlashAttention2,\n+}\n+\n+\n class UniSpeechEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -888,79 +731,6 @@ def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         return outputs\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2AttnAdapterLayer with Wav2Vec2->UniSpeech\n-class UniSpeechAttnAdapterLayer(nn.Module):\n-    def __init__(self, config):\n-        \"\"\"\n-        Implements adapter modules directly with 3D tensor weight as parameters and without using ModuleList to speed\n-        up training throughput.\n-        \"\"\"\n-        super().__init__()\n-        self.input_dim = config.adapter_attn_dim\n-        self.hidden_dim = config.hidden_size\n-\n-        self.norm = nn.LayerNorm(self.hidden_dim)\n-        self.linear_1 = nn.Linear(self.hidden_dim, self.input_dim)\n-        self.act_fn = nn.ReLU()\n-        self.linear_2 = nn.Linear(self.input_dim, self.hidden_dim)\n-\n-    def forward(self, hidden_states: torch.FloatTensor):\n-        hidden_states = self.norm(hidden_states)\n-\n-        hidden_states = self.linear_1(hidden_states)\n-        hidden_states = self.act_fn(hidden_states)\n-        hidden_states = self.linear_2(hidden_states)\n-\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayerStableLayerNorm with Wav2Vec2->UniSpeech, WAV2VEC2->UNISPEECH\n-class UniSpeechEncoderLayerStableLayerNorm(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.attention = UNISPEECH_ATTENTION_CLASSES[config._attn_implementation](\n-            embed_dim=config.hidden_size,\n-            num_heads=config.num_attention_heads,\n-            dropout=config.attention_dropout,\n-            is_decoder=False,\n-        )\n-        self.dropout = nn.Dropout(config.hidden_dropout)\n-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.feed_forward = UniSpeechFeedForward(config)\n-        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-\n-        if getattr(config, \"adapter_attn_dim\", None) is not None:\n-            self.adapter_layer = UniSpeechAttnAdapterLayer(config)\n-        else:\n-            self.adapter_layer = None\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ):\n-        attn_residual = hidden_states\n-        hidden_states = self.layer_norm(hidden_states)\n-        hidden_states, attn_weights, _ = self.attention(\n-            hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-        )\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = attn_residual + hidden_states\n-        hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))\n-\n-        if self.adapter_layer is not None:\n-            hidden_states = hidden_states + self.adapter_layer(hidden_states)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Encoder with Wav2Vec2->UniSpeech\n class UniSpeechEncoder(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1046,7 +816,76 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderStableLayerNorm with Wav2Vec2->UniSpeech\n+class UniSpeechAttnAdapterLayer(nn.Module):\n+    def __init__(self, config):\n+        \"\"\"\n+        Implements adapter modules directly with 3D tensor weight as parameters and without using ModuleList to speed\n+        up training throughput.\n+        \"\"\"\n+        super().__init__()\n+        self.input_dim = config.adapter_attn_dim\n+        self.hidden_dim = config.hidden_size\n+\n+        self.norm = nn.LayerNorm(self.hidden_dim)\n+        self.linear_1 = nn.Linear(self.hidden_dim, self.input_dim)\n+        self.act_fn = nn.ReLU()\n+        self.linear_2 = nn.Linear(self.input_dim, self.hidden_dim)\n+\n+    def forward(self, hidden_states: torch.FloatTensor):\n+        hidden_states = self.norm(hidden_states)\n+\n+        hidden_states = self.linear_1(hidden_states)\n+        hidden_states = self.act_fn(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class UniSpeechEncoderLayerStableLayerNorm(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.attention = UNISPEECH_ATTENTION_CLASSES[config._attn_implementation](\n+            embed_dim=config.hidden_size,\n+            num_heads=config.num_attention_heads,\n+            dropout=config.attention_dropout,\n+            is_decoder=False,\n+        )\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.feed_forward = UniSpeechFeedForward(config)\n+        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        if getattr(config, \"adapter_attn_dim\", None) is not None:\n+            self.adapter_layer = UniSpeechAttnAdapterLayer(config)\n+        else:\n+            self.adapter_layer = None\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ):\n+        attn_residual = hidden_states\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states, attn_weights, _ = self.attention(\n+            hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+        )\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = attn_residual + hidden_states\n+        hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))\n+\n+        if self.adapter_layer is not None:\n+            hidden_states = hidden_states + self.adapter_layer(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n class UniSpeechEncoderStableLayerNorm(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1138,7 +977,7 @@ def forward(\n \n class UniSpeechGumbelVectorQuantizer(nn.Module):\n     \"\"\"\n-    Vector quantization using gumbel softmax. See [CATEGORICAL REPARAMETERIZATION WITH\n+    Vector quantization using gumbel softmax. See `[CATEGORICAL REPARAMETERIZATION WITH\n     GUMBEL-SOFTMAX](https://arxiv.org/pdf/1611.01144.pdf) for more information.\n     \"\"\"\n \n@@ -1149,8 +988,8 @@ def __init__(self, config):\n \n         if config.codevector_dim % self.num_groups != 0:\n             raise ValueError(\n-                f\"`config.codevector_dim {config.codevector_dim} must be divisible by `config.num_codevector_groups`\"\n-                f\" {self.num_groups} for concatenation\"\n+                f\"`config.codevector_dim {config.codevector_dim} must be divisible \"\n+                f\"by `config.num_codevector_groups` {self.num_groups} for concatenation\"\n             )\n \n         # storage for codebook variables (codewords)\n@@ -1283,6 +1122,128 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n         return attention_mask\n \n \n+def _compute_mask_indices(\n+    shape: Tuple[int, int],\n+    mask_prob: float,\n+    mask_length: int,\n+    attention_mask: Optional[torch.LongTensor] = None,\n+    min_masks: int = 0,\n+) -> np.ndarray:\n+    \"\"\"\n+    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n+    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n+    CPU as part of the preprocessing during training.\n+\n+    Args:\n+        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n+               the first element is the batch size and the second element is the length of the axis to span.\n+        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n+                    independently generated mask spans of length `mask_length` is computed by\n+                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n+                    actual percentage will be smaller.\n+        mask_length: size of the mask\n+        min_masks: minimum number of masked spans\n+        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n+                        each batch dimension.\n+    \"\"\"\n+    batch_size, sequence_length = shape\n+\n+    if mask_length < 1:\n+        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n+\n+    if mask_length > sequence_length:\n+        raise ValueError(\n+            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n+            f\" and `sequence_length`: {sequence_length}`\"\n+        )\n+\n+    # epsilon is used for probabilistic rounding\n+    epsilon = np.random.rand(1).item()\n+\n+    def compute_num_masked_span(input_length):\n+        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n+        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n+        num_masked_span = max(num_masked_span, min_masks)\n+\n+        # make sure num masked span <= sequence_length\n+        if num_masked_span * mask_length > sequence_length:\n+            num_masked_span = sequence_length // mask_length\n+\n+        # make sure num_masked span is also <= input_length - (mask_length - 1)\n+        if input_length - (mask_length - 1) < num_masked_span:\n+            num_masked_span = max(input_length - (mask_length - 1), 0)\n+\n+        return num_masked_span\n+\n+    # compute number of masked spans in batch\n+    input_lengths = (\n+        attention_mask.detach().sum(-1).tolist()\n+        if attention_mask is not None\n+        else [sequence_length for _ in range(batch_size)]\n+    )\n+\n+    # SpecAugment mask to fill\n+    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n+    spec_aug_mask_idxs = []\n+\n+    max_num_masked_span = compute_num_masked_span(sequence_length)\n+\n+    if max_num_masked_span == 0:\n+        return spec_aug_mask\n+\n+    for input_length in input_lengths:\n+        # compute num of masked spans for this input\n+        num_masked_span = compute_num_masked_span(input_length)\n+\n+        # get random indices to mask\n+        spec_aug_mask_idx = np.random.choice(\n+            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+        )\n+\n+        # pick first sampled index that will serve as a dummy index to pad vector\n+        # to ensure same dimension for all batches due to probabilistic rounding\n+        # Picking first sample just pads those vectors twice.\n+        if len(spec_aug_mask_idx) == 0:\n+            # this case can only happen if `input_length` is strictly smaller then\n+            # `sequence_length` in which case the last token has to be a padding\n+            # token which we can use as a dummy mask id\n+            dummy_mask_idx = sequence_length - 1\n+        else:\n+            dummy_mask_idx = spec_aug_mask_idx[0]\n+\n+        spec_aug_mask_idx = np.concatenate(\n+            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n+        )\n+        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n+\n+    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n+\n+    # expand masked indices to masked spans\n+    spec_aug_mask_idxs = np.broadcast_to(\n+        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+\n+    # add offset to the starting indexes so that indexes now create a span\n+    offsets = np.arange(mask_length)[None, None, :]\n+    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n+        batch_size, max_num_masked_span * mask_length\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+\n+    # ensure that we cannot have indices larger than sequence_length\n+    if spec_aug_mask_idxs.max() > sequence_length - 1:\n+        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+\n+    # scatter indices to mask\n+    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+\n+    return spec_aug_mask\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 1024]\n+\n+\n UNISPEECH_START_DOCSTRING = r\"\"\"\n     UniSpeech was proposed in [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled\n     Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,\n@@ -1301,7 +1262,6 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n-\n UNISPEECH_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1339,6 +1299,9 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n \"\"\"\n \n \n+UniSpeechBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n+\n @add_start_docstrings(\n     \"The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.\",\n     UNISPEECH_START_DOCSTRING,\n@@ -1361,7 +1324,6 @@ def __init__(self, config: UniSpeechConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states\n     def _mask_hidden_states(\n         self,\n         hidden_states: torch.FloatTensor,\n@@ -1411,7 +1373,7 @@ def _mask_hidden_states(\n     @add_start_docstrings_to_model_forward(UNISPEECH_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Wav2Vec2BaseModelOutput,\n+        output_type=UniSpeechBaseModelOutput,\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n         expected_output=_EXPECTED_OUTPUT_SHAPE,\n@@ -1424,7 +1386,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Wav2Vec2BaseModelOutput]:\n+    ) -> Union[Tuple, UniSpeechBaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1456,7 +1418,7 @@ def forward(\n         if not return_dict:\n             return (hidden_states, extract_features) + encoder_outputs[1:]\n \n-        return Wav2Vec2BaseModelOutput(\n+        return UniSpeechBaseModelOutput(\n             last_hidden_state=hidden_states,\n             extract_features=extract_features,\n             hidden_states=encoder_outputs.hidden_states,\n@@ -1610,6 +1572,13 @@ def forward(\n         )\n \n \n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'mister quilter is the apposl of the midle classes and weare glad to welcom his gosepl'\"\n+_CTC_EXPECTED_LOSS = 17.17\n+\n+\n @add_start_docstrings(\n     \"\"\"UniSpeech Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n     UNISPEECH_START_DOCSTRING,\n@@ -1620,7 +1589,6 @@ def forward(\n             by default.\n     \"\"\",\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->UniSpeech, wav2vec2->unispeech, WAV_2_VEC_2->UNISPEECH\n class UniSpeechForCTC(UniSpeechPreTrainedModel):\n     def __init__(self, config, target_lang: Optional[str] = None):\n         super().__init__(config)\n@@ -1797,7 +1765,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_extractor\n     def freeze_feature_extractor(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n@@ -1810,15 +1777,13 @@ def freeze_feature_extractor(self):\n         )\n         self.freeze_feature_encoder()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_encoder with wav2vec2->unispeech\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n         not be updated during training.\n         \"\"\"\n         self.unispeech.feature_extractor._freeze_parameters()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_base_model with wav2vec2->unispeech\n     def freeze_base_model(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the base model so that its parameters will not\n@@ -1834,7 +1799,6 @@ def freeze_base_model(self):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.forward with Wav2Vec2->UniSpeech, wav2vec2->unispeech\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],"
        },
        {
            "sha": "1096bc559b44ddd603784f9f03e1cbef174ded27",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "added",
            "additions": 563,
            "deletions": 0,
            "changes": 563,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -0,0 +1,563 @@\n+import math\n+import warnings\n+from dataclasses import dataclass\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...modeling_outputs import CausalLMOutput, ModelOutput, SequenceClassifierOutput, Wav2Vec2BaseModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..wav2vec2.modeling_wav2vec2 import (\n+    Wav2Vec2Encoder,\n+    Wav2Vec2EncoderStableLayerNorm,\n+    Wav2Vec2FeatureEncoder,\n+    Wav2Vec2FeatureProjection,\n+    Wav2Vec2ForCTC,\n+    Wav2Vec2ForSequenceClassification,\n+    Wav2Vec2GumbelVectorQuantizer,\n+    Wav2Vec2Model,\n+    Wav2Vec2PositionalConvEmbedding,\n+)\n+from .configuration_unispeech import UniSpeechConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"UniSpeechConfig\"\n+\n+# Base docstring\n+_CHECKPOINT_FOR_DOC = \"patrickvonplaten/unispeech-large-1500h-cv-timit\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 1024]\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'mister quilter is the apposl of the midle classes and weare glad to welcom his gosepl'\"\n+_CTC_EXPECTED_LOSS = 17.17\n+\n+\n+@dataclass\n+class UniSpeechForPreTrainingOutput(ModelOutput):\n+    \"\"\"\n+    Output type of [`UniSpeechForPreTrainingOutput`], with potential hidden states and attentions.\n+\n+    Args:\n+        loss (*optional*, returned when model is in train mode, `torch.FloatTensor` of shape `(1,)`):\n+            Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n+            paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n+            Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n+            projected quantized states.\n+        projected_quantized_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n+            Quantized extracted feature vectors projected to *config.proj_codevector_dim* representing the positive\n+            target vectors for contrastive loss.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    projected_states: Optional[torch.FloatTensor] = None\n+    projected_quantized_states: Optional[torch.FloatTensor] = None\n+    codevector_perplexity: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+class UniSpeechPositionalConvEmbedding(Wav2Vec2PositionalConvEmbedding):\n+    pass\n+\n+\n+class UniSpeechFeatureEncoder(Wav2Vec2FeatureEncoder):\n+    pass\n+\n+\n+class UniSpeechFeatureProjection(Wav2Vec2FeatureProjection):\n+    pass\n+\n+\n+class UniSpeechEncoder(Wav2Vec2Encoder):\n+    pass\n+\n+\n+class UniSpeechEncoderStableLayerNorm(Wav2Vec2EncoderStableLayerNorm):\n+    pass\n+\n+\n+class UniSpeechGumbelVectorQuantizer(Wav2Vec2GumbelVectorQuantizer):\n+    @staticmethod\n+    def _compute_perplexity(probs):\n+        marginal_probs = probs.mean(dim=0)\n+        perplexity = torch.exp(-torch.sum(marginal_probs * torch.log(marginal_probs + 1e-7), dim=-1)).sum()\n+        return perplexity\n+\n+    def forward(self, hidden_states):\n+        batch_size, sequence_length, hidden_size = hidden_states.shape\n+\n+        # project to codevector dim\n+        hidden_states = self.weight_proj(hidden_states)\n+        hidden_states = hidden_states.view(batch_size * sequence_length * self.num_groups, -1)\n+\n+        if self.training:\n+            # sample code vector probs via gumbel in differentiateable way\n+            codevector_probs = nn.functional.gumbel_softmax(\n+                hidden_states.float(), tau=self.temperature, hard=True\n+            ).type_as(hidden_states)\n+\n+            # compute perplexity\n+            codevector_soft_dist = torch.softmax(\n+                hidden_states.view(batch_size * sequence_length, self.num_groups, -1).float(), dim=-1\n+            )\n+            perplexity = self._compute_perplexity(codevector_soft_dist)\n+        else:\n+            # take argmax in non-differentiable way\n+            # comptute hard codevector distribution (one hot)\n+            codevector_idx = hidden_states.argmax(dim=-1)\n+            codevector_probs = hidden_states.new_zeros(*hidden_states.shape).scatter_(\n+                -1, codevector_idx.view(-1, 1), 1.0\n+            )\n+            codevector_probs = codevector_probs.view(batch_size * sequence_length, self.num_groups, -1)\n+\n+            perplexity = self._compute_perplexity(codevector_probs)\n+\n+        codevector_probs = codevector_probs.view(batch_size * sequence_length, -1)\n+        # use probs to retrieve codevectors\n+        codevectors_per_group = codevector_probs.unsqueeze(-1) * self.codevectors\n+        codevectors = codevectors_per_group.view(batch_size * sequence_length, self.num_groups, self.num_vars, -1)\n+        codevectors = codevectors.sum(-2).view(batch_size, sequence_length, -1)\n+\n+        return codevectors, perplexity\n+\n+\n+class UniSpeechPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = UniSpeechConfig\n+    base_model_prefix = \"unispeech\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        # gumbel softmax requires special init\n+        if isinstance(module, UniSpeechGumbelVectorQuantizer):\n+            module.weight_proj.weight.data.normal_(mean=0.0, std=1)\n+            module.weight_proj.bias.data.zero_()\n+            nn.init.uniform_(module.codevectors)\n+        elif isinstance(module, UniSpeechPositionalConvEmbedding):\n+            nn.init.normal_(\n+                module.conv.weight,\n+                mean=0,\n+                std=2 * math.sqrt(1 / (module.conv.kernel_size[0] * module.conv.in_channels)),\n+            )\n+            nn.init.constant_(module.conv.bias, 0)\n+        elif isinstance(module, UniSpeechFeatureProjection):\n+            k = math.sqrt(1 / module.projection.in_features)\n+            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n+            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n+        elif isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            nn.init.kaiming_normal_(module.weight)\n+\n+            if module.bias is not None:\n+                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n+                nn.init.uniform_(module.bias, a=-k, b=k)\n+\n+    def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+\n+        def _conv_out_length(input_length, kernel_size, stride):\n+            # 1D convolutional layer output length formula taken\n+            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n+            return torch.div(input_length - kernel_size, stride, rounding_mode=\"floor\") + 1\n+\n+        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):\n+            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n+\n+        return input_lengths\n+\n+    def _get_feature_vector_attention_mask(self, feature_vector_length: int, attention_mask: torch.LongTensor):\n+        # Effectively attention_mask.sum(-1), but not inplace to be able to run\n+        # on inference mode.\n+        non_padded_lengths = attention_mask.cumsum(dim=-1)[:, -1]\n+        output_lengths = self._get_feat_extract_output_lengths(non_padded_lengths).to(torch.long)\n+        batch_size = attention_mask.shape[0]\n+\n+        attention_mask = torch.zeros(\n+            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n+        )\n+        # these two operations makes sure that all values before the output lengths idxs are attended to\n+        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n+        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n+        return attention_mask\n+\n+\n+UNISPEECH_START_DOCSTRING = r\"\"\"\n+    UniSpeech was proposed in [UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled\n+    Data](https://arxiv.org/abs/2101.07597) by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,\n+    Michael Zeng, Xuedong Huang.\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving etc.).\n+\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n+    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`UniSpeechConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+UNISPEECH_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n+            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n+            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n+        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            <Tip warning={true}>\n+\n+            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==\n+            True`. For all models whose processor has `config.return_attention_mask == False`, `attention_mask` should\n+            **not** be passed to avoid degraded performance when doing batched inference. For such models\n+            `input_values` should simply be padded with 0 and passed without `attention_mask`. Be aware that these\n+            models also yield slightly different results depending on whether `input_values` is padded or not.\n+\n+            </Tip>\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+UniSpeechBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n+\n+@add_start_docstrings(\n+    \"The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.\",\n+    UNISPEECH_START_DOCSTRING,\n+)\n+class UniSpeechModel(UniSpeechPreTrainedModel, Wav2Vec2Model):\n+    def __init__(self, config: UniSpeechConfig):\n+        UniSpeechPreTrainedModel.__init__(config)\n+        self.config = config\n+        self.feature_extractor = UniSpeechFeatureEncoder(config)\n+        self.feature_projection = UniSpeechFeatureProjection(config)\n+\n+        if config.mask_time_prob > 0.0 or config.mask_feature_prob > 0.0:\n+            self.masked_spec_embed = nn.Parameter(torch.Tensor(config.hidden_size).uniform_())\n+\n+        if config.do_stable_layer_norm:\n+            self.encoder = UniSpeechEncoderStableLayerNorm(config)\n+        else:\n+            self.encoder = UniSpeechEncoder(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for UniSpeech\")\n+\n+    def freeze_feature_encoder(self):\n+        raise AttributeError(\"Not needed for UniSpeech\")\n+\n+    @add_start_docstrings_to_model_forward(UNISPEECH_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=UniSpeechBaseModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(\n+        self,\n+        input_values: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        mask_time_indices: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, UniSpeechBaseModelOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        extract_features = self.feature_extractor(input_values)\n+        extract_features = extract_features.transpose(1, 2)\n+\n+        if attention_mask is not None:\n+            # compute reduced attention_mask corresponding to feature vectors\n+            attention_mask = self._get_feature_vector_attention_mask(extract_features.shape[1], attention_mask)\n+\n+        hidden_states, extract_features = self.feature_projection(extract_features)\n+        hidden_states = self._mask_hidden_states(\n+            hidden_states, mask_time_indices=mask_time_indices, attention_mask=attention_mask\n+        )\n+\n+        encoder_outputs = self.encoder(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        hidden_states = encoder_outputs[0]\n+\n+        if not return_dict:\n+            return (hidden_states, extract_features) + encoder_outputs[1:]\n+\n+        return UniSpeechBaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            extract_features=extract_features,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"UniSpeech Model with a vector-quantization module and ctc loss for pre-training.\"\"\", UNISPEECH_START_DOCSTRING\n+)\n+class UniSpeechForPreTraining(UniSpeechPreTrainedModel):\n+    def __init__(self, config: UniSpeechConfig):\n+        super().__init__(config)\n+        self.unispeech = UniSpeechModel(config)\n+        self.dropout_features = nn.Dropout(config.feat_quantizer_dropout)\n+\n+        self.quantizer = UniSpeechGumbelVectorQuantizer(config)\n+        self.project_q = nn.Linear(config.codevector_dim, config.proj_codevector_dim)\n+        self.project_hid = nn.Linear(config.proj_codevector_dim, config.hidden_size)\n+\n+        self.ctc_proj = nn.Linear(config.hidden_size, config.num_ctc_classes)\n+        self.dropout = nn.Dropout(config.final_dropout)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def set_gumbel_temperature(self, temperature: int):\n+        \"\"\"\n+        Set the Gumbel softmax temperature to a given value. Only necessary for training\n+        \"\"\"\n+        self.quantizer.temperature = temperature\n+\n+    def freeze_feature_extractor(self):\n+        \"\"\"\n+        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n+        not be updated during training.\n+        \"\"\"\n+        warnings.warn(\n+            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n+            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n+            FutureWarning,\n+        )\n+        self.freeze_feature_encoder()\n+\n+    def freeze_feature_encoder(self):\n+        \"\"\"\n+        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n+        not be updated during training.\n+        \"\"\"\n+        self.unispeech.feature_extractor._freeze_parameters()\n+\n+    @staticmethod\n+    def compute_contrastive_logits(\n+        target_features: torch.FloatTensor,\n+        negative_features: torch.FloatTensor,\n+        predicted_features: torch.FloatTensor,\n+        temperature: int = 1,\n+    ):\n+        \"\"\"\n+        Compute logits for contrastive loss based using cosine similarity as the distance measure between\n+        `[positive_feature, negative_features]` and `[predicted_features]`. Additionally, temperature can be applied.\n+        \"\"\"\n+        target_features = torch.cat([target_features, negative_features], dim=0)\n+\n+        logits = torch.cosine_similarity(predicted_features.float(), target_features.float(), dim=-1)\n+        logits = logits.type_as(target_features)\n+\n+        # apply temperature\n+        logits = logits / temperature\n+        return logits\n+\n+    @add_start_docstrings_to_model_forward(UNISPEECH_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=UniSpeechForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_values: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, UniSpeechForPreTrainingOutput]:\n+        r\"\"\"\n+        mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict\n+            masked extracted features in *config.proj_codevector_dim* space.\n+        sampled_negative_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length, num_negatives)`, *optional*):\n+            Indices indicating which quantized target vectors are used as negative sampled vectors in contrastive loss.\n+            Required input for pre-training.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoFeatureExtractor, UniSpeechForPreTraining\n+\n+        >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/unispeech-large-1500h-cv\")\n+        >>> model = UniSpeechForPreTraining.from_pretrained(\"microsoft/unispeech-large-1500h-cv\")\n+        >>> # TODO: Add full pretraining example\n+        ```\"\"\"\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.unispeech(\n+            input_values,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        transformer_features = outputs[0]\n+\n+        # quantize all (unmasked) extracted features and project to final vq dim\n+        extract_features = self.dropout_features(outputs[1])\n+        quantized_features, codevector_perplexity = self.quantizer(extract_features)\n+\n+        # project quantized features twice\n+        quantized_features = self.project_q(quantized_features.to(self.project_q.weight.dtype))\n+        quantized_features = self.project_hid(quantized_features)\n+\n+        prob_replace_matrix = torch.empty(transformer_features.size(0), transformer_features.size(1)).fill_(\n+            self.config.replace_prob\n+        )\n+        prob_replace_matrix = prob_replace_matrix.transpose(0, 1)\n+        sampled_replace_matrix = torch.bernoulli(prob_replace_matrix).bool().to(transformer_features.device)\n+        sampled_replace_matrix = sampled_replace_matrix.transpose(0, 1)\n+        sampled_replace_matrix = sampled_replace_matrix.unsqueeze(-1)\n+        logits = transformer_features.masked_fill(sampled_replace_matrix, 0.0) + (\n+            quantized_features.masked_fill(~sampled_replace_matrix, 0.0)\n+        )\n+\n+        # project to ctc units\n+        logits = self.dropout(logits)\n+        logits = self.ctc_proj(logits)\n+\n+        # TODO(PVP) - add negative sampling & loss computation\n+        loss = None\n+        if not return_dict:\n+            if loss is not None:\n+                return (loss, transformer_features, quantized_features, codevector_perplexity) + outputs[2:]\n+            return (transformer_features, quantized_features, codevector_perplexity) + outputs[2:]\n+\n+        return UniSpeechForPreTrainingOutput(\n+            loss=loss,\n+            projected_states=transformer_features,\n+            projected_quantized_states=quantized_features,\n+            codevector_perplexity=codevector_perplexity,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"UniSpeech Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n+    UNISPEECH_START_DOCSTRING,\n+    \"\"\"\n+        target_lang (`str`, *optional*):\n+            Language id of adapter weights. Adapter weights are stored in the format adapter.<lang>.safetensors or\n+            adapter.<lang>.bin. Only relevant when using an instance of [`UniSpeechForCTC`] with adapters. Uses 'eng'\n+            by default.\n+    \"\"\",\n+)\n+class UniSpeechForCTC(Wav2Vec2ForCTC):\n+    @add_start_docstrings_to_model_forward(UNISPEECH_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=CausalLMOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_CTC_EXPECTED_OUTPUT,\n+        expected_loss=_CTC_EXPECTED_LOSS,\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    UniSpeech Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like\n+    SUPERB Keyword Spotting.\n+    \"\"\",\n+    UNISPEECH_START_DOCSTRING,\n+)\n+class UniSpeechForSequenceClassification(Wav2Vec2ForSequenceClassification):\n+    @add_start_docstrings_to_model_forward(UNISPEECH_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=SequenceClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"UniSpeechForCTC\",\n+    \"UniSpeechForPreTraining\",\n+    \"UniSpeechForSequenceClassification\",\n+    \"UniSpeechModel\",\n+    \"UniSpeechPreTrainedModel\",\n+]"
        },
        {
            "sha": "b1f8c4c3466ebcc37568d6c350d6a441568bfe26",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 305,
            "deletions": 357,
            "changes": 662,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1,28 +1,17 @@\n-# coding=utf-8\n-# Copyright 2021 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch UniSpeechSat model.\"\"\"\n-\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/unispeech_sat/modular_unispeech_sat.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_unispeech_sat.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n import warnings\n from dataclasses import dataclass\n from typing import Optional, Tuple, Union\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n-from torch import nn\n+import torch.nn as nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n@@ -32,14 +21,14 @@\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n+    ModelOutput,\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n     Wav2Vec2BaseModelOutput,\n     XVectorOutput,\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n-    ModelOutput,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -56,27 +45,11 @@\n \n logger = logging.get_logger(__name__)\n \n-\n-_HIDDEN_STATES_START_POSITION = 2\n-\n-# General docstring\n-_CONFIG_FOR_DOC = \"UniSpeechSatConfig\"\n-\n # Base docstring\n _CHECKPOINT_FOR_DOC = \"microsoft/unispeech-sat-base-100h-libri-ft\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n \n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = \"'MISTER QUILDER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n-_CTC_EXPECTED_LOSS = 39.88\n-\n-# Frame class docstring\n-_FRAME_CLASS_CHECKPOINT = \"microsoft/unispeech-sat-base-plus-sd\"\n-_FRAME_EXPECTED_OUTPUT = [0, 0]\n-\n-# Speaker Verification docstring\n-_XVECTOR_CHECKPOINT = \"microsoft/unispeech-sat-base-plus-sv\"\n-_XVECTOR_EXPECTED_OUTPUT = 0.97\n+# General docstring\n+_CONFIG_FOR_DOC = \"UniSpeechSatConfig\"\n \n \n @dataclass\n@@ -116,127 +89,62 @@ class UniSpeechSatForPreTrainingOutput(ModelOutput):\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices\n-def _compute_mask_indices(\n-    shape: Tuple[int, int],\n-    mask_prob: float,\n-    mask_length: int,\n-    attention_mask: Optional[torch.LongTensor] = None,\n-    min_masks: int = 0,\n-) -> np.ndarray:\n-    \"\"\"\n-    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n-    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n-    CPU as part of the preprocessing during training.\n+class UniSpeechSatSamePadLayer(nn.Module):\n+    def __init__(self, num_conv_pos_embeddings):\n+        super().__init__()\n+        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n \n-    Args:\n-        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n-               the first element is the batch size and the second element is the length of the axis to span.\n-        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n-                    independently generated mask spans of length `mask_length` is computed by\n-                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n-                    actual percentage will be smaller.\n-        mask_length: size of the mask\n-        min_masks: minimum number of masked spans\n-        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n-                        each batch dimension.\n-    \"\"\"\n-    batch_size, sequence_length = shape\n+    def forward(self, hidden_states):\n+        if self.num_pad_remove > 0:\n+            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n+        return hidden_states\n \n-    if mask_length < 1:\n-        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n \n-    if mask_length > sequence_length:\n-        raise ValueError(\n-            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n-            f\" and `sequence_length`: {sequence_length}`\"\n+class UniSpeechSatPositionalConvEmbedding(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.conv = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=config.num_conv_pos_embeddings,\n+            padding=config.num_conv_pos_embeddings // 2,\n+            groups=config.num_conv_pos_embedding_groups,\n         )\n \n-    # epsilon is used for probabilistic rounding\n-    epsilon = np.random.rand(1).item()\n-\n-    def compute_num_masked_span(input_length):\n-        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n-        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n-        num_masked_span = max(num_masked_span, min_masks)\n-\n-        # make sure num masked span <= sequence_length\n-        if num_masked_span * mask_length > sequence_length:\n-            num_masked_span = sequence_length // mask_length\n-\n-        # make sure num_masked span is also <= input_length - (mask_length - 1)\n-        if input_length - (mask_length - 1) < num_masked_span:\n-            num_masked_span = max(input_length - (mask_length - 1), 0)\n-\n-        return num_masked_span\n-\n-    # compute number of masked spans in batch\n-    input_lengths = (\n-        attention_mask.detach().sum(-1).tolist()\n-        if attention_mask is not None\n-        else [sequence_length for _ in range(batch_size)]\n-    )\n-\n-    # SpecAugment mask to fill\n-    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n-    spec_aug_mask_idxs = []\n-\n-    max_num_masked_span = compute_num_masked_span(sequence_length)\n-\n-    if max_num_masked_span == 0:\n-        return spec_aug_mask\n-\n-    for input_length in input_lengths:\n-        # compute num of masked spans for this input\n-        num_masked_span = compute_num_masked_span(input_length)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n \n-        # get random indices to mask\n-        spec_aug_mask_idx = np.random.choice(\n-            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n-        )\n+        if is_deepspeed_zero3_enabled():\n+            import deepspeed\n \n-        # pick first sampled index that will serve as a dummy index to pad vector\n-        # to ensure same dimension for all batches due to probabilistic rounding\n-        # Picking first sample just pads those vectors twice.\n-        if len(spec_aug_mask_idx) == 0:\n-            # this case can only happen if `input_length` is strictly smaller then\n-            # `sequence_length` in which case the last token has to be a padding\n-            # token which we can use as a dummy mask id\n-            dummy_mask_idx = sequence_length - 1\n+            with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n+                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n+            if hasattr(self.conv, \"parametrizations\"):\n+                weight_g = self.conv.parametrizations.weight.original0\n+                weight_v = self.conv.parametrizations.weight.original1\n+            else:\n+                weight_g = self.conv.weight_g\n+                weight_v = self.conv.weight_v\n+            deepspeed.zero.register_external_parameter(self, weight_v)\n+            deepspeed.zero.register_external_parameter(self, weight_g)\n         else:\n-            dummy_mask_idx = spec_aug_mask_idx[0]\n-\n-        spec_aug_mask_idx = np.concatenate(\n-            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n-        )\n-        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n-\n-    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n-\n-    # expand masked indices to masked spans\n-    spec_aug_mask_idxs = np.broadcast_to(\n-        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+            self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n \n-    # add offset to the starting indexes so that indexes now create a span\n-    offsets = np.arange(mask_length)[None, None, :]\n-    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n-        batch_size, max_num_masked_span * mask_length\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+        self.padding = UniSpeechSatSamePadLayer(config.num_conv_pos_embeddings)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n \n-    # ensure that we cannot have indices larger than sequence_length\n-    if spec_aug_mask_idxs.max() > sequence_length - 1:\n-        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.transpose(1, 2)\n \n-    # scatter indices to mask\n-    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.padding(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n \n-    return spec_aug_mask\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->UniSpeechSat\n class UniSpeechSatNoLayerNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -258,7 +166,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->UniSpeechSat\n class UniSpeechSatLayerNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -286,7 +193,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->UniSpeechSat\n class UniSpeechSatGroupNormConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -311,65 +217,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->UniSpeechSat\n-class UniSpeechSatPositionalConvEmbedding(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.conv = nn.Conv1d(\n-            config.hidden_size,\n-            config.hidden_size,\n-            kernel_size=config.num_conv_pos_embeddings,\n-            padding=config.num_conv_pos_embeddings // 2,\n-            groups=config.num_conv_pos_embedding_groups,\n-        )\n-\n-        weight_norm = nn.utils.weight_norm\n-        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n-            weight_norm = nn.utils.parametrizations.weight_norm\n-\n-        if is_deepspeed_zero3_enabled():\n-            import deepspeed\n-\n-            with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n-                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n-            if hasattr(self.conv, \"parametrizations\"):\n-                weight_g = self.conv.parametrizations.weight.original0\n-                weight_v = self.conv.parametrizations.weight.original1\n-            else:\n-                weight_g = self.conv.weight_g\n-                weight_v = self.conv.weight_v\n-            deepspeed.zero.register_external_parameter(self, weight_v)\n-            deepspeed.zero.register_external_parameter(self, weight_g)\n-        else:\n-            self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n-\n-        self.padding = UniSpeechSatSamePadLayer(config.num_conv_pos_embeddings)\n-        self.activation = ACT2FN[config.feat_extract_activation]\n-\n-    def forward(self, hidden_states):\n-        hidden_states = hidden_states.transpose(1, 2)\n-\n-        hidden_states = self.conv(hidden_states)\n-        hidden_states = self.padding(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n-\n-        hidden_states = hidden_states.transpose(1, 2)\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->UniSpeechSat\n-class UniSpeechSatSamePadLayer(nn.Module):\n-    def __init__(self, num_conv_pos_embeddings):\n-        super().__init__()\n-        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n-\n-    def forward(self, hidden_states):\n-        if self.num_pad_remove > 0:\n-            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->UniSpeechSat\n class UniSpeechSatFeatureEncoder(nn.Module):\n     \"\"\"Construct the features from raw audio waveform\"\"\"\n \n@@ -417,18 +264,6 @@ def forward(self, input_values):\n         return hidden_states\n \n \n-class UniSpeechSatFeatureExtractor(UniSpeechSatFeatureEncoder):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        warnings.warn(\n-            f\"The class `{self.__class__.__name__}` has been depreciated \"\n-            \"and will be removed in Transformers v5. \"\n-            f\"Use `{self.__class__.__bases__[0].__name__}` instead.\",\n-            FutureWarning,\n-        )\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection with Wav2Vec2->UniSpeechSat\n class UniSpeechSatFeatureProjection(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -444,7 +279,6 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartAttention with Bart->UniSpeechSat\n class UniSpeechSatAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -603,7 +437,6 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n-# Copied from transformers.models.bart.modeling_bart.BartFlashAttention2 with Bart->UniSpeechSat\n class UniSpeechSatFlashAttention2(UniSpeechSatAttention):\n     \"\"\"\n     UniSpeechSat flash attention module. This module inherits from `UniSpeechSatAttention` as the weights of the module stays\n@@ -731,7 +564,6 @@ def forward(\n \n \n class UniSpeechSatSdpaAttention(UniSpeechSatAttention):\n-    # Copied from transformers.models.bart.modeling_bart.BartSdpaAttention.forward with Bart->UniSpeechSat\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -837,14 +669,6 @@ def forward(\n         return attn_output, None, past_key_value\n \n \n-UNISPEECHSAT_ATTENTION_CLASSES = {\n-    \"eager\": UniSpeechSatAttention,\n-    \"sdpa\": UniSpeechSatSdpaAttention,\n-    \"flash_attention_2\": UniSpeechSatFlashAttention2,\n-}\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->UniSpeechSat\n class UniSpeechSatFeedForward(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -856,118 +680,52 @@ def __init__(self, config):\n         else:\n             self.intermediate_act_fn = config.hidden_act\n \n-        self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n-        self.output_dropout = nn.Dropout(config.hidden_dropout)\n-\n-    def forward(self, hidden_states):\n-        hidden_states = self.intermediate_dense(hidden_states)\n-        hidden_states = self.intermediate_act_fn(hidden_states)\n-        hidden_states = self.intermediate_dropout(hidden_states)\n-\n-        hidden_states = self.output_dense(hidden_states)\n-        hidden_states = self.output_dropout(hidden_states)\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayer with Wav2Vec2->UniSpeechSat, WAV2VEC2->UNISPEECHSAT\n-class UniSpeechSatEncoderLayer(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.attention = UNISPEECHSAT_ATTENTION_CLASSES[config._attn_implementation](\n-            embed_dim=config.hidden_size,\n-            num_heads=config.num_attention_heads,\n-            dropout=config.attention_dropout,\n-            is_decoder=False,\n-        )\n-\n-        self.dropout = nn.Dropout(config.hidden_dropout)\n-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.feed_forward = UniSpeechSatFeedForward(config)\n-        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-\n-    def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n-        attn_residual = hidden_states\n-        hidden_states, attn_weights, _ = self.attention(\n-            hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n-        )\n-        hidden_states = self.dropout(hidden_states)\n-        hidden_states = attn_residual + hidden_states\n-\n-        hidden_states = self.layer_norm(hidden_states)\n-        hidden_states = hidden_states + self.feed_forward(hidden_states)\n-        hidden_states = self.final_layer_norm(hidden_states)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2AttnAdapterLayer with Wav2Vec2->UniSpeechSat\n-class UniSpeechSatAttnAdapterLayer(nn.Module):\n-    def __init__(self, config):\n-        \"\"\"\n-        Implements adapter modules directly with 3D tensor weight as parameters and without using ModuleList to speed\n-        up training throughput.\n-        \"\"\"\n-        super().__init__()\n-        self.input_dim = config.adapter_attn_dim\n-        self.hidden_dim = config.hidden_size\n-\n-        self.norm = nn.LayerNorm(self.hidden_dim)\n-        self.linear_1 = nn.Linear(self.hidden_dim, self.input_dim)\n-        self.act_fn = nn.ReLU()\n-        self.linear_2 = nn.Linear(self.input_dim, self.hidden_dim)\n-\n-    def forward(self, hidden_states: torch.FloatTensor):\n-        hidden_states = self.norm(hidden_states)\n+        self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)\n+        self.output_dropout = nn.Dropout(config.hidden_dropout)\n \n-        hidden_states = self.linear_1(hidden_states)\n-        hidden_states = self.act_fn(hidden_states)\n-        hidden_states = self.linear_2(hidden_states)\n+    def forward(self, hidden_states):\n+        hidden_states = self.intermediate_dense(hidden_states)\n+        hidden_states = self.intermediate_act_fn(hidden_states)\n+        hidden_states = self.intermediate_dropout(hidden_states)\n \n+        hidden_states = self.output_dense(hidden_states)\n+        hidden_states = self.output_dropout(hidden_states)\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderLayerStableLayerNorm with Wav2Vec2->UniSpeechSat, WAV2VEC2->UNISPEECHSAT\n-class UniSpeechSatEncoderLayerStableLayerNorm(nn.Module):\n+UNISPEECH_SAT_ATTENTION_CLASSES = {\n+    \"eager\": UniSpeechSatAttention,\n+    \"sdpa\": UniSpeechSatSdpaAttention,\n+    \"flash_attention_2\": UniSpeechSatFlashAttention2,\n+}\n+\n+\n+class UniSpeechSatEncoderLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.attention = UNISPEECHSAT_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attention = UNISPEECH_SAT_ATTENTION_CLASSES[config._attn_implementation](\n             embed_dim=config.hidden_size,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_dropout,\n             is_decoder=False,\n         )\n+\n         self.dropout = nn.Dropout(config.hidden_dropout)\n         self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.feed_forward = UniSpeechSatFeedForward(config)\n         self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-        if getattr(config, \"adapter_attn_dim\", None) is not None:\n-            self.adapter_layer = UniSpeechSatAttnAdapterLayer(config)\n-        else:\n-            self.adapter_layer = None\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ):\n+    def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         attn_residual = hidden_states\n-        hidden_states = self.layer_norm(hidden_states)\n         hidden_states, attn_weights, _ = self.attention(\n             hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n         )\n         hidden_states = self.dropout(hidden_states)\n         hidden_states = attn_residual + hidden_states\n-        hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))\n \n-        if self.adapter_layer is not None:\n-            hidden_states = hidden_states + self.adapter_layer(hidden_states)\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = hidden_states + self.feed_forward(hidden_states)\n+        hidden_states = self.final_layer_norm(hidden_states)\n \n         outputs = (hidden_states,)\n \n@@ -977,7 +735,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Encoder with Wav2Vec2->UniSpeechSat\n class UniSpeechSatEncoder(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1063,7 +820,76 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2EncoderStableLayerNorm with Wav2Vec2->UniSpeechSat\n+class UniSpeechSatAttnAdapterLayer(nn.Module):\n+    def __init__(self, config):\n+        \"\"\"\n+        Implements adapter modules directly with 3D tensor weight as parameters and without using ModuleList to speed\n+        up training throughput.\n+        \"\"\"\n+        super().__init__()\n+        self.input_dim = config.adapter_attn_dim\n+        self.hidden_dim = config.hidden_size\n+\n+        self.norm = nn.LayerNorm(self.hidden_dim)\n+        self.linear_1 = nn.Linear(self.hidden_dim, self.input_dim)\n+        self.act_fn = nn.ReLU()\n+        self.linear_2 = nn.Linear(self.input_dim, self.hidden_dim)\n+\n+    def forward(self, hidden_states: torch.FloatTensor):\n+        hidden_states = self.norm(hidden_states)\n+\n+        hidden_states = self.linear_1(hidden_states)\n+        hidden_states = self.act_fn(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class UniSpeechSatEncoderLayerStableLayerNorm(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.attention = UNISPEECH_SAT_ATTENTION_CLASSES[config._attn_implementation](\n+            embed_dim=config.hidden_size,\n+            num_heads=config.num_attention_heads,\n+            dropout=config.attention_dropout,\n+            is_decoder=False,\n+        )\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.feed_forward = UniSpeechSatFeedForward(config)\n+        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        if getattr(config, \"adapter_attn_dim\", None) is not None:\n+            self.adapter_layer = UniSpeechSatAttnAdapterLayer(config)\n+        else:\n+            self.adapter_layer = None\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ):\n+        attn_residual = hidden_states\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states, attn_weights, _ = self.attention(\n+            hidden_states, attention_mask=attention_mask, output_attentions=output_attentions\n+        )\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = attn_residual + hidden_states\n+        hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))\n+\n+        if self.adapter_layer is not None:\n+            hidden_states = hidden_states + self.adapter_layer(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n class UniSpeechSatEncoderStableLayerNorm(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1155,7 +981,7 @@ def forward(\n \n class UniSpeechSatGumbelVectorQuantizer(nn.Module):\n     \"\"\"\n-    Vector quantization using gumbel softmax. See [CATEGORICAL REPARAMETERIZATION WITH\n+    Vector quantization using gumbel softmax. See `[CATEGORICAL REPARAMETERIZATION WITH\n     GUMBEL-SOFTMAX](https://arxiv.org/pdf/1611.01144.pdf) for more information.\n     \"\"\"\n \n@@ -1166,8 +992,8 @@ def __init__(self, config):\n \n         if config.codevector_dim % self.num_groups != 0:\n             raise ValueError(\n-                f\"`config.codevector_dim {config.codevector_dim} must be divisible by `config.num_codevector_groups`\"\n-                f\" {self.num_groups} for concatenation\"\n+                f\"`config.codevector_dim {config.codevector_dim} must be divisible \"\n+                f\"by `config.num_codevector_groups` {self.num_groups} for concatenation\"\n             )\n \n         # storage for codebook variables (codewords)\n@@ -1300,6 +1126,128 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n         return attention_mask\n \n \n+def _compute_mask_indices(\n+    shape: Tuple[int, int],\n+    mask_prob: float,\n+    mask_length: int,\n+    attention_mask: Optional[torch.LongTensor] = None,\n+    min_masks: int = 0,\n+) -> np.ndarray:\n+    \"\"\"\n+    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n+    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n+    CPU as part of the preprocessing during training.\n+\n+    Args:\n+        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n+               the first element is the batch size and the second element is the length of the axis to span.\n+        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n+                    independently generated mask spans of length `mask_length` is computed by\n+                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n+                    actual percentage will be smaller.\n+        mask_length: size of the mask\n+        min_masks: minimum number of masked spans\n+        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n+                        each batch dimension.\n+    \"\"\"\n+    batch_size, sequence_length = shape\n+\n+    if mask_length < 1:\n+        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n+\n+    if mask_length > sequence_length:\n+        raise ValueError(\n+            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n+            f\" and `sequence_length`: {sequence_length}`\"\n+        )\n+\n+    # epsilon is used for probabilistic rounding\n+    epsilon = np.random.rand(1).item()\n+\n+    def compute_num_masked_span(input_length):\n+        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n+        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n+        num_masked_span = max(num_masked_span, min_masks)\n+\n+        # make sure num masked span <= sequence_length\n+        if num_masked_span * mask_length > sequence_length:\n+            num_masked_span = sequence_length // mask_length\n+\n+        # make sure num_masked span is also <= input_length - (mask_length - 1)\n+        if input_length - (mask_length - 1) < num_masked_span:\n+            num_masked_span = max(input_length - (mask_length - 1), 0)\n+\n+        return num_masked_span\n+\n+    # compute number of masked spans in batch\n+    input_lengths = (\n+        attention_mask.detach().sum(-1).tolist()\n+        if attention_mask is not None\n+        else [sequence_length for _ in range(batch_size)]\n+    )\n+\n+    # SpecAugment mask to fill\n+    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n+    spec_aug_mask_idxs = []\n+\n+    max_num_masked_span = compute_num_masked_span(sequence_length)\n+\n+    if max_num_masked_span == 0:\n+        return spec_aug_mask\n+\n+    for input_length in input_lengths:\n+        # compute num of masked spans for this input\n+        num_masked_span = compute_num_masked_span(input_length)\n+\n+        # get random indices to mask\n+        spec_aug_mask_idx = np.random.choice(\n+            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+        )\n+\n+        # pick first sampled index that will serve as a dummy index to pad vector\n+        # to ensure same dimension for all batches due to probabilistic rounding\n+        # Picking first sample just pads those vectors twice.\n+        if len(spec_aug_mask_idx) == 0:\n+            # this case can only happen if `input_length` is strictly smaller then\n+            # `sequence_length` in which case the last token has to be a padding\n+            # token which we can use as a dummy mask id\n+            dummy_mask_idx = sequence_length - 1\n+        else:\n+            dummy_mask_idx = spec_aug_mask_idx[0]\n+\n+        spec_aug_mask_idx = np.concatenate(\n+            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n+        )\n+        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n+\n+    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n+\n+    # expand masked indices to masked spans\n+    spec_aug_mask_idxs = np.broadcast_to(\n+        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+\n+    # add offset to the starting indexes so that indexes now create a span\n+    offsets = np.arange(mask_length)[None, None, :]\n+    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n+        batch_size, max_num_masked_span * mask_length\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+\n+    # ensure that we cannot have indices larger than sequence_length\n+    if spec_aug_mask_idxs.max() > sequence_length - 1:\n+        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+\n+    # scatter indices to mask\n+    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+\n+    return spec_aug_mask\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n+\n+\n UNISPEECH_SAT_START_DOCSTRING = r\"\"\"\n     UniSpeechSat was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n     Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael\n@@ -1318,7 +1266,6 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n-\n UNISPEECH_SAT_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1338,12 +1285,10 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n             <Tip warning={true}>\n \n             `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==\n-            True`. For all models whose processor has `config.return_attention_mask == False`, such as\n-            [microsoft/unispeech-sat-base-100h-libri-ft](https://huggingface.co/microsoft/unispeech-sat-base-100h-libri-ft),\n-            `attention_mask` should **not** be passed to avoid degraded performance when doing batched inference. For\n-            such models `input_values` should simply be padded with 0 and passed without `attention_mask`. Be aware\n-            that these models also yield slightly different results depending on whether `input_values` is padded or\n-            not.\n+            True`. For all models whose processor has `config.return_attention_mask == False`, `attention_mask` should\n+            **not** be passed to avoid degraded performance when doing batched inference. For such models\n+            `input_values` should simply be padded with 0 and passed without `attention_mask`. Be aware that these\n+            models also yield slightly different results depending on whether `input_values` is padded or not.\n \n             </Tip>\n \n@@ -1357,6 +1302,8 @@ def _get_feature_vector_attention_mask(self, feature_vector_length: int, attenti\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n \n+UniSpeechSatBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n \n @add_start_docstrings(\n     \"The bare UniSpeechSat Model transformer outputting raw hidden-states without any specific head on top.\",\n@@ -1379,7 +1326,6 @@ def __init__(self, config: UniSpeechSatConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states\n     def _mask_hidden_states(\n         self,\n         hidden_states: torch.FloatTensor,\n@@ -1429,7 +1375,7 @@ def _mask_hidden_states(\n     @add_start_docstrings_to_model_forward(UNISPEECH_SAT_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Wav2Vec2BaseModelOutput,\n+        output_type=UniSpeechSatBaseModelOutput,\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n         expected_output=_EXPECTED_OUTPUT_SHAPE,\n@@ -1442,7 +1388,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Wav2Vec2BaseModelOutput]:\n+    ) -> Union[Tuple, UniSpeechSatBaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1474,15 +1420,18 @@ def forward(\n         if not return_dict:\n             return (hidden_states, extract_features) + encoder_outputs[1:]\n \n-        return Wav2Vec2BaseModelOutput(\n+        return UniSpeechSatBaseModelOutput(\n             last_hidden_state=hidden_states,\n             extract_features=extract_features,\n             hidden_states=encoder_outputs.hidden_states,\n             attentions=encoder_outputs.attentions,\n         )\n \n \n-@add_start_docstrings(\"\"\"UniSpeechSat Model with a quantizer and `VQ` head on top.\"\"\", UNISPEECH_SAT_START_DOCSTRING)\n+@add_start_docstrings(\n+    \"\"\"UniSpeechSat Model with a vector-quantization module and ctc loss for pre-training.\"\"\",\n+    UNISPEECH_SAT_START_DOCSTRING,\n+)\n class UniSpeechSatForPreTraining(UniSpeechSatPreTrainedModel):\n     def __init__(self, config: UniSpeechSatConfig):\n         super().__init__(config)\n@@ -1529,7 +1478,7 @@ def freeze_feature_encoder(self):\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n         not be updated during training.\n         \"\"\"\n-        self.wav2vec2.feature_extractor._freeze_parameters()\n+        self.unispeech_sat.feature_extractor._freeze_parameters()\n \n     @staticmethod\n     def compute_contrastive_logits(\n@@ -1594,16 +1543,6 @@ def forward(\n         logits = extract_features\n         loss = quantized_features = codevector_perplexity = None\n \n-        # layer normalization (has no effect when `config.do_stable_layer_norm == False`)\n-        #        extract_features = self.layer_norm_for_extract(extract_features)\n-        #        quantized_features, codevector_perplexity = self.quantizer(extract_features)\n-        #\n-        # project quantized features twice\n-        #        quantized_features = self.project_q(quantized_features)\n-        #        quantized_features = self.project_hid(quantized_features)\n-        #\n-        #        loss = None\n-        #        logits = quantized_features\n         if not return_dict:\n             if loss is not None:\n                 return (loss, logits, transformer_features, quantized_features, codevector_perplexity) + outputs[2:]\n@@ -1620,6 +1559,13 @@ def forward(\n         )\n \n \n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'MISTER QUILDER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n+_CTC_EXPECTED_LOSS = 39.88\n+\n+\n @add_start_docstrings(\n     \"\"\"UniSpeechSat Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n     UNISPEECH_SAT_START_DOCSTRING,\n@@ -1630,7 +1576,6 @@ def forward(\n             'eng' by default.\n     \"\"\",\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->UniSpeechSat, wav2vec2->unispeech_sat, WAV_2_VEC_2->UNISPEECH_SAT\n class UniSpeechSatForCTC(UniSpeechSatPreTrainedModel):\n     def __init__(self, config, target_lang: Optional[str] = None):\n         super().__init__(config)\n@@ -1784,8 +1729,8 @@ def forward(\n \n @add_start_docstrings(\n     \"\"\"\n-    UniSpeechSat Model with a sequence classification head on top (a linear layer over the pooled output) for tasks\n-    like SUPERB Keyword Spotting.\n+    UniSpeechSat Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like\n+    SUPERB Keyword Spotting.\n     \"\"\",\n     UNISPEECH_SAT_START_DOCSTRING,\n )\n@@ -1807,7 +1752,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_extractor\n     def freeze_feature_extractor(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n@@ -1820,15 +1764,13 @@ def freeze_feature_extractor(self):\n         )\n         self.freeze_feature_encoder()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_encoder with wav2vec2->unispeech_sat\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n         not be updated during training.\n         \"\"\"\n         self.unispeech_sat.feature_extractor._freeze_parameters()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_base_model with wav2vec2->unispeech_sat\n     def freeze_base_model(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the base model so that its parameters will not\n@@ -1844,7 +1786,6 @@ def freeze_base_model(self):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.forward with Wav2Vec2->UniSpeechSat, wav2vec2->unispeech_sat\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1908,13 +1849,17 @@ def forward(\n         )\n \n \n+# Frame class docstring\n+_FRAME_CLASS_CHECKPOINT = \"microsoft/unispeech-sat-base-plus-sd\"\n+_FRAME_EXPECTED_OUTPUT = [0, 0]\n+\n+\n @add_start_docstrings(\n     \"\"\"\n-    UniSpeech-SAT Model with a frame classification head on top for tasks like Speaker Diarization.\n+    UniSpeechSat Model with a frame classification head on top for tasks like Speaker Diarization.\n     \"\"\",\n     UNISPEECH_SAT_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification with Wav2Vec2->UniSpeechSat, wav2vec2->unispeech_sat, WAV_2_VEC_2->UNISPEECH_SAT\n class UniSpeechSatForAudioFrameClassification(UniSpeechSatPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -2021,7 +1966,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.AMSoftmaxLoss\n class AMSoftmaxLoss(nn.Module):\n     def __init__(self, input_dim, num_labels, scale=30.0, margin=0.4):\n         super(AMSoftmaxLoss, self).__init__()\n@@ -2045,7 +1989,6 @@ def forward(self, hidden_states, labels):\n         return loss\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.TDNNLayer\n class TDNNLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -2061,6 +2004,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if is_peft_available():\n             from peft.tuners.lora import LoraLayer\n \n+        if is_peft_available():\n             if isinstance(self.kernel, LoraLayer):\n                 warnings.warn(\n                     \"Detected LoRA on TDNNLayer. LoRA weights won't be applied due to optimization. \"\n@@ -2077,13 +2021,17 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+# Speaker Verification docstring\n+_XVECTOR_CHECKPOINT = \"microsoft/unispeech-sat-base-plus-sv\"\n+_XVECTOR_EXPECTED_OUTPUT = 0.97\n+\n+\n @add_start_docstrings(\n     \"\"\"\n-    UniSpeech-SAT Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n+    UniSpeechSat Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n     \"\"\",\n     UNISPEECH_SAT_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector with Wav2Vec2->UniSpeechSat, wav2vec2->unispeech_sat, WAV_2_VEC_2->UNISPEECH_SAT\n class UniSpeechSatForXVector(UniSpeechSatPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "44e566068ef5e4d8da7438971c36d8482a9fc4bb",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "added",
            "additions": 610,
            "deletions": 0,
            "changes": 610,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -0,0 +1,610 @@\n+import math\n+import warnings\n+from dataclasses import dataclass\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...modeling_outputs import (\n+    CausalLMOutput,\n+    ModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+    Wav2Vec2BaseModelOutput,\n+    XVectorOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..wav2vec2.modeling_wav2vec2 import (\n+    Wav2Vec2Encoder,\n+    Wav2Vec2EncoderStableLayerNorm,\n+    Wav2Vec2FeatureEncoder,\n+    Wav2Vec2FeatureProjection,\n+    Wav2Vec2ForAudioFrameClassification,\n+    Wav2Vec2ForCTC,\n+    Wav2Vec2ForSequenceClassification,\n+    Wav2Vec2ForXVector,\n+    Wav2Vec2GumbelVectorQuantizer,\n+    Wav2Vec2Model,\n+    Wav2Vec2PositionalConvEmbedding,\n+)\n+from .configuration_unispeech_sat import UniSpeechSatConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"UniSpeechSatConfig\"\n+\n+# Base docstring\n+_CHECKPOINT_FOR_DOC = \"microsoft/unispeech-sat-base-100h-libri-ft\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'MISTER QUILDER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n+_CTC_EXPECTED_LOSS = 39.88\n+\n+# Frame class docstring\n+_FRAME_CLASS_CHECKPOINT = \"microsoft/unispeech-sat-base-plus-sd\"\n+_FRAME_EXPECTED_OUTPUT = [0, 0]\n+\n+# Speaker Verification docstring\n+_XVECTOR_CHECKPOINT = \"microsoft/unispeech-sat-base-plus-sv\"\n+_XVECTOR_EXPECTED_OUTPUT = 0.97\n+\n+\n+@dataclass\n+class UniSpeechSatForPreTrainingOutput(ModelOutput):\n+    \"\"\"\n+    Output type of [`UniSpeechSatForPreTrainingOutput`], with potential hidden states and attentions.\n+\n+    Args:\n+        loss (*optional*, returned when model is in train mode, `torch.FloatTensor` of shape `(1,)`):\n+            Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n+            paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n+            Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n+            projected quantized states.\n+        projected_quantized_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n+            Quantized extracted feature vectors projected to *config.proj_codevector_dim* representing the positive\n+            target vectors for contrastive loss.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    projected_states: Optional[torch.FloatTensor] = None\n+    projected_quantized_states: Optional[torch.FloatTensor] = None\n+    codevector_perplexity: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+class UniSpeechSatPositionalConvEmbedding(Wav2Vec2PositionalConvEmbedding):\n+    pass\n+\n+\n+class UniSpeechSatFeatureEncoder(Wav2Vec2FeatureEncoder):\n+    pass\n+\n+\n+class UniSpeechSatFeatureProjection(Wav2Vec2FeatureProjection):\n+    pass\n+\n+\n+class UniSpeechSatEncoder(Wav2Vec2Encoder):\n+    pass\n+\n+\n+class UniSpeechSatEncoderStableLayerNorm(Wav2Vec2EncoderStableLayerNorm):\n+    pass\n+\n+\n+class UniSpeechSatGumbelVectorQuantizer(Wav2Vec2GumbelVectorQuantizer):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.weight_proj = nn.Linear(config.hidden_size, self.num_groups * self.num_vars)\n+\n+    @staticmethod\n+    def _compute_perplexity(probs, mask=None):\n+        marginal_probs = probs.mean(dim=0)\n+        perplexity = torch.exp(-torch.sum(marginal_probs * torch.log(marginal_probs + 1e-7), dim=-1)).sum()\n+        return perplexity\n+\n+    def forward(self, hidden_states):\n+        batch_size, sequence_length, hidden_size = hidden_states.shape\n+\n+        # project to codevector dim\n+        hidden_states = self.weight_proj(hidden_states)\n+        hidden_states = hidden_states.view(batch_size * sequence_length * self.num_groups, -1)\n+\n+        if self.training:\n+            # sample code vector probs via gumbel in differentiateable way\n+            codevector_probs = nn.functional.gumbel_softmax(\n+                hidden_states.float(), tau=self.temperature, hard=True\n+            ).type_as(hidden_states)\n+\n+            # compute perplexity\n+            codevector_soft_dist = torch.softmax(\n+                hidden_states.view(batch_size * sequence_length, self.num_groups, -1).float(), dim=-1\n+            )\n+            perplexity = self._compute_perplexity(codevector_soft_dist)\n+        else:\n+            # take argmax in non-differentiable way\n+            # comptute hard codevector distribution (one hot)\n+            codevector_idx = hidden_states.argmax(dim=-1)\n+            codevector_probs = hidden_states.new_zeros(*hidden_states.shape).scatter_(\n+                -1, codevector_idx.view(-1, 1), 1.0\n+            )\n+            codevector_probs = codevector_probs.view(batch_size * sequence_length, self.num_groups, -1)\n+\n+            perplexity = self._compute_perplexity(codevector_probs)\n+\n+        codevector_probs = codevector_probs.view(batch_size * sequence_length, -1)\n+        # use probs to retrieve codevectors\n+        codevectors_per_group = codevector_probs.unsqueeze(-1) * self.codevectors\n+        codevectors = codevectors_per_group.view(batch_size * sequence_length, self.num_groups, self.num_vars, -1)\n+        codevectors = codevectors.sum(-2).view(batch_size, sequence_length, -1)\n+\n+        return codevectors, perplexity\n+\n+\n+class UniSpeechSatPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = UniSpeechSatConfig\n+    base_model_prefix = \"unispeech_sat\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        # gumbel softmax requires special init\n+        if isinstance(module, UniSpeechSatGumbelVectorQuantizer):\n+            module.weight_proj.weight.data.normal_(mean=0.0, std=1)\n+            module.weight_proj.bias.data.zero_()\n+            nn.init.uniform_(module.codevectors)\n+        elif isinstance(module, UniSpeechSatPositionalConvEmbedding):\n+            nn.init.normal_(\n+                module.conv.weight,\n+                mean=0,\n+                std=2 * math.sqrt(1 / (module.conv.kernel_size[0] * module.conv.in_channels)),\n+            )\n+            nn.init.constant_(module.conv.bias, 0)\n+        elif isinstance(module, UniSpeechSatFeatureProjection):\n+            k = math.sqrt(1 / module.projection.in_features)\n+            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n+            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n+        elif isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            nn.init.kaiming_normal_(module.weight)\n+\n+            if module.bias is not None:\n+                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n+                nn.init.uniform_(module.bias, a=-k, b=k)\n+\n+    def _get_feat_extract_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+\n+        def _conv_out_length(input_length, kernel_size, stride):\n+            # 1D convolutional layer output length formula taken\n+            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n+            return torch.div(input_length - kernel_size, stride, rounding_mode=\"floor\") + 1\n+\n+        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):\n+            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n+\n+        return input_lengths\n+\n+    def _get_feature_vector_attention_mask(self, feature_vector_length: int, attention_mask: torch.LongTensor):\n+        # Effectively attention_mask.sum(-1), but not inplace to be able to run\n+        # on inference mode.\n+        non_padded_lengths = attention_mask.cumsum(dim=-1)[:, -1]\n+        output_lengths = self._get_feat_extract_output_lengths(non_padded_lengths).to(torch.long)\n+        batch_size = attention_mask.shape[0]\n+\n+        attention_mask = torch.zeros(\n+            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n+        )\n+        # these two operations makes sure that all values before the output lengths idxs are attended to\n+        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n+        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n+        return attention_mask\n+\n+\n+UNISPEECH_SAT_START_DOCSTRING = r\"\"\"\n+    UniSpeechSat was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n+    Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael\n+    Auli.\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving etc.).\n+\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n+    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`UniSpeechSatConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+UNISPEECH_SAT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n+            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n+            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n+        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            <Tip warning={true}>\n+\n+            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==\n+            True`. For all models whose processor has `config.return_attention_mask == False`, `attention_mask` should\n+            **not** be passed to avoid degraded performance when doing batched inference. For such models\n+            `input_values` should simply be padded with 0 and passed without `attention_mask`. Be aware that these\n+            models also yield slightly different results depending on whether `input_values` is padded or not.\n+\n+            </Tip>\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+UniSpeechSatBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n+\n+@add_start_docstrings(\n+    \"The bare UniSpeechSat Model transformer outputting raw hidden-states without any specific head on top.\",\n+    UNISPEECH_SAT_START_DOCSTRING,\n+)\n+class UniSpeechSatModel(UniSpeechSatPreTrainedModel, Wav2Vec2Model):\n+    def __init__(self, config: UniSpeechSatConfig):\n+        UniSpeechSatPreTrainedModel.__init__(config)\n+        self.config = config\n+        self.feature_extractor = UniSpeechSatFeatureEncoder(config)\n+        self.feature_projection = UniSpeechSatFeatureProjection(config)\n+\n+        self.masked_spec_embed = nn.Parameter(torch.Tensor(config.hidden_size).uniform_())\n+\n+        if config.do_stable_layer_norm:\n+            self.encoder = UniSpeechSatEncoderStableLayerNorm(config)\n+        else:\n+            self.encoder = UniSpeechSatEncoder(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for UniSpeechSat\")\n+\n+    def freeze_feature_encoder(self):\n+        raise AttributeError(\"Not needed for UniSpeechSat\")\n+\n+    @add_start_docstrings_to_model_forward(UNISPEECH_SAT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=UniSpeechSatBaseModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(\n+        self,\n+        input_values: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        mask_time_indices: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, UniSpeechSatBaseModelOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        extract_features = self.feature_extractor(input_values)\n+        extract_features = extract_features.transpose(1, 2)\n+\n+        if attention_mask is not None:\n+            # compute reduced attention_mask corresponding to feature vectors\n+            attention_mask = self._get_feature_vector_attention_mask(extract_features.shape[1], attention_mask)\n+\n+        hidden_states, extract_features = self.feature_projection(extract_features)\n+        hidden_states = self._mask_hidden_states(\n+            hidden_states, mask_time_indices=mask_time_indices, attention_mask=attention_mask\n+        )\n+\n+        encoder_outputs = self.encoder(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        hidden_states = encoder_outputs[0]\n+\n+        if not return_dict:\n+            return (hidden_states, extract_features) + encoder_outputs[1:]\n+\n+        return UniSpeechSatBaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            extract_features=extract_features,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"UniSpeechSat Model with a vector-quantization module and ctc loss for pre-training.\"\"\",\n+    UNISPEECH_SAT_START_DOCSTRING,\n+)\n+class UniSpeechSatForPreTraining(UniSpeechSatPreTrainedModel):\n+    def __init__(self, config: UniSpeechSatConfig):\n+        super().__init__(config)\n+        self.unispeech_sat = UniSpeechSatModel(config)\n+        self.dropout_features = nn.Dropout(config.feat_quantizer_dropout)\n+\n+        self.quantizer = UniSpeechSatGumbelVectorQuantizer(config)\n+        self.project_q = nn.Linear(config.codevector_dim, config.proj_codevector_dim)\n+        self.project_hid = nn.Linear(config.hidden_size, config.proj_codevector_dim)\n+\n+        self.dropout = nn.Dropout(config.final_dropout)\n+\n+        self.speaker_proj = nn.Linear(config.hidden_size, config.codevector_dim)\n+        self.label_embeddings_concat = nn.Parameter(torch.FloatTensor(config.num_clusters, config.codevector_dim))\n+        self.label_embeddings_concat.data.zero_()\n+\n+        self.layer_norm_for_extract = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        if self.config.do_stable_layer_norm:\n+            self.layer_norm_for_extract.requires_grad = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def set_gumbel_temperature(self, temperature: int):\n+        \"\"\"\n+        Set the Gumbel softmax temperature to a given value. Only necessary for training\n+        \"\"\"\n+        self.quantizer.temperature = temperature\n+\n+    def freeze_feature_extractor(self):\n+        \"\"\"\n+        Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n+        not be updated during training.\n+        \"\"\"\n+        warnings.warn(\n+            \"The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. \"\n+            \"Please use the equivalent `freeze_feature_encoder` method instead.\",\n+            FutureWarning,\n+        )\n+        self.freeze_feature_encoder()\n+\n+    def freeze_feature_encoder(self):\n+        \"\"\"\n+        Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n+        not be updated during training.\n+        \"\"\"\n+        self.unispeech_sat.feature_extractor._freeze_parameters()\n+\n+    @staticmethod\n+    def compute_contrastive_logits(\n+        target_features: torch.FloatTensor,\n+        negative_features: torch.FloatTensor,\n+        predicted_features: torch.FloatTensor,\n+        temperature: int = 1,\n+    ):\n+        \"\"\"\n+        Compute logits for contrastive loss based using cosine similarity as the distance measure between\n+        `[positive_feature, negative_features]` and `[predicted_features]`. Additionally, temperature can be applied.\n+        \"\"\"\n+        target_features = torch.cat([target_features, negative_features], dim=0)\n+\n+        logits = torch.cosine_similarity(predicted_features.float(), target_features.float(), dim=-1)\n+        logits = logits.type_as(target_features)\n+\n+        # apply temperature\n+        logits = logits / temperature\n+        return logits\n+\n+    @add_start_docstrings_to_model_forward(UNISPEECH_SAT_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=UniSpeechSatForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_values: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, UniSpeechSatForPreTrainingOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoFeatureExtractor, UniSpeechSatForPreTraining\n+        >>> from transformers.models.unispeech_sat.modeling_unispeech_sat import _compute_mask_indices\n+\n+        >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/unispeech-sat-base\")\n+        >>> model = UniSpeechSatForPreTraining.from_pretrained(\"microsoft/unispeech-sat-base\")\n+        >>> # TODO: Add full pretraining example\n+        ```\"\"\"\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.unispeech_sat(\n+            input_values,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        transformer_features = outputs[0]\n+\n+        # quantize all (unmasked) extracted features and project to final vq dim\n+        extract_features = self.dropout_features(outputs[1])\n+\n+        # TODO(PVP) - add pretraining logic and add to tests\n+        logits = extract_features\n+        loss = quantized_features = codevector_perplexity = None\n+\n+        if not return_dict:\n+            if loss is not None:\n+                return (loss, logits, transformer_features, quantized_features, codevector_perplexity) + outputs[2:]\n+            return (logits, transformer_features, quantized_features, codevector_perplexity) + outputs[2:]\n+\n+        return UniSpeechSatForPreTrainingOutput(\n+            loss=loss,\n+            logits=logits,\n+            projected_states=transformer_features,\n+            projected_quantized_states=quantized_features,\n+            codevector_perplexity=codevector_perplexity,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"UniSpeechSat Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n+    UNISPEECH_SAT_START_DOCSTRING,\n+    \"\"\"\n+        target_lang (`str`, *optional*):\n+            Language id of adapter weights. Adapter weights are stored in the format adapter.<lang>.safetensors or\n+            adapter.<lang>.bin. Only relevant when using an instance of [`UniSpeechSatForCTC`] with adapters. Uses\n+            'eng' by default.\n+    \"\"\",\n+)\n+class UniSpeechSatForCTC(Wav2Vec2ForCTC):\n+    @add_start_docstrings_to_model_forward(UNISPEECH_SAT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=CausalLMOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_CTC_EXPECTED_OUTPUT,\n+        expected_loss=_CTC_EXPECTED_LOSS,\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    UniSpeechSat Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like\n+    SUPERB Keyword Spotting.\n+    \"\"\",\n+    UNISPEECH_SAT_START_DOCSTRING,\n+)\n+class UniSpeechSatForSequenceClassification(Wav2Vec2ForSequenceClassification):\n+    @add_start_docstrings_to_model_forward(UNISPEECH_SAT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=SequenceClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    UniSpeechSat Model with a frame classification head on top for tasks like Speaker Diarization.\n+    \"\"\",\n+    UNISPEECH_SAT_START_DOCSTRING,\n+)\n+class UniSpeechSatForAudioFrameClassification(Wav2Vec2ForAudioFrameClassification):\n+    @add_start_docstrings_to_model_forward(UNISPEECH_SAT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_FRAME_CLASS_CHECKPOINT,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_FRAME_EXPECTED_OUTPUT,\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    UniSpeechSat Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n+    \"\"\",\n+    UNISPEECH_SAT_START_DOCSTRING,\n+)\n+class UniSpeechSatForXVector(Wav2Vec2ForXVector):\n+    pass\n+\n+    @add_start_docstrings_to_model_forward(UNISPEECH_SAT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_XVECTOR_CHECKPOINT,\n+        output_type=XVectorOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_XVECTOR_EXPECTED_OUTPUT,\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"UniSpeechSatForAudioFrameClassification\",\n+    \"UniSpeechSatForCTC\",\n+    \"UniSpeechSatForPreTraining\",\n+    \"UniSpeechSatForSequenceClassification\",\n+    \"UniSpeechSatForXVector\",\n+    \"UniSpeechSatModel\",\n+    \"UniSpeechSatPreTrainedModel\",\n+]"
        },
        {
            "sha": "547076205018b7a15e2d7785eeea8c9cc9eed109",
            "filename": "src/transformers/models/wav2vec2/modeling_flax_wav2vec2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_flax_wav2vec2.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -212,7 +212,7 @@ def _sample_negative_indices(features_shape: Tuple, num_negatives: int, attentio\n     return sampled_negative_indices\n \n \n-WAV_2_VEC_2_START_DOCSTRING = r\"\"\"\n+WAV2VEC2_START_DOCSTRING = r\"\"\"\n     Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n     Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael\n     Auli.\n@@ -251,7 +251,7 @@ def _sample_negative_indices(features_shape: Tuple, num_negatives: int, attentio\n \"\"\"\n \n \n-WAV_2_VEC_2_INPUTS_DOCSTRING = r\"\"\"\n+WAV2VEC2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_values (`jnp.ndarray` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n@@ -885,7 +885,7 @@ def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple, params: Froz\n         else:\n             return random_params\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     def __call__(\n         self,\n         input_values,\n@@ -1050,7 +1050,7 @@ def _get_feature_vector_attention_mask(\n \n @add_start_docstrings(\n     \"The bare Wav2Vec2 Model transformer outputting raw hidden-states without any specific head on top.\",\n-    WAV_2_VEC_2_START_DOCSTRING,\n+    WAV2VEC2_START_DOCSTRING,\n )\n class FlaxWav2Vec2Model(FlaxWav2Vec2PreTrainedModel):\n     module_class = FlaxWav2Vec2Module\n@@ -1088,7 +1088,7 @@ class FlaxWav2Vec2Model(FlaxWav2Vec2PreTrainedModel):\n \n overwrite_call_docstring(\n     FlaxWav2Vec2Model,\n-    WAV_2_VEC_2_INPUTS_DOCSTRING + FLAX_WAV2VEC2_MODEL_DOCSTRING,\n+    WAV2VEC2_INPUTS_DOCSTRING + FLAX_WAV2VEC2_MODEL_DOCSTRING,\n )\n append_replace_return_docstrings(\n     FlaxWav2Vec2Model, output_type=FlaxWav2Vec2BaseModelOutput, config_class=Wav2Vec2Config\n@@ -1168,7 +1168,7 @@ def _conv_out_length(input_length, kernel_size, stride):\n \n @add_start_docstrings(\n     \"Wav2Vec2 Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\",\n-    WAV_2_VEC_2_START_DOCSTRING,\n+    WAV2VEC2_START_DOCSTRING,\n )\n class FlaxWav2Vec2ForCTC(FlaxWav2Vec2PreTrainedModel):\n     module_class = FlaxWav2Vec2ForCTCModule\n@@ -1211,7 +1211,7 @@ class FlaxWav2Vec2ForCTC(FlaxWav2Vec2PreTrainedModel):\n \n overwrite_call_docstring(\n     FlaxWav2Vec2ForCTC,\n-    WAV_2_VEC_2_INPUTS_DOCSTRING + FLAX_WAV2VEC2_FOR_CTC_DOCSTRING,\n+    WAV2VEC2_INPUTS_DOCSTRING + FLAX_WAV2VEC2_FOR_CTC_DOCSTRING,\n )\n append_replace_return_docstrings(FlaxWav2Vec2ForCTC, output_type=FlaxCausalLMOutput, config_class=Wav2Vec2Config)\n \n@@ -1315,11 +1315,11 @@ def _conv_out_length(input_length, kernel_size, stride):\n         return input_lengths\n \n \n-@add_start_docstrings(\"\"\"Wav2Vec2 Model with a quantizer and `VQ` head on top.\"\"\", WAV_2_VEC_2_START_DOCSTRING)\n+@add_start_docstrings(\"\"\"Wav2Vec2 Model with a quantizer and `VQ` head on top.\"\"\", WAV2VEC2_START_DOCSTRING)\n class FlaxWav2Vec2ForPreTraining(FlaxWav2Vec2PreTrainedModel):\n     module_class = FlaxWav2Vec2ForPreTrainingModule\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     # overwrite since has `gumbel_temperature` input\n     def __call__(\n         self,\n@@ -1418,7 +1418,7 @@ def __call__(\n \n overwrite_call_docstring(\n     FlaxWav2Vec2ForPreTraining,\n-    WAV_2_VEC_2_INPUTS_DOCSTRING + FLAX_WAV2VEC2_FOR_PRETRAINING_DOCSTRING,\n+    WAV2VEC2_INPUTS_DOCSTRING + FLAX_WAV2VEC2_FOR_PRETRAINING_DOCSTRING,\n )\n append_replace_return_docstrings(\n     FlaxWav2Vec2ForPreTraining, output_type=FlaxWav2Vec2ForPreTrainingOutput, config_class=Wav2Vec2Config"
        },
        {
            "sha": "c385c192a987d5bd7425b6cd7c9ac6245c336c11",
            "filename": "src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1397,7 +1397,7 @@ def _get_feature_vector_attention_mask(\n         return attention_mask\n \n \n-WAV_2_VEC_2_START_DOCSTRING = r\"\"\"\n+WAV2VEC2_START_DOCSTRING = r\"\"\"\n \n     This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -1439,7 +1439,7 @@ def _get_feature_vector_attention_mask(\n             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n-WAV_2_VEC_2_INPUTS_DOCSTRING = r\"\"\"\n+WAV2VEC2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_values (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` `Dict[str, tf.Tensor]` or `Dict[str, np.ndarray]` and each example must have the shape `({0})`):\n             Indices of input sequence tokens in the vocabulary.\n@@ -1497,15 +1497,15 @@ def _get_feature_vector_attention_mask(\n \n @add_start_docstrings(\n     \"The bare TFWav2Vec2 Model transformer outputing raw hidden-states without any specific head on top.\",\n-    WAV_2_VEC_2_START_DOCSTRING,\n+    WAV2VEC2_START_DOCSTRING,\n )\n class TFWav2Vec2Model(TFWav2Vec2PreTrainedModel):\n     def __init__(self, config: Wav2Vec2Config, *inputs, **kwargs):\n         super().__init__(config, *inputs, **kwargs)\n         self.config = config\n         self.wav2vec2 = TFWav2Vec2MainLayer(config, name=\"wav2vec2\")\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=TFBaseModelOutput, config_class=_CONFIG_FOR_DOC)\n     @unpack_inputs\n     def call(\n@@ -1579,7 +1579,7 @@ def build(self, input_shape=None):\n \n @add_start_docstrings(\n     \"\"\"TFWav2Vec2 Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n-    WAV_2_VEC_2_START_DOCSTRING,\n+    WAV2VEC2_START_DOCSTRING,\n )\n class TFWav2Vec2ForCTC(TFWav2Vec2PreTrainedModel):\n     def __init__(self, config: Wav2Vec2Config, *inputs, **kwargs):\n@@ -1612,7 +1612,7 @@ def freeze_feature_encoder(self):\n         self.wav2vec2.feature_extractor.trainable = False\n \n     @unpack_inputs\n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=TFCausalLMOutput, config_class=_CONFIG_FOR_DOC)\n     def call(\n         self,"
        },
        {
            "sha": "2ac0e21486e7c49b22c7d3e78a87c046153610f8",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -63,6 +63,7 @@\n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n+\n logger = logging.get_logger(__name__)\n \n \n@@ -1633,7 +1634,7 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n         self.target_lang = target_lang\n \n \n-WAV_2_VEC_2_START_DOCSTRING = r\"\"\"\n+WAV2VEC2_START_DOCSTRING = r\"\"\"\n     Wav2Vec2 was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n     Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael\n     Auli.\n@@ -1652,7 +1653,7 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n \"\"\"\n \n \n-WAV_2_VEC_2_INPUTS_DOCSTRING = r\"\"\"\n+WAV2VEC2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n             Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n@@ -1692,7 +1693,7 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n \n @add_start_docstrings(\n     \"The bare Wav2Vec2 Model transformer outputting raw hidden-states without any specific head on top.\",\n-    WAV_2_VEC_2_START_DOCSTRING,\n+    WAV2VEC2_START_DOCSTRING,\n )\n class Wav2Vec2Model(Wav2Vec2PreTrainedModel):\n     def __init__(self, config: Wav2Vec2Config):\n@@ -1780,7 +1781,7 @@ def _mask_hidden_states(\n \n         return hidden_states\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n         output_type=Wav2Vec2BaseModelOutput,\n@@ -1841,7 +1842,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\"\"\"Wav2Vec2 Model with a quantizer and `VQ` head on top.\"\"\", WAV_2_VEC_2_START_DOCSTRING)\n+@add_start_docstrings(\"\"\"Wav2Vec2 Model with a quantizer and `VQ` head on top.\"\"\", WAV2VEC2_START_DOCSTRING)\n class Wav2Vec2ForPreTraining(Wav2Vec2PreTrainedModel):\n     def __init__(self, config: Wav2Vec2Config):\n         super().__init__(config)\n@@ -1902,7 +1903,7 @@ def compute_contrastive_logits(\n         logits = logits / temperature\n         return logits\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Wav2Vec2ForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n@@ -2065,7 +2066,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\"\"\"Wav2Vec2 Model with a `language modeling` head on top.\"\"\", WAV_2_VEC_2_START_DOCSTRING)\n+@add_start_docstrings(\"\"\"Wav2Vec2 Model with a `language modeling` head on top.\"\"\", WAV2VEC2_START_DOCSTRING)\n class Wav2Vec2ForMaskedLM(Wav2Vec2PreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -2081,7 +2082,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n         input_values: torch.FloatTensor,\n@@ -2113,7 +2114,7 @@ def forward(\n \n @add_start_docstrings(\n     \"\"\"Wav2Vec2 Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n-    WAV_2_VEC_2_START_DOCSTRING,\n+    WAV2VEC2_START_DOCSTRING,\n     \"\"\"\n         target_lang (`str`, *optional*):\n             Language id of adapter weights. Adapter weights are stored in the format adapter.<lang>.safetensors or\n@@ -2193,7 +2194,7 @@ def freeze_base_model(self):\n         for param in self.wav2vec2.parameters():\n             param.requires_grad = False\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n         output_type=CausalLMOutput,\n@@ -2277,7 +2278,7 @@ def forward(\n     Wav2Vec2 Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like\n     SUPERB Keyword Spotting.\n     \"\"\",\n-    WAV_2_VEC_2_START_DOCSTRING,\n+    WAV2VEC2_START_DOCSTRING,\n )\n class Wav2Vec2ForSequenceClassification(Wav2Vec2PreTrainedModel):\n     def __init__(self, config):\n@@ -2324,7 +2325,7 @@ def freeze_base_model(self):\n         for param in self.wav2vec2.parameters():\n             param.requires_grad = False\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_SEQ_CLASS_CHECKPOINT,\n         output_type=SequenceClassifierOutput,\n@@ -2400,7 +2401,7 @@ def forward(\n     \"\"\"\n     Wav2Vec2 Model with a frame classification head on top for tasks like Speaker Diarization.\n     \"\"\",\n-    WAV_2_VEC_2_START_DOCSTRING,\n+    WAV2VEC2_START_DOCSTRING,\n )\n class Wav2Vec2ForAudioFrameClassification(Wav2Vec2PreTrainedModel):\n     def __init__(self, config):\n@@ -2446,7 +2447,7 @@ def freeze_base_model(self):\n         for param in self.wav2vec2.parameters():\n             param.requires_grad = False\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_FRAME_CLASS_CHECKPOINT,\n         output_type=TokenClassifierOutput,\n@@ -2546,6 +2547,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if is_peft_available():\n             from peft.tuners.lora import LoraLayer\n \n+        if is_peft_available():\n             if isinstance(self.kernel, LoraLayer):\n                 warnings.warn(\n                     \"Detected LoRA on TDNNLayer. LoRA weights won't be applied due to optimization. \"\n@@ -2566,7 +2568,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n     Wav2Vec2 Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n     \"\"\",\n-    WAV_2_VEC_2_START_DOCSTRING,\n+    WAV2VEC2_START_DOCSTRING,\n )\n class Wav2Vec2ForXVector(Wav2Vec2PreTrainedModel):\n     def __init__(self, config):\n@@ -2630,7 +2632,7 @@ def _conv_out_length(input_length, kernel_size, stride):\n \n         return input_lengths\n \n-    @add_start_docstrings_to_model_forward(WAV_2_VEC_2_INPUTS_DOCSTRING)\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_XVECTOR_CHECKPOINT,\n         output_type=XVectorOutput,"
        },
        {
            "sha": "86e9b65b3d5eb6aa98256700426df9d44221de16",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 176,
            "deletions": 240,
            "changes": 416,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1,26 +1,15 @@\n-# coding=utf-8\n-# Copyright 2024 The Seamless Authors and the HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Wav2Vec2-BERT model.\"\"\"\n-\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_wav2vec2_bert.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n import warnings\n from typing import Optional, Tuple, Union\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n@@ -42,213 +31,14 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_peft_available,\n-    logging,\n )\n from .configuration_wav2vec2_bert import Wav2Vec2BertConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n-_HIDDEN_STATES_START_POSITION = 2\n-\n # General docstring\n _CONFIG_FOR_DOC = \"Wav2Vec2BertConfig\"\n \n-# Base docstring\n-_BASE_CHECKPOINT_FOR_DOC = \"facebook/w2v-bert-2.0\"\n-_PRETRAINED_CHECKPOINT_FOR_DOC = \"hf-audio/wav2vec2-bert-CV16-en\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 146, 1024]\n-\n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = \"'mr quilter is the apostle of the middle classes and we are glad to welcome his gospel'\"\n-_CTC_EXPECTED_LOSS = 17.04\n-\n-\n-# Copied from transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2._compute_new_attention_mask\n-def _compute_new_attention_mask(hidden_states: torch.Tensor, seq_lens: torch.Tensor):\n-    \"\"\"\n-    Computes an attention mask of the form `(batch, seq_len)` with an attention for each element in the batch that\n-    stops at the corresponding element in `seq_lens`.\n-    Args:\n-        hidden_states (`torch.FloatTensor` of shape `(batch, seq_len, *)`):\n-            The sequences to mask, where `*` is any number of sequence-specific dimensions including none.\n-        seq_lens (`torch.Tensor` of shape `(batch)`:\n-            Each element represents the length of the sequence at the same index in `hidden_states`\n-    Returns:\n-        `torch.FloatTensor`: The float attention mask of shape `(batch, seq_len)`\n-    \"\"\"\n-    batch_size, mask_seq_len = hidden_states.shape[:2]\n-\n-    indices = torch.arange(mask_seq_len, device=seq_lens.device).expand(batch_size, -1)\n-\n-    bool_mask = indices >= seq_lens.unsqueeze(1).expand(-1, mask_seq_len)\n-\n-    mask = hidden_states.new_ones((batch_size, mask_seq_len))\n-\n-    mask = mask.masked_fill(bool_mask, 0)\n-\n-    return mask\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices\n-def _compute_mask_indices(\n-    shape: Tuple[int, int],\n-    mask_prob: float,\n-    mask_length: int,\n-    attention_mask: Optional[torch.LongTensor] = None,\n-    min_masks: int = 0,\n-) -> np.ndarray:\n-    \"\"\"\n-    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n-    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n-    CPU as part of the preprocessing during training.\n-\n-    Args:\n-        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n-               the first element is the batch size and the second element is the length of the axis to span.\n-        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n-                    independently generated mask spans of length `mask_length` is computed by\n-                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n-                    actual percentage will be smaller.\n-        mask_length: size of the mask\n-        min_masks: minimum number of masked spans\n-        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n-                        each batch dimension.\n-    \"\"\"\n-    batch_size, sequence_length = shape\n-\n-    if mask_length < 1:\n-        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n-\n-    if mask_length > sequence_length:\n-        raise ValueError(\n-            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n-            f\" and `sequence_length`: {sequence_length}`\"\n-        )\n-\n-    # epsilon is used for probabilistic rounding\n-    epsilon = np.random.rand(1).item()\n-\n-    def compute_num_masked_span(input_length):\n-        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n-        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n-        num_masked_span = max(num_masked_span, min_masks)\n-\n-        # make sure num masked span <= sequence_length\n-        if num_masked_span * mask_length > sequence_length:\n-            num_masked_span = sequence_length // mask_length\n-\n-        # make sure num_masked span is also <= input_length - (mask_length - 1)\n-        if input_length - (mask_length - 1) < num_masked_span:\n-            num_masked_span = max(input_length - (mask_length - 1), 0)\n-\n-        return num_masked_span\n-\n-    # compute number of masked spans in batch\n-    input_lengths = (\n-        attention_mask.detach().sum(-1).tolist()\n-        if attention_mask is not None\n-        else [sequence_length for _ in range(batch_size)]\n-    )\n-\n-    # SpecAugment mask to fill\n-    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n-    spec_aug_mask_idxs = []\n-\n-    max_num_masked_span = compute_num_masked_span(sequence_length)\n-\n-    if max_num_masked_span == 0:\n-        return spec_aug_mask\n-\n-    for input_length in input_lengths:\n-        # compute num of masked spans for this input\n-        num_masked_span = compute_num_masked_span(input_length)\n-\n-        # get random indices to mask\n-        spec_aug_mask_idx = np.random.choice(\n-            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n-        )\n-\n-        # pick first sampled index that will serve as a dummy index to pad vector\n-        # to ensure same dimension for all batches due to probabilistic rounding\n-        # Picking first sample just pads those vectors twice.\n-        if len(spec_aug_mask_idx) == 0:\n-            # this case can only happen if `input_length` is strictly smaller then\n-            # `sequence_length` in which case the last token has to be a padding\n-            # token which we can use as a dummy mask id\n-            dummy_mask_idx = sequence_length - 1\n-        else:\n-            dummy_mask_idx = spec_aug_mask_idx[0]\n-\n-        spec_aug_mask_idx = np.concatenate(\n-            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n-        )\n-        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n-\n-    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n-\n-    # expand masked indices to masked spans\n-    spec_aug_mask_idxs = np.broadcast_to(\n-        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n-\n-    # add offset to the starting indexes so that indexes now create a span\n-    offsets = np.arange(mask_length)[None, None, :]\n-    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n-        batch_size, max_num_masked_span * mask_length\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n-\n-    # ensure that we cannot have indices larger than sequence_length\n-    if spec_aug_mask_idxs.max() > sequence_length - 1:\n-        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n-\n-    # scatter indices to mask\n-    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n-\n-    return spec_aug_mask\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._sample_negative_indices\n-def _sample_negative_indices(\n-    features_shape: Tuple, num_negatives: int, mask_time_indices: Optional[np.ndarray] = None\n-):\n-    \"\"\"\n-    Sample `num_negatives` vectors from feature vectors.\n-    \"\"\"\n-    batch_size, sequence_length = features_shape\n-\n-    # generate indices of the positive vectors themselves, repeat them `num_negatives` times\n-    sequence_length_range = np.arange(sequence_length)\n-\n-    # get `num_negatives` random vector indices from the same utterance\n-    sampled_negative_indices = np.zeros(shape=(batch_size, sequence_length, num_negatives), dtype=np.int32)\n-\n-    mask_time_indices = (\n-        mask_time_indices.astype(bool) if mask_time_indices is not None else np.ones(features_shape, dtype=bool)\n-    )\n-\n-    for batch_idx in range(batch_size):\n-        high = mask_time_indices[batch_idx].sum() - 1\n-        mapped_masked_indices = sequence_length_range[mask_time_indices[batch_idx]]\n-\n-        feature_indices = np.broadcast_to(np.arange(high + 1)[:, None], (high + 1, num_negatives))\n-        sampled_indices = np.random.randint(0, high, size=(high + 1, num_negatives))\n-        # avoid sampling the same positive vector, but keep the distribution uniform\n-        sampled_indices[sampled_indices >= feature_indices] += 1\n \n-        # remap to actual indices\n-        sampled_negative_indices[batch_idx][mask_time_indices[batch_idx]] = mapped_masked_indices[sampled_indices]\n-\n-        # correct for batch size\n-        sampled_negative_indices[batch_idx] += batch_idx * sequence_length\n-\n-    return sampled_negative_indices\n-\n-\n-# Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerRotaryPositionalEmbedding with Wav2Vec2Conformer->Wav2Vec2Bert\n class Wav2Vec2BertRotaryPositionalEmbedding(nn.Module):\n     \"\"\"Rotary positional embedding\n     Reference : https://blog.eleuther.ai/rotary-embeddings/ Paper: https://arxiv.org/pdf/2104.09864.pdf\n@@ -284,7 +74,6 @@ def forward(self, hidden_states):\n         return self.cached_rotary_positional_embedding\n \n \n-# Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerRelPositionalEmbedding with Wav2Vec2Conformer->Wav2Vec2Bert\n class Wav2Vec2BertRelPositionalEmbedding(nn.Module):\n     \"\"\"Relative positional encoding module.\"\"\"\n \n@@ -363,7 +152,6 @@ def __init__(self, config, act_fn=None, hidden_size=None):\n         self.output_dense = nn.Linear(config.intermediate_size, hidden_size)\n         self.output_dropout = nn.Dropout(config.hidden_dropout)\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward.forward\n     def forward(self, hidden_states):\n         hidden_states = self.intermediate_dense(hidden_states)\n         hidden_states = self.intermediate_act_fn(hidden_states)\n@@ -556,7 +344,6 @@ def forward(\n \n         return hidden_states, probs\n \n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_rotary_embedding\n     def _apply_rotary_embedding(self, hidden_states, relative_position_embeddings):\n         batch_size, sequence_length, hidden_size = hidden_states.size()\n         hidden_states = hidden_states.view(batch_size, sequence_length, self.num_heads, self.head_size)\n@@ -576,7 +363,6 @@ def _apply_rotary_embedding(self, hidden_states, relative_position_embeddings):\n \n         return hidden_states\n \n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_relative_embeddings\n     def _apply_relative_embeddings(self, query, key, relative_position_embeddings):\n         # 1. project positional embeddings\n         # => (batch, head, 2*time1-1, d_k)\n@@ -823,6 +609,32 @@ def forward(self, hidden_states, attention_mask=None):\n         return hidden_states\n \n \n+# Copied from transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2._compute_new_attention_mask\n+def _compute_new_attention_mask(hidden_states: torch.Tensor, seq_lens: torch.Tensor):\n+    \"\"\"\n+    Computes an attention mask of the form `(batch, seq_len)` with an attention for each element in the batch that\n+    stops at the corresponding element in `seq_lens`.\n+    Args:\n+        hidden_states (`torch.FloatTensor` of shape `(batch, seq_len, *)`):\n+            The sequences to mask, where `*` is any number of sequence-specific dimensions including none.\n+        seq_lens (`torch.Tensor` of shape `(batch)`:\n+            Each element represents the length of the sequence at the same index in `hidden_states`\n+    Returns:\n+        `torch.FloatTensor`: The float attention mask of shape `(batch, seq_len)`\n+    \"\"\"\n+    batch_size, mask_seq_len = hidden_states.shape[:2]\n+\n+    indices = torch.arange(mask_seq_len, device=seq_lens.device).expand(batch_size, -1)\n+\n+    bool_mask = indices >= seq_lens.unsqueeze(1).expand(-1, mask_seq_len)\n+\n+    mask = hidden_states.new_ones((batch_size, mask_seq_len))\n+\n+    mask = mask.masked_fill(bool_mask, 0)\n+\n+    return mask\n+\n+\n class Wav2Vec2BertAdapterLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -911,7 +723,6 @@ def forward(\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerPreTrainedModel with Wav2Vec2Conformer->Wav2Vec2Bert,wav2vec2_conformer->wav2vec2_bert, input_values->input_features\n class Wav2Vec2BertPreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n@@ -995,6 +806,129 @@ def _get_feature_vector_attention_mask(\n         return attention_mask\n \n \n+def _compute_mask_indices(\n+    shape: Tuple[int, int],\n+    mask_prob: float,\n+    mask_length: int,\n+    attention_mask: Optional[torch.LongTensor] = None,\n+    min_masks: int = 0,\n+) -> np.ndarray:\n+    \"\"\"\n+    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n+    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n+    CPU as part of the preprocessing during training.\n+\n+    Args:\n+        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n+               the first element is the batch size and the second element is the length of the axis to span.\n+        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n+                    independently generated mask spans of length `mask_length` is computed by\n+                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n+                    actual percentage will be smaller.\n+        mask_length: size of the mask\n+        min_masks: minimum number of masked spans\n+        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n+                        each batch dimension.\n+    \"\"\"\n+    batch_size, sequence_length = shape\n+\n+    if mask_length < 1:\n+        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n+\n+    if mask_length > sequence_length:\n+        raise ValueError(\n+            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n+            f\" and `sequence_length`: {sequence_length}`\"\n+        )\n+\n+    # epsilon is used for probabilistic rounding\n+    epsilon = np.random.rand(1).item()\n+\n+    def compute_num_masked_span(input_length):\n+        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n+        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n+        num_masked_span = max(num_masked_span, min_masks)\n+\n+        # make sure num masked span <= sequence_length\n+        if num_masked_span * mask_length > sequence_length:\n+            num_masked_span = sequence_length // mask_length\n+\n+        # make sure num_masked span is also <= input_length - (mask_length - 1)\n+        if input_length - (mask_length - 1) < num_masked_span:\n+            num_masked_span = max(input_length - (mask_length - 1), 0)\n+\n+        return num_masked_span\n+\n+    # compute number of masked spans in batch\n+    input_lengths = (\n+        attention_mask.detach().sum(-1).tolist()\n+        if attention_mask is not None\n+        else [sequence_length for _ in range(batch_size)]\n+    )\n+\n+    # SpecAugment mask to fill\n+    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n+    spec_aug_mask_idxs = []\n+\n+    max_num_masked_span = compute_num_masked_span(sequence_length)\n+\n+    if max_num_masked_span == 0:\n+        return spec_aug_mask\n+\n+    for input_length in input_lengths:\n+        # compute num of masked spans for this input\n+        num_masked_span = compute_num_masked_span(input_length)\n+\n+        # get random indices to mask\n+        spec_aug_mask_idx = np.random.choice(\n+            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+        )\n+\n+        # pick first sampled index that will serve as a dummy index to pad vector\n+        # to ensure same dimension for all batches due to probabilistic rounding\n+        # Picking first sample just pads those vectors twice.\n+        if len(spec_aug_mask_idx) == 0:\n+            # this case can only happen if `input_length` is strictly smaller then\n+            # `sequence_length` in which case the last token has to be a padding\n+            # token which we can use as a dummy mask id\n+            dummy_mask_idx = sequence_length - 1\n+        else:\n+            dummy_mask_idx = spec_aug_mask_idx[0]\n+\n+        spec_aug_mask_idx = np.concatenate(\n+            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n+        )\n+        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n+\n+    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n+\n+    # expand masked indices to masked spans\n+    spec_aug_mask_idxs = np.broadcast_to(\n+        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+\n+    # add offset to the starting indexes so that indexes now create a span\n+    offsets = np.arange(mask_length)[None, None, :]\n+    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n+        batch_size, max_num_masked_span * mask_length\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+\n+    # ensure that we cannot have indices larger than sequence_length\n+    if spec_aug_mask_idxs.max() > sequence_length - 1:\n+        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+\n+    # scatter indices to mask\n+    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+\n+    return spec_aug_mask\n+\n+\n+_PRETRAINED_CHECKPOINT_FOR_DOC = \"hf-audio/wav2vec2-bert-CV16-en\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 146, 1024]\n+\n+\n WAV2VEC2_BERT_START_DOCSTRING = r\"\"\"\n     Wav2Vec2Bert was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n     Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael\n@@ -1003,16 +937,16 @@ def _get_feature_vector_attention_mask(\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving etc.).\n \n-    This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module) sub-class. Use it as a\n-    regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n+    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n \n     Parameters:\n         config ([`Wav2Vec2BertConfig`]): Model configuration class with all the parameters of the model.\n             Initializing with a config file does not load the weights associated with the model, only the\n             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n-\n WAV2VEC2_BERT_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1039,6 +973,9 @@ def _get_feature_vector_attention_mask(\n \"\"\"\n \n \n+Wav2Vec2BertBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n+\n @add_start_docstrings(\n     \"The bare Wav2Vec2Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n     WAV2VEC2_BERT_START_DOCSTRING,\n@@ -1064,7 +1001,6 @@ def __init__(self, config: Wav2Vec2BertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states\n     def _mask_hidden_states(\n         self,\n         hidden_states: torch.FloatTensor,\n@@ -1114,7 +1050,7 @@ def _mask_hidden_states(\n     @add_start_docstrings_to_model_forward(WAV2VEC2_BERT_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_PRETRAINED_CHECKPOINT_FOR_DOC,\n-        output_type=Wav2Vec2BaseModelOutput,\n+        output_type=Wav2Vec2BertBaseModelOutput,\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n         expected_output=_EXPECTED_OUTPUT_SHAPE,\n@@ -1127,7 +1063,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Wav2Vec2BaseModelOutput]:\n+    ) -> Union[Tuple, Wav2Vec2BertBaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1159,20 +1095,26 @@ def forward(\n         if not return_dict:\n             return (hidden_states, extract_features) + encoder_outputs[1:]\n \n-        return Wav2Vec2BaseModelOutput(\n+        return Wav2Vec2BertBaseModelOutput(\n             last_hidden_state=hidden_states,\n             extract_features=extract_features,\n             hidden_states=encoder_outputs.hidden_states,\n             attentions=encoder_outputs.attentions,\n         )\n \n \n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'mr quilter is the apostle of the middle classes and we are glad to welcome his gospel'\"\n+_CTC_EXPECTED_LOSS = 17.04\n+\n+\n @add_start_docstrings(\n     \"\"\"Wav2Vec2Bert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n     WAV2VEC2_BERT_START_DOCSTRING,\n )\n class Wav2Vec2BertForCTC(Wav2Vec2BertPreTrainedModel):\n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForCTC.__init__ with Wav2Vec2Conformer->Wav2Vec2Bert,WAV2VEC2_CONFORMER->WAV2VEC2_BERT,wav2vec2_conformer->wav2vec2_bert\n     def __init__(self, config, target_lang: Optional[str] = None):\n         super().__init__(config)\n \n@@ -1277,6 +1219,10 @@ def forward(\n         )\n \n \n+# Base docstring\n+_BASE_CHECKPOINT_FOR_DOC = \"facebook/w2v-bert-2.0\"\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     Wav2Vec2Bert Model with a sequence classification head on top (a linear layer over the pooled output) for\n@@ -1285,7 +1231,6 @@ def forward(\n     WAV2VEC2_BERT_START_DOCSTRING,\n )\n class Wav2Vec2BertForSequenceClassification(Wav2Vec2BertPreTrainedModel):\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.__init__ with Wav2Vec2->Wav2Vec2Bert,wav2vec2->wav2vec2_bert\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1318,7 +1263,6 @@ def freeze_base_model(self):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.forward with Wav2Vec2->Wav2Vec2Bert,wav2vec2->wav2vec2_bert,WAV_2_VEC_2->WAV2VEC2_BERT, input_values->input_features\n     def forward(\n         self,\n         input_features: Optional[torch.Tensor],\n@@ -1389,7 +1333,6 @@ def forward(\n     WAV2VEC2_BERT_START_DOCSTRING,\n )\n class Wav2Vec2BertForAudioFrameClassification(Wav2Vec2BertPreTrainedModel):\n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForAudioFrameClassification.__init__ with Wav2Vec2Conformer->Wav2Vec2Bert,WAV2VEC2_CONFORMER->WAV2VEC2_BERT,wav2vec2_conformer->wav2vec2_bert\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1406,7 +1349,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForAudioFrameClassification.freeze_base_model with wav2vec2_conformer->wav2vec2_bert\n     def freeze_base_model(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the base model so that its parameters will not\n@@ -1422,7 +1364,6 @@ def freeze_base_model(self):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForAudioFrameClassification.forward with wav2vec2_conformer->wav2vec2_bert, input_values->input_features\n     def forward(\n         self,\n         input_features: Optional[torch.Tensor],\n@@ -1477,7 +1418,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.AMSoftmaxLoss\n class AMSoftmaxLoss(nn.Module):\n     def __init__(self, input_dim, num_labels, scale=30.0, margin=0.4):\n         super(AMSoftmaxLoss, self).__init__()\n@@ -1501,7 +1441,6 @@ def forward(self, hidden_states, labels):\n         return loss\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.TDNNLayer\n class TDNNLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -1517,6 +1456,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if is_peft_available():\n             from peft.tuners.lora import LoraLayer\n \n+        if is_peft_available():\n             if isinstance(self.kernel, LoraLayer):\n                 warnings.warn(\n                     \"Detected LoRA on TDNNLayer. LoRA weights won't be applied due to optimization. \"\n@@ -1540,7 +1480,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n     WAV2VEC2_BERT_START_DOCSTRING,\n )\n class Wav2Vec2BertForXVector(Wav2Vec2BertPreTrainedModel):\n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForXVector.__init__ with Wav2Vec2Conformer->Wav2Vec2Bert,WAV2VEC2_CONFORMER->WAV2VEC2_BERT,wav2vec2_conformer->wav2vec2_bert\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1560,7 +1499,6 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForXVector.freeze_base_model with wav2vec2_conformer->wav2vec2_bert\n     def freeze_base_model(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the base model so that its parameters will not\n@@ -1569,7 +1507,6 @@ def freeze_base_model(self):\n         for param in self.wav2vec2_bert.parameters():\n             param.requires_grad = False\n \n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForXVector._get_tdnn_output_lengths\n     def _get_tdnn_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n         \"\"\"\n         Computes the output length of the TDNN layers\n@@ -1592,7 +1529,6 @@ def _conv_out_length(input_length, kernel_size, stride):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForXVector.forward with wav2vec2_conformer->wav2vec2_bert, input_values->input_features\n     def forward(\n         self,\n         input_features: Optional[torch.Tensor],"
        },
        {
            "sha": "b8dc95c67540bbed4f5c4d312cc1b3017ff2b334",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "added",
            "additions": 1169,
            "deletions": 0,
            "changes": 1169,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -0,0 +1,1169 @@\n+import math\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+from torch.nn import CrossEntropyLoss\n+\n+from ...activations import ACT2FN\n+from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    CausalLMOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+    Wav2Vec2BaseModelOutput,\n+    XVectorOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ..wav2vec2.modeling_wav2vec2 import Wav2Vec2FeedForward, Wav2Vec2ForSequenceClassification, Wav2Vec2Model\n+from ..wav2vec2_conformer.modeling_wav2vec2_conformer import (\n+    Wav2Vec2ConformerForAudioFrameClassification,\n+    Wav2Vec2ConformerForCTC,\n+    Wav2Vec2ConformerForXVector,\n+    Wav2Vec2ConformerRelPositionalEmbedding,\n+    Wav2Vec2ConformerRotaryPositionalEmbedding,\n+    Wav2Vec2ConformerSelfAttention,\n+)\n+from .configuration_wav2vec2_bert import Wav2Vec2BertConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"Wav2Vec2BertConfig\"\n+\n+# Base docstring\n+_BASE_CHECKPOINT_FOR_DOC = \"facebook/w2v-bert-2.0\"\n+_PRETRAINED_CHECKPOINT_FOR_DOC = \"hf-audio/wav2vec2-bert-CV16-en\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 146, 1024]\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'mr quilter is the apostle of the middle classes and we are glad to welcome his gospel'\"\n+_CTC_EXPECTED_LOSS = 17.04\n+\n+\n+# Copied from transformers.models.seamless_m4t_v2.modeling_seamless_m4t_v2._compute_new_attention_mask\n+def _compute_new_attention_mask(hidden_states: torch.Tensor, seq_lens: torch.Tensor):\n+    \"\"\"\n+    Computes an attention mask of the form `(batch, seq_len)` with an attention for each element in the batch that\n+    stops at the corresponding element in `seq_lens`.\n+    Args:\n+        hidden_states (`torch.FloatTensor` of shape `(batch, seq_len, *)`):\n+            The sequences to mask, where `*` is any number of sequence-specific dimensions including none.\n+        seq_lens (`torch.Tensor` of shape `(batch)`:\n+            Each element represents the length of the sequence at the same index in `hidden_states`\n+    Returns:\n+        `torch.FloatTensor`: The float attention mask of shape `(batch, seq_len)`\n+    \"\"\"\n+    batch_size, mask_seq_len = hidden_states.shape[:2]\n+\n+    indices = torch.arange(mask_seq_len, device=seq_lens.device).expand(batch_size, -1)\n+\n+    bool_mask = indices >= seq_lens.unsqueeze(1).expand(-1, mask_seq_len)\n+\n+    mask = hidden_states.new_ones((batch_size, mask_seq_len))\n+\n+    mask = mask.masked_fill(bool_mask, 0)\n+\n+    return mask\n+\n+\n+class Wav2Vec2BertRotaryPositionalEmbedding(Wav2Vec2ConformerRotaryPositionalEmbedding, nn.Module):\n+    def __init__(self, config):\n+        nn.Module.__init__()\n+        dim = config.hidden_size // config.num_attention_heads\n+        base = config.rotary_embedding_base\n+\n+        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n+        # Ignore copy\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.cached_sequence_length = None\n+        self.cached_rotary_positional_embedding = None\n+\n+\n+class Wav2Vec2BertRelPositionalEmbedding(Wav2Vec2ConformerRelPositionalEmbedding):\n+    pass\n+\n+\n+class Wav2Vec2BertFeatureProjection(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.layer_norm = nn.LayerNorm(config.feature_projection_input_dim, eps=config.layer_norm_eps)\n+        self.projection = nn.Linear(config.feature_projection_input_dim, config.hidden_size)\n+        self.dropout = nn.Dropout(config.feat_proj_dropout)\n+\n+    def forward(self, hidden_states):\n+        # non-projected hidden states are needed for quantization\n+        norm_hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = self.projection(norm_hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        return hidden_states, norm_hidden_states\n+\n+\n+class Wav2Vec2BertFeedForward(Wav2Vec2FeedForward, nn.Module):\n+    def __init__(self, config, act_fn=None, hidden_size=None):\n+        nn.Module.__init__()\n+        act_fn = act_fn if act_fn is not None else config.hidden_act\n+        hidden_size = hidden_size if hidden_size is not None else config.hidden_size\n+        self.intermediate_dropout = nn.Dropout(config.activation_dropout)\n+\n+        self.intermediate_dense = nn.Linear(hidden_size, config.intermediate_size)\n+        self.intermediate_act_fn = ACT2FN[act_fn] if isinstance(act_fn, str) else act_fn\n+\n+        self.output_dense = nn.Linear(config.intermediate_size, hidden_size)\n+        self.output_dropout = nn.Dropout(config.hidden_dropout)\n+\n+\n+class Wav2Vec2BertConvolutionModule(nn.Module):\n+    \"\"\"Convolution block used in the conformer block\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        if (config.conv_depthwise_kernel_size - 1) % 2 == 1:\n+            raise ValueError(\"`config.conv_depthwise_kernel_size` should be a odd number for 'SAME' padding\")\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.pointwise_conv1 = nn.Conv1d(\n+            config.hidden_size,\n+            2 * config.hidden_size,\n+            kernel_size=1,\n+            stride=1,\n+            padding=0,\n+            bias=False,\n+        )\n+        self.glu = nn.GLU(dim=1)\n+        self.depthwise_conv = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            config.conv_depthwise_kernel_size,\n+            stride=1,\n+            padding=0,\n+            groups=config.hidden_size,\n+            bias=False,\n+        )\n+\n+        self.depthwise_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.pointwise_conv2 = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=1,\n+            stride=1,\n+            padding=0,\n+            bias=False,\n+        )\n+        self.dropout = nn.Dropout(config.conformer_conv_dropout)\n+\n+    def forward(self, hidden_states, attention_mask=None):\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        # Ensure that we do not leak padded positions in depthwise convolution if attention mask is passed.\n+        # Put 0 where necessary\n+        if attention_mask is not None:\n+            hidden_states = hidden_states.masked_fill(~attention_mask.bool().unsqueeze(-1), 0.0)\n+\n+        # exchange the temporal dimension and the feature dimension\n+        hidden_states = hidden_states.transpose(1, 2)\n+\n+        # GLU mechanism\n+        # => (batch, 2*channel, dim)\n+        hidden_states = self.pointwise_conv1(hidden_states)\n+        # => (batch, channel, dim)\n+        hidden_states = self.glu(hidden_states)\n+\n+        # Pad the sequence entirely on the left because of causal convolution.\n+        hidden_states = torch.nn.functional.pad(hidden_states, (self.depthwise_conv.kernel_size[0] - 1, 0))\n+\n+        # 1D Depthwise Conv\n+        hidden_states = self.depthwise_conv(hidden_states)\n+\n+        hidden_states = self.depthwise_layer_norm(hidden_states.transpose(1, 2)).transpose(1, 2)\n+\n+        hidden_states = self.activation(hidden_states)\n+\n+        hidden_states = self.pointwise_conv2(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return hidden_states\n+\n+\n+class Wav2Vec2BertSelfAttention(Wav2Vec2ConformerSelfAttention, nn.Module):\n+    \"\"\"Construct an Wav2Vec2BertSelfAttention object.\n+    Can be enhanced with rotary or relative position embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config, is_adapter_attention=False):\n+        nn.Module.__init__()\n+        hidden_size = config.hidden_size if not is_adapter_attention else config.output_hidden_size\n+\n+        self.head_size = hidden_size // config.num_attention_heads\n+        self.num_heads = config.num_attention_heads\n+        self.position_embeddings_type = config.position_embeddings_type if not is_adapter_attention else None\n+\n+        self.linear_q = nn.Linear(hidden_size, hidden_size)\n+        self.linear_k = nn.Linear(hidden_size, hidden_size)\n+        self.linear_v = nn.Linear(hidden_size, hidden_size)\n+        self.linear_out = nn.Linear(hidden_size, hidden_size)\n+\n+        self.dropout = nn.Dropout(p=config.attention_dropout)\n+\n+        if self.position_embeddings_type == \"relative\":\n+            # linear transformation for positional encoding\n+            self.linear_pos = nn.Linear(hidden_size, hidden_size, bias=False)\n+            # these two learnable bias are used in matrix c and matrix d\n+            # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n+            self.pos_bias_u = nn.Parameter(torch.zeros(self.num_heads, self.head_size))\n+            self.pos_bias_v = nn.Parameter(torch.zeros(self.num_heads, self.head_size))\n+\n+        if self.position_embeddings_type == \"relative_key\":\n+            self.left_max_position_embeddings = config.left_max_position_embeddings\n+            self.right_max_position_embeddings = config.right_max_position_embeddings\n+            num_positions = self.left_max_position_embeddings + self.right_max_position_embeddings + 1\n+            self.distance_embedding = nn.Embedding(num_positions, self.head_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        relative_position_embeddings: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        # self-attention mechanism\n+        batch_size, sequence_length, hidden_size = hidden_states.size()\n+\n+        # make sure query/key states can be != value states\n+        query_key_states = hidden_states\n+        value_states = hidden_states\n+\n+        if self.position_embeddings_type == \"rotary\":\n+            if relative_position_embeddings is None:\n+                raise ValueError(\n+                    \"`relative_position_embeddings` has to be defined when `self.position_embeddings_type == 'rotary'\"\n+                )\n+            query_key_states = self._apply_rotary_embedding(query_key_states, relative_position_embeddings)\n+\n+        # project query_key_states and value_states\n+        query = self.linear_q(query_key_states).view(batch_size, -1, self.num_heads, self.head_size)\n+        key = self.linear_k(query_key_states).view(batch_size, -1, self.num_heads, self.head_size)\n+        value = self.linear_v(value_states).view(batch_size, -1, self.num_heads, self.head_size)\n+\n+        # => (batch, head, time1, d_k)\n+        query = query.transpose(1, 2)\n+        key = key.transpose(1, 2)\n+        value = value.transpose(1, 2)\n+\n+        if self.position_embeddings_type == \"relative\":\n+            if relative_position_embeddings is None:\n+                raise ValueError(\n+                    \"`relative_position_embeddings` has to be defined when `self.position_embeddings_type ==\"\n+                    \" 'relative'\"\n+                )\n+            # apply relative_position_embeddings to qk scores\n+            # as proposed in Transformer_XL: https://arxiv.org/abs/1901.02860\n+            scores = self._apply_relative_embeddings(\n+                query=query, key=key, relative_position_embeddings=relative_position_embeddings\n+            )\n+        else:\n+            scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_size)\n+\n+        if self.position_embeddings_type == \"relative_key\":\n+            query_length, key_length = query.shape[2], key.shape[2]\n+\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n+            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n+            distance = position_ids_r - position_ids_l\n+            distance = torch.clamp(distance, -self.left_max_position_embeddings, self.right_max_position_embeddings)\n+\n+            positional_embedding = self.distance_embedding(distance + self.left_max_position_embeddings)\n+            positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+            relative_position_attn_weights = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            scores = scores + (relative_position_attn_weights / math.sqrt(self.head_size))\n+\n+        # apply attention_mask if necessary\n+        if attention_mask is not None:\n+            scores = scores + attention_mask\n+\n+        # => (batch, head, time1, time2)\n+        probs = torch.softmax(scores, dim=-1)\n+        probs = self.dropout(probs)\n+\n+        # => (batch, head, time1, d_k)\n+        hidden_states = torch.matmul(probs, value)\n+\n+        # => (batch, time1, hidden_size)\n+        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.head_size)\n+        hidden_states = self.linear_out(hidden_states)\n+\n+        return hidden_states, probs\n+\n+\n+class Wav2Vec2BertEncoderLayer(nn.Module):\n+    \"\"\"Conformer block based on https://arxiv.org/abs/2005.08100.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        embed_dim = config.hidden_size\n+        dropout = config.attention_dropout\n+\n+        # Feed-forward 1\n+        self.ffn1_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.ffn1 = Wav2Vec2BertFeedForward(config)\n+\n+        # Self-Attention\n+        self.self_attn_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.self_attn_dropout = nn.Dropout(dropout)\n+        self.self_attn = Wav2Vec2BertSelfAttention(config)\n+\n+        # Conformer Convolution\n+        self.conv_module = Wav2Vec2BertConvolutionModule(config)\n+\n+        # Feed-forward 2\n+        self.ffn2_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.ffn2 = Wav2Vec2BertFeedForward(config)\n+        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        relative_position_embeddings: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        conv_attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        hidden_states = hidden_states\n+\n+        # 1. Feed-Forward 1 layer\n+        residual = hidden_states\n+        hidden_states = self.ffn1_layer_norm(hidden_states)\n+        hidden_states = self.ffn1(hidden_states)\n+        hidden_states = hidden_states * 0.5 + residual\n+        residual = hidden_states\n+\n+        # 2. Self-Attention layer\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+        hidden_states, attn_weigts = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            relative_position_embeddings=relative_position_embeddings,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = self.self_attn_dropout(hidden_states)\n+        hidden_states = hidden_states + residual\n+\n+        # 3. Convolutional Layer\n+        residual = hidden_states\n+        hidden_states = self.conv_module(hidden_states, attention_mask=conv_attention_mask)\n+        hidden_states = residual + hidden_states\n+\n+        # 4. Feed-Forward 2 Layer\n+        residual = hidden_states\n+        hidden_states = self.ffn2_layer_norm(hidden_states)\n+        hidden_states = self.ffn2(hidden_states)\n+        hidden_states = hidden_states * 0.5 + residual\n+        hidden_states = self.final_layer_norm(hidden_states)\n+\n+        return hidden_states, attn_weigts\n+\n+\n+class Wav2Vec2BertEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+\n+        if config.position_embeddings_type == \"relative\":\n+            self.embed_positions = Wav2Vec2BertRelPositionalEmbedding(config)\n+        elif config.position_embeddings_type == \"rotary\":\n+            self.embed_positions = Wav2Vec2BertRotaryPositionalEmbedding(config)\n+        else:\n+            self.embed_positions = None\n+\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layers = nn.ModuleList([Wav2Vec2BertEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask=None,\n+        output_attentions=False,\n+        output_hidden_states=False,\n+        return_dict=True,\n+    ):\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        conv_attention_mask = attention_mask\n+        if attention_mask is not None:\n+            # make sure padded tokens output 0\n+            hidden_states = hidden_states.masked_fill(~attention_mask.bool().unsqueeze(-1), 0.0)\n+\n+            # extend attention_mask\n+            attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n+            attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n+            attention_mask = attention_mask.expand(\n+                attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n+            )\n+\n+        hidden_states = self.dropout(hidden_states)\n+\n+        if self.embed_positions is not None:\n+            relative_position_embeddings = self.embed_positions(hidden_states)\n+        else:\n+            relative_position_embeddings = None\n+\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n+\n+        for i, layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n+            dropout_probability = torch.rand([])\n+\n+            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n+                if self.gradient_checkpointing and self.training:\n+                    layer_outputs = self._gradient_checkpointing_func(\n+                        layer.__call__,\n+                        hidden_states,\n+                        attention_mask,\n+                        relative_position_embeddings,\n+                        output_attentions,\n+                        conv_attention_mask,\n+                    )\n+                else:\n+                    layer_outputs = layer(\n+                        hidden_states,\n+                        attention_mask=attention_mask,\n+                        relative_position_embeddings=relative_position_embeddings,\n+                        output_attentions=output_attentions,\n+                        conv_attention_mask=conv_attention_mask,\n+                    )\n+                hidden_states = layer_outputs[0]\n+\n+            if skip_the_layer:\n+                layer_outputs = (None, None)\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+class Wav2Vec2BertAdapter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        # feature dim might need to be down-projected\n+        if config.output_hidden_size != config.hidden_size:\n+            self.proj = nn.Linear(config.hidden_size, config.output_hidden_size)\n+            self.proj_layer_norm = nn.LayerNorm(config.output_hidden_size, eps=config.layer_norm_eps)\n+        else:\n+            self.proj = self.proj_layer_norm = None\n+        self.layers = nn.ModuleList(Wav2Vec2BertAdapterLayer(config) for _ in range(config.num_adapter_layers))\n+        self.layerdrop = config.layerdrop\n+\n+        self.kernel_size = config.adapter_kernel_size\n+        self.stride = config.adapter_stride\n+\n+    def _compute_sub_sample_lengths_from_attention_mask(self, seq_lens):\n+        if seq_lens is None:\n+            return seq_lens\n+        pad = self.kernel_size // 2\n+        seq_lens = ((seq_lens + 2 * pad - self.kernel_size) / self.stride) + 1\n+        return seq_lens.floor()\n+\n+    def forward(self, hidden_states, attention_mask=None):\n+        # down project hidden_states if necessary\n+        if self.proj is not None and self.proj_layer_norm is not None:\n+            hidden_states = self.proj(hidden_states)\n+            hidden_states = self.proj_layer_norm(hidden_states)\n+\n+        sub_sampled_lengths = None\n+        if attention_mask is not None:\n+            sub_sampled_lengths = (attention_mask.size(1) - (1 - attention_mask.int()).sum(1)).to(hidden_states.device)\n+\n+        for layer in self.layers:\n+            layerdrop_prob = torch.rand([])\n+            sub_sampled_lengths = self._compute_sub_sample_lengths_from_attention_mask(sub_sampled_lengths)\n+            if not self.training or (layerdrop_prob > self.layerdrop):\n+                hidden_states = layer(\n+                    hidden_states, attention_mask=attention_mask, sub_sampled_lengths=sub_sampled_lengths\n+                )\n+\n+        return hidden_states\n+\n+\n+class Wav2Vec2BertAdapterLayer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        embed_dim = config.output_hidden_size\n+        dropout = config.conformer_conv_dropout\n+\n+        self.kernel_size = config.adapter_kernel_size\n+        self.stride = config.adapter_stride\n+\n+        # 1. residual convolution\n+        self.residual_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.residual_conv = nn.Conv1d(\n+            embed_dim,\n+            2 * embed_dim,\n+            self.kernel_size,\n+            stride=self.stride,\n+            padding=self.stride // 2,\n+        )\n+        self.activation = nn.GLU(dim=1)\n+\n+        # Self-Attention\n+        self.self_attn_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.self_attn_conv = nn.Conv1d(\n+            embed_dim,\n+            2 * embed_dim,\n+            self.kernel_size,\n+            stride=self.stride,\n+            padding=self.stride // 2,\n+        )\n+        self.self_attn = Wav2Vec2BertSelfAttention(config, is_adapter_attention=True)\n+        self.self_attn_dropout = nn.Dropout(dropout)\n+\n+        # Feed-forward\n+        self.ffn_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.ffn = Wav2Vec2BertFeedForward(config, act_fn=config.adapter_act, hidden_size=embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        sub_sampled_lengths: Optional[torch.Tensor] = None,\n+    ):\n+        residual = self.residual_layer_norm(hidden_states)\n+\n+        # Apply pooling to the residual to match the sequence length of the\n+        # multi-head attention output.\n+        # (batch, seq_len, feature_dim) -> (batch, feature_dim, seq_len)\n+        residual = residual.transpose(1, 2)\n+        residual = self.residual_conv(residual)\n+        residual = self.activation(residual)\n+        # (batch, feature_dim, seq_len) -> (batch, seq_len, feature_dim)\n+        residual = residual.transpose(1, 2)\n+\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+        # Apply pooling before feeding to the multihead-attention layer.\n+        # (batch, seq_len, feature_dim) -> (batch, feature_dim, seq_len)\n+        hidden_states = hidden_states.transpose(1, 2)\n+        hidden_states = self.self_attn_conv(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        # (batch, feature_dim, seq_len) -> (batch, seq_len, feature_dim)\n+        hidden_states = hidden_states.transpose(1, 2)\n+\n+        if attention_mask is not None:\n+            attention_mask = _compute_new_attention_mask(hidden_states=hidden_states, seq_lens=sub_sampled_lengths)\n+            attention_mask = _prepare_4d_attention_mask(\n+                attention_mask,\n+                hidden_states.dtype,\n+            )\n+\n+        # The rest of the computation is identical to a vanilla Transformer\n+        # encoder layer.\n+        hidden_states, attn_weigths = self.self_attn(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = self.self_attn_dropout(hidden_states)\n+        hidden_states = hidden_states + residual\n+\n+        residual = hidden_states\n+\n+        hidden_states = self.ffn_layer_norm(hidden_states)\n+        hidden_states = self.ffn(hidden_states) + residual\n+\n+        return hidden_states\n+\n+\n+class Wav2Vec2BertPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = Wav2Vec2BertConfig\n+    base_model_prefix = \"wav2vec2_bert\"\n+    main_input_name = \"input_features\"\n+    supports_gradient_checkpointing = True\n+\n+    # Ignore copy\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, Wav2Vec2BertSelfAttention):\n+            if hasattr(module, \"pos_bias_u\"):\n+                nn.init.xavier_uniform_(module.pos_bias_u)\n+            if hasattr(module, \"pos_bias_v\"):\n+                nn.init.xavier_uniform_(module.pos_bias_v)\n+        elif isinstance(module, Wav2Vec2BertFeatureProjection):\n+            k = math.sqrt(1 / module.projection.in_features)\n+            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n+            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n+        elif isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            nn.init.kaiming_normal_(module.weight)\n+\n+            if module.bias is not None:\n+                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n+                nn.init.uniform_(module.bias, a=-k, b=k)\n+\n+    # Ignore copy\n+    def _get_feat_extract_output_lengths(\n+        self, input_lengths: Union[torch.LongTensor, int], add_adapter: Optional[bool] = None\n+    ):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+\n+        add_adapter = self.config.add_adapter if add_adapter is None else add_adapter\n+\n+        def _conv_out_length(input_length, kernel_size, stride, padding):\n+            # 1D convolutional layer output length formula taken\n+            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n+            return torch.div(input_length + 2 * padding - kernel_size, stride, rounding_mode=\"floor\") + 1\n+\n+        if add_adapter:\n+            padding = self.config.adapter_kernel_size // 2\n+            for _ in range(self.config.num_adapter_layers):\n+                input_lengths = _conv_out_length(\n+                    input_lengths, self.config.adapter_kernel_size, self.config.adapter_stride, padding\n+                )\n+\n+        return input_lengths\n+\n+    def _get_feature_vector_attention_mask(\n+        self, feature_vector_length: int, attention_mask: torch.LongTensor, add_adapter=None\n+    ):\n+        # Effectively attention_mask.sum(-1), but not inplace to be able to run\n+        # on inference mode.\n+        non_padded_lengths = attention_mask.cumsum(dim=-1)[:, -1]\n+\n+        output_lengths = self._get_feat_extract_output_lengths(non_padded_lengths, add_adapter=add_adapter)\n+        output_lengths = output_lengths.to(torch.long)\n+\n+        batch_size = attention_mask.shape[0]\n+\n+        attention_mask = torch.zeros(\n+            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n+        )\n+        # these two operations makes sure that all values before the output lengths idxs are attended to\n+        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n+        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n+        return attention_mask\n+\n+\n+WAV2VEC2_BERT_START_DOCSTRING = None\n+\n+WAV2VEC2_BERT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n+            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n+            soundfile`). To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n+        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+Wav2Vec2BertBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n+\n+@add_start_docstrings(\n+    \"The bare Wav2Vec2Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n+    WAV2VEC2_BERT_START_DOCSTRING,\n+)\n+class Wav2Vec2BertModel(Wav2Vec2Model, Wav2Vec2BertPreTrainedModel):\n+    def __init__(self, config: Wav2Vec2BertConfig):\n+        Wav2Vec2BertPreTrainedModel.__init__(config)\n+        self.config = config\n+        self.feature_projection = Wav2Vec2BertFeatureProjection(config)\n+\n+        # model only needs masking vector if mask prob is > 0.0\n+        if config.mask_time_prob > 0.0 or config.mask_feature_prob > 0.0:\n+            self.masked_spec_embed = nn.Parameter(torch.Tensor(config.hidden_size).uniform_())\n+\n+        self.encoder = Wav2Vec2BertEncoder(config)\n+\n+        self.adapter = Wav2Vec2BertAdapter(config) if config.add_adapter else None\n+\n+        self.intermediate_ffn = None\n+        if config.use_intermediate_ffn_before_adapter:\n+            self.intermediate_ffn = Wav2Vec2BertFeedForward(config, act_fn=\"relu\")\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n+\n+    def freeze_feature_encoder(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_BERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_PRETRAINED_CHECKPOINT_FOR_DOC,\n+        output_type=Wav2Vec2BertBaseModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(\n+        self,\n+        input_features: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        mask_time_indices: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, Wav2Vec2BertBaseModelOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        hidden_states, extract_features = self.feature_projection(input_features)\n+        hidden_states = self._mask_hidden_states(\n+            hidden_states, mask_time_indices=mask_time_indices, attention_mask=attention_mask\n+        )\n+\n+        encoder_outputs = self.encoder(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        hidden_states = encoder_outputs[0]\n+\n+        if self.intermediate_ffn:\n+            expanded_hidden_states = self.intermediate_ffn(hidden_states)\n+            hidden_states = hidden_states + 0.5 * expanded_hidden_states\n+\n+        if self.adapter is not None:\n+            hidden_states = self.adapter(hidden_states, attention_mask=attention_mask)\n+\n+        if not return_dict:\n+            return (hidden_states, extract_features) + encoder_outputs[1:]\n+\n+        return Wav2Vec2BertBaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            extract_features=extract_features,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Wav2Vec2Bert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n+    WAV2VEC2_BERT_START_DOCSTRING,\n+)\n+class Wav2Vec2BertForCTC(Wav2Vec2ConformerForCTC):\n+    def __init__(self, config, target_lang: Optional[str] = None):\n+        super().__init__(config)\n+\n+    def freeze_feature_encoder(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_BERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_PRETRAINED_CHECKPOINT_FOR_DOC,\n+        output_type=CausalLMOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_CTC_EXPECTED_OUTPUT,\n+        expected_loss=_CTC_EXPECTED_LOSS,\n+    )\n+    def forward(\n+        self,\n+        input_features: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        labels: Optional[torch.Tensor] = None,\n+    ) -> Union[Tuple, CausalLMOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n+            Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\n+            the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.\n+            All labels set to `-100` are ignored (masked), the loss is only computed for labels in `[0, ...,\n+            config.vocab_size - 1]`.\n+        \"\"\"\n+        if labels is not None and labels.max() >= self.config.vocab_size:\n+            raise ValueError(f\"Label values must be <= vocab_size: {self.config.vocab_size}\")\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.wav2vec2_bert(\n+            input_features,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        hidden_states = outputs[0]\n+        hidden_states = self.dropout(hidden_states)\n+\n+        logits = self.lm_head(hidden_states)\n+\n+        loss = None\n+        if labels is not None:\n+            # retrieve loss input_lengths from attention_mask\n+            attention_mask = (\n+                attention_mask\n+                if attention_mask is not None\n+                else torch.ones(input_features.shape[:2], device=input_features.device, dtype=torch.long)\n+            )\n+            input_lengths = self._get_feat_extract_output_lengths(attention_mask.sum([-1])).to(torch.long)\n+\n+            # assuming that padded tokens are filled with -100\n+            # when not being attended to\n+            labels_mask = labels >= 0\n+            target_lengths = labels_mask.sum(-1)\n+            flattened_targets = labels.masked_select(labels_mask)\n+\n+            # ctc_loss doesn't support fp16\n+            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n+\n+            with torch.backends.cudnn.flags(enabled=False):\n+                loss = nn.functional.ctc_loss(\n+                    log_probs,\n+                    flattened_targets,\n+                    input_lengths,\n+                    target_lengths,\n+                    blank=self.config.pad_token_id,\n+                    reduction=self.config.ctc_loss_reduction,\n+                    zero_infinity=self.config.ctc_zero_infinity,\n+                )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return CausalLMOutput(\n+            loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Wav2Vec2Bert Model with a sequence classification head on top (a linear layer over the pooled output) for\n+    tasks like SUPERB Keyword Spotting.\n+    \"\"\",\n+    WAV2VEC2_BERT_START_DOCSTRING,\n+)\n+class Wav2Vec2BertForSequenceClassification(Wav2Vec2ForSequenceClassification):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n+\n+    def freeze_feature_encoder(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n+\n+    def freeze_base_model(self):\n+        \"\"\"\n+        Calling this function will disable the gradient computation for the base model so that its parameters will not\n+        be updated during training. Only the classification head will be updated.\n+        \"\"\"\n+        for param in self.wav2vec2_bert.parameters():\n+            param.requires_grad = False\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_BERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_BASE_CHECKPOINT_FOR_DOC,\n+        output_type=SequenceClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.forward with Wav2Vec2->Wav2Vec2Bert,wav2vec2->wav2vec2_bert,WAV_2_VEC_2->WAV2VEC2_BERT, input_values->input_features\n+    def forward(\n+        self,\n+        input_features: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        labels: Optional[torch.Tensor] = None,\n+    ) -> Union[Tuple, SequenceClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = True if self.config.use_weighted_layer_sum else output_hidden_states\n+\n+        outputs = self.wav2vec2_bert(\n+            input_features,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        if self.config.use_weighted_layer_sum:\n+            hidden_states = outputs[_HIDDEN_STATES_START_POSITION]\n+            hidden_states = torch.stack(hidden_states, dim=1)\n+            norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)\n+            hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)\n+        else:\n+            hidden_states = outputs[0]\n+\n+        hidden_states = self.projector(hidden_states)\n+        if attention_mask is None:\n+            pooled_output = hidden_states.mean(dim=1)\n+        else:\n+            padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)\n+            expand_padding_mask = padding_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_padding_mask] = 0.0\n+            pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)\n+\n+        logits = self.classifier(pooled_output)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Wav2Vec2Bert Model with a frame classification head on top for tasks like Speaker Diarization.\n+    \"\"\",\n+    WAV2VEC2_BERT_START_DOCSTRING,\n+)\n+class Wav2Vec2BertForAudioFrameClassification(Wav2Vec2ConformerForAudioFrameClassification):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+    def freeze_feature_encoder(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_BERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_BASE_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForAudioFrameClassification.forward with wav2vec2_conformer->wav2vec2_bert, input_values->input_features\n+    def forward(\n+        self,\n+        input_features: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, TokenClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = True if self.config.use_weighted_layer_sum else output_hidden_states\n+\n+        outputs = self.wav2vec2_bert(\n+            input_features,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        if self.config.use_weighted_layer_sum:\n+            hidden_states = outputs[_HIDDEN_STATES_START_POSITION]\n+            hidden_states = torch.stack(hidden_states, dim=1)\n+            norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)\n+            hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)\n+        else:\n+            hidden_states = outputs[0]\n+\n+        logits = self.classifier(hidden_states)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(logits.view(-1, self.num_labels), torch.argmax(labels.view(-1, self.num_labels), axis=1))\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[_HIDDEN_STATES_START_POSITION:]\n+            return output\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Wav2Vec2Bert Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n+    \"\"\",\n+    WAV2VEC2_BERT_START_DOCSTRING,\n+)\n+class Wav2Vec2BertForXVector(Wav2Vec2ConformerForXVector):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+    def freeze_feature_encoder(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Bert\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_BERT_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_BASE_CHECKPOINT_FOR_DOC,\n+        output_type=XVectorOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerForXVector.forward with wav2vec2_conformer->wav2vec2_bert, input_values->input_features\n+    def forward(\n+        self,\n+        input_features: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        labels: Optional[torch.Tensor] = None,\n+    ) -> Union[Tuple, XVectorOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = True if self.config.use_weighted_layer_sum else output_hidden_states\n+\n+        outputs = self.wav2vec2_bert(\n+            input_features,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        if self.config.use_weighted_layer_sum:\n+            hidden_states = outputs[_HIDDEN_STATES_START_POSITION]\n+            hidden_states = torch.stack(hidden_states, dim=1)\n+            norm_weights = nn.functional.softmax(self.layer_weights, dim=-1)\n+            hidden_states = (hidden_states * norm_weights.view(-1, 1, 1)).sum(dim=1)\n+        else:\n+            hidden_states = outputs[0]\n+\n+        hidden_states = self.projector(hidden_states)\n+\n+        for tdnn_layer in self.tdnn:\n+            hidden_states = tdnn_layer(hidden_states)\n+\n+        # Statistic Pooling\n+        if attention_mask is None:\n+            mean_features = hidden_states.mean(dim=1)\n+            std_features = hidden_states.std(dim=1)\n+        else:\n+            feat_extract_output_lengths = self._get_feat_extract_output_lengths(attention_mask.sum(dim=1))\n+            tdnn_output_lengths = self._get_tdnn_output_lengths(feat_extract_output_lengths)\n+            mean_features = []\n+            std_features = []\n+            for i, length in enumerate(tdnn_output_lengths):\n+                mean_features.append(hidden_states[i, :length].mean(dim=0))\n+                std_features.append(hidden_states[i, :length].std(dim=0))\n+            mean_features = torch.stack(mean_features)\n+            std_features = torch.stack(std_features)\n+        statistic_pooling = torch.cat([mean_features, std_features], dim=-1)\n+\n+        output_embeddings = self.feature_extractor(statistic_pooling)\n+        logits = self.classifier(output_embeddings)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.objective(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits, output_embeddings) + outputs[_HIDDEN_STATES_START_POSITION:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return XVectorOutput(\n+            loss=loss,\n+            logits=logits,\n+            embeddings=output_embeddings,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"Wav2Vec2BertForAudioFrameClassification\",\n+    \"Wav2Vec2BertForCTC\",\n+    \"Wav2Vec2BertForSequenceClassification\",\n+    \"Wav2Vec2BertForXVector\",\n+    \"Wav2Vec2BertModel\",\n+    \"Wav2Vec2BertPreTrainedModel\",\n+]"
        },
        {
            "sha": "bd94e44b6166c796c716ed79fe905c2870c7344c",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 219,
            "deletions": 302,
            "changes": 521,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1,27 +1,16 @@\n-# coding=utf-8\n-# Copyright 2022 The Fairseq Authors and the HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Wav2Vec2-Conformer model.\"\"\"\n-\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_wav2vec2_conformer.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n import warnings\n from dataclasses import dataclass\n from typing import Optional, Tuple, Union\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n@@ -43,31 +32,19 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_peft_available,\n-    logging,\n     replace_return_docstrings,\n )\n from .configuration_wav2vec2_conformer import Wav2Vec2ConformerConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n-_HIDDEN_STATES_START_POSITION = 2\n-\n-# General docstring\n-_CONFIG_FOR_DOC = \"Wav2Vec2ConformerConfig\"\n-\n # Base docstring\n _CHECKPOINT_FOR_DOC = \"facebook/wav2vec2-conformer-rope-large-960h-ft\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 292, 1024]\n \n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n-_CTC_EXPECTED_LOSS = 64.21\n+# General docstring\n+_CONFIG_FOR_DOC = \"Wav2Vec2ConformerConfig\"\n \n \n @dataclass\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTrainingOutput with Wav2Vec2->Wav2Vec2Conformer\n class Wav2Vec2ConformerForPreTrainingOutput(ModelOutput):\n     \"\"\"\n     Output type of [`Wav2Vec2ConformerForPreTraining`], with potential hidden states and attentions.\n@@ -109,239 +86,17 @@ class Wav2Vec2ConformerForPreTrainingOutput(ModelOutput):\n     diversity_loss: Optional[torch.FloatTensor] = None\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices\n-def _compute_mask_indices(\n-    shape: Tuple[int, int],\n-    mask_prob: float,\n-    mask_length: int,\n-    attention_mask: Optional[torch.LongTensor] = None,\n-    min_masks: int = 0,\n-) -> np.ndarray:\n-    \"\"\"\n-    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n-    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n-    CPU as part of the preprocessing during training.\n-\n-    Args:\n-        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n-               the first element is the batch size and the second element is the length of the axis to span.\n-        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n-                    independently generated mask spans of length `mask_length` is computed by\n-                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n-                    actual percentage will be smaller.\n-        mask_length: size of the mask\n-        min_masks: minimum number of masked spans\n-        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n-                        each batch dimension.\n-    \"\"\"\n-    batch_size, sequence_length = shape\n-\n-    if mask_length < 1:\n-        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n-\n-    if mask_length > sequence_length:\n-        raise ValueError(\n-            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n-            f\" and `sequence_length`: {sequence_length}`\"\n-        )\n-\n-    # epsilon is used for probabilistic rounding\n-    epsilon = np.random.rand(1).item()\n-\n-    def compute_num_masked_span(input_length):\n-        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n-        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n-        num_masked_span = max(num_masked_span, min_masks)\n-\n-        # make sure num masked span <= sequence_length\n-        if num_masked_span * mask_length > sequence_length:\n-            num_masked_span = sequence_length // mask_length\n-\n-        # make sure num_masked span is also <= input_length - (mask_length - 1)\n-        if input_length - (mask_length - 1) < num_masked_span:\n-            num_masked_span = max(input_length - (mask_length - 1), 0)\n-\n-        return num_masked_span\n-\n-    # compute number of masked spans in batch\n-    input_lengths = (\n-        attention_mask.detach().sum(-1).tolist()\n-        if attention_mask is not None\n-        else [sequence_length for _ in range(batch_size)]\n-    )\n-\n-    # SpecAugment mask to fill\n-    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n-    spec_aug_mask_idxs = []\n-\n-    max_num_masked_span = compute_num_masked_span(sequence_length)\n-\n-    if max_num_masked_span == 0:\n-        return spec_aug_mask\n-\n-    for input_length in input_lengths:\n-        # compute num of masked spans for this input\n-        num_masked_span = compute_num_masked_span(input_length)\n-\n-        # get random indices to mask\n-        spec_aug_mask_idx = np.random.choice(\n-            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n-        )\n-\n-        # pick first sampled index that will serve as a dummy index to pad vector\n-        # to ensure same dimension for all batches due to probabilistic rounding\n-        # Picking first sample just pads those vectors twice.\n-        if len(spec_aug_mask_idx) == 0:\n-            # this case can only happen if `input_length` is strictly smaller then\n-            # `sequence_length` in which case the last token has to be a padding\n-            # token which we can use as a dummy mask id\n-            dummy_mask_idx = sequence_length - 1\n-        else:\n-            dummy_mask_idx = spec_aug_mask_idx[0]\n-\n-        spec_aug_mask_idx = np.concatenate(\n-            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n-        )\n-        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n-\n-    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n-\n-    # expand masked indices to masked spans\n-    spec_aug_mask_idxs = np.broadcast_to(\n-        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n-\n-    # add offset to the starting indexes so that indexes now create a span\n-    offsets = np.arange(mask_length)[None, None, :]\n-    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n-        batch_size, max_num_masked_span * mask_length\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n-\n-    # ensure that we cannot have indices larger than sequence_length\n-    if spec_aug_mask_idxs.max() > sequence_length - 1:\n-        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n-\n-    # scatter indices to mask\n-    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n-\n-    return spec_aug_mask\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._sample_negative_indices\n-def _sample_negative_indices(\n-    features_shape: Tuple, num_negatives: int, mask_time_indices: Optional[np.ndarray] = None\n-):\n-    \"\"\"\n-    Sample `num_negatives` vectors from feature vectors.\n-    \"\"\"\n-    batch_size, sequence_length = features_shape\n-\n-    # generate indices of the positive vectors themselves, repeat them `num_negatives` times\n-    sequence_length_range = np.arange(sequence_length)\n-\n-    # get `num_negatives` random vector indices from the same utterance\n-    sampled_negative_indices = np.zeros(shape=(batch_size, sequence_length, num_negatives), dtype=np.int32)\n-\n-    mask_time_indices = (\n-        mask_time_indices.astype(bool) if mask_time_indices is not None else np.ones(features_shape, dtype=bool)\n-    )\n-\n-    for batch_idx in range(batch_size):\n-        high = mask_time_indices[batch_idx].sum() - 1\n-        mapped_masked_indices = sequence_length_range[mask_time_indices[batch_idx]]\n-\n-        feature_indices = np.broadcast_to(np.arange(high + 1)[:, None], (high + 1, num_negatives))\n-        sampled_indices = np.random.randint(0, high, size=(high + 1, num_negatives))\n-        # avoid sampling the same positive vector, but keep the distribution uniform\n-        sampled_indices[sampled_indices >= feature_indices] += 1\n-\n-        # remap to actual indices\n-        sampled_negative_indices[batch_idx][mask_time_indices[batch_idx]] = mapped_masked_indices[sampled_indices]\n-\n-        # correct for batch size\n-        sampled_negative_indices[batch_idx] += batch_idx * sequence_length\n-\n-    return sampled_negative_indices\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->Wav2Vec2Conformer\n-class Wav2Vec2ConformerNoLayerNormConvLayer(nn.Module):\n-    def __init__(self, config, layer_id=0):\n-        super().__init__()\n-        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n-        self.out_conv_dim = config.conv_dim[layer_id]\n-\n-        self.conv = nn.Conv1d(\n-            self.in_conv_dim,\n-            self.out_conv_dim,\n-            kernel_size=config.conv_kernel[layer_id],\n-            stride=config.conv_stride[layer_id],\n-            bias=config.conv_bias,\n-        )\n-        self.activation = ACT2FN[config.feat_extract_activation]\n-\n-    def forward(self, hidden_states):\n-        hidden_states = self.conv(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->Wav2Vec2Conformer\n-class Wav2Vec2ConformerLayerNormConvLayer(nn.Module):\n-    def __init__(self, config, layer_id=0):\n-        super().__init__()\n-        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n-        self.out_conv_dim = config.conv_dim[layer_id]\n-\n-        self.conv = nn.Conv1d(\n-            self.in_conv_dim,\n-            self.out_conv_dim,\n-            kernel_size=config.conv_kernel[layer_id],\n-            stride=config.conv_stride[layer_id],\n-            bias=config.conv_bias,\n-        )\n-        self.layer_norm = nn.LayerNorm(self.out_conv_dim, elementwise_affine=True)\n-        self.activation = ACT2FN[config.feat_extract_activation]\n-\n-    def forward(self, hidden_states):\n-        hidden_states = self.conv(hidden_states)\n-\n-        hidden_states = hidden_states.transpose(-2, -1)\n-        hidden_states = self.layer_norm(hidden_states)\n-        hidden_states = hidden_states.transpose(-2, -1)\n-\n-        hidden_states = self.activation(hidden_states)\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->Wav2Vec2Conformer\n-class Wav2Vec2ConformerGroupNormConvLayer(nn.Module):\n-    def __init__(self, config, layer_id=0):\n+class Wav2Vec2ConformerSamePadLayer(nn.Module):\n+    def __init__(self, num_conv_pos_embeddings):\n         super().__init__()\n-        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n-        self.out_conv_dim = config.conv_dim[layer_id]\n-\n-        self.conv = nn.Conv1d(\n-            self.in_conv_dim,\n-            self.out_conv_dim,\n-            kernel_size=config.conv_kernel[layer_id],\n-            stride=config.conv_stride[layer_id],\n-            bias=config.conv_bias,\n-        )\n-        self.activation = ACT2FN[config.feat_extract_activation]\n-\n-        self.layer_norm = nn.GroupNorm(num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)\n+        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n \n     def forward(self, hidden_states):\n-        hidden_states = self.conv(hidden_states)\n-        hidden_states = self.layer_norm(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n+        if self.num_pad_remove > 0:\n+            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->Wav2Vec2Conformer\n class Wav2Vec2ConformerPositionalConvEmbedding(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -471,19 +226,78 @@ def forward(self, hidden_states: torch.Tensor):\n         return relative_position_embeddings\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->Wav2Vec2Conformer\n-class Wav2Vec2ConformerSamePadLayer(nn.Module):\n-    def __init__(self, num_conv_pos_embeddings):\n+class Wav2Vec2ConformerNoLayerNormConvLayer(nn.Module):\n+    def __init__(self, config, layer_id=0):\n         super().__init__()\n-        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n+        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n+        self.out_conv_dim = config.conv_dim[layer_id]\n+\n+        self.conv = nn.Conv1d(\n+            self.in_conv_dim,\n+            self.out_conv_dim,\n+            kernel_size=config.conv_kernel[layer_id],\n+            stride=config.conv_stride[layer_id],\n+            bias=config.conv_bias,\n+        )\n+        self.activation = ACT2FN[config.feat_extract_activation]\n \n     def forward(self, hidden_states):\n-        if self.num_pad_remove > 0:\n-            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        return hidden_states\n+\n+\n+class Wav2Vec2ConformerLayerNormConvLayer(nn.Module):\n+    def __init__(self, config, layer_id=0):\n+        super().__init__()\n+        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n+        self.out_conv_dim = config.conv_dim[layer_id]\n+\n+        self.conv = nn.Conv1d(\n+            self.in_conv_dim,\n+            self.out_conv_dim,\n+            kernel_size=config.conv_kernel[layer_id],\n+            stride=config.conv_stride[layer_id],\n+            bias=config.conv_bias,\n+        )\n+        self.layer_norm = nn.LayerNorm(self.out_conv_dim, elementwise_affine=True)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+\n+        hidden_states = hidden_states.transpose(-2, -1)\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = hidden_states.transpose(-2, -1)\n+\n+        hidden_states = self.activation(hidden_states)\n+        return hidden_states\n+\n+\n+class Wav2Vec2ConformerGroupNormConvLayer(nn.Module):\n+    def __init__(self, config, layer_id=0):\n+        super().__init__()\n+        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n+        self.out_conv_dim = config.conv_dim[layer_id]\n+\n+        self.conv = nn.Conv1d(\n+            self.in_conv_dim,\n+            self.out_conv_dim,\n+            kernel_size=config.conv_kernel[layer_id],\n+            stride=config.conv_stride[layer_id],\n+            bias=config.conv_bias,\n+        )\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+\n+        self.layer_norm = nn.GroupNorm(num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->Wav2Vec2Conformer\n class Wav2Vec2ConformerFeatureEncoder(nn.Module):\n     \"\"\"Construct the features from raw audio waveform\"\"\"\n \n@@ -531,7 +345,6 @@ def forward(self, input_values):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection with Wav2Vec2->Wav2Vec2Conformer\n class Wav2Vec2ConformerFeatureProjection(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -547,7 +360,6 @@ def forward(self, hidden_states):\n         return hidden_states, norm_hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->Wav2Vec2Conformer\n class Wav2Vec2ConformerFeedForward(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -943,7 +755,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GumbelVectorQuantizer with Wav2Vec2->Wav2Vec2Conformer\n class Wav2Vec2ConformerGumbelVectorQuantizer(nn.Module):\n     \"\"\"\n     Vector quantization using gumbel softmax. See `[CATEGORICAL REPARAMETERIZATION WITH\n@@ -1020,7 +831,6 @@ def forward(self, hidden_states, mask_time_indices=None):\n         return codevectors, perplexity\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Adapter with Wav2Vec2->Wav2Vec2Conformer\n class Wav2Vec2ConformerAdapter(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1052,7 +862,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2AdapterLayer with Wav2Vec2->Wav2Vec2Conformer\n class Wav2Vec2ConformerAdapterLayer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -1170,6 +979,128 @@ def _get_feature_vector_attention_mask(\n         return attention_mask\n \n \n+def _compute_mask_indices(\n+    shape: Tuple[int, int],\n+    mask_prob: float,\n+    mask_length: int,\n+    attention_mask: Optional[torch.LongTensor] = None,\n+    min_masks: int = 0,\n+) -> np.ndarray:\n+    \"\"\"\n+    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n+    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n+    CPU as part of the preprocessing during training.\n+\n+    Args:\n+        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n+               the first element is the batch size and the second element is the length of the axis to span.\n+        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n+                    independently generated mask spans of length `mask_length` is computed by\n+                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n+                    actual percentage will be smaller.\n+        mask_length: size of the mask\n+        min_masks: minimum number of masked spans\n+        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n+                        each batch dimension.\n+    \"\"\"\n+    batch_size, sequence_length = shape\n+\n+    if mask_length < 1:\n+        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n+\n+    if mask_length > sequence_length:\n+        raise ValueError(\n+            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n+            f\" and `sequence_length`: {sequence_length}`\"\n+        )\n+\n+    # epsilon is used for probabilistic rounding\n+    epsilon = np.random.rand(1).item()\n+\n+    def compute_num_masked_span(input_length):\n+        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n+        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n+        num_masked_span = max(num_masked_span, min_masks)\n+\n+        # make sure num masked span <= sequence_length\n+        if num_masked_span * mask_length > sequence_length:\n+            num_masked_span = sequence_length // mask_length\n+\n+        # make sure num_masked span is also <= input_length - (mask_length - 1)\n+        if input_length - (mask_length - 1) < num_masked_span:\n+            num_masked_span = max(input_length - (mask_length - 1), 0)\n+\n+        return num_masked_span\n+\n+    # compute number of masked spans in batch\n+    input_lengths = (\n+        attention_mask.detach().sum(-1).tolist()\n+        if attention_mask is not None\n+        else [sequence_length for _ in range(batch_size)]\n+    )\n+\n+    # SpecAugment mask to fill\n+    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n+    spec_aug_mask_idxs = []\n+\n+    max_num_masked_span = compute_num_masked_span(sequence_length)\n+\n+    if max_num_masked_span == 0:\n+        return spec_aug_mask\n+\n+    for input_length in input_lengths:\n+        # compute num of masked spans for this input\n+        num_masked_span = compute_num_masked_span(input_length)\n+\n+        # get random indices to mask\n+        spec_aug_mask_idx = np.random.choice(\n+            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+        )\n+\n+        # pick first sampled index that will serve as a dummy index to pad vector\n+        # to ensure same dimension for all batches due to probabilistic rounding\n+        # Picking first sample just pads those vectors twice.\n+        if len(spec_aug_mask_idx) == 0:\n+            # this case can only happen if `input_length` is strictly smaller then\n+            # `sequence_length` in which case the last token has to be a padding\n+            # token which we can use as a dummy mask id\n+            dummy_mask_idx = sequence_length - 1\n+        else:\n+            dummy_mask_idx = spec_aug_mask_idx[0]\n+\n+        spec_aug_mask_idx = np.concatenate(\n+            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n+        )\n+        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n+\n+    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n+\n+    # expand masked indices to masked spans\n+    spec_aug_mask_idxs = np.broadcast_to(\n+        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n+\n+    # add offset to the starting indexes so that indexes now create a span\n+    offsets = np.arange(mask_length)[None, None, :]\n+    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n+        batch_size, max_num_masked_span * mask_length\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n+\n+    # ensure that we cannot have indices larger than sequence_length\n+    if spec_aug_mask_idxs.max() > sequence_length - 1:\n+        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n+\n+    # scatter indices to mask\n+    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+\n+    return spec_aug_mask\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 1024]\n+\n+\n WAV2VEC2_CONFORMER_START_DOCSTRING = r\"\"\"\n     Wav2Vec2Conformer was proposed in [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech\n     Representations](https://arxiv.org/abs/2006.11477) by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael\n@@ -1178,16 +1109,16 @@ def _get_feature_vector_attention_mask(\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving etc.).\n \n-    This model is a PyTorch [nn.Module](https://pytorch.org/docs/stable/nn.html#nn.Module) sub-class. Use it as a\n-    regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n+    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n \n     Parameters:\n         config ([`Wav2Vec2ConformerConfig`]): Model configuration class with all the parameters of the model.\n             Initializing with a config file does not load the weights associated with the model, only the\n             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n-\n WAV2VEC2_CONFORMER_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1226,6 +1157,8 @@ def _get_feature_vector_attention_mask(\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n \n+Wav2Vec2ConformerBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n \n @add_start_docstrings(\n     \"The bare Wav2Vec2Conformer Model transformer outputting raw hidden-states without any specific head on top.\",\n@@ -1249,15 +1182,13 @@ def __init__(self, config: Wav2Vec2ConformerConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model.freeze_feature_encoder\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n         not be updated during training.\n         \"\"\"\n         self.feature_extractor._freeze_parameters()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model._mask_hidden_states\n     def _mask_hidden_states(\n         self,\n         hidden_states: torch.FloatTensor,\n@@ -1307,12 +1238,11 @@ def _mask_hidden_states(\n     @add_start_docstrings_to_model_forward(WAV2VEC2_CONFORMER_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Wav2Vec2BaseModelOutput,\n+        output_type=Wav2Vec2ConformerBaseModelOutput,\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n         expected_output=_EXPECTED_OUTPUT_SHAPE,\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model.forward with wav2vec2->wav2vec2_conformer\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1321,7 +1251,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Wav2Vec2BaseModelOutput]:\n+    ) -> Union[Tuple, Wav2Vec2ConformerBaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1358,7 +1288,7 @@ def forward(\n         if not return_dict:\n             return (hidden_states, extract_features) + encoder_outputs[1:]\n \n-        return Wav2Vec2BaseModelOutput(\n+        return Wav2Vec2ConformerBaseModelOutput(\n             last_hidden_state=hidden_states,\n             extract_features=extract_features,\n             hidden_states=encoder_outputs.hidden_states,\n@@ -1370,7 +1300,6 @@ def forward(\n     \"\"\"Wav2Vec2Conformer Model with a quantizer and `VQ` head on top.\"\"\", WAV2VEC2_CONFORMER_START_DOCSTRING\n )\n class Wav2Vec2ConformerForPreTraining(Wav2Vec2ConformerPreTrainedModel):\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.__init__ with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer\n     def __init__(self, config: Wav2Vec2ConformerConfig):\n         super().__init__(config)\n         self.wav2vec2_conformer = Wav2Vec2ConformerModel(config)\n@@ -1384,14 +1313,12 @@ def __init__(self, config: Wav2Vec2ConformerConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.set_gumbel_temperature\n     def set_gumbel_temperature(self, temperature: int):\n         \"\"\"\n         Set the Gumbel softmax temperature to a given value. Only necessary for training\n         \"\"\"\n         self.quantizer.temperature = temperature\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.freeze_feature_encoder with wav2vec2->wav2vec2_conformer\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1400,7 +1327,6 @@ def freeze_feature_encoder(self):\n         self.wav2vec2_conformer.feature_extractor._freeze_parameters()\n \n     @staticmethod\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.compute_contrastive_logits\n     def compute_contrastive_logits(\n         target_features: torch.FloatTensor,\n         negative_features: torch.FloatTensor,\n@@ -1423,7 +1349,6 @@ def compute_contrastive_logits(\n \n     @add_start_docstrings_to_model_forward(WAV2VEC2_CONFORMER_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Wav2Vec2ConformerForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForPreTraining.forward with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer,wav2vec2_conformer-base->wav2vec2-conformer-rel-pos-large\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1452,8 +1377,8 @@ def forward(\n         >>> from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer import _compute_mask_indices, _sample_negative_indices\n         >>> from datasets import load_dataset\n \n-        >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-conformer-rel-pos-large\")\n-        >>> model = Wav2Vec2ConformerForPreTraining.from_pretrained(\"facebook/wav2vec2-conformer-rel-pos-large\")\n+        >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2_conformer-base\")\n+        >>> model = Wav2Vec2ConformerForPreTraining.from_pretrained(\"facebook/wav2vec2_conformer-base\")\n \n         >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         >>> input_values = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values  # Batch size 1\n@@ -1585,12 +1510,18 @@ def forward(\n         )\n \n \n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n+_CTC_EXPECTED_LOSS = 64.21\n+\n+\n @add_start_docstrings(\n     \"\"\"Wav2Vec2Conformer Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n     WAV2VEC2_CONFORMER_START_DOCSTRING,\n )\n class Wav2Vec2ConformerForCTC(Wav2Vec2ConformerPreTrainedModel):\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.__init__ with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer\n     def __init__(self, config, target_lang: Optional[str] = None):\n         super().__init__(config)\n \n@@ -1614,7 +1545,6 @@ def __init__(self, config, target_lang: Optional[str] = None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.freeze_feature_encoder with wav2vec2->wav2vec2_conformer\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1630,7 +1560,6 @@ def freeze_feature_encoder(self):\n         expected_output=_CTC_EXPECTED_OUTPUT,\n         expected_loss=_CTC_EXPECTED_LOSS,\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC.forward with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1710,7 +1639,6 @@ def forward(\n     WAV2VEC2_CONFORMER_START_DOCSTRING,\n )\n class Wav2Vec2ConformerForSequenceClassification(Wav2Vec2ConformerPreTrainedModel):\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.__init__ with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1728,7 +1656,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_encoder with wav2vec2->wav2vec2_conformer\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n@@ -1751,7 +1678,6 @@ def freeze_base_model(self):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.forward with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer,WAV_2_VEC_2->WAV2VEC2_CONFORMER\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1822,7 +1748,6 @@ def forward(\n     WAV2VEC2_CONFORMER_START_DOCSTRING,\n )\n class Wav2Vec2ConformerForAudioFrameClassification(Wav2Vec2ConformerPreTrainedModel):\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification.__init__ with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer,WAV_2_VEC_2->WAV2VEC2_CONFORMER\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1839,15 +1764,13 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification.freeze_feature_encoder with wav2vec2->wav2vec2_conformer\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n         not be updated during training.\n         \"\"\"\n         self.wav2vec2_conformer.feature_extractor._freeze_parameters()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification.freeze_base_model with wav2vec2->wav2vec2_conformer\n     def freeze_base_model(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the base model so that its parameters will not\n@@ -1863,7 +1786,6 @@ def freeze_base_model(self):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification.forward with wav2vec2->wav2vec2_conformer\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1918,7 +1840,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.AMSoftmaxLoss\n class AMSoftmaxLoss(nn.Module):\n     def __init__(self, input_dim, num_labels, scale=30.0, margin=0.4):\n         super(AMSoftmaxLoss, self).__init__()\n@@ -1942,7 +1863,6 @@ def forward(self, hidden_states, labels):\n         return loss\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.TDNNLayer\n class TDNNLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -1958,6 +1878,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if is_peft_available():\n             from peft.tuners.lora import LoraLayer\n \n+        if is_peft_available():\n             if isinstance(self.kernel, LoraLayer):\n                 warnings.warn(\n                     \"Detected LoRA on TDNNLayer. LoRA weights won't be applied due to optimization. \"\n@@ -2000,15 +1921,13 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector.freeze_feature_encoder with wav2vec2->wav2vec2_conformer\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n         not be updated during training.\n         \"\"\"\n         self.wav2vec2_conformer.feature_extractor._freeze_parameters()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector.freeze_base_model with wav2vec2->wav2vec2_conformer\n     def freeze_base_model(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the base model so that its parameters will not\n@@ -2017,7 +1936,6 @@ def freeze_base_model(self):\n         for param in self.wav2vec2_conformer.parameters():\n             param.requires_grad = False\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector._get_tdnn_output_lengths with wav2vec2->wav2vec2_conformer\n     def _get_tdnn_output_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n         \"\"\"\n         Computes the output length of the TDNN layers\n@@ -2040,7 +1958,6 @@ def _conv_out_length(input_length, kernel_size, stride):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector.forward with Wav2Vec2->Wav2Vec2Conformer,wav2vec2->wav2vec2_conformer,WAV_2_VEC_2->WAV2VEC2_CONFORMER\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],"
        },
        {
            "sha": "c2d101385fada5e60767826464f037e03d178651",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "added",
            "additions": 892,
            "deletions": 0,
            "changes": 892,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -0,0 +1,892 @@\n+import math\n+from dataclasses import dataclass\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    CausalLMOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+    Wav2Vec2BaseModelOutput,\n+    XVectorOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    ModelOutput,\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..wav2vec2.modeling_wav2vec2 import (\n+    Wav2Vec2Adapter,\n+    Wav2Vec2AdapterLayer,\n+    Wav2Vec2FeatureEncoder,\n+    Wav2Vec2FeatureProjection,\n+    Wav2Vec2FeedForward,\n+    Wav2Vec2ForAudioFrameClassification,\n+    Wav2Vec2ForCTC,\n+    Wav2Vec2ForPreTraining,\n+    Wav2Vec2ForSequenceClassification,\n+    Wav2Vec2ForXVector,\n+    Wav2Vec2GumbelVectorQuantizer,\n+    Wav2Vec2Model,\n+    Wav2Vec2PositionalConvEmbedding,\n+)\n+from .configuration_wav2vec2_conformer import Wav2Vec2ConformerConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_HIDDEN_STATES_START_POSITION = 2\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"Wav2Vec2ConformerConfig\"\n+\n+# Base docstring\n+_CHECKPOINT_FOR_DOC = \"facebook/wav2vec2-conformer-rope-large-960h-ft\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 1024]\n+\n+# CTC docstring\n+_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n+_CTC_EXPECTED_LOSS = 64.21\n+\n+\n+@dataclass\n+class Wav2Vec2ConformerForPreTrainingOutput(ModelOutput):\n+    \"\"\"\n+    Output type of [`Wav2Vec2ConformerForPreTraining`], with potential hidden states and attentions.\n+\n+    Args:\n+        loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n+            Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the [official\n+            paper](https://arxiv.org/pdf/2006.11477.pdf) . (classification) loss.\n+        projected_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n+            Hidden-states of the model projected to *config.proj_codevector_dim* that can be used to predict the masked\n+            projected quantized states.\n+        projected_quantized_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.proj_codevector_dim)`):\n+            Quantized extracted feature vectors projected to *config.proj_codevector_dim* representing the positive\n+            target vectors for contrastive loss.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        contrastive_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n+            The contrastive loss (L_m) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .\n+        diversity_loss (*optional*, returned when `sample_negative_indices` are passed, `torch.FloatTensor` of shape `(1,)`):\n+            The diversity loss (L_d) as stated in the [official paper](https://arxiv.org/pdf/2006.11477.pdf) .\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    projected_states: Optional[torch.FloatTensor] = None\n+    projected_quantized_states: Optional[torch.FloatTensor] = None\n+    codevector_perplexity: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    contrastive_loss: Optional[torch.FloatTensor] = None\n+    diversity_loss: Optional[torch.FloatTensor] = None\n+\n+\n+class Wav2Vec2ConformerPositionalConvEmbedding(Wav2Vec2PositionalConvEmbedding):\n+    pass\n+\n+\n+class Wav2Vec2ConformerRotaryPositionalEmbedding(nn.Module):\n+    \"\"\"Rotary positional embedding\n+    Reference : https://blog.eleuther.ai/rotary-embeddings/ Paper: https://arxiv.org/pdf/2104.09864.pdf\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        dim = config.hidden_size // config.num_attention_heads\n+        base = config.rotary_embedding_base\n+\n+        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n+        self.register_buffer(\"inv_freq\", inv_freq)\n+        self.cached_sequence_length = None\n+        self.cached_rotary_positional_embedding = None\n+\n+    def forward(self, hidden_states):\n+        sequence_length = hidden_states.shape[1]\n+\n+        if sequence_length == self.cached_sequence_length and self.cached_rotary_positional_embedding is not None:\n+            return self.cached_rotary_positional_embedding\n+\n+        self.cached_sequence_length = sequence_length\n+        # Embeddings are computed in the dtype of the inv_freq constant\n+        time_stamps = torch.arange(sequence_length).type_as(self.inv_freq)\n+        freqs = torch.einsum(\"i,j->ij\", time_stamps, self.inv_freq)\n+        embeddings = torch.cat((freqs, freqs), dim=-1)\n+\n+        cos_embeddings = embeddings.cos()[:, None, None, :]\n+        sin_embeddings = embeddings.sin()[:, None, None, :]\n+        # Computed embeddings are cast to the dtype of the hidden state inputs\n+        self.cached_rotary_positional_embedding = torch.stack([cos_embeddings, sin_embeddings]).type_as(hidden_states)\n+        return self.cached_rotary_positional_embedding\n+\n+\n+class Wav2Vec2ConformerRelPositionalEmbedding(nn.Module):\n+    \"\"\"Relative positional encoding module.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.max_len = config.max_source_positions\n+        self.d_model = config.hidden_size\n+        self.pe = None\n+        self.extend_pe(torch.tensor(0.0).expand(1, self.max_len))\n+\n+    def extend_pe(self, x):\n+        # Reset the positional encodings\n+        if self.pe is not None:\n+            # self.pe contains both positive and negative parts\n+            # the length of self.pe is 2 * input_len - 1\n+            if self.pe.size(1) >= x.size(1) * 2 - 1:\n+                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n+                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n+                return\n+        # Suppose `i` is the position of query vector and `j` is the\n+        # position of key vector. We use positive relative positions when keys\n+        # are to the left (i>j) and negative relative positions otherwise (i<j).\n+        pe_positive = torch.zeros(x.size(1), self.d_model)\n+        pe_negative = torch.zeros(x.size(1), self.d_model)\n+        position = torch.arange(0, x.size(1), dtype=torch.int64).float().unsqueeze(1)\n+        div_term = torch.exp(\n+            torch.arange(0, self.d_model, 2, dtype=torch.int64).float() * -(math.log(10000.0) / self.d_model)\n+        )\n+        pe_positive[:, 0::2] = torch.sin(position * div_term)\n+        pe_positive[:, 1::2] = torch.cos(position * div_term)\n+        pe_negative[:, 0::2] = torch.sin(-1 * position * div_term)\n+        pe_negative[:, 1::2] = torch.cos(-1 * position * div_term)\n+\n+        # Reverse the order of positive indices and concat both positive and\n+        # negative indices. This is used to support the shifting trick\n+        # as in https://arxiv.org/abs/1901.02860\n+        pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n+        pe_negative = pe_negative[1:].unsqueeze(0)\n+        pe = torch.cat([pe_positive, pe_negative], dim=1)\n+        self.pe = pe.to(device=x.device, dtype=x.dtype)\n+\n+    def forward(self, hidden_states: torch.Tensor):\n+        self.extend_pe(hidden_states)\n+        start_idx = self.pe.size(1) // 2 - hidden_states.size(1) + 1\n+        end_idx = self.pe.size(1) // 2 + hidden_states.size(1)\n+        relative_position_embeddings = self.pe[:, start_idx:end_idx]\n+\n+        return relative_position_embeddings\n+\n+\n+class Wav2Vec2ConformerFeatureEncoder(Wav2Vec2FeatureEncoder):\n+    pass\n+\n+\n+class Wav2Vec2ConformerFeatureProjection(Wav2Vec2FeatureProjection):\n+    pass\n+\n+\n+class Wav2Vec2ConformerFeedForward(Wav2Vec2FeedForward):\n+    pass\n+\n+\n+class Wav2Vec2ConformerConvolutionModule(nn.Module):\n+    \"\"\"Convolution block used in the conformer block\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        if (config.conv_depthwise_kernel_size - 1) % 2 == 1:\n+            raise ValueError(\"`config.conv_depthwise_kernel_size` should be a odd number for 'SAME' padding\")\n+        self.layer_norm = nn.LayerNorm(config.hidden_size)\n+        self.pointwise_conv1 = nn.Conv1d(\n+            config.hidden_size,\n+            2 * config.hidden_size,\n+            kernel_size=1,\n+            stride=1,\n+            padding=0,\n+            bias=False,\n+        )\n+        self.glu = nn.GLU(dim=1)\n+        self.depthwise_conv = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            config.conv_depthwise_kernel_size,\n+            stride=1,\n+            padding=(config.conv_depthwise_kernel_size - 1) // 2,\n+            groups=config.hidden_size,\n+            bias=False,\n+        )\n+        self.batch_norm = nn.BatchNorm1d(config.hidden_size)\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.pointwise_conv2 = nn.Conv1d(\n+            config.hidden_size,\n+            config.hidden_size,\n+            kernel_size=1,\n+            stride=1,\n+            padding=0,\n+            bias=False,\n+        )\n+        self.dropout = nn.Dropout(config.conformer_conv_dropout)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.layer_norm(hidden_states)\n+        # exchange the temporal dimension and the feature dimension\n+        hidden_states = hidden_states.transpose(1, 2)\n+\n+        # GLU mechanism\n+        # => (batch, 2*channel, dim)\n+        hidden_states = self.pointwise_conv1(hidden_states)\n+        # => (batch, channel, dim)\n+        hidden_states = self.glu(hidden_states)\n+\n+        # 1D Depthwise Conv\n+        hidden_states = self.depthwise_conv(hidden_states)\n+        hidden_states = self.batch_norm(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+\n+        hidden_states = self.pointwise_conv2(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return hidden_states\n+\n+\n+class Wav2Vec2ConformerSelfAttention(nn.Module):\n+    \"\"\"Construct an Wav2Vec2ConformerSelfAttention object.\n+    Can be enhanced with rotary or relative position embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.head_size = config.hidden_size // config.num_attention_heads\n+        self.num_heads = config.num_attention_heads\n+        self.position_embeddings_type = config.position_embeddings_type\n+\n+        self.linear_q = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.linear_k = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.linear_v = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.linear_out = nn.Linear(config.hidden_size, config.hidden_size)\n+\n+        self.dropout = nn.Dropout(p=config.attention_dropout)\n+\n+        if self.position_embeddings_type == \"relative\":\n+            # linear transformation for positional encoding\n+            self.linear_pos = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n+            # these two learnable bias are used in matrix c and matrix d\n+            # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n+            self.pos_bias_u = nn.Parameter(torch.zeros(self.num_heads, self.head_size))\n+            self.pos_bias_v = nn.Parameter(torch.zeros(self.num_heads, self.head_size))\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        relative_position_embeddings: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        # self-attention mechanism\n+        batch_size, sequence_length, hidden_size = hidden_states.size()\n+\n+        # make sure query/key states can be != value states\n+        query_key_states = hidden_states\n+        value_states = hidden_states\n+\n+        if self.position_embeddings_type == \"rotary\":\n+            if relative_position_embeddings is None:\n+                raise ValueError(\n+                    \"`relative_position_embeddings` has to be defined when `self.position_embeddings_type == 'rotary'\"\n+                )\n+            query_key_states = self._apply_rotary_embedding(query_key_states, relative_position_embeddings)\n+\n+        # project query_key_states and value_states\n+        query = self.linear_q(query_key_states).view(batch_size, -1, self.num_heads, self.head_size)\n+        key = self.linear_k(query_key_states).view(batch_size, -1, self.num_heads, self.head_size)\n+        value = self.linear_v(value_states).view(batch_size, -1, self.num_heads, self.head_size)\n+\n+        # => (batch, head, time1, d_k)\n+        query = query.transpose(1, 2)\n+        key = key.transpose(1, 2)\n+        value = value.transpose(1, 2)\n+\n+        if self.position_embeddings_type == \"relative\":\n+            if relative_position_embeddings is None:\n+                raise ValueError(\n+                    \"`relative_position_embeddings` has to be defined when `self.position_embeddings_type ==\"\n+                    \" 'relative'\"\n+                )\n+            # apply relative_position_embeddings to qk scores\n+            # as proposed in Transformer_XL: https://arxiv.org/abs/1901.02860\n+            scores = self._apply_relative_embeddings(\n+                query=query, key=key, relative_position_embeddings=relative_position_embeddings\n+            )\n+        else:\n+            scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_size)\n+\n+        # apply attention_mask if necessary\n+        if attention_mask is not None:\n+            scores = scores + attention_mask\n+\n+        # => (batch, head, time1, time2)\n+        probs = torch.softmax(scores, dim=-1)\n+        probs = self.dropout(probs)\n+\n+        # => (batch, head, time1, d_k)\n+        hidden_states = torch.matmul(probs, value)\n+\n+        # => (batch, time1, hidden_size)\n+        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.head_size)\n+        hidden_states = self.linear_out(hidden_states)\n+\n+        return hidden_states, probs\n+\n+    def _apply_rotary_embedding(self, hidden_states, relative_position_embeddings):\n+        batch_size, sequence_length, hidden_size = hidden_states.size()\n+        hidden_states = hidden_states.view(batch_size, sequence_length, self.num_heads, self.head_size)\n+\n+        cos = relative_position_embeddings[0, :sequence_length, ...]\n+        sin = relative_position_embeddings[1, :sequence_length, ...]\n+\n+        # rotate hidden_states with rotary embeddings\n+        hidden_states = hidden_states.transpose(0, 1)\n+        rotated_states_begin = hidden_states[..., : self.head_size // 2]\n+        rotated_states_end = hidden_states[..., self.head_size // 2 :]\n+        rotated_states = torch.cat((-rotated_states_end, rotated_states_begin), dim=rotated_states_begin.ndim - 1)\n+        hidden_states = (hidden_states * cos) + (rotated_states * sin)\n+        hidden_states = hidden_states.transpose(0, 1)\n+\n+        hidden_states = hidden_states.view(batch_size, sequence_length, self.num_heads * self.head_size)\n+\n+        return hidden_states\n+\n+    def _apply_relative_embeddings(self, query, key, relative_position_embeddings):\n+        # 1. project positional embeddings\n+        # => (batch, head, 2*time1-1, d_k)\n+        proj_relative_position_embeddings = self.linear_pos(relative_position_embeddings)\n+        proj_relative_position_embeddings = proj_relative_position_embeddings.view(\n+            relative_position_embeddings.size(0), -1, self.num_heads, self.head_size\n+        )\n+        proj_relative_position_embeddings = proj_relative_position_embeddings.transpose(1, 2)\n+        proj_relative_position_embeddings = proj_relative_position_embeddings.transpose(2, 3)\n+\n+        # 2. Add bias to query\n+        # => (batch, head, time1, d_k)\n+        query = query.transpose(1, 2)\n+        q_with_bias_u = (query + self.pos_bias_u).transpose(1, 2)\n+        q_with_bias_v = (query + self.pos_bias_v).transpose(1, 2)\n+\n+        # 3. attention score: first compute matrix a and matrix c\n+        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n+        # => (batch, head, time1, time2)\n+        scores_ac = torch.matmul(q_with_bias_u, key.transpose(-2, -1))\n+\n+        # 4. then compute matrix b and matrix d\n+        # => (batch, head, time1, 2*time1-1)\n+        scores_bd = torch.matmul(q_with_bias_v, proj_relative_position_embeddings)\n+\n+        # 5. shift matrix b and matrix d\n+        zero_pad = torch.zeros((*scores_bd.size()[:3], 1), device=scores_bd.device, dtype=scores_bd.dtype)\n+        scores_bd_padded = torch.cat([zero_pad, scores_bd], dim=-1)\n+        scores_bd_padded_shape = scores_bd.size()[:2] + (scores_bd.shape[3] + 1, scores_bd.shape[2])\n+        scores_bd_padded = scores_bd_padded.view(*scores_bd_padded_shape)\n+        scores_bd = scores_bd_padded[:, :, 1:].view_as(scores_bd)\n+        scores_bd = scores_bd[:, :, :, : scores_bd.size(-1) // 2 + 1]\n+\n+        # 6. sum matrices\n+        # => (batch, head, time1, time2)\n+        scores = (scores_ac + scores_bd) / math.sqrt(self.head_size)\n+\n+        return scores\n+\n+\n+class Wav2Vec2ConformerEncoderLayer(nn.Module):\n+    \"\"\"Conformer block based on https://arxiv.org/abs/2005.08100.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        embed_dim = config.hidden_size\n+        dropout = config.attention_dropout\n+\n+        # Feed-forward 1\n+        self.ffn1_layer_norm = nn.LayerNorm(embed_dim)\n+        self.ffn1 = Wav2Vec2ConformerFeedForward(config)\n+\n+        # Self-Attention\n+        self.self_attn_layer_norm = nn.LayerNorm(embed_dim)\n+        self.self_attn_dropout = nn.Dropout(dropout)\n+        self.self_attn = Wav2Vec2ConformerSelfAttention(config)\n+\n+        # Conformer Convolution\n+        self.conv_module = Wav2Vec2ConformerConvolutionModule(config)\n+\n+        # Feed-forward 2\n+        self.ffn2_layer_norm = nn.LayerNorm(embed_dim)\n+        self.ffn2 = Wav2Vec2ConformerFeedForward(config)\n+        self.final_layer_norm = nn.LayerNorm(embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        relative_position_embeddings: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ):\n+        hidden_states = hidden_states\n+\n+        # 1. Feed-Forward 1 layer\n+        residual = hidden_states\n+        hidden_states = self.ffn1_layer_norm(hidden_states)\n+        hidden_states = self.ffn1(hidden_states)\n+        hidden_states = hidden_states * 0.5 + residual\n+        residual = hidden_states\n+\n+        # 2. Self-Attention layer\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+        hidden_states, attn_weigts = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            relative_position_embeddings=relative_position_embeddings,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = self.self_attn_dropout(hidden_states)\n+        hidden_states = hidden_states + residual\n+\n+        # 3. Convolutional Layer\n+        residual = hidden_states\n+        hidden_states = self.conv_module(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # 4. Feed-Forward 2 Layer\n+        residual = hidden_states\n+        hidden_states = self.ffn2_layer_norm(hidden_states)\n+        hidden_states = self.ffn2(hidden_states)\n+        hidden_states = hidden_states * 0.5 + residual\n+        hidden_states = self.final_layer_norm(hidden_states)\n+\n+        return hidden_states, attn_weigts\n+\n+\n+class Wav2Vec2ConformerEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+\n+        if config.position_embeddings_type == \"relative\":\n+            self.embed_positions = Wav2Vec2ConformerRelPositionalEmbedding(config)\n+        elif config.position_embeddings_type == \"rotary\":\n+            self.embed_positions = Wav2Vec2ConformerRotaryPositionalEmbedding(config)\n+        else:\n+            self.embed_positions = None\n+\n+        self.pos_conv_embed = Wav2Vec2ConformerPositionalConvEmbedding(config)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layers = nn.ModuleList([Wav2Vec2ConformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask=None,\n+        output_attentions=False,\n+        output_hidden_states=False,\n+        return_dict=True,\n+    ):\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        if attention_mask is not None:\n+            # make sure padded tokens output 0\n+            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_attention_mask] = 0.0\n+\n+            # extend attention_mask\n+            attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n+            attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n+            attention_mask = attention_mask.expand(\n+                attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n+            )\n+\n+        hidden_states = self.dropout(hidden_states)\n+\n+        if self.embed_positions is not None:\n+            relative_position_embeddings = self.embed_positions(hidden_states)\n+        else:\n+            relative_position_embeddings = None\n+\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n+\n+        for i, layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n+            dropout_probability = torch.rand([])\n+\n+            skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n+                if self.gradient_checkpointing and self.training:\n+                    layer_outputs = self._gradient_checkpointing_func(\n+                        layer.__call__,\n+                        hidden_states,\n+                        attention_mask,\n+                        relative_position_embeddings,\n+                        output_attentions,\n+                    )\n+                else:\n+                    layer_outputs = layer(\n+                        hidden_states,\n+                        attention_mask=attention_mask,\n+                        relative_position_embeddings=relative_position_embeddings,\n+                        output_attentions=output_attentions,\n+                    )\n+                hidden_states = layer_outputs[0]\n+\n+            if skip_the_layer:\n+                layer_outputs = (None, None)\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        hidden_states = self.layer_norm(hidden_states)\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+class Wav2Vec2ConformerGumbelVectorQuantizer(Wav2Vec2GumbelVectorQuantizer):\n+    pass\n+\n+\n+class Wav2Vec2ConformerAdapter(Wav2Vec2Adapter):\n+    pass\n+\n+\n+class Wav2Vec2ConformerAdapterLayer(Wav2Vec2AdapterLayer):\n+    pass\n+\n+\n+class Wav2Vec2ConformerPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = Wav2Vec2ConformerConfig\n+    base_model_prefix = \"wav2vec2_conformer\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        # Wav2Vec2ForPreTraining last 2 linear layers need standard Linear init.\n+        if isinstance(module, Wav2Vec2ConformerForPreTraining):\n+            module.project_hid.reset_parameters()\n+            module.project_q.reset_parameters()\n+            module.project_hid._is_hf_initialized = True\n+            module.project_q._is_hf_initialized = True\n+        # gumbel softmax requires special init\n+        elif isinstance(module, Wav2Vec2ConformerGumbelVectorQuantizer):\n+            module.weight_proj.weight.data.normal_(mean=0.0, std=1)\n+            module.weight_proj.bias.data.zero_()\n+            nn.init.uniform_(module.codevectors)\n+        elif isinstance(module, Wav2Vec2ConformerSelfAttention):\n+            if hasattr(module, \"pos_bias_u\"):\n+                nn.init.xavier_uniform_(module.pos_bias_u)\n+            if hasattr(module, \"pos_bias_v\"):\n+                nn.init.xavier_uniform_(module.pos_bias_v)\n+        elif isinstance(module, Wav2Vec2ConformerPositionalConvEmbedding):\n+            nn.init.normal_(\n+                module.conv.weight,\n+                mean=0,\n+                std=2 * math.sqrt(1 / (module.conv.kernel_size[0] * module.conv.in_channels)),\n+            )\n+            nn.init.constant_(module.conv.bias, 0)\n+        elif isinstance(module, Wav2Vec2ConformerFeatureProjection):\n+            k = math.sqrt(1 / module.projection.in_features)\n+            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n+            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n+        elif isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            nn.init.kaiming_normal_(module.weight)\n+\n+            if module.bias is not None:\n+                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n+                nn.init.uniform_(module.bias, a=-k, b=k)\n+\n+    def _get_feat_extract_output_lengths(\n+        self, input_lengths: Union[torch.LongTensor, int], add_adapter: Optional[bool] = None\n+    ):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+\n+        add_adapter = self.config.add_adapter if add_adapter is None else add_adapter\n+\n+        def _conv_out_length(input_length, kernel_size, stride):\n+            # 1D convolutional layer output length formula taken\n+            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n+            return torch.div(input_length - kernel_size, stride, rounding_mode=\"floor\") + 1\n+\n+        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):\n+            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n+\n+        if add_adapter:\n+            for _ in range(self.config.num_adapter_layers):\n+                input_lengths = _conv_out_length(input_lengths, 1, self.config.adapter_stride)\n+\n+        return input_lengths\n+\n+    def _get_feature_vector_attention_mask(\n+        self, feature_vector_length: int, attention_mask: torch.LongTensor, add_adapter=None\n+    ):\n+        # Effectively attention_mask.sum(-1), but not inplace to be able to run\n+        # on inference mode.\n+        non_padded_lengths = attention_mask.cumsum(dim=-1)[:, -1]\n+\n+        output_lengths = self._get_feat_extract_output_lengths(non_padded_lengths, add_adapter=add_adapter)\n+        output_lengths = output_lengths.to(torch.long)\n+\n+        batch_size = attention_mask.shape[0]\n+\n+        attention_mask = torch.zeros(\n+            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n+        )\n+        # these two operations makes sure that all values before the output lengths idxs are attended to\n+        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n+        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n+        return attention_mask\n+\n+\n+WAV2VEC2_CONFORMER_START_DOCSTRING = None  # will be automatically redefined\n+\n+WAV2VEC2_CONFORMER_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n+            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n+            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n+        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            <Tip warning={true}>\n+\n+            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==\n+            True`. For all models whose processor has `config.return_attention_mask == False`, such as\n+            [wav2vec2-conformer-rel-pos-large](https://huggingface.co/facebook/wav2vec2-conformer-rel-pos-large),\n+            `attention_mask` should **not** be passed to avoid degraded performance when doing batched inference. For\n+            such models `input_values` should simply be padded with 0 and passed without `attention_mask`. Be aware\n+            that these models also yield slightly different results depending on whether `input_values` is padded or\n+            not.\n+\n+            </Tip>\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+Wav2Vec2ConformerBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n+\n+@add_start_docstrings(\n+    \"The bare Wav2Vec2Conformer Model transformer outputting raw hidden-states without any specific head on top.\",\n+    WAV2VEC2_CONFORMER_START_DOCSTRING,\n+)\n+class Wav2Vec2ConformerModel(Wav2Vec2ConformerPreTrainedModel, Wav2Vec2Model):\n+    def __init__(self, config: Wav2Vec2ConformerConfig):\n+        Wav2Vec2ConformerPreTrainedModel.__init__(config)\n+        self.config = config\n+        self.feature_extractor = Wav2Vec2ConformerFeatureEncoder(config)\n+        self.feature_projection = Wav2Vec2ConformerFeatureProjection(config)\n+\n+        # model only needs masking vector if mask prob is > 0.0\n+        if config.mask_time_prob > 0.0 or config.mask_feature_prob > 0.0:\n+            self.masked_spec_embed = nn.Parameter(torch.Tensor(config.hidden_size).uniform_())\n+\n+        self.encoder = Wav2Vec2ConformerEncoder(config)\n+\n+        self.adapter = Wav2Vec2ConformerAdapter(config) if config.add_adapter else None\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_CONFORMER_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=Wav2Vec2ConformerBaseModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Wav2Vec2Conformer Model with a quantizer and `VQ` head on top.\"\"\", WAV2VEC2_CONFORMER_START_DOCSTRING\n+)\n+class Wav2Vec2ConformerForPreTraining(Wav2Vec2ForPreTraining):\n+    def __init__(self, config: Wav2Vec2ConformerConfig):\n+        super().__init__(config)\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_CONFORMER_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Wav2Vec2ConformerForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(self, **super_kwargs) -> Union[Tuple, Wav2Vec2ConformerForPreTrainingOutput]:\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Wav2Vec2Conformer Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n+    WAV2VEC2_CONFORMER_START_DOCSTRING,\n+)\n+class Wav2Vec2ConformerForCTC(Wav2Vec2ForCTC):\n+    def __init__(self, config, target_lang: Optional[str] = None):\n+        super().__init__(config)\n+\n+    def tie_weights(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n+\n+    def freeze_base_model(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_CONFORMER_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=CausalLMOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_CTC_EXPECTED_OUTPUT,\n+        expected_loss=_CTC_EXPECTED_LOSS,\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Wav2Vec2Conformer Model with a sequence classification head on top (a linear layer over the pooled output) for\n+    tasks like SUPERB Keyword Spotting.\n+    \"\"\",\n+    WAV2VEC2_CONFORMER_START_DOCSTRING,\n+)\n+class Wav2Vec2ConformerForSequenceClassification(Wav2Vec2ForSequenceClassification):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_CONFORMER_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=SequenceClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Wav2Vec2Conformer Model with a frame classification head on top for tasks like Speaker Diarization.\n+    \"\"\",\n+    WAV2VEC2_CONFORMER_START_DOCSTRING,\n+)\n+class Wav2Vec2ConformerForAudioFrameClassification(Wav2Vec2ForAudioFrameClassification):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_CONFORMER_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Wav2Vec2Conformer Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n+    \"\"\",\n+    WAV2VEC2_CONFORMER_START_DOCSTRING,\n+)\n+class Wav2Vec2ConformerForXVector(Wav2Vec2ForXVector):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+    def freeze_feature_extractor(self):\n+        raise AttributeError(\"Not needed for Wav2Vec2Conformer\")\n+\n+    @add_start_docstrings_to_model_forward(WAV2VEC2_CONFORMER_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=XVectorOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"Wav2Vec2ConformerForAudioFrameClassification\",\n+    \"Wav2Vec2ConformerForCTC\",\n+    \"Wav2Vec2ConformerForPreTraining\",\n+    \"Wav2Vec2ConformerForSequenceClassification\",\n+    \"Wav2Vec2ConformerForXVector\",\n+    \"Wav2Vec2ConformerModel\",\n+    \"Wav2Vec2ConformerPreTrainedModel\",\n+]"
        },
        {
            "sha": "1c3c09d1a70f7cf5d732839b0ca403c58aad1275",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 385,
            "deletions": 425,
            "changes": 810,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -1,28 +1,17 @@\n-# coding=utf-8\n-# Copyright 2021 The Fairseq Authors, Microsoft Research, and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch WavLM model.\"\"\"\n-\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/wavlm/modular_wavlm.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_wavlm.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n import warnings\n from typing import Optional, Tuple, Union\n \n import numpy as np\n import torch\n+import torch.nn as nn\n import torch.nn.functional as F\n-import torch.utils.checkpoint\n-from torch import nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n@@ -49,225 +38,22 @@\n \n logger = logging.get_logger(__name__)\n \n-\n-_HIDDEN_STATES_START_POSITION = 2\n-\n-# General docstring\n-_CONFIG_FOR_DOC = \"WavLMConfig\"\n-\n-# Base docstring\n _CHECKPOINT_FOR_DOC = \"patrickvonplaten/wavlm-libri-clean-100h-base-plus\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n-\n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = \"'mister quilter is the aposle of the middle classes and we are glad to welcome his gospel'\"\n-_CTC_EXPECTED_LOSS = 12.51\n-\n-# Frame class docstring\n-_FRAME_CLASS_CHECKPOINT = \"microsoft/wavlm-base-plus-sd\"\n-_FRAME_EXPECTED_OUTPUT = [0, 0]\n-\n-# Speaker Verification docstring\n-_XVECTOR_CHECKPOINT = \"microsoft/wavlm-base-plus-sv\"\n-_XVECTOR_EXPECTED_OUTPUT = 0.97\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2._compute_mask_indices\n-def _compute_mask_indices(\n-    shape: Tuple[int, int],\n-    mask_prob: float,\n-    mask_length: int,\n-    attention_mask: Optional[torch.LongTensor] = None,\n-    min_masks: int = 0,\n-) -> np.ndarray:\n-    \"\"\"\n-    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n-    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n-    CPU as part of the preprocessing during training.\n-\n-    Args:\n-        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n-               the first element is the batch size and the second element is the length of the axis to span.\n-        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n-                    independently generated mask spans of length `mask_length` is computed by\n-                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n-                    actual percentage will be smaller.\n-        mask_length: size of the mask\n-        min_masks: minimum number of masked spans\n-        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n-                        each batch dimension.\n-    \"\"\"\n-    batch_size, sequence_length = shape\n-\n-    if mask_length < 1:\n-        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n-\n-    if mask_length > sequence_length:\n-        raise ValueError(\n-            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n-            f\" and `sequence_length`: {sequence_length}`\"\n-        )\n-\n-    # epsilon is used for probabilistic rounding\n-    epsilon = np.random.rand(1).item()\n-\n-    def compute_num_masked_span(input_length):\n-        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n-        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n-        num_masked_span = max(num_masked_span, min_masks)\n-\n-        # make sure num masked span <= sequence_length\n-        if num_masked_span * mask_length > sequence_length:\n-            num_masked_span = sequence_length // mask_length\n-\n-        # make sure num_masked span is also <= input_length - (mask_length - 1)\n-        if input_length - (mask_length - 1) < num_masked_span:\n-            num_masked_span = max(input_length - (mask_length - 1), 0)\n-\n-        return num_masked_span\n-\n-    # compute number of masked spans in batch\n-    input_lengths = (\n-        attention_mask.detach().sum(-1).tolist()\n-        if attention_mask is not None\n-        else [sequence_length for _ in range(batch_size)]\n-    )\n-\n-    # SpecAugment mask to fill\n-    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n-    spec_aug_mask_idxs = []\n-\n-    max_num_masked_span = compute_num_masked_span(sequence_length)\n-\n-    if max_num_masked_span == 0:\n-        return spec_aug_mask\n-\n-    for input_length in input_lengths:\n-        # compute num of masked spans for this input\n-        num_masked_span = compute_num_masked_span(input_length)\n-\n-        # get random indices to mask\n-        spec_aug_mask_idx = np.random.choice(\n-            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n-        )\n-\n-        # pick first sampled index that will serve as a dummy index to pad vector\n-        # to ensure same dimension for all batches due to probabilistic rounding\n-        # Picking first sample just pads those vectors twice.\n-        if len(spec_aug_mask_idx) == 0:\n-            # this case can only happen if `input_length` is strictly smaller then\n-            # `sequence_length` in which case the last token has to be a padding\n-            # token which we can use as a dummy mask id\n-            dummy_mask_idx = sequence_length - 1\n-        else:\n-            dummy_mask_idx = spec_aug_mask_idx[0]\n-\n-        spec_aug_mask_idx = np.concatenate(\n-            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n-        )\n-        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n-\n-    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n-\n-    # expand masked indices to masked spans\n-    spec_aug_mask_idxs = np.broadcast_to(\n-        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n-\n-    # add offset to the starting indexes so that indexes now create a span\n-    offsets = np.arange(mask_length)[None, None, :]\n-    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n-        batch_size, max_num_masked_span * mask_length\n-    )\n-    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n-\n-    # ensure that we cannot have indices larger than sequence_length\n-    if spec_aug_mask_idxs.max() > sequence_length - 1:\n-        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n-\n-    # scatter indices to mask\n-    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n-\n-    return spec_aug_mask\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2NoLayerNormConvLayer with Wav2Vec2->WavLM\n-class WavLMNoLayerNormConvLayer(nn.Module):\n-    def __init__(self, config, layer_id=0):\n-        super().__init__()\n-        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n-        self.out_conv_dim = config.conv_dim[layer_id]\n-\n-        self.conv = nn.Conv1d(\n-            self.in_conv_dim,\n-            self.out_conv_dim,\n-            kernel_size=config.conv_kernel[layer_id],\n-            stride=config.conv_stride[layer_id],\n-            bias=config.conv_bias,\n-        )\n-        self.activation = ACT2FN[config.feat_extract_activation]\n-\n-    def forward(self, hidden_states):\n-        hidden_states = self.conv(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2LayerNormConvLayer with Wav2Vec2->WavLM\n-class WavLMLayerNormConvLayer(nn.Module):\n-    def __init__(self, config, layer_id=0):\n-        super().__init__()\n-        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n-        self.out_conv_dim = config.conv_dim[layer_id]\n \n-        self.conv = nn.Conv1d(\n-            self.in_conv_dim,\n-            self.out_conv_dim,\n-            kernel_size=config.conv_kernel[layer_id],\n-            stride=config.conv_stride[layer_id],\n-            bias=config.conv_bias,\n-        )\n-        self.layer_norm = nn.LayerNorm(self.out_conv_dim, elementwise_affine=True)\n-        self.activation = ACT2FN[config.feat_extract_activation]\n-\n-    def forward(self, hidden_states):\n-        hidden_states = self.conv(hidden_states)\n-\n-        hidden_states = hidden_states.transpose(-2, -1)\n-        hidden_states = self.layer_norm(hidden_states)\n-        hidden_states = hidden_states.transpose(-2, -1)\n-\n-        hidden_states = self.activation(hidden_states)\n-        return hidden_states\n+_CONFIG_FOR_DOC = \"WavLMConfig\"\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2GroupNormConvLayer with Wav2Vec2->WavLM\n-class WavLMGroupNormConvLayer(nn.Module):\n-    def __init__(self, config, layer_id=0):\n+class WavLMSamePadLayer(nn.Module):\n+    def __init__(self, num_conv_pos_embeddings):\n         super().__init__()\n-        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n-        self.out_conv_dim = config.conv_dim[layer_id]\n-\n-        self.conv = nn.Conv1d(\n-            self.in_conv_dim,\n-            self.out_conv_dim,\n-            kernel_size=config.conv_kernel[layer_id],\n-            stride=config.conv_stride[layer_id],\n-            bias=config.conv_bias,\n-        )\n-        self.activation = ACT2FN[config.feat_extract_activation]\n-\n-        self.layer_norm = nn.GroupNorm(num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)\n+        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n \n     def forward(self, hidden_states):\n-        hidden_states = self.conv(hidden_states)\n-        hidden_states = self.layer_norm(hidden_states)\n-        hidden_states = self.activation(hidden_states)\n+        if self.num_pad_remove > 0:\n+            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->WavLM\n class WavLMPositionalConvEmbedding(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -313,75 +99,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2SamePadLayer with Wav2Vec2->WavLM\n-class WavLMSamePadLayer(nn.Module):\n-    def __init__(self, num_conv_pos_embeddings):\n-        super().__init__()\n-        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n-\n-    def forward(self, hidden_states):\n-        if self.num_pad_remove > 0:\n-            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n-        return hidden_states\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureEncoder with Wav2Vec2->WavLM\n-class WavLMFeatureEncoder(nn.Module):\n-    \"\"\"Construct the features from raw audio waveform\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-\n-        if config.feat_extract_norm == \"group\":\n-            conv_layers = [WavLMGroupNormConvLayer(config, layer_id=0)] + [\n-                WavLMNoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n-            ]\n-        elif config.feat_extract_norm == \"layer\":\n-            conv_layers = [WavLMLayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)]\n-        else:\n-            raise ValueError(\n-                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n-            )\n-        self.conv_layers = nn.ModuleList(conv_layers)\n-        self.gradient_checkpointing = False\n-        self._requires_grad = True\n-\n-    def _freeze_parameters(self):\n-        for param in self.parameters():\n-            param.requires_grad = False\n-        self._requires_grad = False\n-\n-    def forward(self, input_values):\n-        hidden_states = input_values[:, None]\n-\n-        # make sure hidden_states require grad for gradient_checkpointing\n-        if self._requires_grad and self.training:\n-            hidden_states.requires_grad = True\n-\n-        for conv_layer in self.conv_layers:\n-            if self._requires_grad and self.gradient_checkpointing and self.training:\n-                hidden_states = self._gradient_checkpointing_func(\n-                    conv_layer.__call__,\n-                    hidden_states,\n-                )\n-            else:\n-                hidden_states = conv_layer(hidden_states)\n-\n-        return hidden_states\n-\n-\n-class WavLMFeatureExtractor(WavLMFeatureEncoder):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        warnings.warn(\n-            f\"The class `{self.__class__.__name__}` has been depreciated \"\n-            \"and will be removed in Transformers v5. \"\n-            f\"Use `{self.__class__.__bases__[0].__name__}` instead.\",\n-            FutureWarning,\n-        )\n-\n-\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeatureProjection with Wav2Vec2->WavLM\n class WavLMFeatureProjection(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -563,7 +280,6 @@ def _relative_positions_bucket(self, relative_positions: torch.FloatTensor) -> t\n         return relative_buckets\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2FeedForward with Wav2Vec2->WavLM\n class WavLMFeedForward(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -869,41 +585,264 @@ def _compute_perplexity(probs):\n     def forward(self, hidden_states):\n         batch_size, sequence_length, hidden_size = hidden_states.shape\n \n-        # project to codevector dim\n-        hidden_states = self.weight_proj(hidden_states)\n-        hidden_states = hidden_states.view(batch_size * sequence_length * self.num_groups, -1)\n+        # project to codevector dim\n+        hidden_states = self.weight_proj(hidden_states)\n+        hidden_states = hidden_states.view(batch_size * sequence_length * self.num_groups, -1)\n+\n+        if self.training:\n+            # sample code vector probs via gumbel in differentiateable way\n+            codevector_probs = nn.functional.gumbel_softmax(hidden_states.float(), tau=self.temperature, hard=True)\n+            codevector_probs = codevector_probs.type_as(hidden_states)\n+\n+            # compute perplexity\n+            codevector_soft_dist = torch.softmax(\n+                hidden_states.view(batch_size * sequence_length, self.num_groups, -1).float(), dim=-1\n+            )\n+            perplexity = self._compute_perplexity(codevector_soft_dist)\n+        else:\n+            # take argmax in non-differentiable way\n+            # comptute hard codevector distribution (one hot)\n+            codevector_idx = hidden_states.argmax(dim=-1)\n+            codevector_probs = hidden_states.new_zeros(*hidden_states.shape).scatter_(\n+                -1, codevector_idx.view(-1, 1), 1.0\n+            )\n+            codevector_probs = codevector_probs.view(batch_size * sequence_length, self.num_groups, -1)\n+\n+            perplexity = self._compute_perplexity(codevector_probs)\n+\n+        codevector_probs = codevector_probs.view(batch_size * sequence_length, -1)\n+        # use probs to retrieve codevectors\n+        codevectors_per_group = codevector_probs.unsqueeze(-1) * self.codevectors\n+        codevectors = codevectors_per_group.view(batch_size * sequence_length, self.num_groups, self.num_vars, -1)\n+        codevectors = codevectors.sum(-2).view(batch_size, sequence_length, -1)\n+\n+        return codevectors, perplexity\n+\n+\n+class WavLMPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = WavLMConfig\n+    base_model_prefix = \"wavlm\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        # gumbel softmax requires special init\n+        if isinstance(module, WavLMGumbelVectorQuantizer):\n+            module.weight_proj.weight.data.normal_(mean=0.0, std=1)\n+            module.weight_proj.bias.data.zero_()\n+            nn.init.uniform_(module.codevectors)\n+        elif isinstance(module, WavLMPositionalConvEmbedding):\n+            nn.init.normal_(\n+                module.conv.weight,\n+                mean=0,\n+                std=2 * math.sqrt(1 / (module.conv.kernel_size[0] * module.conv.in_channels)),\n+            )\n+            nn.init.constant_(module.conv.bias, 0)\n+        elif isinstance(module, WavLMFeatureProjection):\n+            k = math.sqrt(1 / module.projection.in_features)\n+            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n+            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n+        elif isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            nn.init.kaiming_normal_(module.weight)\n+\n+            if module.bias is not None:\n+                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n+                nn.init.uniform_(module.bias, a=-k, b=k)\n+\n+    def _get_feat_extract_output_lengths(\n+        self, input_lengths: Union[torch.LongTensor, int], add_adapter: Optional[bool] = None\n+    ):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+\n+        add_adapter = self.config.add_adapter if add_adapter is None else add_adapter\n+\n+        def _conv_out_length(input_length, kernel_size, stride):\n+            # 1D convolutional layer output length formula taken\n+            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n+            return torch.div(input_length - kernel_size, stride, rounding_mode=\"floor\") + 1\n+\n+        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):\n+            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n+\n+        if add_adapter:\n+            for _ in range(self.config.num_adapter_layers):\n+                input_lengths = _conv_out_length(input_lengths, 1, self.config.adapter_stride)\n+\n+        return input_lengths\n+\n+    def _get_feature_vector_attention_mask(\n+        self, feature_vector_length: int, attention_mask: torch.LongTensor, add_adapter=None\n+    ):\n+        # Effectively attention_mask.sum(-1), but not inplace to be able to run\n+        # on inference mode.\n+        non_padded_lengths = attention_mask.cumsum(dim=-1)[:, -1]\n+\n+        output_lengths = self._get_feat_extract_output_lengths(non_padded_lengths, add_adapter=add_adapter)\n+        output_lengths = output_lengths.to(torch.long)\n+\n+        batch_size = attention_mask.shape[0]\n+\n+        attention_mask = torch.zeros(\n+            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n+        )\n+        # these two operations makes sure that all values before the output lengths idxs are attended to\n+        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n+        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n+        return attention_mask\n+\n+\n+class WavLMNoLayerNormConvLayer(nn.Module):\n+    def __init__(self, config, layer_id=0):\n+        super().__init__()\n+        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n+        self.out_conv_dim = config.conv_dim[layer_id]\n+\n+        self.conv = nn.Conv1d(\n+            self.in_conv_dim,\n+            self.out_conv_dim,\n+            kernel_size=config.conv_kernel[layer_id],\n+            stride=config.conv_stride[layer_id],\n+            bias=config.conv_bias,\n+        )\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        return hidden_states\n+\n+\n+class WavLMLayerNormConvLayer(nn.Module):\n+    def __init__(self, config, layer_id=0):\n+        super().__init__()\n+        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n+        self.out_conv_dim = config.conv_dim[layer_id]\n+\n+        self.conv = nn.Conv1d(\n+            self.in_conv_dim,\n+            self.out_conv_dim,\n+            kernel_size=config.conv_kernel[layer_id],\n+            stride=config.conv_stride[layer_id],\n+            bias=config.conv_bias,\n+        )\n+        self.layer_norm = nn.LayerNorm(self.out_conv_dim, elementwise_affine=True)\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+\n+        hidden_states = hidden_states.transpose(-2, -1)\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = hidden_states.transpose(-2, -1)\n+\n+        hidden_states = self.activation(hidden_states)\n+        return hidden_states\n+\n+\n+class WavLMGroupNormConvLayer(nn.Module):\n+    def __init__(self, config, layer_id=0):\n+        super().__init__()\n+        self.in_conv_dim = config.conv_dim[layer_id - 1] if layer_id > 0 else 1\n+        self.out_conv_dim = config.conv_dim[layer_id]\n+\n+        self.conv = nn.Conv1d(\n+            self.in_conv_dim,\n+            self.out_conv_dim,\n+            kernel_size=config.conv_kernel[layer_id],\n+            stride=config.conv_stride[layer_id],\n+            bias=config.conv_bias,\n+        )\n+        self.activation = ACT2FN[config.feat_extract_activation]\n+\n+        self.layer_norm = nn.GroupNorm(num_groups=self.out_conv_dim, num_channels=self.out_conv_dim, affine=True)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        return hidden_states\n+\n+\n+class WavLMFeatureEncoder(nn.Module):\n+    \"\"\"Construct the features from raw audio waveform\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        if config.feat_extract_norm == \"group\":\n+            conv_layers = [WavLMGroupNormConvLayer(config, layer_id=0)] + [\n+                WavLMNoLayerNormConvLayer(config, layer_id=i + 1) for i in range(config.num_feat_extract_layers - 1)\n+            ]\n+        elif config.feat_extract_norm == \"layer\":\n+            conv_layers = [WavLMLayerNormConvLayer(config, layer_id=i) for i in range(config.num_feat_extract_layers)]\n+        else:\n+            raise ValueError(\n+                f\"`config.feat_extract_norm` is {config.feat_extract_norm}, but has to be one of ['group', 'layer']\"\n+            )\n+        self.conv_layers = nn.ModuleList(conv_layers)\n+        self.gradient_checkpointing = False\n+        self._requires_grad = True\n+\n+    def _freeze_parameters(self):\n+        for param in self.parameters():\n+            param.requires_grad = False\n+        self._requires_grad = False\n+\n+    def forward(self, input_values):\n+        hidden_states = input_values[:, None]\n+\n+        # make sure hidden_states require grad for gradient_checkpointing\n+        if self._requires_grad and self.training:\n+            hidden_states.requires_grad = True\n+\n+        for conv_layer in self.conv_layers:\n+            if self._requires_grad and self.gradient_checkpointing and self.training:\n+                hidden_states = self._gradient_checkpointing_func(\n+                    conv_layer.__call__,\n+                    hidden_states,\n+                )\n+            else:\n+                hidden_states = conv_layer(hidden_states)\n \n-        if self.training:\n-            # sample code vector probs via gumbel in differentiateable way\n-            codevector_probs = nn.functional.gumbel_softmax(hidden_states.float(), tau=self.temperature, hard=True)\n-            codevector_probs = codevector_probs.type_as(hidden_states)\n+        return hidden_states\n \n-            # compute perplexity\n-            codevector_soft_dist = torch.softmax(\n-                hidden_states.view(batch_size * sequence_length, self.num_groups, -1).float(), dim=-1\n-            )\n-            perplexity = self._compute_perplexity(codevector_soft_dist)\n-        else:\n-            # take argmax in non-differentiable way\n-            # comptute hard codevector distribution (one hot)\n-            codevector_idx = hidden_states.argmax(dim=-1)\n-            codevector_probs = hidden_states.new_zeros(*hidden_states.shape).scatter_(\n-                -1, codevector_idx.view(-1, 1), 1.0\n-            )\n-            codevector_probs = codevector_probs.view(batch_size * sequence_length, self.num_groups, -1)\n \n-            perplexity = self._compute_perplexity(codevector_probs)\n+class WavLMAdapterLayer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.conv = nn.Conv1d(\n+            config.output_hidden_size,\n+            2 * config.output_hidden_size,\n+            config.adapter_kernel_size,\n+            stride=config.adapter_stride,\n+            padding=1,\n+        )\n \n-        codevector_probs = codevector_probs.view(batch_size * sequence_length, -1)\n-        # use probs to retrieve codevectors\n-        codevectors_per_group = codevector_probs.unsqueeze(-1) * self.codevectors\n-        codevectors = codevectors_per_group.view(batch_size * sequence_length, self.num_groups, self.num_vars, -1)\n-        codevectors = codevectors.sum(-2).view(batch_size, sequence_length, -1)\n+    def forward(self, hidden_states):\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = nn.functional.glu(hidden_states, dim=1)\n \n-        return codevectors, perplexity\n+        return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Adapter with Wav2Vec2->WavLM\n class WavLMAdapter(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -935,111 +874,126 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2AdapterLayer with Wav2Vec2->WavLM\n-class WavLMAdapterLayer(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.conv = nn.Conv1d(\n-            config.output_hidden_size,\n-            2 * config.output_hidden_size,\n-            config.adapter_kernel_size,\n-            stride=config.adapter_stride,\n-            padding=1,\n+def _compute_mask_indices(\n+    shape: Tuple[int, int],\n+    mask_prob: float,\n+    mask_length: int,\n+    attention_mask: Optional[torch.LongTensor] = None,\n+    min_masks: int = 0,\n+) -> np.ndarray:\n+    \"\"\"\n+    Computes random mask spans for a given shape. Used to implement [SpecAugment: A Simple Data Augmentation Method for\n+    ASR](https://arxiv.org/abs/1904.08779). Note that this method is not optimized to run on TPU and should be run on\n+    CPU as part of the preprocessing during training.\n+\n+    Args:\n+        shape: The shape for which to compute masks. This should be of a tuple of size 2 where\n+               the first element is the batch size and the second element is the length of the axis to span.\n+        mask_prob:  The percentage of the whole axis (between 0 and 1) which will be masked. The number of\n+                    independently generated mask spans of length `mask_length` is computed by\n+                    `mask_prob*shape[1]/mask_length`. Note that due to overlaps, `mask_prob` is an upper bound and the\n+                    actual percentage will be smaller.\n+        mask_length: size of the mask\n+        min_masks: minimum number of masked spans\n+        attention_mask: A (right-padded) attention mask which independently shortens the feature axis of\n+                        each batch dimension.\n+    \"\"\"\n+    batch_size, sequence_length = shape\n+\n+    if mask_length < 1:\n+        raise ValueError(\"`mask_length` has to be bigger than 0.\")\n+\n+    if mask_length > sequence_length:\n+        raise ValueError(\n+            f\"`mask_length` has to be smaller than `sequence_length`, but got `mask_length`: {mask_length}\"\n+            f\" and `sequence_length`: {sequence_length}`\"\n         )\n \n-    def forward(self, hidden_states):\n-        hidden_states = self.conv(hidden_states)\n-        hidden_states = nn.functional.glu(hidden_states, dim=1)\n+    # epsilon is used for probabilistic rounding\n+    epsilon = np.random.rand(1).item()\n \n-        return hidden_states\n+    def compute_num_masked_span(input_length):\n+        \"\"\"Given input length, compute how many spans should be masked\"\"\"\n+        num_masked_span = int(mask_prob * input_length / mask_length + epsilon)\n+        num_masked_span = max(num_masked_span, min_masks)\n \n+        # make sure num masked span <= sequence_length\n+        if num_masked_span * mask_length > sequence_length:\n+            num_masked_span = sequence_length // mask_length\n \n-class WavLMPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n+        # make sure num_masked span is also <= input_length - (mask_length - 1)\n+        if input_length - (mask_length - 1) < num_masked_span:\n+            num_masked_span = max(input_length - (mask_length - 1), 0)\n \n-    config_class = WavLMConfig\n-    base_model_prefix = \"wavlm\"\n-    main_input_name = \"input_values\"\n-    supports_gradient_checkpointing = True\n+        return num_masked_span\n \n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        # gumbel softmax requires special init\n-        if isinstance(module, WavLMGumbelVectorQuantizer):\n-            module.weight_proj.weight.data.normal_(mean=0.0, std=1)\n-            module.weight_proj.bias.data.zero_()\n-            nn.init.uniform_(module.codevectors)\n-        elif isinstance(module, WavLMPositionalConvEmbedding):\n-            nn.init.normal_(\n-                module.conv.weight,\n-                mean=0,\n-                std=2 * math.sqrt(1 / (module.conv.kernel_size[0] * module.conv.in_channels)),\n-            )\n-            nn.init.constant_(module.conv.bias, 0)\n-        elif isinstance(module, WavLMFeatureProjection):\n-            k = math.sqrt(1 / module.projection.in_features)\n-            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n-            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n-        elif isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+    # compute number of masked spans in batch\n+    input_lengths = (\n+        attention_mask.detach().sum(-1).tolist()\n+        if attention_mask is not None\n+        else [sequence_length for _ in range(batch_size)]\n+    )\n \n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Conv1d):\n-            nn.init.kaiming_normal_(module.weight)\n+    # SpecAugment mask to fill\n+    spec_aug_mask = np.zeros((batch_size, sequence_length), dtype=bool)\n+    spec_aug_mask_idxs = []\n \n-            if module.bias is not None:\n-                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n-                nn.init.uniform_(module.bias, a=-k, b=k)\n+    max_num_masked_span = compute_num_masked_span(sequence_length)\n \n-    def _get_feat_extract_output_lengths(\n-        self, input_lengths: Union[torch.LongTensor, int], add_adapter: Optional[bool] = None\n-    ):\n-        \"\"\"\n-        Computes the output length of the convolutional layers\n-        \"\"\"\n+    if max_num_masked_span == 0:\n+        return spec_aug_mask\n \n-        add_adapter = self.config.add_adapter if add_adapter is None else add_adapter\n+    for input_length in input_lengths:\n+        # compute num of masked spans for this input\n+        num_masked_span = compute_num_masked_span(input_length)\n \n-        def _conv_out_length(input_length, kernel_size, stride):\n-            # 1D convolutional layer output length formula taken\n-            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n-            return torch.div(input_length - kernel_size, stride, rounding_mode=\"floor\") + 1\n+        # get random indices to mask\n+        spec_aug_mask_idx = np.random.choice(\n+            np.arange(input_length - (mask_length - 1)), num_masked_span, replace=False\n+        )\n \n-        for kernel_size, stride in zip(self.config.conv_kernel, self.config.conv_stride):\n-            input_lengths = _conv_out_length(input_lengths, kernel_size, stride)\n+        # pick first sampled index that will serve as a dummy index to pad vector\n+        # to ensure same dimension for all batches due to probabilistic rounding\n+        # Picking first sample just pads those vectors twice.\n+        if len(spec_aug_mask_idx) == 0:\n+            # this case can only happen if `input_length` is strictly smaller then\n+            # `sequence_length` in which case the last token has to be a padding\n+            # token which we can use as a dummy mask id\n+            dummy_mask_idx = sequence_length - 1\n+        else:\n+            dummy_mask_idx = spec_aug_mask_idx[0]\n \n-        if add_adapter:\n-            for _ in range(self.config.num_adapter_layers):\n-                input_lengths = _conv_out_length(input_lengths, 1, self.config.adapter_stride)\n+        spec_aug_mask_idx = np.concatenate(\n+            [spec_aug_mask_idx, np.ones(max_num_masked_span - num_masked_span, dtype=np.int32) * dummy_mask_idx]\n+        )\n+        spec_aug_mask_idxs.append(spec_aug_mask_idx)\n \n-        return input_lengths\n+    spec_aug_mask_idxs = np.array(spec_aug_mask_idxs)\n \n-    def _get_feature_vector_attention_mask(\n-        self, feature_vector_length: int, attention_mask: torch.LongTensor, add_adapter=None\n-    ):\n-        # Effectively attention_mask.sum(-1), but not inplace to be able to run\n-        # on inference mode.\n-        non_padded_lengths = attention_mask.cumsum(dim=-1)[:, -1]\n+    # expand masked indices to masked spans\n+    spec_aug_mask_idxs = np.broadcast_to(\n+        spec_aug_mask_idxs[:, :, None], (batch_size, max_num_masked_span, mask_length)\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs.reshape(batch_size, max_num_masked_span * mask_length)\n \n-        output_lengths = self._get_feat_extract_output_lengths(non_padded_lengths, add_adapter=add_adapter)\n-        output_lengths = output_lengths.to(torch.long)\n+    # add offset to the starting indexes so that indexes now create a span\n+    offsets = np.arange(mask_length)[None, None, :]\n+    offsets = np.broadcast_to(offsets, (batch_size, max_num_masked_span, mask_length)).reshape(\n+        batch_size, max_num_masked_span * mask_length\n+    )\n+    spec_aug_mask_idxs = spec_aug_mask_idxs + offsets\n \n-        batch_size = attention_mask.shape[0]\n+    # ensure that we cannot have indices larger than sequence_length\n+    if spec_aug_mask_idxs.max() > sequence_length - 1:\n+        spec_aug_mask_idxs[spec_aug_mask_idxs > sequence_length - 1] = sequence_length - 1\n \n-        attention_mask = torch.zeros(\n-            (batch_size, feature_vector_length), dtype=attention_mask.dtype, device=attention_mask.device\n-        )\n-        # these two operations makes sure that all values before the output lengths idxs are attended to\n-        attention_mask[(torch.arange(attention_mask.shape[0], device=attention_mask.device), output_lengths - 1)] = 1\n-        attention_mask = attention_mask.flip([-1]).cumsum(-1).flip([-1]).bool()\n-        return attention_mask\n+    # scatter indices to mask\n+    np.put_along_axis(spec_aug_mask, spec_aug_mask_idxs, 1, -1)\n+\n+    return spec_aug_mask\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n \n \n WAVLM_START_DOCSTRING = r\"\"\"\n@@ -1061,7 +1015,6 @@ def _get_feature_vector_attention_mask(\n             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n-\n WAVLM_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n@@ -1098,12 +1051,13 @@ def _get_feature_vector_attention_mask(\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n \"\"\"\n \n+WavLMBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n \n @add_start_docstrings(\n     \"The bare WavLM Model transformer outputting raw hidden-states without any specific head on top.\",\n     WAVLM_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2Model with Wav2Vec2->WavLM, wav2vec2->wavlm, WAV_2_VEC_2->WAVLM, WavLMBaseModelOutput->Wav2Vec2BaseModelOutput\n class WavLMModel(WavLMPreTrainedModel):\n     def __init__(self, config: WavLMConfig):\n         super().__init__(config)\n@@ -1193,7 +1147,7 @@ def _mask_hidden_states(\n     @add_start_docstrings_to_model_forward(WAVLM_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Wav2Vec2BaseModelOutput,\n+        output_type=WavLMBaseModelOutput,\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n         expected_output=_EXPECTED_OUTPUT_SHAPE,\n@@ -1206,7 +1160,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[Tuple, Wav2Vec2BaseModelOutput]:\n+    ) -> Union[Tuple, WavLMBaseModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1243,19 +1197,24 @@ def forward(\n         if not return_dict:\n             return (hidden_states, extract_features) + encoder_outputs[1:]\n \n-        return Wav2Vec2BaseModelOutput(\n+        return WavLMBaseModelOutput(\n             last_hidden_state=hidden_states,\n             extract_features=extract_features,\n             hidden_states=encoder_outputs.hidden_states,\n             attentions=encoder_outputs.attentions,\n         )\n \n \n+_HIDDEN_STATES_START_POSITION = 2\n+\n+_CTC_EXPECTED_OUTPUT = \"'mister quilter is the aposle of the middle classes and we are glad to welcome his gospel'\"\n+_CTC_EXPECTED_LOSS = 12.51\n+\n+\n @add_start_docstrings(\n     \"\"\"WavLM Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n     WAVLM_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForCTC with Wav2Vec2->WavLM, wav2vec2->wavlm, WAV_2_VEC_2->WAVLM\n class WavLMForCTC(WavLMPreTrainedModel):\n     def __init__(self, config, target_lang: Optional[str] = None):\n         super().__init__(config)\n@@ -1432,7 +1391,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_extractor\n     def freeze_feature_extractor(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameters will\n@@ -1445,15 +1403,13 @@ def freeze_feature_extractor(self):\n         )\n         self.freeze_feature_encoder()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_feature_encoder with wav2vec2->wavlm\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will\n         not be updated during training.\n         \"\"\"\n         self.wavlm.feature_extractor._freeze_parameters()\n \n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.freeze_base_model with wav2vec2->wavlm\n     def freeze_base_model(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the base model so that its parameters will not\n@@ -1469,7 +1425,6 @@ def freeze_base_model(self):\n         config_class=_CONFIG_FOR_DOC,\n         modality=\"audio\",\n     )\n-    # Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForSequenceClassification.forward with Wav2Vec2->WavLM, wav2vec2->wavlm\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1533,13 +1488,16 @@ def forward(\n         )\n \n \n+_FRAME_CLASS_CHECKPOINT = \"microsoft/wavlm-base-plus-sd\"\n+_FRAME_EXPECTED_OUTPUT = [0, 0]\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     WavLM Model with a frame classification head on top for tasks like Speaker Diarization.\n     \"\"\",\n     WAVLM_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForAudioFrameClassification with Wav2Vec2->WavLM, wav2vec2->wavlm, WAV_2_VEC_2->WAVLM\n class WavLMForAudioFrameClassification(WavLMPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1646,7 +1604,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.AMSoftmaxLoss\n class AMSoftmaxLoss(nn.Module):\n     def __init__(self, input_dim, num_labels, scale=30.0, margin=0.4):\n         super(AMSoftmaxLoss, self).__init__()\n@@ -1670,7 +1627,6 @@ def forward(self, hidden_states, labels):\n         return loss\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.TDNNLayer\n class TDNNLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -1686,6 +1642,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         if is_peft_available():\n             from peft.tuners.lora import LoraLayer\n \n+        if is_peft_available():\n             if isinstance(self.kernel, LoraLayer):\n                 warnings.warn(\n                     \"Detected LoRA on TDNNLayer. LoRA weights won't be applied due to optimization. \"\n@@ -1702,13 +1659,16 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+_XVECTOR_CHECKPOINT = \"microsoft/wavlm-base-plus-sv\"\n+_XVECTOR_EXPECTED_OUTPUT = 0.97\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     WavLM Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n     \"\"\",\n     WAVLM_START_DOCSTRING,\n )\n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2ForXVector with Wav2Vec2->WavLM, wav2vec2->wavlm, WAV_2_VEC_2->WAVLM\n class WavLMForXVector(WavLMPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "9ae9170fec56f52933453001ce36530d3e717943",
            "filename": "src/transformers/models/wavlm/modular_wavlm.py",
            "status": "added",
            "additions": 758,
            "deletions": 0,
            "changes": 758,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -0,0 +1,758 @@\n+import math\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+\n+from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_outputs import (\n+    BaseModelOutput,\n+    CausalLMOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+    Wav2Vec2BaseModelOutput,\n+    XVectorOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ..wav2vec2.modeling_wav2vec2 import (\n+    Wav2Vec2FeatureProjection,\n+    Wav2Vec2FeedForward,\n+    Wav2Vec2ForAudioFrameClassification,\n+    Wav2Vec2ForCTC,\n+    Wav2Vec2ForSequenceClassification,\n+    Wav2Vec2ForXVector,\n+    Wav2Vec2Model,\n+    Wav2Vec2PositionalConvEmbedding,\n+    Wav2Vec2PreTrainedModel,\n+)\n+from .configuration_wavlm import WavLMConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"WavLMConfig\"\n+\n+_CHECKPOINT_FOR_DOC = \"patrickvonplaten/wavlm-libri-clean-100h-base-plus\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n+\n+_CTC_EXPECTED_OUTPUT = \"'mister quilter is the aposle of the middle classes and we are glad to welcome his gospel'\"\n+_CTC_EXPECTED_LOSS = 12.51\n+\n+_FRAME_CLASS_CHECKPOINT = \"microsoft/wavlm-base-plus-sd\"\n+_FRAME_EXPECTED_OUTPUT = [0, 0]\n+\n+_XVECTOR_CHECKPOINT = \"microsoft/wavlm-base-plus-sv\"\n+_XVECTOR_EXPECTED_OUTPUT = 0.97\n+\n+\n+class WavLMPositionalConvEmbedding(Wav2Vec2PositionalConvEmbedding):\n+    pass\n+\n+\n+class WavLMFeatureProjection(Wav2Vec2FeatureProjection):\n+    pass\n+\n+\n+class WavLMAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(\n+        self,\n+        embed_dim: int,\n+        num_heads: int,\n+        dropout: float = 0.0,\n+        num_buckets: int = 320,\n+        max_distance: int = 800,\n+        has_relative_position_bias: bool = True,\n+    ):\n+        super().__init__()\n+        self.embed_dim = embed_dim\n+        self.num_heads = num_heads\n+        self.dropout = dropout\n+        self.head_dim = embed_dim // num_heads\n+\n+        if (self.head_dim * num_heads) != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n+                f\" and `num_heads`: {num_heads}).\"\n+            )\n+        self.scaling = self.head_dim**-0.5\n+\n+        self.k_proj = nn.Linear(embed_dim, embed_dim)\n+        self.v_proj = nn.Linear(embed_dim, embed_dim)\n+        self.q_proj = nn.Linear(embed_dim, embed_dim)\n+        self.out_proj = nn.Linear(embed_dim, embed_dim)\n+\n+        self.num_buckets = num_buckets\n+        self.max_distance = max_distance\n+\n+        self.gru_rel_pos_const = nn.Parameter(torch.ones(1, self.num_heads, 1, 1))\n+        self.gru_rel_pos_linear = nn.Linear(self.head_dim, 8)\n+\n+        if has_relative_position_bias:\n+            self.rel_attn_embed = nn.Embedding(self.num_buckets, self.num_heads)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_bias: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        index=0,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        \"\"\"Attention layer with relative attention\"\"\"\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # first pass of attention layer creates position bias\n+        if position_bias is None:\n+            position_bias = self.compute_bias(tgt_len, tgt_len)\n+            position_bias = (\n+                position_bias.unsqueeze(0).repeat(bsz, 1, 1, 1).view(bsz * self.num_heads, tgt_len, tgt_len)\n+            )\n+\n+        # Compute relative position bias:\n+        # 1) get reshape hidden_states\n+        gated_hidden_states = hidden_states.view(hidden_states.shape[:-1] + (self.num_heads, -1))\n+        gated_hidden_states = gated_hidden_states.permute(0, 2, 1, 3)\n+\n+        # 2) project hidden states\n+        relative_position_proj = self.gru_rel_pos_linear(gated_hidden_states)\n+        relative_position_proj = relative_position_proj.view(gated_hidden_states.shape[:-1] + (2, 4)).sum(-1)\n+\n+        # 3) compute gate for position bias from projected hidden states\n+        gate_a, gate_b = torch.sigmoid(relative_position_proj).chunk(2, dim=-1)\n+        gate_output = gate_a * (gate_b * self.gru_rel_pos_const - 1.0) + 2.0\n+\n+        # 4) apply gate to position bias to compute gated position_bias\n+        gated_position_bias = gate_output.view(bsz * self.num_heads, -1, 1) * position_bias\n+        gated_position_bias = gated_position_bias.view((-1, tgt_len, tgt_len))\n+\n+        attn_output, attn_weights = self.torch_multi_head_self_attention(\n+            hidden_states, attention_mask, gated_position_bias, output_attentions\n+        )\n+\n+        return attn_output, attn_weights, position_bias\n+\n+    def torch_multi_head_self_attention(\n+        self,\n+        hidden_states: torch.FloatTensor,\n+        attention_mask: Union[torch.LongTensor, torch.BoolTensor],\n+        gated_position_bias: torch.FloatTensor,\n+        output_attentions: bool,\n+    ) -> (torch.FloatTensor, torch.FloatTensor):\n+        \"\"\"simple wrapper around torch's multi_head_attention_forward function\"\"\"\n+        # self-attention assumes q = k = v\n+        query = key = value = hidden_states.transpose(0, 1)\n+        key_padding_mask = attention_mask.ne(1) if attention_mask is not None else None\n+\n+        # disable bias and add_zero_attn\n+        bias_k = bias_v = None\n+        add_zero_attn = False\n+\n+        # PyTorch 1.3.0 has F.multi_head_attention_forward defined\n+        # so no problem with backwards compatibility\n+        attn_output, attn_weights = F.multi_head_attention_forward(\n+            query,\n+            key,\n+            value,\n+            self.embed_dim,\n+            self.num_heads,\n+            torch.empty([0]),\n+            torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)),\n+            bias_k,\n+            bias_v,\n+            add_zero_attn,\n+            self.dropout,\n+            self.out_proj.weight,\n+            self.out_proj.bias,\n+            self.training,\n+            key_padding_mask,\n+            output_attentions,\n+            gated_position_bias,\n+            use_separate_proj_weight=True,\n+            q_proj_weight=self.q_proj.weight,\n+            k_proj_weight=self.k_proj.weight,\n+            v_proj_weight=self.v_proj.weight,\n+        )\n+\n+        # [Seq_Len, Batch Size, ...] -> [Batch Size, Seq_Len, ...]\n+        attn_output = attn_output.transpose(0, 1)\n+\n+        if attn_weights is not None:\n+            # IMPORTANT: Attention weights are averaged weights\n+            # here which should not be the case. This is an open issue\n+            # on PyTorch: https://github.com/pytorch/pytorch/issues/32590\n+            attn_weights = attn_weights[:, None].broadcast_to(\n+                attn_weights.shape[:1] + (self.num_heads,) + attn_weights.shape[1:]\n+            )\n+\n+        return attn_output, attn_weights\n+\n+    def compute_bias(self, query_length: int, key_length: int) -> torch.FloatTensor:\n+        context_position = torch.arange(query_length, dtype=torch.long)[:, None]\n+        memory_position = torch.arange(key_length, dtype=torch.long)[None, :]\n+        relative_position = memory_position - context_position\n+        relative_position_bucket = self._relative_positions_bucket(relative_position)\n+        relative_position_bucket = relative_position_bucket.to(self.rel_attn_embed.weight.device)\n+        values = self.rel_attn_embed(relative_position_bucket)\n+        values = values.permute([2, 0, 1])\n+        return values\n+\n+    def _relative_positions_bucket(self, relative_positions: torch.FloatTensor) -> torch.FloatTensor:\n+        num_buckets = self.num_buckets // 2\n+\n+        relative_buckets = (relative_positions > 0).to(torch.long) * num_buckets\n+        relative_positions = torch.abs(relative_positions)\n+\n+        max_exact = num_buckets // 2\n+        is_small = relative_positions < max_exact\n+\n+        relative_positions_if_large = torch.log(relative_positions.float() / max_exact)\n+        relative_positions_if_large = relative_positions_if_large / math.log(self.max_distance / max_exact)\n+        relative_positions_if_large = relative_positions_if_large * (num_buckets - max_exact)\n+        relative_position_if_large = (max_exact + relative_positions_if_large).to(torch.long)\n+        relative_position_if_large = torch.min(\n+            relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)\n+        )\n+\n+        relative_buckets += torch.where(is_small, relative_positions, relative_position_if_large)\n+        return relative_buckets\n+\n+\n+class WavLMFeedForward(Wav2Vec2FeedForward):\n+    pass\n+\n+\n+class WavLMEncoderLayer(nn.Module):\n+    def __init__(self, config: WavLMConfig, has_relative_position_bias: bool = True):\n+        super().__init__()\n+        self.attention = WavLMAttention(\n+            embed_dim=config.hidden_size,\n+            num_heads=config.num_attention_heads,\n+            dropout=config.attention_dropout,\n+            num_buckets=config.num_buckets,\n+            max_distance=config.max_bucket_distance,\n+            has_relative_position_bias=has_relative_position_bias,\n+        )\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.feed_forward = WavLMFeedForward(config)\n+        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+    def forward(self, hidden_states, attention_mask=None, position_bias=None, output_attentions=False, index=0):\n+        attn_residual = hidden_states\n+        hidden_states, attn_weights, position_bias = self.attention(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            position_bias=position_bias,\n+            output_attentions=output_attentions,\n+            index=index,\n+        )\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = attn_residual + hidden_states\n+\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        hidden_states = hidden_states + self.feed_forward(hidden_states)\n+        hidden_states = self.final_layer_norm(hidden_states)\n+\n+        outputs = (hidden_states, position_bias)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class WavLMEncoderLayerStableLayerNorm(nn.Module):\n+    def __init__(self, config: WavLMConfig, has_relative_position_bias: bool = True):\n+        super().__init__()\n+        self.attention = WavLMAttention(\n+            embed_dim=config.hidden_size,\n+            num_heads=config.num_attention_heads,\n+            dropout=config.attention_dropout,\n+            num_buckets=config.num_buckets,\n+            max_distance=config.max_bucket_distance,\n+            has_relative_position_bias=has_relative_position_bias,\n+        )\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.feed_forward = WavLMFeedForward(config)\n+        self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+    def forward(self, hidden_states, attention_mask=None, position_bias=None, output_attentions=False):\n+        attn_residual = hidden_states\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states, attn_weights, position_bias = self.attention(\n+            hidden_states,\n+            attention_mask=attention_mask,\n+            position_bias=position_bias,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = attn_residual + hidden_states\n+        hidden_states = hidden_states + self.feed_forward(self.final_layer_norm(hidden_states))\n+\n+        outputs = (hidden_states, position_bias)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class WavLMEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.pos_conv_embed = WavLMPositionalConvEmbedding(config)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layers = nn.ModuleList(\n+            [WavLMEncoderLayer(config, has_relative_position_bias=(i == 0)) for i in range(config.num_hidden_layers)]\n+        )\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask=None,\n+        output_attentions=False,\n+        output_hidden_states=False,\n+        return_dict=True,\n+    ):\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        if attention_mask is not None:\n+            # make sure padded tokens output 0\n+            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_attention_mask] = 0\n+\n+        position_embeddings = self.pos_conv_embed(hidden_states)\n+        hidden_states = hidden_states + position_embeddings\n+        hidden_states = self.layer_norm(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n+        position_bias = None\n+\n+        for i, layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n+            dropout_probability = torch.rand([])\n+\n+            skip_the_layer = self.training and i > 0 and (dropout_probability < self.config.layerdrop)\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n+                if self.gradient_checkpointing and self.training:\n+                    layer_outputs = self._gradient_checkpointing_func(\n+                        layer.__call__,\n+                        hidden_states,\n+                        attention_mask,\n+                        position_bias,\n+                        output_attentions,\n+                    )\n+                else:\n+                    layer_outputs = layer(\n+                        hidden_states,\n+                        attention_mask=attention_mask,\n+                        position_bias=position_bias,\n+                        output_attentions=output_attentions,\n+                        index=i,\n+                    )\n+\n+                hidden_states, position_bias = layer_outputs[:2]\n+\n+            if skip_the_layer:\n+                layer_outputs = (None, None, None)\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[2],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+class WavLMEncoderStableLayerNorm(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.pos_conv_embed = WavLMPositionalConvEmbedding(config)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout)\n+        self.layers = nn.ModuleList(\n+            [\n+                WavLMEncoderLayerStableLayerNorm(config, has_relative_position_bias=(i == 0))\n+                for i in range(config.num_hidden_layers)\n+            ]\n+        )\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        attention_mask=None,\n+        output_attentions=False,\n+        output_hidden_states=False,\n+        return_dict=True,\n+    ):\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        if attention_mask is not None:\n+            # make sure padded tokens are not attended to\n+            expand_attention_mask = attention_mask.unsqueeze(-1).repeat(1, 1, hidden_states.shape[2])\n+            hidden_states[~expand_attention_mask] = 0\n+\n+        position_embeddings = self.pos_conv_embed(hidden_states)\n+        hidden_states = hidden_states + position_embeddings\n+        hidden_states = self.dropout(hidden_states)\n+\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n+        position_bias = None\n+\n+        for i, layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n+            dropout_probability = torch.rand([])\n+\n+            skip_the_layer = self.training and i > 0 and (dropout_probability < self.config.layerdrop)\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n+                # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n+                if self.gradient_checkpointing and self.training:\n+                    layer_outputs = self._gradient_checkpointing_func(\n+                        layer.__call__,\n+                        hidden_states,\n+                        attention_mask,\n+                        position_bias,\n+                        output_attentions,\n+                    )\n+                else:\n+                    layer_outputs = layer(\n+                        hidden_states,\n+                        attention_mask=attention_mask,\n+                        output_attentions=output_attentions,\n+                        position_bias=position_bias,\n+                    )\n+                hidden_states, position_bias = layer_outputs[:2]\n+\n+            if skip_the_layer:\n+                layer_outputs = (None, None, None)\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[2],)\n+\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_self_attentions\n+        )\n+\n+\n+class WavLMGumbelVectorQuantizer(nn.Module):\n+    \"\"\"\n+    Vector quantization using gumbel softmax. See [CATEGORICAL REPARAMETERIZATION WITH\n+    GUMBEL-SOFTMAX](https://arxiv.org/pdf/1611.01144.pdf) for more information.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_groups = config.num_codevector_groups\n+        self.num_vars = config.num_codevectors_per_group\n+\n+        if config.codevector_dim % self.num_groups != 0:\n+            raise ValueError(\n+                f\"`config.codevector_dim {config.codevector_dim} must be divisible\"\n+                f\" by `config.num_codevector_groups` {self.num_groups} \"\n+                \"for concatenation.\"\n+            )\n+\n+        # storage for codebook variables (codewords)\n+        self.codevectors = nn.Parameter(\n+            torch.FloatTensor(1, self.num_groups * self.num_vars, config.codevector_dim // self.num_groups)\n+        )\n+        self.weight_proj = nn.Linear(config.conv_dim[-1], self.num_groups * self.num_vars)\n+\n+        # can be decayed for training\n+        self.temperature = 2\n+\n+    @staticmethod\n+    def _compute_perplexity(probs):\n+        marginal_probs = probs.mean(dim=0)\n+        perplexity = torch.exp(-torch.sum(marginal_probs * torch.log(marginal_probs + 1e-7), dim=-1)).sum()\n+        return perplexity\n+\n+    def forward(self, hidden_states):\n+        batch_size, sequence_length, hidden_size = hidden_states.shape\n+\n+        # project to codevector dim\n+        hidden_states = self.weight_proj(hidden_states)\n+        hidden_states = hidden_states.view(batch_size * sequence_length * self.num_groups, -1)\n+\n+        if self.training:\n+            # sample code vector probs via gumbel in differentiateable way\n+            codevector_probs = nn.functional.gumbel_softmax(hidden_states.float(), tau=self.temperature, hard=True)\n+            codevector_probs = codevector_probs.type_as(hidden_states)\n+\n+            # compute perplexity\n+            codevector_soft_dist = torch.softmax(\n+                hidden_states.view(batch_size * sequence_length, self.num_groups, -1).float(), dim=-1\n+            )\n+            perplexity = self._compute_perplexity(codevector_soft_dist)\n+        else:\n+            # take argmax in non-differentiable way\n+            # comptute hard codevector distribution (one hot)\n+            codevector_idx = hidden_states.argmax(dim=-1)\n+            codevector_probs = hidden_states.new_zeros(*hidden_states.shape).scatter_(\n+                -1, codevector_idx.view(-1, 1), 1.0\n+            )\n+            codevector_probs = codevector_probs.view(batch_size * sequence_length, self.num_groups, -1)\n+\n+            perplexity = self._compute_perplexity(codevector_probs)\n+\n+        codevector_probs = codevector_probs.view(batch_size * sequence_length, -1)\n+        # use probs to retrieve codevectors\n+        codevectors_per_group = codevector_probs.unsqueeze(-1) * self.codevectors\n+        codevectors = codevectors_per_group.view(batch_size * sequence_length, self.num_groups, self.num_vars, -1)\n+        codevectors = codevectors.sum(-2).view(batch_size, sequence_length, -1)\n+\n+        return codevectors, perplexity\n+\n+\n+class WavLMPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = WavLMConfig\n+    base_model_prefix = \"wavlm\"\n+    main_input_name = \"input_values\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = False\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        # gumbel softmax requires special init\n+        if isinstance(module, WavLMGumbelVectorQuantizer):\n+            module.weight_proj.weight.data.normal_(mean=0.0, std=1)\n+            module.weight_proj.bias.data.zero_()\n+            nn.init.uniform_(module.codevectors)\n+        elif isinstance(module, WavLMPositionalConvEmbedding):\n+            nn.init.normal_(\n+                module.conv.weight,\n+                mean=0,\n+                std=2 * math.sqrt(1 / (module.conv.kernel_size[0] * module.conv.in_channels)),\n+            )\n+            nn.init.constant_(module.conv.bias, 0)\n+        elif isinstance(module, WavLMFeatureProjection):\n+            k = math.sqrt(1 / module.projection.in_features)\n+            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n+            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n+        elif isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, nn.Conv1d):\n+            nn.init.kaiming_normal_(module.weight)\n+\n+            if module.bias is not None:\n+                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n+                nn.init.uniform_(module.bias, a=-k, b=k)\n+\n+    def _get_adapters(self):\n+        raise AttributeError(\"Not needed for WavLM\")\n+\n+    def init_adapter_layers(self):\n+        raise AttributeError(\"Not needed for WavLM\")\n+\n+    def load_adapter(self):\n+        raise AttributeError(\"Not needed for WavLM\")\n+\n+\n+WAVLM_START_DOCSTRING = r\"\"\"\n+    WavLM was proposed in [WavLM: Unified Speech Representation Learning with Labeled and Unlabeled\n+    Data](https://arxiv.org/abs/2110.13900) by Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo\n+    Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian,\n+    Jian Wu, Michael Zeng, Xiangzhan Yu, Furu Wei.\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving etc.).\n+\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n+    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`WavLMConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+WAVLM_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n+            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n+            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2Processor.__call__`] for details.\n+        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            <Tip warning={true}>\n+\n+            `attention_mask` should only be passed if the corresponding processor has `config.return_attention_mask ==\n+            True`. For all models whose processor has `config.return_attention_mask == False`, `attention_mask` should\n+            **not** be passed to avoid degraded performance when doing batched inference. For such models\n+            `input_values` should simply be padded with 0 and passed without `attention_mask`. Be aware that these\n+            models also yield slightly different results depending on whether `input_values` is padded or not.\n+\n+            </Tip>\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+WavLMBaseModelOutput = Wav2Vec2BaseModelOutput\n+\n+\n+@add_start_docstrings(\n+    \"The bare WavLM Model transformer outputting raw hidden-states without any specific head on top.\",\n+    WAVLM_START_DOCSTRING,\n+)\n+class WavLMModel(Wav2Vec2Model):\n+    @add_start_docstrings_to_model_forward(WAVLM_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=WavLMBaseModelOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(self, **super_kwargs):\n+        return super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"WavLM Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n+    WAVLM_START_DOCSTRING,\n+)\n+class WavLMForCTC(Wav2Vec2ForCTC):\n+    @add_start_docstrings_to_model_forward(WAVLM_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=CausalLMOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_CTC_EXPECTED_OUTPUT,\n+        expected_loss=_CTC_EXPECTED_LOSS,\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    WavLM Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like\n+    SUPERB Keyword Spotting.\n+    \"\"\",\n+    WAVLM_START_DOCSTRING,\n+)\n+class WavLMForSequenceClassification(Wav2Vec2ForSequenceClassification):\n+    @add_start_docstrings_to_model_forward(WAVLM_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=SequenceClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    WavLM Model with a frame classification head on top for tasks like Speaker Diarization.\n+    \"\"\",\n+    WAVLM_START_DOCSTRING,\n+)\n+class WavLMForAudioFrameClassification(Wav2Vec2ForAudioFrameClassification):\n+    @add_start_docstrings_to_model_forward(WAVLM_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_FRAME_CLASS_CHECKPOINT,\n+        output_type=TokenClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_FRAME_EXPECTED_OUTPUT,\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    WavLM Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n+    \"\"\",\n+    WAVLM_START_DOCSTRING,\n+)\n+class WavLMForXVector(Wav2Vec2ForXVector):\n+    pass\n+\n+    @add_start_docstrings_to_model_forward(WAVLM_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_XVECTOR_CHECKPOINT,\n+        output_type=XVectorOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"audio\",\n+        expected_output=_XVECTOR_EXPECTED_OUTPUT,\n+    )\n+    def forward(self, **super_kwargs):\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"WavLMForAudioFrameClassification\",\n+    \"WavLMForCTC\",\n+    \"WavLMForSequenceClassification\",\n+    \"WavLMForXVector\",\n+    \"WavLMModel\",\n+    \"WavLMPreTrainedModel\",\n+]"
        },
        {
            "sha": "7f1e1b922429e27baa8b33162ab76cd1f8a47b23",
            "filename": "tests/models/wav2vec2_bert/test_modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -50,9 +50,9 @@\n         Wav2Vec2BertForXVector,\n         Wav2Vec2BertModel,\n     )\n+    from transformers.models.wav2vec2.modeling_wav2vec2 import _sample_negative_indices\n     from transformers.models.wav2vec2_bert.modeling_wav2vec2_bert import (\n         _compute_mask_indices,\n-        _sample_negative_indices,\n     )\n \n "
        },
        {
            "sha": "6a884ba36bab99b4300c417f84d4a62af8a75f38",
            "filename": "tests/models/wav2vec2_conformer/test_modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -55,10 +55,10 @@\n         Wav2Vec2FeatureExtractor,\n         Wav2Vec2Processor,\n     )\n+    from transformers.models.wav2vec2.modeling_wav2vec2 import _sample_negative_indices\n     from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer import (\n         Wav2Vec2ConformerGumbelVectorQuantizer,\n         _compute_mask_indices,\n-        _sample_negative_indices,\n     )\n \n "
        },
        {
            "sha": "0962056270d8242bf6b1c677743af1496cc44f36",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 23,
            "deletions": 4,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=f74d7da836ba9e78e98f3bd735ad84cd2cd48cf3",
            "patch": "@@ -79,8 +79,11 @@ def replace(match):\n \n def get_cased_name(lowercase_name: str) -> str:\n     \"\"\"From a model name in lowercase in the format `my_model`, return the cased name in the format `MyModel`.\"\"\"\n+    alt_lowercase_name = lowercase_name.replace(\"_\", \"-\")\n     if lowercase_name in CONFIG_MAPPING_NAMES:\n         return CONFIG_MAPPING_NAMES[lowercase_name].replace(\"Config\", \"\")\n+    elif alt_lowercase_name in CONFIG_MAPPING_NAMES:\n+        return CONFIG_MAPPING_NAMES[alt_lowercase_name].replace(\"Config\", \"\")\n     else:\n         return \"\".join(x.title() for x in lowercase_name.split(\"_\"))\n \n@@ -106,6 +109,8 @@ class ReplaceNameTransformer(m.MatcherDecoratableTransformer):\n \n     def __init__(self, old_name: str, new_name: str, original_new_model_name: str = \"\", only_doc: bool = False):\n         super().__init__()\n+        old_name = old_name.replace(\"-\", \"_\")\n+        new_name = new_name.replace(\"-\", \"_\")\n         self.old_name = old_name\n         self.new_name = new_name\n         self.cased_new_name = get_cased_name(self.new_name)\n@@ -535,7 +540,7 @@ def forward(...):\n \n \n # Top-level variables that match the following patterns will always use the value in the `modular_xxx.py` file\n-ASSIGNMENTS_REGEX_TO_KEEP = [r\"_CHECKPOINT\", r\"_EXPECTED\", r\"_FOR_DOC\"]\n+ASSIGNMENTS_REGEX_TO_KEEP = [r\"_CHECKPOINT\", r\"_EXPECTED\", r\"_FOR_DOC\", r\"_HIDDEN_STATES_START_POSITION\"]\n \n # Top-level variables that match the following patterns will use the value in the `modular_xxx.py` file only if they are not None\n ASSIGNMENTS_REGEX_TO_KEEP_IF_NOT_NONE = [r\"_DOCSTRING\"]\n@@ -616,6 +621,7 @@ def __init__(self, python_module: cst.Module):\n         self.object_dependency_mapping = defaultdict(set)          # immediate function/assignment dependency mapping (i.e. dependencies immediately in the function/assignment definition)\n         self.assignments: Dict[str, cst.SimpleStatementLine] = {}  # mapping of global assignments names to Nodes\n         self.current_function = None                               # this keeps track of the current module-scope function\n+        self.current_class = None                                  # this keeps track of the current module-scope class\n         self.current_assignment = None                             # this keeps track of the current module-scope assignment\n         # this keeps track of objects imported from modeling files (`from .configuration import Config`) -> `Config` should not be a dependency\n         self.objects_imported_from_modeling = set()\n@@ -672,14 +678,18 @@ def leave_FunctionDef(self, node):\n \n     def visit_If(self, node):\n         # If we are inside a function, do not add the import to the list of imports\n-        if self.current_function is None:\n+        if self.current_function is None and self.current_class is None:\n             for stmt in node.body.body:\n                 if m.matches(stmt, m.SimpleStatementLine(body=[m.ImportFrom() | m.Import()])):\n                     self.imports.append(node)\n \n     def visit_ClassDef(self, node: ClassDef) -> None:\n         \"\"\"Record class nodes to create their dependencies at the end.\"\"\"\n         self.classes[node.name.value] = node\n+        self.current_class = node.name.value\n+\n+    def leave_ClassDef(self, node):\n+        self.current_class = None\n \n     def visit_Name(self, node: cst.Call):\n         \"\"\"This is used to create a mapping from module-scope functions and assignments to objects used inside them.\"\"\"\n@@ -1024,11 +1034,20 @@ def replace_class_node(\n             new_decorators = (\n                 updated_methods[name].decorators if len(updated_methods[name].decorators) > 0 else func.decorators\n             )\n+\n+            # Keep return annotation in `modular_xxx.py` if any, else original return annotation\n+            new_return_annotation = updated_methods[name].returns if updated_methods[name].returns else func.returns\n+\n             if not re.match(\n                 r\"\\ndef .*\\(.*\\):\\n    raise.*Error\\(.*\",\n                 mapper.python_module.code_for_node(updated_methods[name]),\n             ):\n-                func = func.with_changes(body=updated_methods[name].body, params=new_params, decorators=new_decorators)\n+                func = func.with_changes(\n+                    body=updated_methods[name].body,\n+                    params=new_params,\n+                    decorators=new_decorators,\n+                    returns=new_return_annotation,\n+                )\n             else:\n                 continue\n \n@@ -1136,7 +1155,7 @@ def append_new_import_node(\n     import_node = node.body[0]\n     names_to_keep = []\n     for name in import_node.names:\n-        name_value = name.evaluated_name\n+        name_value = name.evaluated_alias or name.evaluated_name\n         if name_value not in unused_imports and name_value not in added_names:\n             names_to_keep.append(name.with_changes(comma=cst.MaybeSentinel.DEFAULT))\n             added_names.add(name_value)"
        }
    ],
    "stats": {
        "total": 8900,
        "additions": 6687,
        "deletions": 2213
    }
}