{
    "author": "dhruvmalik007",
    "message": "create ijepa modelcard (ref : PR  #36979 ). (#39354)\n\n* wip: adding first version of the IJEPA model card.\n\n* refactor based on the @stevhliu feedbacks\n\n* refactor:\n- revert the accidental removal of the autodoc api description and the image reerece architecture\n\n- general context updation.\n\n* - changes of model for example quantization.\n- merging the  quantization content.",
    "sha": "787a0128a92926c615b29abfa8dec40087c1d875",
    "files": [
        {
            "sha": "146b673732e05b7052420a713390a058b090924f",
            "filename": "docs/source/en/model_doc/ijepa.md",
            "status": "modified",
            "additions": 80,
            "deletions": 34,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/787a0128a92926c615b29abfa8dec40087c1d875/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/787a0128a92926c615b29abfa8dec40087c1d875/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md?ref=787a0128a92926c615b29abfa8dec40087c1d875",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -14,53 +14,107 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n # I-JEPA\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n+[I-JEPA](https://huggingface.co/papers/2301.08243) is a self-supervised learning method that learns semantic image representations by predicting parts of an image from other parts of the image. It compares the abstract representations of the image (rather than pixel level comparisons), which avoids the typical pitfalls of data augmentation bias and pixel-level details that don't capture semantic meaning.\n+\n+You can find the original I-JEPA checkpoints under the [AI at Meta](https://huggingface.co/facebook/models?search=ijepa) organization.\n+> [!TIP]\n+> This model was contributed by [jmtzt](https://huggingface.co/jmtzt).\n \n-## Overview\n \n-The I-JEPA model was proposed in [Image-based Joint-Embedding Predictive Architecture](https://huggingface.co/papers/2301.08243) by Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas.\n-I-JEPA is a self-supervised learning method that predicts the representations of one part of an image based on other parts of the same image. This approach focuses on learning semantic features without relying on pre-defined invariances from hand-crafted data transformations, which can bias specific tasks, or on filling in pixel-level details, which often leads to less meaningful representations.\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/ijepa_architecture.jpg\">\n \n-The abstract from the paper is the following:\n \n-This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image- based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample tar- get blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transform- ers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.\n+> Click on the I-JEPA models in the right sidebar for more examples of how to apply I-JEPA to different image representation and classification tasks.\n \n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/ijepa_architecture.jpg\"\n-alt=\"drawing\" width=\"600\"/>\n+The example below demonstrates how to extract image features with [`Pipeline`] or the [`AutoModel`] class.\n \n-<small> I-JEPA architecture. Taken from the <a href=\"https://huggingface.co/papers/2301.08243\">original paper.</a> </small>\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-This model was contributed by [jmtzt](https://huggingface.co/jmtzt).\n-The original code can be found [here](https://github.com/facebookresearch/ijepa).\n+```py\n+import torch\n+from transformers import pipeline\n+feature_extractor = pipeline(\n+    task=\"image-feature-extraction\",\n+    model=\"facebook/ijepa_vith14_1k\",\n+    device=0,\n+    torch_dtype=torch.bfloat16\n+)\n+features = feature_extractor(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\", return_tensors=True)  \n+\n+print(f\"Feature shape: {features.shape}\")\n \n-## How to use\n+```\n \n-Here is how to use this model for image feature extraction:\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-```python\n+```py\n import requests\n import torch\n from PIL import Image\n from torch.nn.functional import cosine_similarity\n+from transformers import AutoModel, AutoProcessor  \n+\n+url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"  \n+url_2 = \"http://images.cocodataset.org/val2017/000000219578.jpg\"\n+image_1 = Image.open(requests.get(url_1, stream=True).raw)\n+image_2 = Image.open(requests.get(url_2, stream=True).raw)\n+\n+processor = AutoProcessor.from_pretrained(\"facebook/ijepa_vith14_1k\")  \n+model = AutoModel.from_pretrained(\"facebook/ijepa_vith14_1k\", torch_dtype=\"auto\", attn_implementation=\"sdpa\")  \n+\n+\n+def infer(image):  \n+    inputs = processor(image, return_tensors=\"pt\")  \n+    outputs = model(**inputs)  \n+    return outputs.last_hidden_state.mean(dim=1)  \n+\n \n-from transformers import AutoModel, AutoProcessor\n+embed_1 = infer(image_1)  \n+embed_2 = infer(image_2)  \n+\n+similarity = cosine_similarity(embed_1, embed_2)  \n+print(similarity)\n+```\n+</hfoption>\n+</hfoptions>\n+\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to 4-bits.\n+\n+```py\n+import torch\n+from transformers import BitsAndBytesConfig, AutoModel, AutoProcessor\n+from datasets import load_dataset\n+\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=torch.bfloat16,\n+    bnb_4bit_use_double_quant=True,\n+)\n \n url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n url_2 = \"http://images.cocodataset.org/val2017/000000219578.jpg\"\n image_1 = Image.open(requests.get(url_1, stream=True).raw)\n image_2 = Image.open(requests.get(url_2, stream=True).raw)\n \n-model_id = \"facebook/ijepa_vith14_1k\"\n-processor = AutoProcessor.from_pretrained(model_id)\n-model = AutoModel.from_pretrained(model_id)\n+processor = AutoProcessor.from_pretrained(\"facebook/ijepa_vitg16_22k\")\n+model = AutoModel.from_pretrained(\"facebook/ijepa_vitg16_22k\", quantization_config=quantization_config, torch_dtype=\"auto\", attn_implementation=\"sdpa\")\n+\n \n-@torch.no_grad()\n def infer(image):\n     inputs = processor(image, return_tensors=\"pt\")\n     outputs = model(**inputs)\n@@ -74,15 +128,6 @@ similarity = cosine_similarity(embed_1, embed_2)\n print(similarity)\n ```\n \n-## Resources\n-\n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with I-JEPA.\n-\n-<PipelineTag pipeline=\"image-classification\"/>\n-\n-- [`IJepaForImageClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb).\n-- See also: [Image classification task guide](../tasks/image_classification)\n-\n ## IJepaConfig\n \n [[autodoc]] IJepaConfig\n@@ -95,4 +140,5 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n ## IJepaForImageClassification\n \n [[autodoc]] IJepaForImageClassification\n-    - forward\n\\ No newline at end of file\n+    - forward\n+"
        }
    ],
    "stats": {
        "total": 114,
        "additions": 80,
        "deletions": 34
    }
}