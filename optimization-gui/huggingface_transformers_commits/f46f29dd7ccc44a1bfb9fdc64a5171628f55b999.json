{
    "author": "cyyever",
    "message": "Remove more PyTorch 2.2 compatible code (#40337)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "f46f29dd7ccc44a1bfb9fdc64a5171628f55b999",
    "files": [
        {
            "sha": "c714b9bb0c0741d0fb2d4d7bd8fa34841adea09a",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=f46f29dd7ccc44a1bfb9fdc64a5171628f55b999",
            "patch": "@@ -23,7 +23,6 @@\n \n import torch\n import torch.utils.checkpoint\n-from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -45,7 +44,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, get_torch_version, logging\n+from ...utils import ModelOutput, auto_docstring, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_bert import BertConfig\n \n@@ -326,7 +325,6 @@ class BertSdpaSelfAttention(BertSelfAttention):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from BertSelfAttention\n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n@@ -405,14 +403,6 @@ def forward(\n                 if is_cross_attention:\n                     past_key_values.is_updated[self.layer_idx] = True\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n-            query_layer = query_layer.contiguous()\n-            key_layer = key_layer.contiguous()\n-            value_layer = value_layer.contiguous()\n-\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create"
        },
        {
            "sha": "f3a2918d4610eca04d2d4fbfb2c6805e344ebbf6",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=f46f29dd7ccc44a1bfb9fdc64a5171628f55b999",
            "patch": "@@ -20,7 +20,6 @@\n \n import torch\n import torch.utils.checkpoint\n-from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -41,7 +40,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils import auto_docstring, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_camembert import CamembertConfig\n \n@@ -277,7 +276,6 @@ class CamembertSdpaSelfAttention(CamembertSelfAttention):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from CamembertSelfAttention\n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n@@ -356,14 +354,6 @@ def forward(\n                 if is_cross_attention:\n                     past_key_values.is_updated[self.layer_idx] = True\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n-            query_layer = query_layer.contiguous()\n-            key_layer = key_layer.contiguous()\n-            value_layer = value_layer.contiguous()\n-\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create"
        },
        {
            "sha": "c840d47fbf70335cbc39b3bb98a77075851b402d",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=f46f29dd7ccc44a1bfb9fdc64a5171628f55b999",
            "patch": "@@ -20,7 +20,6 @@\n \n import torch\n import torch.utils.checkpoint\n-from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -41,7 +40,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils import auto_docstring, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_roberta import RobertaConfig\n \n@@ -276,7 +275,6 @@ class RobertaSdpaSelfAttention(RobertaSelfAttention):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from RobertaSelfAttention\n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n@@ -355,14 +353,6 @@ def forward(\n                 if is_cross_attention:\n                     past_key_values.is_updated[self.layer_idx] = True\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n-            query_layer = query_layer.contiguous()\n-            key_layer = key_layer.contiguous()\n-            value_layer = value_layer.contiguous()\n-\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create"
        },
        {
            "sha": "76d9dc8318ec13bce100c7f4b62ab52fe01e15ab",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=f46f29dd7ccc44a1bfb9fdc64a5171628f55b999",
            "patch": "@@ -20,7 +20,6 @@\n \n import torch\n import torch.utils.checkpoint\n-from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -41,7 +40,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils import auto_docstring, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_xlm_roberta import XLMRobertaConfig\n \n@@ -277,7 +276,6 @@ class XLMRobertaSdpaSelfAttention(XLMRobertaSelfAttention):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from XLMRobertaSelfAttention\n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n@@ -356,14 +354,6 @@ def forward(\n                 if is_cross_attention:\n                     past_key_values.is_updated[self.layer_idx] = True\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n-            query_layer = query_layer.contiguous()\n-            key_layer = key_layer.contiguous()\n-            value_layer = value_layer.contiguous()\n-\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create"
        },
        {
            "sha": "0e720fa60664e23b8bb606c8c82c7bf07fefba0f",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=f46f29dd7ccc44a1bfb9fdc64a5171628f55b999",
            "patch": "@@ -19,7 +19,6 @@\n \n import torch\n import torch.utils.checkpoint\n-from packaging import version\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n@@ -40,7 +39,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, get_torch_version, logging\n+from ...utils import auto_docstring, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_xlm_roberta_xl import XLMRobertaXLConfig\n \n@@ -274,7 +273,6 @@ class XLMRobertaXLSdpaSelfAttention(XLMRobertaXLSelfAttention):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n         self.dropout_prob = config.attention_probs_dropout_prob\n-        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n \n     # Adapted from XLMRobertaXLSelfAttention\n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n@@ -353,14 +351,6 @@ def forward(\n                 if is_cross_attention:\n                     past_key_values.is_updated[self.layer_idx] = True\n \n-        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n-        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577\n-        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n-            query_layer = query_layer.contiguous()\n-            key_layer = key_layer.contiguous()\n-            value_layer = value_layer.contiguous()\n-\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create"
        },
        {
            "sha": "126c98462376e7d20f5a586f9ba9b321ab2d0593",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 13,
            "deletions": 31,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f46f29dd7ccc44a1bfb9fdc64a5171628f55b999/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=f46f29dd7ccc44a1bfb9fdc64a5171628f55b999",
            "patch": "@@ -29,11 +29,9 @@\n from typing import Any, Callable, Optional, TypedDict\n \n import numpy as np\n-from packaging import version\n \n from ..utils import logging\n from .import_utils import (\n-    get_torch_version,\n     is_flax_available,\n     is_mlx_available,\n     is_tf_available,\n@@ -351,23 +349,14 @@ def __init_subclass__(cls) -> None:\n         `static_graph=True` with modules that output `ModelOutput` subclasses.\n         \"\"\"\n         if is_torch_available():\n-            if version.parse(get_torch_version()) >= version.parse(\"2.2\"):\n-                from torch.utils._pytree import register_pytree_node\n-\n-                register_pytree_node(\n-                    cls,\n-                    _model_output_flatten,\n-                    partial(_model_output_unflatten, output_type=cls),\n-                    serialized_type_name=f\"{cls.__module__}.{cls.__name__}\",\n-                )\n-            else:\n-                from torch.utils._pytree import _register_pytree_node\n+            from torch.utils._pytree import register_pytree_node\n \n-                _register_pytree_node(\n-                    cls,\n-                    _model_output_flatten,\n-                    partial(_model_output_unflatten, output_type=cls),\n-                )\n+            register_pytree_node(\n+                cls,\n+                _model_output_flatten,\n+                partial(_model_output_unflatten, output_type=cls),\n+                serialized_type_name=f\"{cls.__module__}.{cls.__name__}\",\n+            )\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -494,19 +483,12 @@ def _model_output_unflatten(\n     ) -> ModelOutput:\n         return output_type(**dict(zip(context, values)))\n \n-    if version.parse(get_torch_version()) >= version.parse(\"2.2\"):\n-        _torch_pytree.register_pytree_node(\n-            ModelOutput,\n-            _model_output_flatten,\n-            partial(_model_output_unflatten, output_type=ModelOutput),\n-            serialized_type_name=f\"{ModelOutput.__module__}.{ModelOutput.__name__}\",\n-        )\n-    else:\n-        _torch_pytree._register_pytree_node(\n-            ModelOutput,\n-            _model_output_flatten,\n-            partial(_model_output_unflatten, output_type=ModelOutput),\n-        )\n+    _torch_pytree.register_pytree_node(\n+        ModelOutput,\n+        _model_output_flatten,\n+        partial(_model_output_unflatten, output_type=ModelOutput),\n+        serialized_type_name=f\"{ModelOutput.__module__}.{ModelOutput.__name__}\",\n+    )\n \n \n class ExplicitEnum(str, Enum):"
        }
    ],
    "stats": {
        "total": 104,
        "additions": 18,
        "deletions": 86
    }
}