{
    "author": "vasqu",
    "message": "[`Padding-Free Attention`] Fix packed FA attention with pos ids only  (#42801)\n\n* fix position ids\n\n* style\n\n* fix",
    "sha": "d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
    "files": [
        {
            "sha": "1132cc5a6cd310e903052f497ea55191943b05d7",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -342,7 +342,6 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         batch_size, seq_length = hidden_states.shape[:-1]"
        },
        {
            "sha": "f8eeef52b39c16f11b7bb9f975332ca282fb7e3f",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -369,7 +369,6 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         batch_size, seq_length = hidden_states.shape[:-1]"
        },
        {
            "sha": "86de994963e48900f297a02c37e356605ca6b8b6",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -525,7 +525,6 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[EncoderDecoderCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n         self_attn_cache = past_key_values"
        },
        {
            "sha": "759e450b7bcb5242dd27f3fd5bfcb857f61f7361",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -314,7 +314,6 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[EncoderDecoderCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]]:\n         self_attn_cache = past_key_values"
        },
        {
            "sha": "74aaf8fc1276393520f1343022a607f64ca5a664",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -297,7 +297,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "cf4d16da6f538a9b24cb3ebf5c9b51f384e4c34d",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -321,7 +321,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "3cc8c7301e7dfe4526198cd054f08695bb64fdfe",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -240,7 +240,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "09cc7fff45d73213b401bd7311025e592f04e771",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -260,7 +260,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "ed622fa486db42d70498d106245597fe54242891",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -252,7 +252,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "751d36413eb264e4c8be01e4e690ff0124be365a",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -344,7 +344,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -374,7 +373,6 @@ def forward(\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n             sliding_window=self.sliding_window,\n-            position_ids=position_ids,\n             s_aux=self.sinks,  # diff with Llama\n             **kwargs,\n         )"
        },
        {
            "sha": "b04b5d53b2bd2adbf94ac8a120c583d83446540d",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -269,7 +269,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         input_shape = hidden_states.shape[:-1]\n@@ -299,7 +298,6 @@ def forward(\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n             sliding_window=self.sliding_window,\n-            position_ids=position_ids,\n             s_aux=self.sinks,  # diff with Llama\n             **kwargs,\n         )"
        },
        {
            "sha": "3b9fc6a115c4fbd201d82abf9efc6bd431902679",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -384,7 +384,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "505123291a2251379180a7387fe5388b818f919f",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -233,7 +233,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Lfm2HybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "539de3c2cd245674f3bde27ce6e639822ec6aa43",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -452,7 +452,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Lfm2MoeHybridConvCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "8dc730c7f8f8ba9b08dec30e35ac2dc8ff428030",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -247,7 +247,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "8490684895470e5921abb0dd539f8bdc996cdabf",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -122,7 +122,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "b5fda4b2f151db4941eb59720a6a55c5d6cc12cb",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -241,7 +241,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "2557891dd333dbfc21226d343edd3be01a026dbb",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -219,7 +219,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "9c00911a66d993687eda0cf05c6cf7204a861b22",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -206,7 +206,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "c1f60d60374b454dac94f809d08c901e2c2e9191",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -92,7 +92,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "5a40187596c6ec0578acd157d7c28606c00234ac",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -425,7 +425,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Zamba2HybridDynamicCache] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "58425790fae0ba93b192cb2f49dc1b93fdd1a53b",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -232,7 +232,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Zamba2HybridDynamicCache] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        position_ids: Optional[torch.Tensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         input_shape = hidden_states.shape[:-1]"
        },
        {
            "sha": "5b7a10ad71868351a0df59a5dc4928f6cdd9e8f9",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=d1eda63fa38ff29e7d5d930eb3eeff9fa75e1a9d",
            "patch": "@@ -703,6 +703,8 @@ class TransformersKwargs(TypedDict, total=False):\n             Maximum sequence length for query state.\n         max_length_k (`int`, *optional*):\n             Maximum sequence length for key state.\n+        position_ids (`torch.LongTensor`, *optional*)\n+            Indices of positions of each input sequence tokens.\n     \"\"\"\n \n     num_items_in_batch: Optional[\"torch.Tensor\"]\n@@ -713,6 +715,7 @@ class TransformersKwargs(TypedDict, total=False):\n     cu_seq_lens_k: Optional[\"torch.LongTensor\"]\n     max_length_q: int | None\n     max_length_k: int | None\n+    position_ids: Optional[\"torch.LongTensor\"]\n \n \n def is_timm_config_dict(config_dict: dict[str, Any]) -> bool:"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 3,
        "deletions": 24
    }
}