{
    "author": "yonigozlan",
    "message": "Add Got-OCR 2 Fast image processor and refactor slow one (#36185)\n\n* refactor image processor slow got ocr\n\n* add working image processor fast\n\n* fix fast image processor, update doc\n\n* use one big loop for processing patches",
    "sha": "2c5d038f9204ffbc78acd398238dd5231341e648",
    "files": [
        {
            "sha": "33607033b49f5f463af0eb13d32a7e3087fa1aa1",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 25,
            "deletions": 14,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -44,13 +44,14 @@ The original code can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2\n \n ```python\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n \n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n->>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n >>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n->>> inputs = processor(image, return_tensors=\"pt\").to(device)\n+>>> inputs = processor(image, return_tensors=\"pt\", device=device).to(device)\n \n >>> generate_ids = model.generate(\n ...     **inputs,\n@@ -68,15 +69,16 @@ The original code can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2\n \n ```python\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n \n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n->>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n >>> image1 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n >>> image2 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/image_ocr.jpg\"\n \n->>> inputs = processor([image1, image2], return_tensors=\"pt\").to(device)\n+>>> inputs = processor([image1, image2], return_tensors=\"pt\", device=device).to(device)\n \n >>> generate_ids = model.generate(\n ...     **inputs,\n@@ -96,13 +98,14 @@ GOT-OCR2 can also generate formatted text, such as markdown or LaTeX. Here is an\n \n ```python\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n \n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n->>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n >>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/latex.png\"\n->>> inputs = processor(image, return_tensors=\"pt\", format=True).to(device)\n+>>> inputs = processor(image, return_tensors=\"pt\", format=True, device=device).to(device)\n \n >>> generate_ids = model.generate(\n ...     **inputs,\n@@ -124,14 +127,15 @@ Here is an example of how to process multiple pages at once:\n \n ```python\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n \n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n->>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n >>> image1 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/page1.png\"\n >>> image2 = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/page2.png\"\n->>> inputs = processor([image1, image2], return_tensors=\"pt\", multi_page=True, format=True).to(device)\n+>>> inputs = processor([image1, image2], return_tensors=\"pt\", multi_page=True, format=True, device=device).to(device)\n \n >>> generate_ids = model.generate(\n ...     **inputs,\n@@ -153,13 +157,14 @@ Here is an example of how to process cropped patches:\n ```python\n >>> import torch\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n \n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", torch_dtype=torch.bfloat16, device_map=device)\n->>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n >>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/one_column.png\"\n->>> inputs = processor(image, return_tensors=\"pt\", format=True, crop_to_patches=True, max_patches=3).to(device)\n+>>> inputs = processor(image, return_tensors=\"pt\", format=True, crop_to_patches=True, max_patches=3, device=device).to(device)\n \n >>> generate_ids = model.generate(\n ...     **inputs,\n@@ -179,13 +184,14 @@ GOT supports interactive OCR, where the user can specify the region to be recogn\n \n ```python\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n \n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n->>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n >>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/multi_box.png\"\n->>> inputs = processor(image, return_tensors=\"pt\", color=\"green\").to(device) # or box=[x1, y1, x2, y2] for coordinates (image pixels)\n+>>> inputs = processor(image, return_tensors=\"pt\", color=\"green\", device=device).to(device) # or box=[x1, y1, x2, y2] for coordinates (image pixels)\n \n >>> generate_ids = model.generate(\n ...     **inputs,\n@@ -206,14 +212,15 @@ Here is an example of how to process sheet music:\n \n ```python\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n >>> import verovio\n \n >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n->>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\")\n+>>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n >>> image = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_got_ocr/resolve/main/sheet_music.png\"\n->>> inputs = processor(image, return_tensors=\"pt\", format=True).to(device)\n+>>> inputs = processor(image, return_tensors=\"pt\", format=True, device=device).to(device)\n \n >>> generate_ids = model.generate(\n ...     **inputs,\n@@ -258,6 +265,10 @@ alt=\"drawing\" width=\"600\"/>\n \n [[autodoc]] GotOcr2ImageProcessor\n \n+## GotOcr2ImageProcessorFast\n+\n+[[autodoc]] GotOcr2ImageProcessorFast\n+\n ## GotOcr2Processor\n \n [[autodoc]] GotOcr2Processor"
        },
        {
            "sha": "f05a7b3b2c19ff20fce374d04d58cded3aaa4e76",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -1330,6 +1330,7 @@\n     _import_structure[\"models.deit\"].append(\"DeiTImageProcessorFast\")\n     _import_structure[\"models.depth_pro\"].append(\"DepthProImageProcessorFast\")\n     _import_structure[\"models.detr\"].append(\"DetrImageProcessorFast\")\n+    _import_structure[\"models.got_ocr2\"].append(\"GotOcr2ImageProcessorFast\")\n     _import_structure[\"models.llava\"].append(\"LlavaImageProcessorFast\")\n     _import_structure[\"models.llava_next\"].append(\"LlavaNextImageProcessorFast\")\n     _import_structure[\"models.llava_onevision\"].append(\"LlavaOnevisionImageProcessorFast\")\n@@ -6526,6 +6527,7 @@\n         from .models.deit import DeiTImageProcessorFast\n         from .models.depth_pro import DepthProImageProcessorFast\n         from .models.detr import DetrImageProcessorFast\n+        from .models.got_ocr2 import GotOcr2ImageProcessorFast\n         from .models.llava import LlavaImageProcessorFast\n         from .models.llava_next import LlavaNextImageProcessorFast\n         from .models.llava_onevision import LlavaOnevisionImageProcessorFast"
        },
        {
            "sha": "180d156359b21267d28539c856245c5b1af93e47",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -88,7 +88,7 @@\n             (\"fuyu\", (\"FuyuImageProcessor\",)),\n             (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"glpn\", (\"GLPNImageProcessor\",)),\n-            (\"got_ocr2\", (\"GotOcr2ImageProcessor\",)),\n+            (\"got_ocr2\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n             (\"grounding-dino\", (\"GroundingDinoImageProcessor\",)),\n             (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"hiera\", (\"BitImageProcessor\",)),"
        },
        {
            "sha": "00b6ccc53fc0efb0fc88c2f95586276cd40010fe",
            "filename": "src/transformers/models/got_ocr2/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2F__init__.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_got_ocr2 import *\n     from .image_processing_got_ocr2 import *\n+    from .image_processing_got_ocr2_fast import *\n     from .modeling_got_ocr2 import *\n     from .processing_got_ocr2 import *\n "
        },
        {
            "sha": "d052f4a543f641a63402b595b8d2cc35f538a59c",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 79,
            "deletions": 66,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -1,9 +1,3 @@\n-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#           This file was automatically generated from src/transformers/models/got_ocr2/modular_got_ocr2.py.\n-#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#             the file from the modular. If any change should be done, please apply the change to the\n-#                          modular_got_ocr2.py file directly. One of our CI enforces this.\n-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n #\n@@ -18,7 +12,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n+\"\"\"Image processor class for Got-OCR-2.\"\"\"\n \n from functools import lru_cache\n from typing import Dict, List, Optional, Tuple, Union\n@@ -27,11 +21,9 @@\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import (\n-    _rescale_for_pil_conversion,\n     convert_to_rgb,\n     resize,\n     to_channel_dimension_format,\n-    to_pil_image,\n )\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n@@ -142,6 +134,15 @@ class GotOcr2ImageProcessor(BaseImageProcessor):\n         size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n             Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n             method.\n+        crop_to_patches (`bool`, *optional*, defaults to `False`):\n+            Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n+            `preprocess` method.\n+        min_patches (`int`, *optional*, defaults to 1):\n+            The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+            set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n+        max_patches (`int`, *optional*, defaults to 12):\n+            The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+            set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n             Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n             overridden by the `resample` parameter in the `preprocess` method.\n@@ -172,6 +173,9 @@ def __init__(\n         self,\n         do_resize: bool = True,\n         size: Dict[str, int] = None,\n+        crop_to_patches: bool = False,\n+        min_patches: int = 1,\n+        max_patches: int = 12,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -187,6 +191,9 @@ def __init__(\n \n         self.do_resize = do_resize\n         self.size = size\n+        self.crop_to_patches = crop_to_patches\n+        self.min_patches = min_patches\n+        self.max_patches = max_patches\n         self.resample = resample\n         self.do_rescale = do_rescale\n         self.rescale_factor = rescale_factor\n@@ -249,6 +256,9 @@ def preprocess(\n         images: ImageInput,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,\n+        crop_to_patches: Optional[bool] = None,\n+        min_patches: Optional[int] = None,\n+        max_patches: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n@@ -274,6 +284,14 @@ def preprocess(\n                 `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n                 is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n                 edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n+            crop_to_patches (`bool`, *optional*, defaults to `self.crop_to_patches`):\n+                Whether to crop the image to patches.\n+            min_patches (`int`, *optional*, defaults to `self.min_patches`):\n+                The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+                set to `True`.\n+            max_patches (`int`, *optional*, defaults to `self.max_patches`):\n+                The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+                set to `True`.\n             resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`.\n             do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n@@ -308,6 +326,9 @@ def preprocess(\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n         \"\"\"\n         do_resize = do_resize if do_resize is not None else self.do_resize\n+        crop_to_patches = crop_to_patches if crop_to_patches is not None else self.crop_to_patches\n+        min_patches = min_patches if min_patches is not None else self.min_patches\n+        max_patches = max_patches if max_patches is not None else self.max_patches\n         resample = resample if resample is not None else self.resample\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n         rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n@@ -353,40 +374,52 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        if do_resize:\n-            images = [\n-                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_normalize:\n+        if crop_to_patches and max_patches > 1:\n             images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n+                self.crop_image_to_patches(\n+                    image,\n+                    min_patches=min_patches,\n+                    max_patches=max_patches,\n+                    patch_size=size,\n+                    data_format=input_data_format,\n+                )\n                 for image in images\n             ]\n+            num_patches = np.array([len(image) for image in images])\n+            images = [image for images_list in images for image in images_list]\n+        else:\n+            num_patches = np.array([1] * len(images))\n+\n+        for i, image in enumerate(images):\n+            if do_resize:\n+                images[i] = self.resize(image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                images[i] = self.rescale(image=images[i], scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                images[i] = self.normalize(\n+                    image=images[i],\n+                    mean=image_mean,\n+                    std=image_std,\n+                    input_data_format=input_data_format,\n+                )\n \n-        images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n-        ]\n+            images[i] = to_channel_dimension_format(images[i], data_format, input_channel_dim=input_data_format)\n \n-        encoded_outputs = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)\n+        encoded_outputs = BatchFeature(\n+            data={\"pixel_values\": images, \"num_patches\": num_patches}, tensor_type=return_tensors\n+        )\n \n         return encoded_outputs\n \n     def crop_image_to_patches(\n         self,\n-        image: ImageInput,\n+        images: np.ndarray,\n         min_patches: int,\n         max_patches: int,\n         use_thumbnail: bool = True,\n         patch_size: Union[Tuple, int, dict] = None,\n-        return_numpy: bool = False,\n         data_format: ChannelDimension = None,\n     ):\n         \"\"\"\n@@ -396,8 +429,8 @@ def crop_image_to_patches(\n         The aspect ratio of the patches grid is chosen to be the closest to the original image aspect ratio.\n \n         Args:\n-            image (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`):\n-                The image to be cropped. The image can be a PIL image, NumPy array or PyTorch tensor.\n+            images (`np.ndarray`):\n+                The image to be cropped.\n             min_patches (`int`):\n                 The minimum number of patches to be extracted from the image.\n             max_patches (`int`):\n@@ -406,24 +439,17 @@ def crop_image_to_patches(\n                 Whether to add a thumbnail image to the list of cropped patches.\n             patch_size (`int`, `Tuple[int, int]`, `dict`, *optional*):\n                 The size of the output patches.\n-            return_numpy (`bool`, *optional*, defaults to `False`):\n-                Whether to return the cropped images as NumPy arrays.\n             data_format (`ChannelDimension`, *optional*):\n                 The format of the image data. If `None`, the format is inferred from the input image.\n \n         Returns:\n             List[`PIL.Image.Image`] or List[np.ndarray]: The list of cropped images.\n         \"\"\"\n-        patch_size = patch_size if patch_size is not None else self.size\n-        patch_size = get_size_dict(patch_size, default_to_square=True)\n-        original_size = get_size_dict(image.size, height_width_order=False)\n-        do_rescale = False\n-        if not isinstance(image, PIL.Image.Image):\n-            do_rescale = _rescale_for_pil_conversion(image)\n-            image = to_pil_image(image, do_rescale=do_rescale)\n-\n+        if data_format is None:\n+            data_format = infer_channel_dimension_format(images)\n+        images = to_channel_dimension_format(images, ChannelDimension.FIRST, data_format)\n         patch_size_height, patch_size_width = patch_size[\"height\"], patch_size[\"width\"]\n-        original_height, original_width = original_size[\"height\"], original_size[\"width\"]\n+        original_height, original_width = images.shape[-2:]\n         # find the closest aspect ratio to the target\n         num_columns, num_rows = get_optimal_tiled_canvas(\n             (original_height, original_width), (patch_size_height, patch_size_width), min_patches, max_patches\n@@ -435,8 +461,12 @@ def crop_image_to_patches(\n         num_blocks = num_columns * num_rows\n \n         # resize the image so that each patch is of patch_size\n-        resized_image = image.resize((target_width, target_height))\n-\n+        resized_image = self.resize(\n+            images,\n+            {\"height\": target_height, \"width\": target_width},\n+            data_format=ChannelDimension.FIRST,\n+            input_data_format=ChannelDimension.FIRST,\n+        )\n         # split the image into patches\n         processed_images = []\n         for i in range(num_blocks):\n@@ -449,33 +479,16 @@ def crop_image_to_patches(\n                 (row + 1) * patch_size_height,\n             )\n             # split the image\n-            patch_image = resized_image.crop(box)\n+            patch_image = resized_image[..., box[1] : box[3], box[0] : box[2]]\n+            patch_image = to_channel_dimension_format(patch_image, data_format, ChannelDimension.FIRST)\n             processed_images.append(patch_image)\n \n         if use_thumbnail and len(processed_images) != 1:\n-            thumbnail_img = image.resize((patch_size_width, patch_size_height))\n+            thumbnail_img = self.resize(\n+                images, patch_size, data_format=data_format, input_data_format=ChannelDimension.FIRST\n+            )\n             processed_images.append(thumbnail_img)\n \n-        if return_numpy:\n-            processed_images_numpy = []\n-            for processed_image in processed_images:\n-                processed_image = np.array(processed_image)\n-                # If the input image channel dimension was of size 1, then it is dropped when converting to a PIL image\n-                # so we need to add it back if necessary.\n-                processed_image = (\n-                    np.expand_dims(processed_image, axis=-1) if processed_image.ndim == 2 else processed_image\n-                )\n-                # The image is always in channels last format after converting from a PIL image\n-                if data_format is not None:\n-                    processed_image = to_channel_dimension_format(\n-                        processed_image, data_format, input_channel_dim=ChannelDimension.LAST\n-                    )\n-                # If an image was rescaled to be in the range [0, 255] before converting to a PIL image, then we need to\n-                # rescale it back to the original range.\n-                processed_image = self.rescale(processed_image, 1 / 255) if do_rescale else processed_image\n-                processed_images_numpy.append(processed_image)\n-            processed_images = processed_images_numpy\n-\n         return processed_images\n \n "
        },
        {
            "sha": "5103f73b1166e3dfa1c96541a8a914b4fc16a1c3",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "added",
            "additions": 257,
            "deletions": 0,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -0,0 +1,257 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Got-OCR-2.\"\"\"\n+\n+from typing import List, Optional, Tuple, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorInitKwargs,\n+    DefaultFastImageProcessorPreprocessKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+from .image_processing_got_ocr2 import get_optimal_tiled_canvas\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class GotOcr2ImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+\n+\n+class GotOcr2ImageProcessorPreprocessKwargs(DefaultFastImageProcessorPreprocessKwargs):\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast GotOcr2 image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        crop_to_patches (`bool`, *optional*, defaults to `False`):\n+            Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n+            `preprocess` method.\n+        min_patches (`int`, *optional*, defaults to 1):\n+            The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+            set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n+        max_patches (`int`, *optional*, defaults to 12):\n+            The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+            set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+    \"\"\",\n+)\n+class GotOcr2ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = OPENAI_CLIP_MEAN\n+    image_std = OPENAI_CLIP_STD\n+    size = {\"height\": 384, \"width\": 384}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    crop_to_patches = False\n+    min_patches = 1\n+    max_patches = 12\n+    valid_init_kwargs = GotOcr2ImageProcessorInitKwargs\n+    valid_preprocess_kwargs = GotOcr2ImageProcessorPreprocessKwargs\n+\n+    def __init__(self, **kwargs: Unpack[GotOcr2ImageProcessorInitKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+            crop_to_patches (`bool`, *optional*, defaults to `False`):\n+                Whether to crop the image to patches. Can be overridden by the `crop_to_patches` parameter in the\n+                `preprocess` method.\n+            min_patches (`int`, *optional*, defaults to 1):\n+                The minimum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+                set to `True`. Can be overridden by the `min_patches` parameter in the `preprocess` method.\n+            max_patches (`int`, *optional*, defaults to 12):\n+                The maximum number of patches to be extracted from the image. Only has an effect if `crop_to_patches` is\n+                set to `True`. Can be overridden by the `max_patches` parameter in the `preprocess` method.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[GotOcr2ImageProcessorPreprocessKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def crop_image_to_patches(\n+        self,\n+        images: \"torch.Tensor\",\n+        min_patches: int,\n+        max_patches: int,\n+        use_thumbnail: bool = True,\n+        patch_size: Union[Tuple, int, dict] = None,\n+        interpolation: Optional[\"F.InterpolationMode\"] = None,\n+    ):\n+        \"\"\"\n+        Crop the images to patches and return a list of cropped images.\n+        The number of patches and their grid arrangement are determined by the original image size,\n+        the target patch size and the minimum and maximum number of patches.\n+        The aspect ratio of the patches grid is chosen to be the closest to the original image aspect ratio.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                The images to be cropped.\n+            min_patches (`int`):\n+                The minimum number of patches to be extracted from the image.\n+            max_patches (`int`):\n+                The maximum number of patches to be extracted from the image.\n+            use_thumbnail (`bool`, *optional*, defaults to `True`):\n+                Whether to add a thumbnail image to the list of cropped patches.\n+            patch_size (`int`, `Tuple[int, int]`, `dict`, *optional*):\n+                The size of the output patches.\n+                The format of the image data. If `None`, the format is inferred from the input image.\n+\n+        Returns:\n+            List[`PIL.Image.Image`] or List[np.ndarray]: The list of cropped images.\n+        \"\"\"\n+        patch_size_height, patch_size_width = patch_size.height, patch_size.width\n+        original_height, original_width = images.shape[-2:]\n+        # find the closest aspect ratio to the target\n+        num_columns, num_rows = get_optimal_tiled_canvas(\n+            (original_height, original_width), (patch_size_height, patch_size_width), min_patches, max_patches\n+        )\n+\n+        # calculate the target width and height\n+        target_width = patch_size_width * num_columns\n+        target_height = patch_size_height * num_rows\n+        num_blocks = num_columns * num_rows\n+\n+        # resize the image so that each patch is of patch_size\n+        resized_image = self.resize(\n+            images, SizeDict(height=target_height, width=target_width), interpolation=interpolation\n+        )\n+        # split the image into patches\n+        processed_images = []\n+        for i in range(num_blocks):\n+            column = i % num_columns\n+            row = i // num_columns\n+            box = (\n+                column * patch_size_width,\n+                row * patch_size_height,\n+                (column + 1) * patch_size_width,\n+                (row + 1) * patch_size_height,\n+            )\n+            # split the image\n+            patch_image = resized_image[..., box[1] : box[3], box[0] : box[2]]\n+            processed_images.append(patch_image)\n+\n+        if use_thumbnail and len(processed_images) != 1:\n+            thumbnail_img = self.resize(images, patch_size, interpolation=interpolation)\n+            processed_images.append(thumbnail_img)\n+\n+        processed_images = torch.stack(processed_images, dim=0).transpose(0, 1).contiguous()\n+\n+        return processed_images\n+\n+    def _preprocess(\n+        self,\n+        images: List[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        crop_to_patches: bool,\n+        min_patches: int,\n+        max_patches: int,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> BatchFeature:\n+        if crop_to_patches:\n+            grouped_images, grouped_images_index = group_images_by_shape(images)\n+            processed_images_grouped = {}\n+            num_patches = {}\n+            for shape, stacked_images in grouped_images.items():\n+                stacked_images = self.crop_image_to_patches(\n+                    stacked_images,\n+                    min_patches,\n+                    max_patches,\n+                    patch_size=size,\n+                    interpolation=interpolation,\n+                )\n+                processed_images_grouped[shape] = stacked_images\n+                num_patches[shape] = [stacked_images.shape[1]] * stacked_images.shape[0]\n+            images = reorder_images(processed_images_grouped, grouped_images_index)\n+            images = [image for images_list in images for image in images_list]\n+            num_patches = reorder_images(num_patches, grouped_images_index)\n+        else:\n+            num_patches = [1] * len(images)\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(\n+            data={\"pixel_values\": processed_images, \"num_patches\": num_patches}, tensor_type=return_tensors\n+        )\n+\n+\n+__all__ = [\"GotOcr2ImageProcessorFast\"]"
        },
        {
            "sha": "918ee2bb039d1eba5ac2f2afa683a603589b1cae",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -32,11 +32,7 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    replace_return_docstrings,\n-)\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n from ..auto import AutoModelForCausalLM\n from .configuration_got_ocr2 import GotOcr2Config, GotOcr2VisionConfig\n "
        },
        {
            "sha": "2dc500b8d6c32a04dd0a2f8f989e41ccd1ac2034",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 451,
            "changes": 452,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -14,35 +14,20 @@\n # limitations under the License.\n \n \n-from functools import lru_cache\n from typing import List, Optional, Tuple, Union\n \n-import numpy as np\n import torch\n import torch.nn as nn\n import torch.utils.checkpoint\n \n-from transformers.models.blip.image_processing_blip import BlipImageProcessor\n from transformers.models.llava.modeling_llava import (\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n     LlavaPreTrainedModel,\n )\n from transformers.models.sam.modeling_sam import SamMLPBlock, SamVisionAttention, SamVisionEncoder, SamVisionLayer\n-from transformers.processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n-from transformers.tokenization_utils_base import (\n-    PreTokenizedInput,\n-    TextInput,\n-)\n \n from ...configuration_utils import PretrainedConfig\n-from ...image_processing_utils import BatchFeature, get_size_dict\n-from ...image_transforms import (\n-    _rescale_for_pil_conversion,\n-    to_channel_dimension_format,\n-    to_pil_image,\n-)\n-from ...image_utils import ChannelDimension, ImageInput\n from ...utils import (\n     add_start_docstrings_to_model_forward,\n     is_vision_available,\n@@ -53,9 +38,7 @@\n \n \n if is_vision_available():\n-    import PIL\n-\n-    from ...image_utils import load_images\n+    pass\n \n logger = logging.get_logger(__name__)\n \n@@ -246,437 +229,6 @@ def __init__(\n __all__ = [\"GotOcr2VisionConfig\", \"GotOcr2Config\"]\n \n \n-class GotOcr2TextKwargs(TextKwargs, total=False):\n-    format: Optional[bool]\n-\n-\n-class GotOcr2ImagesKwargs(ImagesKwargs, total=False):\n-    box: Optional[Union[List, Tuple[float, float], Tuple[float, float, float, float]]]\n-    color: Optional[str]\n-    num_image_tokens: Optional[int]\n-    multi_page: Optional[bool]\n-    crop_to_patches: Optional[bool]\n-    min_patches: Optional[int]\n-    max_patches: Optional[int]\n-\n-\n-class GotOcr2ProcessorKwargs(ProcessingKwargs, total=False):\n-    text_kwargs: GotOcr2TextKwargs\n-    images_kwargs: GotOcr2ImagesKwargs\n-    _defaults = {\n-        \"text_kwargs\": {\n-            \"padding\": False,\n-            \"format\": False,\n-        },\n-        \"images_kwargs\": {\n-            \"num_image_tokens\": 256,\n-            \"multi_page\": False,\n-            \"crop_to_patches\": False,\n-            \"min_patches\": 1,\n-            \"max_patches\": 12,\n-        },\n-    }\n-\n-\n-def preprocess_box_annotation(box: Union[List, Tuple], image_size: Tuple[int, int]) -> List:\n-    \"\"\"\n-    Convert box annotation to the format [x1, y1, x2, y2] in the range [0, 1000].\n-    \"\"\"\n-    width, height = image_size\n-    if len(box) == 4:\n-        box[0] = int(box[0] / width * 1000)\n-        box[1] = int(box[1] / height * 1000)\n-        box[2] = int(box[2] / width * 1000)\n-        box[3] = int(box[3] / height * 1000)\n-    else:\n-        raise ValueError(\"Box must be a list or tuple of lists in the form [x1, y1, x2, y2].\")\n-\n-    return list(box)\n-\n-\n-# Similar to image_processing_mllama.get_all_supported_aspect_ratios\n-@lru_cache(maxsize=10)\n-def get_all_supported_aspect_ratios(min_image_tiles: int, max_image_tiles: int) -> List[Tuple[int, int]]:\n-    \"\"\"\n-    Computes all allowed aspect ratios for a given minimum and maximum number of input tiles.\n-\n-    This function calculates all possible arrangements of tiles that can be formed\n-    within the constraint of the minimum and maximum number of tiles. Each arrangement is\n-    represented by its aspect ratio (width/height) and the corresponding tile configuration.\n-\n-    Args:\n-        min_image_tiles (`int`):\n-            The minimum number of tiles allowed.\n-        max_image_tiles (`int`):\n-            The maximum number of tiles allowed.\n-\n-    Returns:\n-        `List[Tuple[int, int]]`: A list of tuples, each tuple representing a valid (width, height)\n-        configuration in terms of number of tiles.\n-\n-    Example:\n-        >>> get_all_supported_aspect_ratios(1, 4)\n-        [(1, 1), (1, 2), (2, 1), (1, 3), (3, 1), (1, 4), (2, 2), (4, 1)]\n-\n-    \"\"\"\n-    aspect_ratios = []\n-    for width in range(1, max_image_tiles + 1):\n-        for height in range(1, max_image_tiles + 1):\n-            if width * height <= max_image_tiles and width * height >= min_image_tiles:\n-                aspect_ratios.append((width, height))\n-\n-    aspect_ratios = sorted(aspect_ratios, key=lambda x: x[0] * x[1])\n-\n-    return aspect_ratios\n-\n-\n-@lru_cache(maxsize=100)\n-def get_optimal_tiled_canvas(\n-    original_image_size: Tuple[int, int],\n-    target_tile_size: Tuple[int, int],\n-    min_image_tiles: int,\n-    max_image_tiles: int,\n-) -> Tuple[int, int]:\n-    \"\"\"\n-    Given a minimum and maximum number of tiles, find the canvas with the closest aspect ratio to the\n-    original image aspect ratio.\n-    In case of tie-breaking condition when two canvases have the same aspect ratio difference, we favor the canvas with\n-    more tiles, until the area covered by the tiles is more than twice the target area, in order to avoid unnecessarily\n-    excessive tiling.\n-    \"\"\"\n-    possible_tile_arrangements = get_all_supported_aspect_ratios(min_image_tiles, max_image_tiles)\n-\n-    original_height, original_width = original_image_size\n-    target_tile_height, target_tile_width = target_tile_size\n-    aspect_ratio = original_width / original_height\n-    area = original_width * original_height\n-\n-    # find the grid with the best aspect ratio\n-    best_ratio_diff = float(\"inf\")\n-    best_grid = (1, 1)\n-    for grid in possible_tile_arrangements:\n-        grid_aspect_ratio = grid[0] / grid[1]\n-        ratio_diff = abs(aspect_ratio - grid_aspect_ratio)\n-        if ratio_diff < best_ratio_diff:\n-            best_ratio_diff = ratio_diff\n-            best_grid = grid\n-        elif ratio_diff == best_ratio_diff:\n-            # if the aspect ratio difference is the same, we favor the grid with more patches\n-            # until the area covered by the patches is more than twice the original image area\n-            if area > 0.5 * target_tile_height * target_tile_width * grid[0] * grid[1]:\n-                best_grid = grid\n-\n-    return best_grid\n-\n-\n-class GotOcr2ImageProcessor(BlipImageProcessor):\n-    def crop_image_to_patches(\n-        self,\n-        image: ImageInput,\n-        min_patches: int,\n-        max_patches: int,\n-        use_thumbnail: bool = True,\n-        patch_size: Union[Tuple, int, dict] = None,\n-        return_numpy: bool = False,\n-        data_format: ChannelDimension = None,\n-    ):\n-        \"\"\"\n-        Crop the image to patches and return a list of cropped images.\n-        The number of patches and their grid arrangement are determined by the original image size,\n-        the target patch size and the minimum and maximum number of patches.\n-        The aspect ratio of the patches grid is chosen to be the closest to the original image aspect ratio.\n-\n-        Args:\n-            image (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`):\n-                The image to be cropped. The image can be a PIL image, NumPy array or PyTorch tensor.\n-            min_patches (`int`):\n-                The minimum number of patches to be extracted from the image.\n-            max_patches (`int`):\n-                The maximum number of patches to be extracted from the image.\n-            use_thumbnail (`bool`, *optional*, defaults to `True`):\n-                Whether to add a thumbnail image to the list of cropped patches.\n-            patch_size (`int`, `Tuple[int, int]`, `dict`, *optional*):\n-                The size of the output patches.\n-            return_numpy (`bool`, *optional*, defaults to `False`):\n-                Whether to return the cropped images as NumPy arrays.\n-            data_format (`ChannelDimension`, *optional*):\n-                The format of the image data. If `None`, the format is inferred from the input image.\n-\n-        Returns:\n-            List[`PIL.Image.Image`] or List[np.ndarray]: The list of cropped images.\n-        \"\"\"\n-        patch_size = patch_size if patch_size is not None else self.size\n-        patch_size = get_size_dict(patch_size, default_to_square=True)\n-        original_size = get_size_dict(image.size, height_width_order=False)\n-        do_rescale = False\n-        if not isinstance(image, PIL.Image.Image):\n-            do_rescale = _rescale_for_pil_conversion(image)\n-            image = to_pil_image(image, do_rescale=do_rescale)\n-\n-        patch_size_height, patch_size_width = patch_size[\"height\"], patch_size[\"width\"]\n-        original_height, original_width = original_size[\"height\"], original_size[\"width\"]\n-        # find the closest aspect ratio to the target\n-        num_columns, num_rows = get_optimal_tiled_canvas(\n-            (original_height, original_width), (patch_size_height, patch_size_width), min_patches, max_patches\n-        )\n-\n-        # calculate the target width and height\n-        target_width = patch_size_width * num_columns\n-        target_height = patch_size_height * num_rows\n-        num_blocks = num_columns * num_rows\n-\n-        # resize the image so that each patch is of patch_size\n-        resized_image = image.resize((target_width, target_height))\n-\n-        # split the image into patches\n-        processed_images = []\n-        for i in range(num_blocks):\n-            column = i % num_columns\n-            row = i // num_columns\n-            box = (\n-                column * patch_size_width,\n-                row * patch_size_height,\n-                (column + 1) * patch_size_width,\n-                (row + 1) * patch_size_height,\n-            )\n-            # split the image\n-            patch_image = resized_image.crop(box)\n-            processed_images.append(patch_image)\n-\n-        if use_thumbnail and len(processed_images) != 1:\n-            thumbnail_img = image.resize((patch_size_width, patch_size_height))\n-            processed_images.append(thumbnail_img)\n-\n-        if return_numpy:\n-            processed_images_numpy = []\n-            for processed_image in processed_images:\n-                processed_image = np.array(processed_image)\n-                # If the input image channel dimension was of size 1, then it is dropped when converting to a PIL image\n-                # so we need to add it back if necessary.\n-                processed_image = (\n-                    np.expand_dims(processed_image, axis=-1) if processed_image.ndim == 2 else processed_image\n-                )\n-                # The image is always in channels last format after converting from a PIL image\n-                if data_format is not None:\n-                    processed_image = to_channel_dimension_format(\n-                        processed_image, data_format, input_channel_dim=ChannelDimension.LAST\n-                    )\n-                # If an image was rescaled to be in the range [0, 255] before converting to a PIL image, then we need to\n-                # rescale it back to the original range.\n-                processed_image = self.rescale(processed_image, 1 / 255) if do_rescale else processed_image\n-                processed_images_numpy.append(processed_image)\n-            processed_images = processed_images_numpy\n-\n-        return processed_images\n-\n-\n-class GotOcr2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a GotOcr2 processor which wraps a [`GotOcr2ImageProcessor`] and\n-    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n-    tokenizer functionalities. See the [`~GotOcr2Processor.__call__`] and [`~GotOcr2Processor.decode`] for more information.\n-    Args:\n-        image_processor ([`GotOcr2ImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n-    attributes = [\"image_processor\", \"tokenizer\"]\n-    valid_kwargs = [\"chat_template\"]\n-    image_processor_class = \"GotOcr2ImageProcessor\"\n-    tokenizer_class = \"PreTrainedTokenizerFast\"\n-\n-    def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n-        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n-\n-        self.message_start_token = \"<|im_start|>\"\n-        self.message_end_token = \"<|im_end|>\"\n-        self.img_start_token = \"<img>\"\n-        self.img_end_token = \"</img>\"\n-        self.img_pad_token = \"<imgpad>\"\n-        self.system_query = \"system\\nYou should follow the instructions carefully and explain your answers in detail.\"\n-\n-    def _make_list_of_inputs(self, images, text, box, color, multi_page):\n-        if not isinstance(images, (list, tuple)):\n-            images = [images]\n-            if multi_page:\n-                logger.warning(\"Multi-page inference is enabled but only one image is passed.\")\n-                images = [images]\n-        elif isinstance(images[0], (list, tuple)) and not multi_page:\n-            raise ValueError(\"Nested images are only supported with `multi_page` set to `True`.\")\n-        elif not isinstance(images[0], (list, tuple)) and multi_page:\n-            images = [images]\n-\n-        if isinstance(text, str):\n-            text = [text]\n-\n-        if not isinstance(box[0], (list, tuple)):\n-            # Use the same box for all images\n-            box = [box for _ in range(len(images))]\n-        if not isinstance(color, (list, tuple)):\n-            color = [color for _ in range(len(images))]\n-\n-        return images, text, box, color\n-\n-    def __call__(\n-        self,\n-        images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n-        audio=None,\n-        videos=None,\n-        **kwargs: Unpack[GotOcr2ProcessorKwargs],\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text if `text`\n-        is not `None`, otherwise encode default OCR queries which depends on the `format`, `box`, `color`, `multi_page` and\n-        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwrags` arguments to\n-        GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            format (`bool`, *optional*):\n-                If set, will add the format token to the query, and the model will return the OCR result with formatting.\n-            box (`List[float]`, `List[Tuple[float, float]]`, `List[Tuple[float, float, float, float]]`, *optional*):\n-                The box annotation to be added to the query. If a list of floats or a tuple of floats is provided, it\n-                will be interpreted as [x1, y1, x2, y2]. If a list of tuples is provided, each tuple should be in the\n-                form (x1, y1, x2, y2).\n-            color (`str`, *optional*):\n-                The color annotation to be added to the query. The model will return the OCR result within the box with\n-                the specified color.\n-            multi_page (`bool`, *optional*):\n-                If set, will enable multi-page inference. The model will return the OCR result across multiple pages.\n-            crop_to_patches (`bool`, *optional*):\n-                If set, will crop the image to patches. The model will return the OCR result upon the patch reference.\n-            min_patches (`int`, *optional*):\n-                The minimum number of patches to be cropped from the image. Only used when `crop_to_patches` is set to\n-                `True`.\n-            max_patches (`int`, *optional*):\n-                The maximum number of patches to be cropped from the image. Only used when `crop_to_patches` is set to\n-                `True`.\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n-        Returns:\n-            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n-        \"\"\"\n-\n-        output_kwargs = self._merge_kwargs(\n-            GotOcr2ProcessorKwargs,\n-            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n-            **kwargs,\n-        )\n-        format_output = output_kwargs[\"text_kwargs\"].pop(\"format\")\n-        num_image_tokens = output_kwargs[\"images_kwargs\"].pop(\"num_image_tokens\")\n-        box = output_kwargs[\"images_kwargs\"].pop(\"box\", [None])\n-        color = output_kwargs[\"images_kwargs\"].pop(\"color\", None)\n-        multi_page = output_kwargs[\"images_kwargs\"].pop(\"multi_page\")\n-        crop_to_patches = output_kwargs[\"images_kwargs\"].pop(\"crop_to_patches\")\n-        min_patches = output_kwargs[\"images_kwargs\"].pop(\"min_patches\")\n-        max_patches = output_kwargs[\"images_kwargs\"].pop(\"max_patches\")\n-\n-        images, text, box, color = self._make_list_of_inputs(images, text, box, color, multi_page)\n-\n-        # Load images as we need to know the image size\n-        images = load_images(images)\n-        if text is None:\n-            text = []\n-            for index, (image_group, box_single, color_single) in enumerate(zip(images, box, color)):\n-                if crop_to_patches:\n-                    image_group = self.image_processor.crop_image_to_patches(\n-                        image_group,\n-                        patch_size=output_kwargs[\"images_kwargs\"].get(\"size\"),\n-                        min_patches=min_patches,\n-                        max_patches=max_patches,\n-                    )\n-                    images[index] = image_group\n-                num_images = len(image_group) if (multi_page or crop_to_patches) else 1\n-                if box_single[0] is not None:\n-                    box_single = preprocess_box_annotation(box_single, image_group.size)\n-                query = (\n-                    f\"{f'[{color_single}] ' if color_single is not None else ''}\"\n-                    f\"{str(box_single) if box_single[0] is not None else ''} \"\n-                    \"OCR\"\n-                    f\"{' with format' if format_output else ''}\"\n-                    f\"{' across multi pages' if multi_page else ''}\"\n-                    f\"{' upon the patch reference' if crop_to_patches else ''}\"\n-                    \": \"\n-                )\n-                prompt = (\n-                    self.message_start_token\n-                    + self.system_query\n-                    + self.message_end_token\n-                    + self.message_start_token\n-                    + \"user\\n\"\n-                    + self.img_start_token\n-                    + self.img_pad_token * num_image_tokens * num_images\n-                    + self.img_end_token\n-                    + \"\\n\"\n-                    + query\n-                    + self.message_end_token\n-                    + self.message_start_token\n-                    + \"assistant\\n\"\n-                )\n-                text.append(prompt)\n-        elif crop_to_patches:\n-            for index, (image_group, box_single, color_single) in enumerate(zip(images, box, color)):\n-                image_group = self.image_processor.crop_image_to_patches(\n-                    image_group,\n-                    patch_size=output_kwargs[\"images_kwargs\"].get(\"size\"),\n-                    min_patches=min_patches,\n-                    max_patches=max_patches,\n-                )\n-                images[index] = image_group\n-\n-        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-        if multi_page or crop_to_patches:\n-            # flatten images\n-            images = [image for image_group in images for image in image_group]\n-        image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n-\n-        return BatchFeature(data={**text_inputs, **image_inputs})\n-\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(tokenizer_input_names) + list(image_processor_input_names)\n-\n-\n class GotOcr2MLPBlock(SamMLPBlock):\n     pass\n \n@@ -972,8 +524,6 @@ def forward(\n __all__ = [\n     \"GotOcr2VisionConfig\",\n     \"GotOcr2Config\",\n-    \"GotOcr2Processor\",\n     \"GotOcr2PreTrainedModel\",\n     \"GotOcr2ForConditionalGeneration\",\n-    \"GotOcr2ImageProcessor\",\n ]"
        },
        {
            "sha": "398ec36c9e75a9ee101bf3aeacf26257aebccc9e",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 37,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -1,9 +1,3 @@\n-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-#           This file was automatically generated from src/transformers/models/got_ocr2/modular_got_ocr2.py.\n-#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#             the file from the modular. If any change should be done, please apply the change to the\n-#                          modular_got_ocr2.py file directly. One of our CI enforces this.\n-#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n #\n@@ -22,6 +16,8 @@\n \n from typing import List, Optional, Tuple, Union\n \n+import numpy as np\n+\n from transformers.processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from transformers.tokenization_utils_base import PreTokenizedInput, TextInput\n \n@@ -100,7 +96,7 @@ class GotOcr2Processor(ProcessorMixin):\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n     valid_kwargs = [\"chat_template\"]\n-    image_processor_class = \"GotOcr2ImageProcessor\"\n+    image_processor_class = \"AutoImageProcessor\"\n     tokenizer_class = \"PreTrainedTokenizerFast\"\n \n     def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n@@ -205,28 +201,29 @@ def __call__(\n         box = output_kwargs[\"images_kwargs\"].pop(\"box\", [None])\n         color = output_kwargs[\"images_kwargs\"].pop(\"color\", None)\n         multi_page = output_kwargs[\"images_kwargs\"].pop(\"multi_page\")\n-        crop_to_patches = output_kwargs[\"images_kwargs\"].pop(\"crop_to_patches\")\n-        min_patches = output_kwargs[\"images_kwargs\"].pop(\"min_patches\")\n-        max_patches = output_kwargs[\"images_kwargs\"].pop(\"max_patches\")\n \n+        crop_to_patches = output_kwargs[\"images_kwargs\"].get(\"crop_to_patches\")\n         images, text, box, color = self._make_list_of_inputs(images, text, box, color, multi_page)\n-\n+        if multi_page:\n+            # save the number of pages per batch\n+            num_pages_per_batch = [len(image_group) for image_group in images]\n+            # flatten the list of images\n+            images = [image for image_group in images for image in image_group]\n+        else:\n+            num_pages_per_batch = [1 for _ in range(len(images))]\n         # Load images as we need to know the image size\n         images = load_images(images)\n+        image_sizes = [image.size for image in images]\n+        image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+        num_patches_array = image_inputs.pop(\"num_patches\")\n         if text is None:\n             text = []\n-            for index, (image_group, box_single, color_single) in enumerate(zip(images, box, color)):\n-                if crop_to_patches:\n-                    image_group = self.image_processor.crop_image_to_patches(\n-                        image_group,\n-                        patch_size=output_kwargs[\"images_kwargs\"].get(\"size\"),\n-                        min_patches=min_patches,\n-                        max_patches=max_patches,\n-                    )\n-                    images[index] = image_group\n-                num_images = len(image_group) if (multi_page or crop_to_patches) else 1\n+            patch_indices = np.cumsum(num_pages_per_batch)\n+            for index, (num_pages, box_single, color_single) in enumerate(zip(num_pages_per_batch, box, color)):\n+                current_patch_index = patch_indices[index - 1] if index > 0 else 0\n+                num_patches = sum(num_patches_array[current_patch_index : current_patch_index + num_pages])\n                 if box_single[0] is not None:\n-                    box_single = preprocess_box_annotation(box_single, image_group.size)\n+                    box_single = preprocess_box_annotation(box_single, image_sizes[index])\n                 query = (\n                     f\"{f'[{color_single}] ' if color_single is not None else ''}\"\n                     f\"{str(box_single) if box_single[0] is not None else ''} \"\n@@ -243,7 +240,7 @@ def __call__(\n                     + self.message_start_token\n                     + \"user\\n\"\n                     + self.img_start_token\n-                    + self.img_pad_token * num_image_tokens * num_images\n+                    + self.img_pad_token * num_image_tokens * num_patches\n                     + self.img_end_token\n                     + \"\\n\"\n                     + query\n@@ -252,22 +249,8 @@ def __call__(\n                     + \"assistant\\n\"\n                 )\n                 text.append(prompt)\n-        elif crop_to_patches:\n-            for index, (image_group, box_single, color_single) in enumerate(zip(images, box, color)):\n-                image_group = self.image_processor.crop_image_to_patches(\n-                    image_group,\n-                    patch_size=output_kwargs[\"images_kwargs\"].get(\"size\"),\n-                    min_patches=min_patches,\n-                    max_patches=max_patches,\n-                )\n-                images[index] = image_group\n \n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n-        if multi_page or crop_to_patches:\n-            # flatten images\n-            images = [image for image_group in images for image in image_group]\n-        image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n-\n         return BatchFeature(data={**text_inputs, **image_inputs})\n \n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "de62c4ae7cb190a9a62011332147105ee1dc9b1e",
            "filename": "src/transformers/utils/dummy_torchvision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -58,6 +58,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torchvision\"])\n \n \n+class GotOcr2ImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"torchvision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchvision\"])\n+\n+\n class LlavaImageProcessorFast(metaclass=DummyObject):\n     _backends = [\"torchvision\"]\n "
        },
        {
            "sha": "93cd347dea62958a2441f27a20fc8d609c859400",
            "filename": "tests/models/got_ocr2/test_image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 75,
            "deletions": 12,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/2c5d038f9204ffbc78acd398238dd5231341e648/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2c5d038f9204ffbc78acd398238dd5231341e648/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py?ref=2c5d038f9204ffbc78acd398238dd5231341e648",
            "patch": "@@ -16,15 +16,22 @@\n \n import unittest\n \n+from transformers.image_utils import SizeDict\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n+if is_torch_available():\n+    import torch\n+\n if is_vision_available():\n     from transformers import GotOcr2ImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import GotOcr2ImageProcessorFast\n+\n \n class GotOcr2ImageProcessingTester(unittest.TestCase):\n     def __init__(\n@@ -89,6 +96,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class GotOcr2ProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = GotOcr2ImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = GotOcr2ImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -99,17 +107,72 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processor, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processor, \"size\"))\n-        self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processor, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processor, \"image_std\"))\n-        self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processor, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processor, \"size\"))\n+            self.assertTrue(hasattr(image_processor, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processor, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processor, \"image_std\"))\n+            self.assertTrue(hasattr(image_processor, \"do_convert_rgb\"))\n+\n+    def test_slow_fast_equivalence_crop_to_patches(self):\n+        dummy_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)[0]\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict, crop_to_patches=True)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict, crop_to_patches=True)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(encoding_slow.num_patches, encoding_fast.num_patches)\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )\n+\n+    def test_slow_fast_equivalence_batched_crop_to_patches(self):\n+        # Prepare image inputs so that we have two groups of images with equal resolution with a group of images with\n+        # different resolutions in between\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        dummy_images += self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        dummy_images += self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict, crop_to_patches=True)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict, crop_to_patches=True)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(encoding_slow.num_patches, encoding_fast.num_patches)\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )\n \n     def test_crop_to_patches(self):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)[0]\n-        processed_images = image_processor.crop_image_to_patches(image, 1, 6, use_thumbnail=True)\n+        # test slow image processor\n+        image_processor = self.image_processor_list[0](**self.image_processor_dict)\n+        image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)[0]\n+        processed_images = image_processor.crop_image_to_patches(\n+            image,\n+            min_patches=1,\n+            max_patches=6,\n+            use_thumbnail=True,\n+            patch_size={\"height\": 20, \"width\": 20},\n+        )\n         self.assertEqual(len(processed_images), 5)\n-        self.assertEqual(processed_images[0].size, (20, 20))\n+        self.assertEqual(processed_images[0].shape[:2], (20, 20))\n+\n+        # test fast image processor (process batch)\n+        image_processor = self.image_processor_list[1](**self.image_processor_dict)\n+        image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)[0]\n+        processed_images = image_processor.crop_image_to_patches(\n+            image.unsqueeze(0),\n+            min_patches=1,\n+            max_patches=6,\n+            use_thumbnail=True,\n+            patch_size=SizeDict(height=20, width=20),\n+        )\n+        self.assertEqual(len(processed_images[0]), 5)\n+        self.assertEqual(processed_images.shape[-2:], (20, 20))"
        }
    ],
    "stats": {
        "total": 1055,
        "additions": 469,
        "deletions": 586
    }
}