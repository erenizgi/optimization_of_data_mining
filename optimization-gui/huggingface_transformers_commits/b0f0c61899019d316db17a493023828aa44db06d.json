{
    "author": "gante",
    "message": "Add SynthID (watermerking by Google DeepMind) (#34350)\n\n* Add SynthIDTextWatermarkLogitsProcessor\r\n\r\n* esolving comments.\r\n\r\n* Resolving comments.\r\n\r\n* esolving commits,\r\n\r\n* Improving SynthIDWatermark tests.\r\n\r\n* switch to PT version\r\n\r\n* detector as pretrained model + style\r\n\r\n* update training + style\r\n\r\n* rebase\r\n\r\n* Update logits_process.py\r\n\r\n* Improving SynthIDWatermark tests.\r\n\r\n* Shift detector training to wikitext negatives and stabilize with lower learning rate.\r\n\r\n* Clean up.\r\n\r\n* in for 7B\r\n\r\n* cleanup\r\n\r\n* upport python 3.8.\r\n\r\n* README and final cleanup.\r\n\r\n* HF Hub upload and initiaze.\r\n\r\n* Update requirements for synthid_text.\r\n\r\n* Adding SynthIDTextWatermarkDetector.\r\n\r\n* Detector testing.\r\n\r\n* Documentation changes.\r\n\r\n* Copyrights fix.\r\n\r\n* Fix detector api.\r\n\r\n* ironing out errors\r\n\r\n* ironing out errors\r\n\r\n* training checks\r\n\r\n* make fixup and make fix-copies\r\n\r\n* docstrings and add to docs\r\n\r\n* copyright\r\n\r\n* BC\r\n\r\n* test docstrings\r\n\r\n* move import\r\n\r\n* protect type hints\r\n\r\n* top level imports\r\n\r\n* watermarking example\r\n\r\n* direct imports\r\n\r\n* tpr fpr meaning\r\n\r\n* process_kwargs\r\n\r\n* SynthIDTextWatermarkingConfig docstring\r\n\r\n* assert -> exception\r\n\r\n* example updates\r\n\r\n* no immutable dict (cant be serialized)\r\n\r\n* pack fn\r\n\r\n* einsum equivalent\r\n\r\n* import order\r\n\r\n* fix test on gpu\r\n\r\n* add detector example\r\n\r\n---------\r\n\r\nCo-authored-by: Sumedh Ghaisas <sumedhg@google.com>\r\nCo-authored-by: Marc Sun <marc@huggingface.co>\r\nCo-authored-by: sumedhghaisas2 <138781311+sumedhghaisas2@users.noreply.github.com>\r\nCo-authored-by: raushan <raushan@huggingface.co>",
    "sha": "b0f0c61899019d316db17a493023828aa44db06d",
    "files": [
        {
            "sha": "946940cb0194812e25792f4f1c84ab5046107ca5",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -185,6 +185,9 @@ generation.\n [[autodoc]] SuppressTokensLogitsProcessor\n     - __call__\n \n+[[autodoc]] SynthIDTextWatermarkLogitsProcessor\n+    - __call__\n+\n [[autodoc]] TemperatureLogitsWarper\n     - __call__\n \n@@ -418,5 +421,20 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n \n ## Watermark Utils\n \n+[[autodoc]] WatermarkingConfig\n+    - __call__\n+\n [[autodoc]] WatermarkDetector\n     - __call__\n+\n+[[autodoc]] BayesianDetectorConfig\n+    - __call__\n+\n+[[autodoc]] BayesianDetectorModel\n+    - __call__\n+\n+[[autodoc]] SynthIDTextWatermarkingConfig\n+    - __call__\n+\n+[[autodoc]] SynthIDTextWatermarkDetector\n+    - __call__"
        },
        {
            "sha": "76a0f1381cd6bc413d575c80ec974fe26682bb75",
            "filename": "docs/source/en/main_classes/text_generation.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -41,8 +41,6 @@ like token streaming.\n \t- validate\n \t- get_generation_mode\n \n-[[autodoc]] generation.WatermarkingConfig\n-\n ## GenerationMixin\n \n [[autodoc]] GenerationMixin"
        },
        {
            "sha": "30ab999037374d21158a50b42cec3a026c1fbff3",
            "filename": "examples/research_projects/synthid_text/README.md",
            "status": "added",
            "additions": 34,
            "deletions": 0,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/examples%2Fresearch_projects%2Fsynthid_text%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/examples%2Fresearch_projects%2Fsynthid_text%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fsynthid_text%2FREADME.md?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -0,0 +1,34 @@\n+# SynthID Text\n+\n+This project showcases the use of SynthIDText for watermarking LLMs. The code shown in this repo also\n+demostrates the training of the detector for detecting such watermarked text. This detector can be uploaded onto\n+a private HF hub repo (private for security reasons) and can be initialized again through pretrained model loading also shown in this script.\n+\n+See our blog post: https://huggingface.co/blog/synthid-text\n+\n+\n+## Python version\n+\n+User would need python 3.9 to run this example.\n+\n+## Installation and running\n+\n+Once you install transformers you would need to install requirements for this project through requirements.txt provided in this folder.\n+\n+```\n+pip install -r requirements.txt\n+```\n+\n+## To run the detector training\n+\n+```\n+python detector_training.py --model_name=google/gemma-7b-it\n+```\n+\n+Check the script for more parameters are are tunable and check out paper at link\n+https://www.nature.com/articles/s41586-024-08025-4 for more information on these parameters.\n+\n+## Caveat\n+\n+Make sure to run the training of the detector and the detection on the same hardware\n+CPU, GPU or TPU to get consistent results (we use detecterministic randomness which is hardware dependent)."
        },
        {
            "sha": "35d0ea22f42b23f3463b2a7fec8fc955bbb617a5",
            "filename": "examples/research_projects/synthid_text/detector_training.py",
            "status": "added",
            "additions": 502,
            "deletions": 0,
            "changes": 502,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/examples%2Fresearch_projects%2Fsynthid_text%2Fdetector_training.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/examples%2Fresearch_projects%2Fsynthid_text%2Fdetector_training.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fsynthid_text%2Fdetector_training.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -0,0 +1,502 @@\n+# coding=utf-8\n+# Copyright 2024 Google DeepMind.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import dataclasses\n+import enum\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+\n+from transformers import (\n+    AutoModelForCausalLM,\n+    AutoTokenizer,\n+    BayesianDetectorConfig,\n+    BayesianDetectorModel,\n+    SynthIDTextWatermarkDetector,\n+    SynthIDTextWatermarkingConfig,\n+    SynthIDTextWatermarkLogitsProcessor,\n+)\n+from utils import (\n+    get_tokenized_uwm_outputs,\n+    get_tokenized_wm_outputs,\n+    process_raw_model_outputs,\n+    update_fn_if_fpr_tpr,\n+    upload_model_to_hf,\n+)\n+\n+\n+@enum.unique\n+class ValidationMetric(enum.Enum):\n+    \"\"\"Direction along the z-axis.\"\"\"\n+\n+    TPR_AT_FPR = \"tpr_at_fpr\"\n+    CROSS_ENTROPY = \"cross_entropy\"\n+\n+\n+@dataclasses.dataclass\n+class TrainingArguments:\n+    \"\"\"Training arguments pertaining to the training loop itself.\"\"\"\n+\n+    eval_metric: Optional[str] = dataclasses.field(\n+        default=ValidationMetric.TPR_AT_FPR, metadata={\"help\": \"The evaluation metric used.\"}\n+    )\n+\n+\n+def train_detector(\n+    detector: torch.nn.Module,\n+    g_values: torch.Tensor,\n+    mask: torch.Tensor,\n+    watermarked: torch.Tensor,\n+    epochs: int = 250,\n+    learning_rate: float = 1e-3,\n+    minibatch_size: int = 64,\n+    seed: int = 0,\n+    l2_weight: float = 0.0,\n+    shuffle: bool = True,\n+    g_values_val: Optional[torch.Tensor] = None,\n+    mask_val: Optional[torch.Tensor] = None,\n+    watermarked_val: Optional[torch.Tensor] = None,\n+    verbose: bool = False,\n+    validation_metric: ValidationMetric = ValidationMetric.TPR_AT_FPR,\n+) -> Tuple[Dict[str, Any], float]:\n+    \"\"\"Trains a Bayesian detector model.\n+\n+    Args:\n+      g_values: g-values of shape [num_train, seq_len, watermarking_depth].\n+      mask: A binary array shape [num_train, seq_len] indicating which g-values\n+        should be used. g-values with mask value 0 are discarded.\n+      watermarked: A binary array of shape [num_train] indicating whether the\n+        example is watermarked (0: unwatermarked, 1: watermarked).\n+      epochs: Number of epochs to train for.\n+      learning_rate: Learning rate for optimizer.\n+      minibatch_size: Minibatch size for training. Note that a minibatch\n+        requires ~ 32 * minibatch_size * seq_len * watermarked_depth *\n+        watermarked_depth bits of memory.\n+      seed: Seed for parameter initialization.\n+      l2_weight: Weight to apply to L2 regularization for delta parameters.\n+      shuffle: Whether to shuffle before training.\n+      g_values_val: Validation g-values of shape [num_val, seq_len,\n+        watermarking_depth].\n+      mask_val: Validation mask of shape [num_val, seq_len].\n+      watermarked_val: Validation watermark labels of shape [num_val].\n+      verbose: Boolean indicating verbosity of training. If true, the loss will\n+        be printed. Defaulted to False.\n+      use_tpr_fpr_for_val: Whether to use TPR@FPR=1% as metric for validation.\n+        If false, use cross entropy loss.\n+\n+    Returns:\n+      Tuple of\n+        training_history: Training history keyed by epoch number where the\n+        values are\n+          dictionaries containing the loss, validation loss, and model\n+          parameters,\n+          keyed by\n+          'loss', 'val_loss', and 'params', respectively.\n+        min_val_loss: Minimum validation loss achieved during training.\n+    \"\"\"\n+\n+    # Set the random seed for reproducibility\n+    torch.manual_seed(seed)\n+\n+    # Shuffle the data if required\n+    if shuffle:\n+        indices = torch.randperm(len(g_values))\n+        g_values = g_values[indices]\n+        mask = mask[indices]\n+        watermarked = watermarked[indices]\n+\n+    # Initialize optimizer\n+    optimizer = torch.optim.Adam(detector.parameters(), lr=learning_rate)\n+    history = {}\n+    min_val_loss = float(\"inf\")\n+\n+    for epoch in range(epochs):\n+        losses = []\n+        detector.train()\n+        num_batches = len(g_values) // minibatch_size\n+        for i in range(0, len(g_values), minibatch_size):\n+            end = i + minibatch_size\n+            if end > len(g_values):\n+                break\n+            loss_batch_weight = l2_weight / num_batches\n+\n+            optimizer.zero_grad()\n+            loss = detector(\n+                g_values=g_values[i:end],\n+                mask=mask[i:end],\n+                labels=watermarked[i:end],\n+                loss_batch_weight=loss_batch_weight,\n+            )[1]\n+            loss.backward()\n+            optimizer.step()\n+            losses.append(loss.item())\n+        train_loss = sum(losses) / len(losses)\n+\n+        val_losses = []\n+        if g_values_val is not None:\n+            detector.eval()\n+            if validation_metric == ValidationMetric.TPR_AT_FPR:\n+                val_loss = update_fn_if_fpr_tpr(\n+                    detector,\n+                    g_values_val,\n+                    mask_val,\n+                    watermarked_val,\n+                    minibatch_size=minibatch_size,\n+                )\n+            else:\n+                for i in range(0, len(g_values_val), minibatch_size):\n+                    end = i + minibatch_size\n+                    if end > len(g_values_val):\n+                        break\n+                    with torch.no_grad():\n+                        v_loss = detector(\n+                            g_values=g_values_val[i:end],\n+                            mask=mask_val[i:end],\n+                            labels=watermarked_val[i:end],\n+                            loss_batch_weight=0,\n+                        )[1]\n+                    val_losses.append(v_loss.item())\n+                val_loss = sum(val_losses) / len(val_losses)\n+\n+        # Store training history\n+        history[epoch + 1] = {\"loss\": train_loss, \"val_loss\": val_loss}\n+        if verbose:\n+            if val_loss is not None:\n+                print(f\"Epoch {epoch}: loss {loss} (train), {val_loss} (val)\")\n+            else:\n+                print(f\"Epoch {epoch}: loss {loss} (train)\")\n+\n+        if val_loss is not None and val_loss < min_val_loss:\n+            min_val_loss = val_loss\n+            best_val_epoch = epoch\n+\n+    if verbose:\n+        print(f\"Best val Epoch: {best_val_epoch}, min_val_loss: {min_val_loss}\")\n+\n+    return history, min_val_loss\n+\n+\n+def train_best_detector(\n+    tokenized_wm_outputs: Union[List[np.ndarray], np.ndarray],\n+    tokenized_uwm_outputs: Union[List[np.ndarray], np.ndarray],\n+    logits_processor: SynthIDTextWatermarkLogitsProcessor,\n+    tokenizer: Any,\n+    torch_device: torch.device,\n+    test_size: float = 0.3,\n+    pos_truncation_length: Optional[int] = 200,\n+    neg_truncation_length: Optional[int] = 100,\n+    max_padded_length: int = 2300,\n+    n_epochs: int = 50,\n+    learning_rate: float = 2.1e-2,\n+    l2_weights: np.ndarray = np.logspace(-3, -2, num=4),\n+    verbose: bool = False,\n+    validation_metric: ValidationMetric = ValidationMetric.TPR_AT_FPR,\n+):\n+    \"\"\"Train and return the best detector given range of hyperparameters.\n+\n+    In practice, we have found that tuning pos_truncation_length,\n+    neg_truncation_length, n_epochs, learning_rate and l2_weights can help\n+    improve the performance of the detector. We reccommend tuning these\n+    parameters for your data.\n+    \"\"\"\n+    l2_weights = list(l2_weights)\n+\n+    (\n+        train_g_values,\n+        train_masks,\n+        train_labels,\n+        cv_g_values,\n+        cv_masks,\n+        cv_labels,\n+    ) = process_raw_model_outputs(\n+        logits_processor,\n+        tokenizer,\n+        pos_truncation_length,\n+        neg_truncation_length,\n+        max_padded_length,\n+        tokenized_wm_outputs,\n+        test_size,\n+        tokenized_uwm_outputs,\n+        torch_device,\n+    )\n+\n+    best_detector = None\n+    lowest_loss = float(\"inf\")\n+    val_losses = []\n+    for l2_weight in l2_weights:\n+        config = BayesianDetectorConfig(watermarking_depth=len(logits_processor.keys))\n+        detector = BayesianDetectorModel(config).to(torch_device)\n+        _, min_val_loss = train_detector(\n+            detector=detector,\n+            g_values=train_g_values,\n+            mask=train_masks,\n+            watermarked=train_labels,\n+            g_values_val=cv_g_values,\n+            mask_val=cv_masks,\n+            watermarked_val=cv_labels,\n+            learning_rate=learning_rate,\n+            l2_weight=l2_weight,\n+            epochs=n_epochs,\n+            verbose=verbose,\n+            validation_metric=validation_metric,\n+        )\n+        val_losses.append(min_val_loss)\n+        if min_val_loss < lowest_loss:\n+            lowest_loss = min_val_loss\n+            best_detector = detector\n+    return best_detector, lowest_loss\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--model_name\",\n+        type=str,\n+        default=\"google/gemma-2b-it\",\n+        help=(\"LM model to train the detector for.\"),\n+    )\n+    parser.add_argument(\n+        \"--temperature\",\n+        type=float,\n+        default=1.0,\n+        help=(\"Temperature to sample from the model.\"),\n+    )\n+    parser.add_argument(\n+        \"--top_k\",\n+        type=int,\n+        default=40,\n+        help=(\"Top K for sampling.\"),\n+    )\n+    parser.add_argument(\n+        \"--top_p\",\n+        type=float,\n+        default=1.0,\n+        help=(\"Top P for sampling.\"),\n+    )\n+    parser.add_argument(\n+        \"--num_negatives\",\n+        type=int,\n+        default=10000,\n+        help=(\"Number of negatives for detector training.\"),\n+    )\n+    parser.add_argument(\n+        \"--pos_batch_size\",\n+        type=int,\n+        default=32,\n+        help=(\"Batch size of watermarked positives while sampling.\"),\n+    )\n+    parser.add_argument(\n+        \"--num_pos_batch\",\n+        type=int,\n+        default=313,\n+        help=(\"Number of positive batches for training.\"),\n+    )\n+    parser.add_argument(\n+        \"--generation_length\",\n+        type=int,\n+        default=512,\n+        help=(\"Generation length for sampling.\"),\n+    )\n+    parser.add_argument(\n+        \"--save_model_to_hf_hub\",\n+        action=\"store_true\",\n+        help=(\"Whether to save the trained model HF hub. By default it will be a private repo.\"),\n+    )\n+    parser.add_argument(\n+        \"--load_from_hf_hub\",\n+        action=\"store_true\",\n+        help=(\n+            \"Whether to load trained detector model from HF Hub, make sure its the model trained on the same model \"\n+            \"we are loading in the script.\"\n+        ),\n+    )\n+    parser.add_argument(\n+        \"--hf_hub_model_name\",\n+        type=str,\n+        default=None,\n+        help=(\"HF hub model name for loading of saving the model.\"),\n+    )\n+    parser.add_argument(\n+        \"--eval_detector_on_prompts\",\n+        action=\"store_true\",\n+        help=(\"Evaluate detector on a prompt and print probability of watermark.\"),\n+    )\n+\n+    args = parser.parse_args()\n+    model_name = args.model_name\n+    temperature = args.temperature\n+    top_k = args.top_k\n+    top_p = args.top_p\n+    num_negatives = args.num_negatives\n+    pos_batch_size = args.pos_batch_size\n+    num_pos_batch = args.num_pos_batch\n+    if num_pos_batch < 10:\n+        raise ValueError(\"--num_pos_batch should be greater than 10.\")\n+    generation_length = args.generation_length\n+    save_model_to_hf_hub = args.save_model_to_hf_hub\n+    load_from_hf_hub = args.load_from_hf_hub\n+    repo_name = args.hf_hub_model_name\n+    eval_detector_on_prompts = args.eval_detector_on_prompts\n+\n+    NEG_BATCH_SIZE = 32\n+\n+    # Truncate outputs to this length for training.\n+    POS_TRUNCATION_LENGTH = 200\n+    NEG_TRUNCATION_LENGTH = 100\n+    # Pad trucated outputs to this length for equal shape across all batches.\n+    MAX_PADDED_LENGTH = 1000\n+\n+    DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n+    if DEVICE.type not in (\"cuda\", \"tpu\"):\n+        raise ValueError(\"We have found the training stable on GPU and TPU, we are working on\" \" a fix for CPUs\")\n+\n+    model = None\n+    if not load_from_hf_hub:\n+        # Change this to make your watermark unique. Check documentation in the paper to understand the\n+        # impact of these parameters.\n+        DEFAULT_WATERMARKING_CONFIG = {\n+            \"ngram_len\": 5,  # This corresponds to H=4 context window size in the paper.\n+            \"keys\": [\n+                654,\n+                400,\n+                836,\n+                123,\n+                340,\n+                443,\n+                597,\n+                160,\n+                57,\n+                29,\n+                590,\n+                639,\n+                13,\n+                715,\n+                468,\n+                990,\n+                966,\n+                226,\n+                324,\n+                585,\n+                118,\n+                504,\n+                421,\n+                521,\n+                129,\n+                669,\n+                732,\n+                225,\n+                90,\n+                960,\n+            ],\n+            \"sampling_table_size\": 2**16,\n+            \"sampling_table_seed\": 0,\n+            \"context_history_size\": 1024,\n+        }\n+        watermark_config = SynthIDTextWatermarkingConfig(**DEFAULT_WATERMARKING_CONFIG)\n+\n+        model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n+        tokenizer = AutoTokenizer.from_pretrained(model_name)\n+        tokenizer.pad_token = tokenizer.eos_token\n+\n+        logits_processor = SynthIDTextWatermarkLogitsProcessor(**DEFAULT_WATERMARKING_CONFIG, device=DEVICE)\n+        tokenized_wm_outputs = get_tokenized_wm_outputs(\n+            model,\n+            tokenizer,\n+            watermark_config,\n+            num_pos_batch,\n+            pos_batch_size,\n+            temperature,\n+            generation_length,\n+            top_k,\n+            top_p,\n+            DEVICE,\n+        )\n+        tokenized_uwm_outputs = get_tokenized_uwm_outputs(num_negatives, NEG_BATCH_SIZE, tokenizer, DEVICE)\n+\n+        best_detector, lowest_loss = train_best_detector(\n+            tokenized_wm_outputs=tokenized_wm_outputs,\n+            tokenized_uwm_outputs=tokenized_uwm_outputs,\n+            logits_processor=logits_processor,\n+            tokenizer=tokenizer,\n+            torch_device=DEVICE,\n+            test_size=0.3,\n+            pos_truncation_length=POS_TRUNCATION_LENGTH,\n+            neg_truncation_length=NEG_TRUNCATION_LENGTH,\n+            max_padded_length=MAX_PADDED_LENGTH,\n+            n_epochs=100,\n+            learning_rate=3e-3,\n+            l2_weights=[\n+                0,\n+            ],\n+            verbose=True,\n+            validation_metric=ValidationMetric.TPR_AT_FPR,\n+        )\n+    else:\n+        if repo_name is None:\n+            raise ValueError(\"When loading from pretrained detector model name cannot be None.\")\n+        best_detector = BayesianDetectorModel.from_pretrained(repo_name).to(DEVICE)\n+\n+    best_detector.config.set_detector_information(\n+        model_name=model_name, watermarking_config=DEFAULT_WATERMARKING_CONFIG\n+    )\n+    if save_model_to_hf_hub:\n+        upload_model_to_hf(best_detector, repo_name)\n+\n+    # Evaluate model response with the detector\n+    if eval_detector_on_prompts:\n+        model_name = best_detector.config.model_name\n+        watermark_config_dict = best_detector.config.watermarking_config\n+        logits_processor = SynthIDTextWatermarkLogitsProcessor(**watermark_config_dict, device=DEVICE)\n+        tokenizer = AutoTokenizer.from_pretrained(model_name)\n+        tokenizer.pad_token = tokenizer.eos_token\n+        synthid_text_detector = SynthIDTextWatermarkDetector(best_detector, logits_processor, tokenizer)\n+\n+        if model is None:\n+            model = AutoModelForCausalLM.from_pretrained(model_name).to(DEVICE)\n+        watermarking_config = SynthIDTextWatermarkingConfig(**watermark_config_dict)\n+\n+        prompts = [\"Write a essay on cats.\"]\n+        inputs = tokenizer(\n+            prompts,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(DEVICE)\n+\n+        _, inputs_len = inputs[\"input_ids\"].shape\n+\n+        outputs = model.generate(\n+            **inputs,\n+            watermarking_config=watermarking_config,\n+            do_sample=True,\n+            max_length=inputs_len + generation_length,\n+            temperature=temperature,\n+            top_k=40,\n+            top_p=1.0,\n+        )\n+        outputs = outputs[:, inputs_len:]\n+        result = synthid_text_detector(outputs)\n+\n+        # You should set this based on expected fpr (false positive rate) and tpr (true positive rate).\n+        # Check our demo at HF Spaces for more info.\n+        upper_threshold = 0.95\n+        lower_threshold = 0.12\n+        if result[0][0] > upper_threshold:\n+            print(\"The text is watermarked.\")\n+        elif lower_threshold < result[0][0] < upper_threshold:\n+            print(\"It is hard to determine if the text is watermarked or not.\")\n+        else:\n+            print(\"The text is not watermarked.\")"
        },
        {
            "sha": "9e40a93ee08f09b6204da34ba8fd76a555aef1a7",
            "filename": "examples/research_projects/synthid_text/requirements.txt",
            "status": "added",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/examples%2Fresearch_projects%2Fsynthid_text%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/examples%2Fresearch_projects%2Fsynthid_text%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fsynthid_text%2Frequirements.txt?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -0,0 +1,5 @@\n+tensorflow-datasets>=4.9.3\n+torch >= 1.3\n+datasets\n+scikit-learn\n+tensorflow"
        },
        {
            "sha": "abcb6ca2f2825503906d5500a184ce4664f3fbbf",
            "filename": "examples/research_projects/synthid_text/utils.py",
            "status": "added",
            "additions": 408,
            "deletions": 0,
            "changes": 408,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/examples%2Fresearch_projects%2Fsynthid_text%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/examples%2Fresearch_projects%2Fsynthid_text%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fsynthid_text%2Futils.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -0,0 +1,408 @@\n+# coding=utf-8\n+# Copyright 2024 Google DeepMind.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import gc\n+from typing import Any, List, Optional, Tuple\n+\n+import datasets\n+import numpy as np\n+import tensorflow as tf\n+import tensorflow_datasets as tfds\n+import torch\n+import tqdm\n+from huggingface_hub import HfApi, create_repo\n+from huggingface_hub.utils import RepositoryNotFoundError\n+from sklearn import model_selection\n+\n+import transformers\n+\n+\n+def pad_to_len(\n+    arr: torch.Tensor,\n+    target_len: int,\n+    left_pad: bool,\n+    eos_token: int,\n+    device: torch.device,\n+) -> torch.Tensor:\n+    \"\"\"Pad or truncate array to given length.\"\"\"\n+    if arr.shape[1] < target_len:\n+        shape_for_ones = list(arr.shape)\n+        shape_for_ones[1] = target_len - shape_for_ones[1]\n+        padded = (\n+            torch.ones(\n+                shape_for_ones,\n+                device=device,\n+                dtype=torch.long,\n+            )\n+            * eos_token\n+        )\n+        if not left_pad:\n+            arr = torch.concatenate((arr, padded), dim=1)\n+        else:\n+            arr = torch.concatenate((padded, arr), dim=1)\n+    else:\n+        arr = arr[:, :target_len]\n+    return arr\n+\n+\n+def filter_and_truncate(\n+    outputs: torch.Tensor,\n+    truncation_length: Optional[int],\n+    eos_token_mask: torch.Tensor,\n+) -> torch.Tensor:\n+    \"\"\"Filter and truncate outputs to given length.\n+\n+    Args:\n+    outputs: output tensor of shape [batch_size, output_len]\n+    truncation_length: Length to truncate the final output.\n+    eos_token_mask: EOS token mask of shape [batch_size, output_len]\n+\n+    Returns:\n+    output tensor of shape [batch_size, truncation_length].\n+    \"\"\"\n+    if truncation_length:\n+        outputs = outputs[:, :truncation_length]\n+        truncation_mask = torch.sum(eos_token_mask, dim=1) >= truncation_length\n+        return outputs[truncation_mask, :]\n+    return outputs\n+\n+\n+def process_outputs_for_training(\n+    all_outputs: List[torch.Tensor],\n+    logits_processor: transformers.generation.SynthIDTextWatermarkLogitsProcessor,\n+    tokenizer: Any,\n+    pos_truncation_length: Optional[int],\n+    neg_truncation_length: Optional[int],\n+    max_length: int,\n+    is_cv: bool,\n+    is_pos: bool,\n+    torch_device: torch.device,\n+) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n+    \"\"\"Process raw model outputs into format understandable by the detector.\n+\n+    Args:\n+    all_outputs: sequence of outputs of shape [batch_size, output_len].\n+    logits_processor: logits processor used for watermarking.\n+    tokenizer: tokenizer used for the model.\n+    pos_truncation_length: Length to truncate wm outputs.\n+    neg_truncation_length: Length to truncate uwm outputs.\n+    max_length: Length to pad truncated outputs so that all processed entries.\n+        have same shape.\n+    is_cv: Process given outputs for cross validation.\n+    is_pos: Process given outputs for positives.\n+    torch_device: torch device to use.\n+\n+    Returns:\n+    Tuple of\n+        all_masks: list of masks of shape [batch_size, max_length].\n+        all_g_values: list of g_values of shape [batch_size, max_length, depth].\n+    \"\"\"\n+    all_masks = []\n+    all_g_values = []\n+    for outputs in tqdm.tqdm(all_outputs):\n+        # outputs is of shape [batch_size, output_len].\n+        # output_len can differ from batch to batch.\n+        eos_token_mask = logits_processor.compute_eos_token_mask(\n+            input_ids=outputs,\n+            eos_token_id=tokenizer.eos_token_id,\n+        )\n+        if is_pos or is_cv:\n+            # filter with length for positives for both train and CV.\n+            # We also filter for length when CV negatives are processed.\n+            outputs = filter_and_truncate(outputs, pos_truncation_length, eos_token_mask)\n+        elif not is_pos and not is_cv:\n+            outputs = filter_and_truncate(outputs, neg_truncation_length, eos_token_mask)\n+\n+        # If no filtered outputs skip this batch.\n+        if outputs.shape[0] == 0:\n+            continue\n+\n+        # All outputs are padded to max-length with eos-tokens.\n+        outputs = pad_to_len(outputs, max_length, False, tokenizer.eos_token_id, torch_device)\n+        # outputs shape [num_filtered_entries, max_length]\n+\n+        eos_token_mask = logits_processor.compute_eos_token_mask(\n+            input_ids=outputs,\n+            eos_token_id=tokenizer.eos_token_id,\n+        )\n+\n+        context_repetition_mask = logits_processor.compute_context_repetition_mask(\n+            input_ids=outputs,\n+        )\n+\n+        # context_repetition_mask of shape [num_filtered_entries, max_length -\n+        # (ngram_len - 1)].\n+        context_repetition_mask = pad_to_len(context_repetition_mask, max_length, True, 0, torch_device)\n+        # We pad on left to get same max_length shape.\n+        # context_repetition_mask of shape [num_filtered_entries, max_length].\n+        combined_mask = context_repetition_mask * eos_token_mask\n+\n+        g_values = logits_processor.compute_g_values(\n+            input_ids=outputs,\n+        )\n+\n+        # g_values of shape [num_filtered_entries, max_length - (ngram_len - 1),\n+        # depth].\n+        g_values = pad_to_len(g_values, max_length, True, 0, torch_device)\n+\n+        # We pad on left to get same max_length shape.\n+        # g_values of shape [num_filtered_entries, max_length, depth].\n+        all_masks.append(combined_mask)\n+        all_g_values.append(g_values)\n+    return all_masks, all_g_values\n+\n+\n+def tpr_at_fpr(detector, detector_inputs, w_true, minibatch_size, target_fpr=0.01) -> torch.Tensor:\n+    \"\"\"Calculates true positive rate (TPR) at false positive rate (FPR)=target_fpr.\"\"\"\n+    positive_idxs = w_true == 1\n+    negative_idxs = w_true == 0\n+    num_samples = detector_inputs[0].size(0)\n+\n+    w_preds = []\n+    for start in range(0, num_samples, minibatch_size):\n+        end = start + minibatch_size\n+        detector_inputs_ = (\n+            detector_inputs[0][start:end],\n+            detector_inputs[1][start:end],\n+        )\n+        with torch.no_grad():\n+            w_pred = detector(*detector_inputs_)[0]\n+        w_preds.append(w_pred)\n+\n+    w_pred = torch.cat(w_preds, dim=0)  # Concatenate predictions\n+    positive_scores = w_pred[positive_idxs]\n+    negative_scores = w_pred[negative_idxs]\n+\n+    # Calculate the FPR threshold\n+    # Note: percentile -> quantile\n+    fpr_threshold = torch.quantile(negative_scores, 1 - target_fpr)\n+    # Note: need to switch to FP32 since torch.mean doesn't work with torch.bool\n+    return torch.mean((positive_scores >= fpr_threshold).to(dtype=torch.float32)).item()  # TPR\n+\n+\n+def update_fn_if_fpr_tpr(detector, g_values_val, mask_val, watermarked_val, minibatch_size):\n+    \"\"\"Loss function for negative TPR@FPR=1% as the validation loss.\"\"\"\n+    tpr_ = tpr_at_fpr(\n+        detector=detector,\n+        detector_inputs=(g_values_val, mask_val),\n+        w_true=watermarked_val,\n+        minibatch_size=minibatch_size,\n+    )\n+    return -tpr_\n+\n+\n+def process_raw_model_outputs(\n+    logits_processor,\n+    tokenizer,\n+    pos_truncation_length,\n+    neg_truncation_length,\n+    max_padded_length,\n+    tokenized_wm_outputs,\n+    test_size,\n+    tokenized_uwm_outputs,\n+    torch_device,\n+):\n+    # Split data into train and CV\n+    train_wm_outputs, cv_wm_outputs = model_selection.train_test_split(tokenized_wm_outputs, test_size=test_size)\n+\n+    train_uwm_outputs, cv_uwm_outputs = model_selection.train_test_split(tokenized_uwm_outputs, test_size=test_size)\n+\n+    process_kwargs = {\n+        \"logits_processor\": logits_processor,\n+        \"tokenizer\": tokenizer,\n+        \"pos_truncation_length\": pos_truncation_length,\n+        \"neg_truncation_length\": neg_truncation_length,\n+        \"max_length\": max_padded_length,\n+        \"torch_device\": torch_device,\n+    }\n+\n+    # Process both train and CV data for training\n+    wm_masks_train, wm_g_values_train = process_outputs_for_training(\n+        [torch.tensor(outputs, device=torch_device, dtype=torch.long) for outputs in train_wm_outputs],\n+        is_pos=True,\n+        is_cv=False,\n+        **process_kwargs,\n+    )\n+    wm_masks_cv, wm_g_values_cv = process_outputs_for_training(\n+        [torch.tensor(outputs, device=torch_device, dtype=torch.long) for outputs in cv_wm_outputs],\n+        is_pos=True,\n+        is_cv=True,\n+        **process_kwargs,\n+    )\n+    uwm_masks_train, uwm_g_values_train = process_outputs_for_training(\n+        [torch.tensor(outputs, device=torch_device, dtype=torch.long) for outputs in train_uwm_outputs],\n+        is_pos=False,\n+        is_cv=False,\n+        **process_kwargs,\n+    )\n+    uwm_masks_cv, uwm_g_values_cv = process_outputs_for_training(\n+        [torch.tensor(outputs, device=torch_device, dtype=torch.long) for outputs in cv_uwm_outputs],\n+        is_pos=False,\n+        is_cv=True,\n+        **process_kwargs,\n+    )\n+\n+    # We get list of data; here we concat all together to be passed to the detector.\n+    def pack(mask, g_values):\n+        mask = torch.cat(mask, dim=0)\n+        g = torch.cat(g_values, dim=0)\n+        return mask, g\n+\n+    wm_masks_train, wm_g_values_train = pack(wm_masks_train, wm_g_values_train)\n+    # Note: Use float instead of bool. Otherwise, the entropy calculation doesn't work\n+    wm_labels_train = torch.ones((wm_masks_train.shape[0],), dtype=torch.float, device=torch_device)\n+\n+    wm_masks_cv, wm_g_values_cv = pack(wm_masks_cv, wm_g_values_cv)\n+    wm_labels_cv = torch.ones((wm_masks_cv.shape[0],), dtype=torch.float, device=torch_device)\n+\n+    uwm_masks_train, uwm_g_values_train = pack(uwm_masks_train, uwm_g_values_train)\n+    uwm_labels_train = torch.zeros((uwm_masks_train.shape[0],), dtype=torch.float, device=torch_device)\n+\n+    uwm_masks_cv, uwm_g_values_cv = pack(uwm_masks_cv, uwm_g_values_cv)\n+    uwm_labels_cv = torch.zeros((uwm_masks_cv.shape[0],), dtype=torch.float, device=torch_device)\n+\n+    # Concat pos and negatives data together.\n+    train_g_values = torch.cat((wm_g_values_train, uwm_g_values_train), dim=0).squeeze()\n+    train_labels = torch.cat((wm_labels_train, uwm_labels_train), axis=0).squeeze()\n+    train_masks = torch.cat((wm_masks_train, uwm_masks_train), axis=0).squeeze()\n+\n+    cv_g_values = torch.cat((wm_g_values_cv, uwm_g_values_cv), axis=0).squeeze()\n+    cv_labels = torch.cat((wm_labels_cv, uwm_labels_cv), axis=0).squeeze()\n+    cv_masks = torch.cat((wm_masks_cv, uwm_masks_cv), axis=0).squeeze()\n+\n+    # Shuffle data.\n+    shuffled_idx = torch.randperm(train_g_values.shape[0])  # Use torch for GPU compatibility\n+\n+    train_g_values = train_g_values[shuffled_idx]\n+    train_labels = train_labels[shuffled_idx]\n+    train_masks = train_masks[shuffled_idx]\n+\n+    # Shuffle the cross-validation data\n+    shuffled_idx_cv = torch.randperm(cv_g_values.shape[0])  # Use torch for GPU compatibility\n+    cv_g_values = cv_g_values[shuffled_idx_cv]\n+    cv_labels = cv_labels[shuffled_idx_cv]\n+    cv_masks = cv_masks[shuffled_idx_cv]\n+\n+    # Del some variables so we free up GPU memory.\n+    del (\n+        wm_g_values_train,\n+        wm_labels_train,\n+        wm_masks_train,\n+        wm_g_values_cv,\n+        wm_labels_cv,\n+        wm_masks_cv,\n+    )\n+    gc.collect()\n+    torch.cuda.empty_cache()\n+\n+    return train_g_values, train_masks, train_labels, cv_g_values, cv_masks, cv_labels\n+\n+\n+def get_tokenized_uwm_outputs(num_negatives, neg_batch_size, tokenizer, device):\n+    dataset, info = tfds.load(\"wikipedia/20230601.en\", split=\"train\", with_info=True)\n+    dataset = dataset.take(num_negatives)\n+\n+    # Convert the dataset to a DataFrame\n+    df = tfds.as_dataframe(dataset, info)\n+    ds = tf.data.Dataset.from_tensor_slices(dict(df))\n+    tf.random.set_seed(0)\n+    ds = ds.shuffle(buffer_size=10_000)\n+    ds = ds.batch(batch_size=neg_batch_size)\n+\n+    tokenized_uwm_outputs = []\n+    # Pad to this length (on the right) for batching.\n+    padded_length = 1000\n+    for i, batch in tqdm.tqdm(enumerate(ds)):\n+        responses = [val.decode() for val in batch[\"text\"].numpy()]\n+        inputs = tokenizer(\n+            responses,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(device)\n+        inputs = inputs[\"input_ids\"].cpu().numpy()\n+        if inputs.shape[1] >= padded_length:\n+            inputs = inputs[:, :padded_length]\n+        else:\n+            inputs = np.concatenate(\n+                [inputs, np.ones((neg_batch_size, padded_length - inputs.shape[1])) * tokenizer.eos_token_id], axis=1\n+            )\n+        tokenized_uwm_outputs.append(inputs)\n+        if len(tokenized_uwm_outputs) * neg_batch_size > num_negatives:\n+            break\n+    return tokenized_uwm_outputs\n+\n+\n+def get_tokenized_wm_outputs(\n+    model,\n+    tokenizer,\n+    watermark_config,\n+    num_pos_batches,\n+    pos_batch_size,\n+    temperature,\n+    max_output_len,\n+    top_k,\n+    top_p,\n+    device,\n+):\n+    eli5_prompts = datasets.load_dataset(\"Pavithree/eli5\")\n+\n+    wm_outputs = []\n+\n+    for batch_id in tqdm.tqdm(range(num_pos_batches)):\n+        prompts = eli5_prompts[\"train\"][\"title\"][batch_id * pos_batch_size : (batch_id + 1) * pos_batch_size]\n+        prompts = [prompt.strip('\"') for prompt in prompts]\n+        inputs = tokenizer(\n+            prompts,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(device)\n+        _, inputs_len = inputs[\"input_ids\"].shape\n+\n+        outputs = model.generate(\n+            **inputs,\n+            watermarking_config=watermark_config,\n+            do_sample=True,\n+            max_length=inputs_len + max_output_len,\n+            temperature=temperature,\n+            top_k=top_k,\n+            top_p=top_p,\n+        )\n+\n+        wm_outputs.append(outputs[:, inputs_len:].cpu().detach())\n+\n+        del outputs, inputs, prompts\n+        gc.collect()\n+\n+    gc.collect()\n+    torch.cuda.empty_cache()\n+    return wm_outputs\n+\n+\n+def upload_model_to_hf(model, hf_repo_name: str, private: bool = True):\n+    api = HfApi()\n+\n+    # Check if the repository exists\n+    try:\n+        api.repo_info(repo_id=hf_repo_name, use_auth_token=True)\n+        print(f\"Repository '{hf_repo_name}' already exists.\")\n+    except RepositoryNotFoundError:\n+        # If the repository does not exist, create it\n+        print(f\"Repository '{hf_repo_name}' not found. Creating it...\")\n+        create_repo(repo_id=hf_repo_name, private=private, use_auth_token=True)\n+        print(f\"Repository '{hf_repo_name}' created successfully.\")\n+\n+    # Push the model to the Hugging Face Hub\n+    print(f\"Uploading model to Hugging Face repo '{hf_repo_name}'...\")\n+    model.push_to_hub(repo_id=hf_repo_name, use_auth_token=True)"
        },
        {
            "sha": "771e3e8f0ae8b827147b6af9175fa5f728c178d9",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -1301,6 +1301,8 @@\n     _import_structure[\"generation\"].extend(\n         [\n             \"AlternatingCodebooksLogitsProcessor\",\n+            \"BayesianDetectorConfig\",\n+            \"BayesianDetectorModel\",\n             \"BeamScorer\",\n             \"BeamSearchScorer\",\n             \"ClassifierFreeGuidanceLogitsProcessor\",\n@@ -1339,6 +1341,9 @@\n             \"StopStringCriteria\",\n             \"SuppressTokensAtBeginLogitsProcessor\",\n             \"SuppressTokensLogitsProcessor\",\n+            \"SynthIDTextWatermarkDetector\",\n+            \"SynthIDTextWatermarkingConfig\",\n+            \"SynthIDTextWatermarkLogitsProcessor\",\n             \"TemperatureLogitsWarper\",\n             \"TopKLogitsWarper\",\n             \"TopPLogitsWarper\",\n@@ -6213,6 +6218,8 @@\n         )\n         from .generation import (\n             AlternatingCodebooksLogitsProcessor,\n+            BayesianDetectorConfig,\n+            BayesianDetectorModel,\n             BeamScorer,\n             BeamSearchScorer,\n             ClassifierFreeGuidanceLogitsProcessor,\n@@ -6251,6 +6258,9 @@\n             StopStringCriteria,\n             SuppressTokensAtBeginLogitsProcessor,\n             SuppressTokensLogitsProcessor,\n+            SynthIDTextWatermarkDetector,\n+            SynthIDTextWatermarkingConfig,\n+            SynthIDTextWatermarkLogitsProcessor,\n             TemperatureLogitsWarper,\n             TopKLogitsWarper,\n             TopPLogitsWarper,"
        },
        {
            "sha": "b487fa3c7fe6ec04c355be158c945a03a7848e66",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 22,
            "deletions": 2,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -18,7 +18,13 @@\n \n \n _import_structure = {\n-    \"configuration_utils\": [\"GenerationConfig\", \"GenerationMode\", \"WatermarkingConfig\"],\n+    \"configuration_utils\": [\n+        \"BaseWatermarkingConfig\",\n+        \"GenerationConfig\",\n+        \"GenerationMode\",\n+        \"SynthIDTextWatermarkingConfig\",\n+        \"WatermarkingConfig\",\n+    ],\n     \"streamers\": [\"TextIteratorStreamer\", \"TextStreamer\"],\n }\n \n@@ -71,6 +77,7 @@\n         \"SequenceBiasLogitsProcessor\",\n         \"SuppressTokensLogitsProcessor\",\n         \"SuppressTokensAtBeginLogitsProcessor\",\n+        \"SynthIDTextWatermarkLogitsProcessor\",\n         \"TemperatureLogitsWarper\",\n         \"TopKLogitsWarper\",\n         \"TopPLogitsWarper\",\n@@ -110,6 +117,9 @@\n     _import_structure[\"watermarking\"] = [\n         \"WatermarkDetector\",\n         \"WatermarkDetectorOutput\",\n+        \"BayesianDetectorModel\",\n+        \"BayesianDetectorConfig\",\n+        \"SynthIDTextWatermarkDetector\",\n     ]\n \n try:\n@@ -179,7 +189,13 @@\n     ]\n \n if TYPE_CHECKING:\n-    from .configuration_utils import GenerationConfig, GenerationMode, WatermarkingConfig\n+    from .configuration_utils import (\n+        BaseWatermarkingConfig,\n+        GenerationConfig,\n+        GenerationMode,\n+        SynthIDTextWatermarkingConfig,\n+        WatermarkingConfig,\n+    )\n     from .streamers import TextIteratorStreamer, TextStreamer\n \n     try:\n@@ -217,6 +233,7 @@\n             SequenceBiasLogitsProcessor,\n             SuppressTokensAtBeginLogitsProcessor,\n             SuppressTokensLogitsProcessor,\n+            SynthIDTextWatermarkLogitsProcessor,\n             TemperatureLogitsWarper,\n             TopKLogitsWarper,\n             TopPLogitsWarper,\n@@ -254,6 +271,9 @@\n             SampleEncoderDecoderOutput,\n         )\n         from .watermarking import (\n+            BayesianDetectorConfig,\n+            BayesianDetectorModel,\n+            SynthIDTextWatermarkDetector,\n             WatermarkDetector,\n             WatermarkDetectorOutput,\n         )"
        },
        {
            "sha": "c460a19885afc58a0261a7126f662c3694153f74",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 165,
            "deletions": 56,
            "changes": 221,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -18,8 +18,9 @@\n import json\n import os\n import warnings\n+from abc import ABC, abstractmethod\n from dataclasses import dataclass, is_dataclass\n-from typing import TYPE_CHECKING, Any, Dict, Optional, Union\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n \n from .. import __version__\n from ..configuration_utils import PretrainedConfig\n@@ -59,6 +60,7 @@\n         StaticCache,\n         StaticCacheConfig,\n     )\n+    from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n \n     NEEDS_CACHE_CONFIG[\"quantized\"] = QuantizedCacheConfig\n     NEEDS_CACHE_CONFIG[\"static\"] = StaticCacheConfig\n@@ -280,23 +282,10 @@ class GenerationConfig(PushToHubMixin):\n         low_memory (`bool`, *optional*):\n             Switch to sequential beam search and sequential topk for contrastive search to reduce peak memory.\n             Used with beam search and contrastive search.\n-        watermarking_config (`WatermarkingConfig` or `dict`, *optional*):\n-            Arguments used to watermark the model outputs by adding a small bias to randomly selected set of \"green\" tokens.\n-            If passed as `Dict`, it will be converted to a `WatermarkingConfig` internally.\n-            See [this paper](https://arxiv.org/abs/2306.04634) for more details. Accepts the following keys:\n-            - greenlist_ratio (`float`):\n-                Used for watermarking. The ratio of \"green\" tokens used to the vocabulary size. Defaults to 0.25.\n-            - bias (`float`):\n-                Used with watermarking. The bias added to the selected \"green\" tokens' logits. Defaults to 2.0.\n-            - hashing_key (`int`):\n-                Hahsing key used for watermarking. Defaults to 15485863 (the millionth prime).\n-            - seeding_scheme (`str`):\n-                Algorithm to use for watermarking. Accepts values:\n-                    - \"lefthash\" (default): \"green\" tokens selection depend on the last token (Algorithm 2 from the paper)\n-                    - \"selfhash\": \"green\" tokens selection depends on the current token itself (Algorithm 3 from the paper)\n-                        The downside of this scheme is that it considers all possible next tokens and can be slower than \"lefthash\".\n-            - context_width (`int`):\n-                The context length of previous tokens to use in seeding. Higher context length makes watermarking more robust.\n+        watermarking_config (`BaseWatermarkingConfig` or `dict`, *optional*):\n+            Arguments used to watermark the model outputs by adding a small bias to randomly selected set of \"green\"\n+            tokens. See the docs of [`SynthIDTextWatermarkingConfig`] and [`WatermarkingConfig`] for more\n+            details. If passed as `Dict`, it will be converted to a `WatermarkingConfig` internally.\n \n         > Parameters that define the output variables of generate\n \n@@ -430,7 +419,7 @@ def __init__(self, **kwargs):\n         watermarking_config = kwargs.pop(\"watermarking_config\", None)\n         if watermarking_config is None:\n             self.watermarking_config = None\n-        elif isinstance(watermarking_config, WatermarkingConfig):\n+        elif isinstance(watermarking_config, BaseWatermarkingConfig):\n             self.watermarking_config = watermarking_config\n         else:\n             self.watermarking_config = WatermarkingConfig.from_dict(watermarking_config)\n@@ -766,7 +755,15 @@ def validate(self, is_init=False):\n \n         # 6.  check watermarking arguments\n         if self.watermarking_config is not None:\n-            if not isinstance(self.watermarking_config, WatermarkingConfig):\n+            if not (\n+                isinstance(self.watermarking_config, WatermarkingConfig)\n+                or isinstance(self.watermarking_config, SynthIDTextWatermarkingConfig)\n+            ):\n+                warnings.warn(\n+                    \"`watermarking_config` as a dict is deprecated. Please construct `watermarking_config` object with \"\n+                    \"`WatermarkingConfig` or `SynthIDTextWatermarkingConfig` class.\",\n+                    FutureWarning,\n+                )\n                 self.watermarking_config = WatermarkingConfig.from_dict(self.watermarking_config)\n             self.watermarking_config.validate()\n \n@@ -1287,52 +1284,20 @@ def update(self, **kwargs):\n \n \n @dataclass\n-class WatermarkingConfig:\n-    \"\"\"\n-    Class that holds arguments for watermark generation and should be passed into `GenerationConfig` during `generate`.\n-    See [this paper](https://arxiv.org/abs/2306.04634) for more details on the arguments.\n-\n-    Accepts the following keys:\n-        - greenlist_ratio (`float`):\n-            Used for watermarking. The ratio of \"green\" tokens used to the vocabulary size. Defaults to 0.25.\n-        - bias (`float`):\n-            Used with watermarking. The bias added to the selected \"green\" tokens' logits. Defaults to 2.0.\n-        - hashing_key (`int`):\n-            Hashing key used for watermarking. Defaults to 15485863 (the millionth prime).\n-        - seeding_scheme (`str`):\n-            Algorithm to use for watermarking. Accepts values:\n-                - \"lefthash\" (default): \"green\" tokens selection depend on the last token (Algorithm 2 from the paper)\n-                - \"selfhash\": \"green\" tokens selection depends on the current token itself (Algorithm 3 from the paper)\n-                    The downside of this scheme is that it considers all possible next tokens and can be slower than \"lefthash\".\n-        - context_width(`int`):\n-            The context length of previous tokens to use in seeding. Higher context length makes watermarking more robust.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        greenlist_ratio: Optional[float] = 0.25,\n-        bias: Optional[float] = 2.0,\n-        hashing_key: Optional[int] = 15485863,\n-        seeding_scheme: Optional[str] = \"lefthash\",\n-        context_width: Optional[int] = 1,\n-    ):\n-        self.greenlist_ratio = greenlist_ratio\n-        self.bias = bias\n-        self.hashing_key = hashing_key\n-        self.seeding_scheme = seeding_scheme\n-        self.context_width = context_width\n+class BaseWatermarkingConfig(ABC):\n+    \"\"\"Generic watermarking config\"\"\"\n \n     @classmethod\n     def from_dict(cls, config_dict, **kwargs):\n         \"\"\"\n-        Constructs a WatermarkingConfig instance from a dictionary of parameters.\n+        Constructs a BaseWatermarkingConfig instance from a dictionary of parameters.\n \n         Args:\n             config_dict (Dict[str, Any]): Dictionary containing configuration parameters.\n             **kwargs: Additional keyword arguments to override dictionary values.\n \n         Returns:\n-            WatermarkingConfig: Instance of WatermarkingConfig constructed from the dictionary.\n+            BaseWatermarkingConfig: Instance of BaseWatermarkingConfig constructed from the dictionary.\n         \"\"\"\n         config = cls(**config_dict)\n         to_remove = []\n@@ -1394,6 +1359,49 @@ def update(self, **kwargs):\n             if hasattr(self, key):\n                 setattr(self, key, value)\n \n+    @abstractmethod\n+    def validate(self): ...\n+\n+    @abstractmethod\n+    def construct_processor(self, vocab_size): ...\n+\n+\n+@dataclass\n+class WatermarkingConfig(BaseWatermarkingConfig):\n+    \"\"\"\n+    Class that holds arguments for watermark generation and should be passed into `GenerationConfig` during `generate`.\n+    See [this paper](https://arxiv.org/abs/2306.04634) for more details on the arguments.\n+\n+    Accepts the following keys:\n+        - greenlist_ratio (`float`):\n+            Used for watermarking. The ratio of \"green\" tokens used to the vocabulary size. Defaults to 0.25.\n+        - bias (`float`):\n+            Used with watermarking. The bias added to the selected \"green\" tokens' logits. Defaults to 2.0.\n+        - hashing_key (`int`):\n+            Hashing key used for watermarking. Defaults to 15485863 (the millionth prime).\n+        - seeding_scheme (`str`):\n+            Algorithm to use for watermarking. Accepts values:\n+                - \"lefthash\" (default): \"green\" tokens selection depend on the last token (Algorithm 2 from the paper)\n+                - \"selfhash\": \"green\" tokens selection depends on the current token itself (Algorithm 3 from the paper)\n+                    The downside of this scheme is that it considers all possible next tokens and can be slower than \"lefthash\".\n+        - context_width(`int`):\n+            The context length of previous tokens to use in seeding. Higher context length makes watermarking more robust.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        greenlist_ratio: Optional[float] = 0.25,\n+        bias: Optional[float] = 2.0,\n+        hashing_key: Optional[int] = 15485863,\n+        seeding_scheme: Optional[str] = \"lefthash\",\n+        context_width: Optional[int] = 1,\n+    ):\n+        self.greenlist_ratio = greenlist_ratio\n+        self.bias = bias\n+        self.hashing_key = hashing_key\n+        self.seeding_scheme = seeding_scheme\n+        self.context_width = context_width\n+\n     def validate(self):\n         watermark_missing_arg_msg = (\n             \"Some of the keys in `watermarking_config` are defined incorrectly. `{key}` should be {correct_value}` \"\n@@ -1423,3 +1431,104 @@ def validate(self):\n                     found_value=self.context_width,\n                 ),\n             )\n+\n+    def construct_processor(self, vocab_size: int, device) -> \"WatermarkLogitsProcessor\":\n+        return WatermarkLogitsProcessor(\n+            vocab_size=vocab_size,\n+            device=device,\n+            greenlist_ratio=self.greenlist_ratio,\n+            bias=self.bias,\n+            hashing_key=self.hashing_key,\n+            seeding_scheme=self.seeding_scheme,\n+            context_width=self.context_width,\n+        )\n+\n+\n+@dataclass\n+class SynthIDTextWatermarkingConfig(BaseWatermarkingConfig):\n+    \"\"\"\n+    Class that holds arguments for watermark generation and should be passed into `GenerationConfig` during `generate`.\n+    See [this paper](https://www.nature.com/articles/s41586-024-08025-4) for more details on the arguments.\n+\n+    Args:\n+        ngram_len (`int`):\n+            Ngram length.\n+        keys (`List[int]`):\n+            A sequence of watermarking keys, one for each depth.\n+        context_history_size (`int`, *optional*, defaults to 1024):\n+            Size of the tensor to keep track of seen contexts.\n+        sampling_table_seed (`int`, *optional*, defaults to 0):\n+            Random seed to generate the sampling table.\n+        sampling_table_size (`int`, *optional*, defaults to 65536):\n+            Size of the sampling table.\n+        skip_first_ngram_calls (`bool`, *optional*, defaults to `False`):\n+            Whether to skip first ngram calls.\n+        debug_mode (`bool`, optional, *optional*, defaults to `False`):\n+            Logits are modified to uniform one got before watermarking modification is applied. This is to test the\n+            implementation.\n+\n+    Examples:\n+    ```python\n+    >>> from transformers import AutoModelForCausalLM, AutoTokenizer, SynthIDTextWatermarkingConfig\n+\n+    >>> tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b-it')\n+    >>> model = AutoModelForCausalLM.from_pretrained('google/gemma-2-2b-it')\n+\n+    >>> # SynthID Text configuration\n+    >>> watermarking_config = SynthIDTextWatermarkingConfig(\n+    ...     keys=[654, 400, 836, 123, 340, 443, 597, 160, 57],\n+    ...     ngram_len=5,\n+    ... )\n+\n+    >>> # Generation with watermarking\n+    >>> tokenized_prompts = tokenizer([\"your prompts here\"])\n+    >>> output_sequences = model.generate(\n+    ...     **tokenized_prompts, watermarking_config=watermarking_config, do_sample=True,\n+    ... )\n+    >>> watermarked_text = tokenizer.batch_decode(output_sequences)\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        ngram_len: int,\n+        keys: List[int],\n+        context_history_size: int = 1024,\n+        sampling_table_seed: int = 0,\n+        sampling_table_size: int = 2**16,\n+        skip_first_ngram_calls: bool = False,\n+        debug_mode: bool = False,\n+    ):\n+        self.ngram_len = ngram_len\n+        self.keys = keys\n+        self.sampling_table_size = sampling_table_size\n+        self.sampling_table_seed = sampling_table_seed\n+        self.context_history_size = context_history_size\n+        self.skip_first_ngram_calls = skip_first_ngram_calls\n+        self.debug_mode = debug_mode\n+\n+    def validate(self):\n+        watermark_missing_arg_msg = (\n+            \"Some of the keys in `watermarking_config` are defined incorrectly. `{key}` should be {correct_value}` \"\n+            \"but found {found_value}\"\n+        )\n+        if self.sampling_table_size > 2**24:\n+            raise ValueError(\n+                watermark_missing_arg_msg.format(\n+                    key=\"sampling_table_size\",\n+                    correct_value=\"< 2**24\",\n+                    found_value=self.sampling_table_size,\n+                ),\n+            )\n+\n+    def construct_processor(self, vocab_size: int, device) -> \"WatermarkLogitsProcessor\":\n+        return SynthIDTextWatermarkLogitsProcessor(\n+            ngram_len=self.ngram_len,\n+            keys=self.keys,\n+            sampling_table_size=self.sampling_table_size,\n+            sampling_table_seed=self.sampling_table_seed,\n+            context_history_size=self.context_history_size,\n+            device=device,\n+            skip_first_ngram_calls=self.skip_first_ngram_calls,\n+            debug_mode=self.debug_mode,\n+        )"
        },
        {
            "sha": "fde95c7a85652f2495eaf31b130955bada9b29d2",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 477,
            "deletions": 1,
            "changes": 478,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -1,5 +1,5 @@\n # coding=utf-8\n-# Copyright 2020 The HuggingFace Inc. team\n+# Copyright 2024 The HuggingFace Inc. team and Google DeepMind.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -2460,6 +2460,7 @@ def _score_rejection_sampling(self, input_seq: torch.LongTensor, scores: torch.F\n                 final_greenlist.append(greedy_predictions[i])\n         return torch.tensor(final_greenlist, device=input_seq.device)\n \n+    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n     def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n         if input_ids.shape[-1] < self.context_width:\n             logger.warning(\n@@ -2477,3 +2478,478 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n             scores_processed[b_idx, greenlist_ids] = scores_processed[b_idx, greenlist_ids] + self.bias\n \n         return scores_processed\n+\n+\n+class SynthIDTextWatermarkState:\n+    \"\"\"SynthID watermarking state.\"\"\"\n+\n+    def __init__(\n+        self,\n+        batch_size: int,\n+        ngram_len: int,\n+        context_history_size: int,\n+        device: torch.device,\n+    ):\n+        \"\"\"Initializes the state.\n+\n+        Args:\n+            batch_size (`int`): Batch size.\n+            ngram_len (`int`): Ngram length.\n+            context_history_size (`int`): Size of the tensor to keep track of seen contexts.\n+            device (`int`): Device to use.\n+        \"\"\"\n+        self.context = torch.zeros(\n+            (batch_size, ngram_len - 1),\n+            dtype=torch.int64,\n+            device=device,\n+        )\n+        self.context_history = torch.zeros(\n+            (batch_size, context_history_size),\n+            dtype=torch.int64,\n+            device=device,\n+        )\n+        self.num_calls = 0\n+\n+\n+class SynthIDTextWatermarkLogitsProcessor(LogitsProcessor):\n+    r\"\"\"\n+    Logits processor that implements watermarking techniques for text generation models.\n+    This class facilitates the application of SynthID text watermarking, a method for embedding imperceptible signals\n+    into generated text to aid in detecting synthetic content. It operates by subtly manipulating the probabilities of\n+    token selection during text generation in a manner that can be reliably recovered later for verification.\n+\n+    Key Features:\n+    * **State Management:** Maintains internal state to track token sequences and generate watermarking keys\n+    dynamically.\n+\n+    * **Key Generation:** Computes hashes based on token sequences and watermarking parameters to create unique keys\n+    for each position.\n+\n+    * **G-Value Sampling:** Employs a pre-computed sampling table to sample watermarking values (g-values) based on\n+    the generated keys.\n+\n+    * **Score Adjustment:** Applies calculated g-values to modify token probabilities during generation, embedding the\n+    watermark.\n+\n+    * **Context Repetition Handling:** Incorporates logic to avoid watermarking tokens in repeated contexts,\n+    preserving naturalness.\n+\n+    * **EOS Token Masking:** Supports masking end-of-sentence tokens to prevent their inclusion in watermarking\n+    calculations.\n+\n+    * **Utility Functions:** Provides functions to compute g-values directly, check for context repetition, create\n+    EOS token masks, and estimate expected mean g-values.\n+\n+    Refer to paper url: https://www.nature.com/articles/s41586-024-08025-4 for more details around this.\n+\n+    Args:\n+        ngram_len (`int`):\n+            Ngram length.\n+        keys (`List[int]`):\n+            A sequence of watermarking keys, one for each depth.\n+        sampling_table_size (`int`):\n+            Size of the sampling table.\n+        sampling_table_seed (`int`):\n+            Random seed to generate the sampling table.\n+        context_history_size (`int`):\n+            Size of the tensor to keep track of seen contexts.\n+        device (`torch.device`):\n+            Device to use.\n+        skip_first_ngram_calls (`bool`, *optional*, defaults to `False`):\n+            Whether to skip first ngram calls.\n+        debug_mode (`bool`, optional, *optional*, defaults to `False`):\n+            Logits are modified to uniform one got before watermarking modification is applied. This is to test the\n+            implementation.\n+\n+    Examples:\n+    ```python\n+    >>> from transformers import AutoModelForCausalLM, AutoTokenizer, SynthIDTextWatermarkingConfig\n+\n+    >>> tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b-it')\n+    >>> model = AutoModelForCausalLM.from_pretrained('google/gemma-2-2b-it')\n+\n+    >>> # SynthID Text configuration\n+    >>> watermarking_config = SynthIDTextWatermarkingConfig(\n+    ...     keys=[654, 400, 836, 123, 340, 443, 597, 160, 57],\n+    ...     ngram_len=5,\n+    ... )\n+\n+    >>> # Generation with watermarking\n+    >>> tokenized_prompts = tokenizer([\"your prompts here\"])\n+    >>> output_sequences = model.generate(\n+    ...     **tokenized_prompts, watermarking_config=watermarking_config, do_sample=True,\n+    ... )\n+    >>> watermarked_text = tokenizer.batch_decode(output_sequences)\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        ngram_len: int,\n+        keys: List[int],\n+        sampling_table_size: int,\n+        sampling_table_seed: int,\n+        context_history_size: int,\n+        device: torch.device,\n+        skip_first_ngram_calls: bool = False,\n+        debug_mode: bool = False,\n+    ):\n+        self.ngram_len = ngram_len\n+        self.keys = torch.tensor(keys, device=device)\n+\n+        generator = torch.Generator(device=device).manual_seed(sampling_table_seed)\n+        # A random sampling table is pre-computed and modulo table size is applied to map from a hash of ngram keys to\n+        # g values, this is similar to the hashtable implementation used in\n+        # https://github.com/facebookresearch/three_bricks. We note that the hashing employed in this repository is\n+        # different from that used to watermark the Gemini App, and hence the detectors trained based on the\n+        # hashing in this repository will not transfer to text generated by the Gemini App.\n+        self.sampling_table = torch.randint(\n+            low=0,\n+            high=2,\n+            size=(sampling_table_size,),\n+            generator=generator,\n+            device=device,\n+        )\n+        self.context_history_size = context_history_size\n+        self.device = device\n+        self.state = None\n+        self.skip_first_ngram_calls = skip_first_ngram_calls\n+        self.debug_mode = debug_mode\n+\n+    def _init_state(self, batch_size: int):\n+        \"\"\"Initializes the state.\"\"\"\n+        self.state = SynthIDTextWatermarkState(\n+            batch_size=batch_size,\n+            ngram_len=self.ngram_len,\n+            context_history_size=self.context_history_size,\n+            device=self.device,\n+        )\n+\n+    def update_scores(self, scores: torch.FloatTensor, g_values: torch.FloatTensor) -> torch.FloatTensor:\n+        \"\"\"Updates scores using the g values.\n+\n+        We assume that the scores are in the log space.\n+        Args:\n+            scores (`torch.FloatTensor`): Scores (batch_size, vocab_size).\n+            g_values (`torch.FloatTensor`): G valus (batch_size, vocab_size, depth).\n+\n+        Returns:\n+            Updated scores (batch_size, vocab_size).\n+        \"\"\"\n+        _, _, depth = g_values.shape\n+\n+        probs = torch.softmax(scores, dim=1)\n+\n+        for i in range(depth):\n+            g_values_at_depth = g_values[:, :, i]\n+            g_mass_at_depth = (g_values_at_depth * probs).sum(axis=1, keepdims=True)\n+            probs = probs * (1 + g_values_at_depth - g_mass_at_depth)\n+\n+        log_probs = torch.log(probs)\n+        log_probs = torch.where(torch.isfinite(log_probs), log_probs, torch.finfo(log_probs.dtype).min)\n+        return log_probs\n+\n+    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)\n+    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n+        self._check_input_ids_shape(input_ids)\n+        batch_size, vocab_size = scores.shape\n+\n+        if self.debug_mode:\n+            scores = torch.ones_like(scores)\n+\n+        # Currently indices is just a arange to compute watermarking on the desnse logits.\n+        all_indices = torch.stack([torch.arange(vocab_size, device=self.device) for _ in range(batch_size)])\n+\n+        if self.state is None:\n+            # Initialize watermarking state if it does not exist.\n+            self._init_state(batch_size)\n+        else:\n+            # Append last input id (which is the input id added in last call) to the\n+            # previous context so we have the context to be used for current\n+            # watermarking.\n+            self.state.context = torch.concat(\n+                (self.state.context, input_ids[:, -1:]),\n+                dim=1,\n+            )\n+            self.state.context = self.state.context[:, 1:]\n+\n+        if self.state is None:\n+            raise ValueError(\"self.state can't be None! Call `self._init_state` to initialize the state.\")\n+\n+        self.state.num_calls += 1\n+\n+        # Don't watermark the first ngram_len - 1 tokens if set.\n+        if self.skip_first_ngram_calls and self.state.num_calls < self.ngram_len:\n+            return scores\n+\n+        # 2. Generate random keys for each ngram key combination.\n+        ngram_keys, hash_result_with_just_context = self._compute_keys(self.state.context, all_indices)\n+        # ngram_keys shape [batch_size, top_k, depth]\n+\n+        # 3. Sample g values.\n+        g_values = self.sample_g_values(ngram_keys)\n+        # g_values shape [batch_size, top_k, depth]\n+\n+        # 4. Modify scores.\n+        updated_scores = self.update_scores(scores, g_values)\n+        # updated scores shape [batch_size, top_k]\n+\n+        # 5. Check if the current watermarking context was previously used, if yes skip watermarking.\n+        hash_result_with_just_context = hash_result_with_just_context[:, None]\n+        is_repeated_context = (self.state.context_history == hash_result_with_just_context).any(\n+            dim=1,\n+            keepdim=True,\n+        )\n+        self.state.context_history = torch.concat(\n+            (hash_result_with_just_context, self.state.context_history),\n+            dim=1,\n+        )[:, :-1]\n+\n+        updated_watermarked_scores = torch.where(\n+            is_repeated_context,\n+            input=scores,\n+            other=updated_scores,\n+        )\n+        return updated_watermarked_scores\n+\n+    def accumulate_hash(\n+        self,\n+        current_hash: torch.LongTensor,\n+        data: torch.LongTensor,\n+        multiplier: int = 6364136223846793005,\n+        increment: int = 1,\n+    ) -> torch.LongTensor:\n+        \"\"\"\n+        Accumulate hash of data on current hash.\n+\n+        Method uses adapted linear congruential generator with newlib/musl parameters.\n+\n+        This function has following property -\n+        f(x, data[T]) = f(f(x, data[:T - 1]), data[T])\n+\n+        This function expects current_hash.shape and data.shape[:-1] to\n+        match/broadcastable.\n+\n+        Args:\n+            current_hash (`torch.LongTensor`):\n+                (shape,)\n+            data (`torch.LongTensor`):\n+                (shape, tensor_len)\n+            multiplier (`int`, optional, *optional*, defaults to 6364136223846793005):\n+                multiplier of linear congruential generator\n+            increment (`int`, optional, *optional*, defaults to 1):\n+                increment of linear congruential generator\n+\n+        Returns:\n+            updated hash (shape,)\n+        \"\"\"\n+        for i in range(data.shape[-1]):\n+            current_hash = torch.add(current_hash, data[..., i])\n+            current_hash = torch.mul(current_hash, multiplier)\n+            current_hash = torch.add(current_hash, increment)\n+        return current_hash\n+\n+    def compute_ngram_keys(self, ngrams: torch.LongTensor) -> torch.LongTensor:\n+        \"\"\"Computes random keys for each ngram and depth.\n+\n+        Args:\n+            ngrams (`torch.LongTensor`):\n+                Ngrams (batch_size, num_ngrams, ngram_len).\n+\n+        Returns:\n+            ngram keys (batch_size, num_ngrams, depth).\n+        \"\"\"\n+        if len(ngrams.shape) != 3:\n+            raise ValueError(\n+                \"Ngrams should be of shape (batch_size, num_ngrams, ngram_len), but\" f\" is {ngrams.shape}\"\n+            )\n+        if ngrams.shape[2] != self.ngram_len:\n+            raise ValueError(\n+                \"Ngrams should be of shape (batch_size, num_ngrams, ngram_len),\"\n+                f\" where ngram_len is {self.ngram_len}, but is {ngrams.shape}\"\n+            )\n+        batch_size, _, _ = ngrams.shape\n+\n+        hash_result = torch.ones(batch_size, device=self.device, dtype=torch.long)\n+        # hash_result shape [batch_size,]\n+        # ngrams shape [batch_size, num_ngrams, ngram_len]\n+        hash_result = torch.vmap(self.accumulate_hash, in_dims=(None, 1), out_dims=1)(hash_result, ngrams)\n+        # hash_result shape [batch_size, num_ngrams]\n+\n+        keys = self.keys[None, None, :, None]\n+        # hash_result shape [batch_size, num_ngrams]\n+        # keys shape [1, 1, depth, 1]\n+        hash_result = torch.vmap(self.accumulate_hash, in_dims=(None, 2), out_dims=2)(hash_result, keys)\n+        # hash_result shape [batch_size, num_ngrams, depth]\n+\n+        return hash_result\n+\n+    def _compute_keys(\n+        self, n_minus_1_grams: torch.LongTensor, indices: torch.LongTensor\n+    ) -> Tuple[torch.LongTensor, torch.LongTensor]:\n+        \"\"\"Computes random keys for each ngram and depth.\n+\n+        Args:\n+            n_minus_1_grams (`torch.LongTensor`):\n+                Ngrams (batch_size, ngram_len - 1).\n+            indices (`torch.LongTensor`):\n+                indices of the continuations (batch_size, num_indices)\n+\n+        Returns:\n+            Ngram keys (batch_size, num_indices, depth).\n+        \"\"\"\n+        batch_size, _ = n_minus_1_grams.shape\n+\n+        hash_result = torch.ones(batch_size, device=self.device, dtype=torch.long)\n+        # First hash n_minus_1 gram, for each batch entry we have a single\n+        # n_minus_1 gram context.\n+        # hash_result shape [batch_size]\n+        # n_minus_1_gram shape [batch_size, ngram_len - 1]\n+        hash_result_with_just_context = self.accumulate_hash(hash_result, n_minus_1_grams)\n+        # hash_result shape [batch_size,]\n+        # Indices is of shape [batch_size, num_indices], so we make it\n+        # [batch_size, num_indices, 1] so we can vmap over num_indices dim.\n+        hash_result = torch.vmap(self.accumulate_hash, in_dims=(None, 1), out_dims=1)(\n+            hash_result_with_just_context, indices[:, :, None]\n+        )\n+        # hash_result shape [batch_size, num_indices]\n+        # Basically we have a hash for each batch entry and each indices\n+        # Now we add watermarking keys to this hash.\n+        # keys are of shape [depth,]\n+        # We add batch, num_indices and data dimension to this making it\n+        # [1, 1, depth, 1].\n+        # So we can vmap over the depth dimension for compute_hash\n+        keys = self.keys[None, None, :, None]\n+        hash_result = torch.vmap(self.accumulate_hash, in_dims=(None, 2), out_dims=2)(hash_result, keys)\n+        # hash_result shape should be [batch_size, num_indices, depth]\n+        return hash_result, hash_result_with_just_context\n+\n+    def sample_g_values(self, ngram_keys: torch.LongTensor) -> torch.LongTensor:\n+        \"\"\"\n+        Samples g values from Bernoulli distribution.\n+\n+        It is not possible to pass random keys in a vectorized way in torch. Instead\n+        we pre-compute a random sampling table, and use apply modulo table size to\n+        map from ngram keys (int64) to g values.\n+\n+        Args:\n+            ngram_keys (`torch.LongTensor`):\n+                Random keys (batch_size, num_ngrams, depth).\n+\n+        Returns:\n+            G values (batch_size, num_ngrams, depth).\n+        \"\"\"\n+        (sampling_table_size,) = self.sampling_table.shape\n+        sampling_table = self.sampling_table.reshape((1, 1, sampling_table_size))\n+        ngram_keys = ngram_keys % sampling_table_size\n+        return torch.take_along_dim(sampling_table, indices=ngram_keys, dim=2)\n+\n+    def _check_input_ids_shape(self, input_ids: torch.LongTensor):\n+        \"\"\"Checks the shape of input ids.\"\"\"\n+        if len(input_ids.shape) != 2:\n+            raise ValueError(\"Input ids should be of shape (batch_size, input_len), but is\" f\" {input_ids.shape}\")\n+\n+    def compute_g_values(self, input_ids: torch.LongTensor) -> torch.LongTensor:\n+        \"\"\"\n+        Computes g values for each ngram from the given sequence of tokens.\n+\n+        Args:\n+            input_ids (`torch.LongTensor`):\n+                Input token ids (batch_size, input_len).\n+\n+        Returns:\n+            G values (batch_size, input_len - (ngram_len - 1), depth).\n+        \"\"\"\n+        self._check_input_ids_shape(input_ids)\n+        ngrams = input_ids.unfold(dimension=1, size=self.ngram_len, step=1)\n+        ngram_keys = self.compute_ngram_keys(ngrams)\n+        return self.sample_g_values(ngram_keys)\n+\n+    def compute_context_repetition_mask(self, input_ids: torch.LongTensor) -> torch.LongTensor:\n+        \"\"\"\n+        Computes repetition mask.\n+\n+        0 and 1 stand for repeated and not repeated context n-1 grams respectively.\n+\n+        Args:\n+            input_ids (`torch.LongTensor`):\n+                Input token ids (batch_size, input_len).\n+\n+        Returns:\n+            Repetitions mask (batch_size, input_len - (ngram_len - 1)).\n+        \"\"\"\n+        self._check_input_ids_shape(input_ids)\n+        batch_size, _ = input_ids.shape\n+        state = SynthIDTextWatermarkState(\n+            batch_size=batch_size,\n+            ngram_len=self.ngram_len,\n+            context_history_size=self.context_history_size,\n+            device=self.device,\n+        )\n+        contexts = input_ids[:, :-1].unfold(\n+            dimension=1,\n+            size=self.ngram_len - 1,\n+            step=1,\n+        )\n+        _, num_contexts, _ = contexts.shape\n+\n+        are_repeated_contexts = []\n+        for i in range(num_contexts):\n+            context = contexts[:, i, :]\n+            hash_result = torch.ones(batch_size, device=self.device, dtype=torch.long)\n+            context_hash = self.accumulate_hash(hash_result, context)[:, None]\n+            is_repeated_context = (state.context_history == context_hash).any(\n+                dim=1,\n+                keepdim=True,\n+            )\n+            are_repeated_contexts.append(is_repeated_context)\n+            state.context_history = torch.concat(\n+                (context_hash, state.context_history),\n+                dim=1,\n+            )[:, :-1]\n+        are_repeated_contexts = torch.concat(are_repeated_contexts, dim=1)\n+\n+        return torch.logical_not(are_repeated_contexts)\n+\n+    def compute_eos_token_mask(self, input_ids: torch.LongTensor, eos_token_id: int) -> torch.LongTensor:\n+        \"\"\"\n+        Computes repetitions mask.\n+\n+        1 stands for ngrams that don't contain EOS tokens and vice versa.\n+\n+        Args:\n+            input_ids (`torch.LongTensor`):\n+                Input token ids (batch_size, input_len).\n+            eos_token_id (`int`):\n+                EOS token ID.\n+\n+        Returns:\n+            EOS token mask (batch_size, input_len).\n+        \"\"\"\n+        self._check_input_ids_shape(input_ids)\n+        noneos_masks = []\n+        all_eos_equated = input_ids == eos_token_id\n+        for eos_equated in all_eos_equated:\n+            nonzero_idx = torch.nonzero(eos_equated)\n+            noneos_mask = torch.ones_like(eos_equated)\n+            if nonzero_idx.shape[0] != 0:\n+                noneos_mask[nonzero_idx[0][0] :] = 0\n+            noneos_masks.append(noneos_mask)\n+        return torch.stack(noneos_masks, dim=0)\n+\n+    def expected_mean_g_value(self, vocab_size: int, coinflip_prob: float = 0.5) -> float:\n+        \"\"\"\n+        Compute expected mean g-value after watermarking, assuming uniform LM dist.\n+\n+        This is the theoretical expected value for single-layer watermarking.\n+\n+        Args:\n+            vocab_size (`int`):\n+                The size of the vocabulary.\n+            coinflip_prob arg_name (`float`, *optional*, defaults to 0.5):\n+                Probability of 1 in boolean prf.\n+\n+        Returns:\n+            The expected mean g-value for watermarked text.\n+        \"\"\"\n+        return coinflip_prob + coinflip_prob * (1 - coinflip_prob) * (1 - (1 / vocab_size))"
        },
        {
            "sha": "700ea0443f4dbd1442d6070a4fc5c48c5cedf199",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -92,7 +92,6 @@\n     TopPLogitsWarper,\n     TypicalLogitsWarper,\n     UnbatchedClassifierFreeGuidanceLogitsProcessor,\n-    WatermarkLogitsProcessor,\n )\n from .stopping_criteria import (\n     ConfidenceCriteria,\n@@ -1011,15 +1010,7 @@ def _get_logits_processor(\n             )\n         if generation_config.watermarking_config is not None:\n             processors.append(\n-                WatermarkLogitsProcessor(\n-                    vocab_size=self.config.vocab_size,\n-                    device=device,\n-                    greenlist_ratio=generation_config.watermarking_config.greenlist_ratio,\n-                    bias=generation_config.watermarking_config.bias,\n-                    hashing_key=generation_config.watermarking_config.hashing_key,\n-                    seeding_scheme=generation_config.watermarking_config.seeding_scheme,\n-                    context_width=generation_config.watermarking_config.context_width,\n-                )\n+                generation_config.watermarking_config.construct_processor(self.config.vocab_size, device)\n             )\n \n         # TODO (joao): find a strategy to specify the order of the processors"
        },
        {
            "sha": "da90c03dd0da89cdf98b2eede63f1eca2b3b9197",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 316,
            "deletions": 6,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -1,5 +1,5 @@\n # coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team\n+# Copyright 2024 The HuggingFace Inc. team and Google DeepMind.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -16,19 +16,22 @@\n import collections\n from dataclasses import dataclass\n from functools import lru_cache\n-from typing import Dict, Optional, Union\n+from typing import Any, Dict, Optional, Tuple, Union\n \n import numpy as np\n+import torch\n+from torch import nn\n+from torch.nn import BCELoss\n \n-from ..configuration_utils import PretrainedConfig\n-from ..utils import is_torch_available, logging\n-from .configuration_utils import WatermarkingConfig\n+from ..modeling_utils import PreTrainedModel\n+from ..utils import ModelOutput, is_torch_available, logging\n+from .configuration_utils import PretrainedConfig, WatermarkingConfig\n \n \n if is_torch_available():\n     import torch\n \n-    from .logits_process import WatermarkLogitsProcessor\n+    from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n \n \n logger = logging.get_logger(__name__)\n@@ -237,3 +240,310 @@ def __call__(\n                 confidence=confidence,\n             )\n         return prediction\n+\n+\n+class BayesianDetectorConfig(PretrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`BayesianDetectorModel`]. It is used to\n+    instantiate a Bayesian Detector model according to the specified arguments.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        watermarking_depth (`int`, *optional*):\n+            The number of tournament layers.\n+        base_rate (`float1`, *optional*, defaults to 0.5):\n+            Prior probability P(w) that a text is watermarked.\n+    \"\"\"\n+\n+    def __init__(self, watermarking_depth: int = None, base_rate: float = 0.5, **kwargs):\n+        self.watermarking_depth = watermarking_depth\n+        self.base_rate = base_rate\n+        # These can be set later to store information about this detector.\n+        self.model_name = None\n+        self.watermarking_config = None\n+\n+        super().__init__(**kwargs)\n+\n+    def set_detector_information(self, model_name, watermarking_config):\n+        self.model_name = model_name\n+        self.watermarking_config = watermarking_config\n+\n+\n+@dataclass\n+class BayesianWatermarkDetectorModelOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of models predicting if the text is watermarked.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss.\n+        posterior_probabilities (`torch.FloatTensor` of shape `(1,)`):\n+            Multiple choice classification loss.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    posterior_probabilities: Optional[torch.FloatTensor] = None\n+\n+\n+class BayesianDetectorWatermarkedLikelihood(nn.Module):\n+    \"\"\"Watermarked likelihood model for binary-valued g-values.\n+\n+    This takes in g-values and returns p(g_values|watermarked).\n+    \"\"\"\n+\n+    def __init__(self, watermarking_depth: int):\n+        \"\"\"Initializes the model parameters.\"\"\"\n+        super().__init__()\n+        self.watermarking_depth = watermarking_depth\n+        self.beta = torch.nn.Parameter(-2.5 + 0.001 * torch.randn(1, 1, watermarking_depth))\n+        self.delta = torch.nn.Parameter(0.001 * torch.randn(1, 1, self.watermarking_depth, watermarking_depth))\n+\n+    def _compute_latents(self, g_values: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"Computes the unique token probability distribution given g-values.\n+\n+        Args:\n+            g_values (`torch.Tensor` of shape `(batch_size, seq_len, watermarking_depth)`):\n+                PRF values.\n+\n+        Returns:\n+            p_one_unique_token and p_two_unique_tokens, both of shape\n+            [batch_size, seq_len, watermarking_depth]. p_one_unique_token[i,t,l]\n+            gives the probability of there being one unique token in a tournament\n+            match on layer l, on timestep t, for batch item i.\n+            p_one_unique_token[i,t,l] + p_two_unique_token[i,t,l] = 1.\n+        \"\"\"\n+        # Tile g-values to produce feature vectors for predicting the latents\n+        # for each layer in the tournament; our model for the latents psi is a\n+        # logistic regression model psi = sigmoid(delta * x + beta).\n+\n+        # [batch_size, seq_len, watermarking_depth, watermarking_depth]\n+        x = torch.repeat_interleave(torch.unsqueeze(g_values, dim=-2), self.watermarking_depth, axis=-2)\n+\n+        # mask all elements above -1 diagonal for autoregressive factorization\n+        x = torch.tril(x, diagonal=-1)\n+\n+        # [batch_size, seq_len, watermarking_depth]\n+        # (i, j, k, l) x (i, j, k, l) -> (i, j, k) einsum equivalent\n+        logits = (self.delta[..., None, :] @ x.type(self.delta.dtype)[..., None]).squeeze() + self.beta\n+\n+        p_two_unique_tokens = torch.sigmoid(logits)\n+        p_one_unique_token = 1 - p_two_unique_tokens\n+        return p_one_unique_token, p_two_unique_tokens\n+\n+    def forward(self, g_values: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Computes the likelihoods P(g_values|watermarked).\n+\n+        Args:\n+            g_values (`torch.Tensor` of shape `(batch_size, seq_len, watermarking_depth)`):\n+                g-values (values 0 or 1)\n+\n+        Returns:\n+            p(g_values|watermarked) of shape [batch_size, seq_len, watermarking_depth].\n+        \"\"\"\n+        p_one_unique_token, p_two_unique_tokens = self._compute_latents(g_values)\n+\n+        # P(g_tl | watermarked) is equal to\n+        # 0.5 * [ (g_tl+0.5) * p_two_unique_tokens + p_one_unique_token].\n+        return 0.5 * ((g_values + 0.5) * p_two_unique_tokens + p_one_unique_token)\n+\n+\n+class BayesianDetectorModel(PreTrainedModel):\n+    r\"\"\"\n+    Bayesian classifier for watermark detection.\n+\n+    This detector uses Bayes' rule to compute a watermarking score, which is the sigmoid of the log of ratio of the\n+    posterior probabilities P(watermarked|g_values) and P(unwatermarked|g_values). Please see the section on\n+    BayesianScore in the paper for further details.\n+    Paper URL: https://www.nature.com/articles/s41586-024-08025-4\n+\n+    Note that this detector only works with non-distortionary Tournament-based watermarking using the Bernoulli(0.5)\n+    g-value distribution.\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`BayesianDetectorConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+    \"\"\"\n+\n+    config_class = BayesianDetectorConfig\n+    base_model_prefix = \"model\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        self.watermarking_depth = config.watermarking_depth\n+        self.base_rate = config.base_rate\n+        self.likelihood_model_watermarked = BayesianDetectorWatermarkedLikelihood(\n+            watermarking_depth=self.watermarking_depth\n+        )\n+        self.prior = torch.nn.Parameter(torch.tensor([self.base_rate]))\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights.\"\"\"\n+        if isinstance(module, nn.Parameter):\n+            module.weight.data.normal_(mean=0.0, std=0.02)\n+\n+    def _compute_posterior(\n+        self,\n+        likelihoods_watermarked: torch.Tensor,\n+        likelihoods_unwatermarked: torch.Tensor,\n+        mask: torch.Tensor,\n+        prior: float,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Compute posterior P(w|g) given likelihoods, mask and prior.\n+\n+        Args:\n+            likelihoods_watermarked (`torch.Tensor` of shape `(batch, length, depth)`):\n+                Likelihoods P(g_values|watermarked) of g-values under watermarked model.\n+            likelihoods_unwatermarked (`torch.Tensor` of shape `(batch, length, depth)`):\n+                Likelihoods P(g_values|unwatermarked) of g-values under unwatermarked model.\n+            mask (`torch.Tensor` of shape `(batch, length)`):\n+                A binary array indicating which g-values should be used. g-values with mask value 0 are discarded.\n+            prior (`float`):\n+                the prior probability P(w) that the text is watermarked.\n+\n+        Returns:\n+            Posterior probability P(watermarked|g_values), shape [batch].\n+        \"\"\"\n+        mask = torch.unsqueeze(mask, dim=-1)\n+        prior = torch.clamp(prior, min=1e-5, max=1 - 1e-5)\n+        log_likelihoods_watermarked = torch.log(torch.clamp(likelihoods_watermarked, min=1e-30, max=float(\"inf\")))\n+        log_likelihoods_unwatermarked = torch.log(torch.clamp(likelihoods_unwatermarked, min=1e-30, max=float(\"inf\")))\n+        log_odds = log_likelihoods_watermarked - log_likelihoods_unwatermarked\n+\n+        # Sum relative surprisals (log odds) across all token positions and layers.\n+        relative_surprisal_likelihood = torch.einsum(\"i...->i\", log_odds * mask)\n+\n+        # Compute the relative surprisal prior\n+        relative_surprisal_prior = torch.log(prior) - torch.log(1 - prior)\n+\n+        # Combine prior and likelihood.\n+        # [batch_size]\n+        relative_surprisal = relative_surprisal_prior + relative_surprisal_likelihood\n+\n+        # Compute the posterior probability P(w|g) = sigmoid(relative_surprisal).\n+        return torch.sigmoid(relative_surprisal)\n+\n+    def forward(\n+        self,\n+        g_values: torch.Tensor,\n+        mask: torch.Tensor,\n+        labels: Optional[torch.Tensor] = None,\n+        loss_batch_weight=1,\n+        return_dict=False,\n+    ) -> BayesianWatermarkDetectorModelOutput:\n+        \"\"\"\n+        Computes the watermarked posterior P(watermarked|g_values).\n+\n+        Args:\n+            g_values (`torch.Tensor` of shape `(batch_size, seq_len, watermarking_depth, ...)`):\n+                g-values (with values 0 or 1)\n+            mask:\n+                A binary array shape [batch_size, seq_len] indicating which g-values should be used. g-values with mask\n+                value 0 are discarded.\n+\n+        Returns:\n+            p(watermarked | g_values), of shape [batch_size].\n+        \"\"\"\n+\n+        likelihoods_watermarked = self.likelihood_model_watermarked(g_values)\n+        likelihoods_unwatermarked = 0.5 * torch.ones_like(g_values)\n+        out = self._compute_posterior(\n+            likelihoods_watermarked=likelihoods_watermarked,\n+            likelihoods_unwatermarked=likelihoods_unwatermarked,\n+            mask=mask,\n+            prior=self.prior,\n+        )\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = BCELoss()\n+            loss_unwweight = torch.sum(self.likelihood_model_watermarked.delta**2)\n+            loss_weight = loss_unwweight * loss_batch_weight\n+            loss = loss_fct(torch.clamp(out, 1e-5, 1 - 1e-5), labels) + loss_weight\n+\n+        if not return_dict:\n+            return (out,) if loss is None else (out, loss)\n+\n+        return BayesianWatermarkDetectorModelOutput(loss=loss, posterior_probabilities=out)\n+\n+\n+class SynthIDTextWatermarkDetector:\n+    r\"\"\"\n+    SynthID text watermark detector class.\n+\n+    This class has to be initialized with the trained bayesian detector module check script\n+    in examples/synthid_text/detector_training.py for example in training/saving/loading this\n+    detector module. The folder also showcases example use case of this detector.\n+\n+    Parameters:\n+        detector_module ([`BayesianDetectorModel`]):\n+            Bayesian detector module object initialized with parameters.\n+            Check examples/research_projects/synthid_text/detector_training.py for usage.\n+        logits_processor (`SynthIDTextWatermarkLogitsProcessor`):\n+            The logits processor used for watermarking.\n+        tokenizer (`Any`):\n+            The tokenizer used for the model.\n+\n+    Examples:\n+    ```python\n+    >>> from transformers import (\n+    ...     AutoTokenizer, BayesianDetectorModel, SynthIDTextWatermarkLogitsProcessor, SynthIDTextWatermarkDetector\n+    ... )\n+\n+    >>> # Load the detector. See examples/research_projects/synthid_text for training a detector.\n+    >>> detector_model = BayesianDetectorModel.from_pretrained(\"joaogante/dummy_synthid_detector\")\n+    >>> logits_processor = SynthIDTextWatermarkLogitsProcessor(\n+    ...     **detector_model.config.watermarking_config, device=\"cpu\"\n+    ... )\n+    >>> tokenizer = AutoTokenizer.from_pretrained(detector_model.config.model_name)\n+    >>> detector = SynthIDTextWatermarkDetector(detector_model, logits_processor, tokenizer)\n+\n+    >>> # Test whether a certain string is watermarked\n+    >>> test_input = tokenizer([\"This is a test input\"], return_tensors=\"pt\")\n+    >>> is_watermarked = detector(test_input.input_ids)\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        detector_module: BayesianDetectorModel,\n+        logits_processor: SynthIDTextWatermarkLogitsProcessor,\n+        tokenizer: Any,\n+    ):\n+        self.detector_module = detector_module\n+        self.logits_processor = logits_processor\n+        self.tokenizer = tokenizer\n+\n+    def __call__(self, tokenized_outputs: torch.Tensor):\n+        # eos mask is computed, skip first ngram_len - 1 tokens\n+        # eos_mask will be of shape [batch_size, output_len]\n+        eos_token_mask = self.logits_processor.compute_eos_token_mask(\n+            input_ids=tokenized_outputs,\n+            eos_token_id=self.tokenizer.eos_token_id,\n+        )[:, self.logits_processor.ngram_len - 1 :]\n+\n+        # context repetition mask is computed\n+        context_repetition_mask = self.logits_processor.compute_context_repetition_mask(\n+            input_ids=tokenized_outputs,\n+        )\n+        # context repitition mask shape [batch_size, output_len - (ngram_len - 1)]\n+\n+        combined_mask = context_repetition_mask * eos_token_mask\n+\n+        g_values = self.logits_processor.compute_g_values(\n+            input_ids=tokenized_outputs,\n+        )\n+        # g values shape [batch_size, output_len - (ngram_len - 1), depth]\n+        return self.detector_module(g_values, combined_mask)"
        },
        {
            "sha": "36e1ff2cfe65c42ae819e0533034c674c9d3e4c7",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -191,6 +191,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class BayesianDetectorConfig(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class BayesianDetectorModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class BeamScorer(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n@@ -457,6 +471,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class SynthIDTextWatermarkDetector(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class SynthIDTextWatermarkingConfig(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class SynthIDTextWatermarkLogitsProcessor(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class TemperatureLogitsWarper(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "aeebb5c4c53d24f450846b425aeaf8c322ea1074",
            "filename": "tests/generation/test_logits_process.py",
            "status": "modified",
            "additions": 186,
            "deletions": 0,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/tests%2Fgeneration%2Ftest_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/tests%2Fgeneration%2Ftest_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_logits_process.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -16,6 +16,7 @@\n import unittest\n from typing import List, Union\n \n+import numpy as np\n from parameterized import parameterized\n \n from transformers import is_torch_available\n@@ -48,6 +49,7 @@\n         PrefixConstrainedLogitsProcessor,\n         RepetitionPenaltyLogitsProcessor,\n         SequenceBiasLogitsProcessor,\n+        SynthIDTextWatermarkLogitsProcessor,\n         TemperatureLogitsWarper,\n         TopKLogitsWarper,\n         TopPLogitsWarper,\n@@ -975,3 +977,187 @@ def test_watermarking_processor(self):\n         scores_wo_bias = scores[:, -1].clone()\n         out = watermark(input_ids=input_ids, scores=scores)\n         self.assertTrue((out[:, 1] == scores_wo_bias + watermark.bias).all())\n+\n+    @parameterized.expand([(5, 3, 10000), (10, 5, 1000)])\n+    def test_synthidtext_watermarking_processor_bias_uniformity(self, ngram_len, num_layers, vocab_size):\n+        \"\"\"Test SynthID watermarked distribution bias uniformity over iterations.\"\"\"\n+        torch.manual_seed(0)\n+        np.random.seed(0)\n+        watermarking_config = {\n+            \"ngram_len\": ngram_len,\n+            \"keys\": np.random.randint(low=0, high=2**16, size=(num_layers,)),\n+            \"sampling_table_size\": 2**16,\n+            \"sampling_table_seed\": 0,\n+            \"context_history_size\": 512,\n+            \"device\": torch_device,\n+        }\n+        batch_size = 100000\n+        ngrams = torch.randint(\n+            low=0,\n+            high=vocab_size,\n+            size=(batch_size, ngram_len),\n+            device=torch_device,\n+        )\n+\n+        logits_processor = SynthIDTextWatermarkLogitsProcessor(**watermarking_config)\n+        g_values = logits_processor.compute_g_values(ngrams)\n+        g_values_mean = torch.mean(torch.mean(g_values.float(), dim=0))\n+        self.assertAlmostEqual(g_values_mean, 0.5, delta=0.01)\n+\n+    @parameterized.expand([(10000, 3), (1000, 20)])\n+    def test_synthidtext_watermark_processor_bias_uniformity_across_vocab(self, vocab_size, num_layers):\n+        \"\"\"Test SynthID watermarked distribution bias uniformity over vocabs of the model.\"\"\"\n+        batch_size = 1000\n+        ngram_len = 5\n+        torch.manual_seed(0)\n+        np.random.seed(0)\n+        watermarking_config = {\n+            \"ngram_len\": ngram_len,\n+            \"keys\": np.random.randint(low=0, high=2**16, size=(num_layers,)),\n+            \"sampling_table_size\": 2**16,\n+            \"sampling_table_seed\": 0,\n+            \"context_history_size\": 512,\n+            \"device\": torch_device,\n+        }\n+        n_minus_1_grams = torch.randint(\n+            low=0,\n+            high=vocab_size,\n+            size=(batch_size, watermarking_config[\"ngram_len\"] - 1),\n+            device=torch_device,\n+        )\n+\n+        logits_processor = SynthIDTextWatermarkLogitsProcessor(**watermarking_config)\n+        ngram_keys, _ = logits_processor._compute_keys(\n+            n_minus_1_grams,\n+            torch.stack([torch.arange(vocab_size, device=torch_device) for _ in range(batch_size)]),\n+        )\n+\n+        g_values = logits_processor.sample_g_values(ngram_keys)\n+        # g_values shape should be [batch_size, vocab_size, num_layers]\n+        g_values_mean = torch.mean(torch.mean(g_values.float(), dim=1))\n+        self.assertAlmostEqual(g_values_mean, 0.5, delta=0.001)\n+\n+    @parameterized.expand([(2, \"uniform\"), (10, \"uniform\"), (2, \"random\"), (10, \"random\")])\n+    def test_synthidtext_watermark_processor_distributional_convergence(self, vocab_size, logits_type):\n+        \"\"\"Check if watermarked distribution converges to unwatermarked logits distribution.\"\"\"\n+        batch_size = 1500\n+        num_keys = 1000\n+\n+        updated_softmaxes = 0\n+        np.random.seed(0)\n+        torch.manual_seed(0)\n+        if logits_type == \"uniform\":\n+            fixed_logits = torch.ones((batch_size, vocab_size), device=torch_device)\n+        elif logits_type == \"random\":\n+            fixed_logits = torch.rand(\n+                (\n+                    1,\n+                    vocab_size,\n+                ),\n+                device=torch_device,\n+            )\n+            fixed_logits = fixed_logits.repeat(batch_size, 1)\n+        else:\n+            raise ValueError(f\"Unrecognized logits_type {logits_type}\")\n+        for _ in range(num_keys):\n+            watermarking_config = {\n+                \"ngram_len\": 5,\n+                \"keys\": np.random.randint(0, 10**9, size=(1,), dtype=np.int64),\n+                \"sampling_table_size\": 2**16,\n+                \"sampling_table_seed\": 0,\n+                \"context_history_size\": 1024,\n+                \"device\": torch_device,\n+            }\n+\n+            logits_processor = SynthIDTextWatermarkLogitsProcessor(**watermarking_config)\n+\n+            ngrams = torch.randint(\n+                low=0,\n+                high=vocab_size,\n+                size=(batch_size, watermarking_config[\"ngram_len\"]),\n+                device=torch_device,\n+            )\n+\n+            # Insert ngram-1 into logit_processor state.\n+            for idx in range(watermarking_config[\"ngram_len\"] - 1):\n+                _ = logits_processor(ngrams[:, :idx], fixed_logits)\n+\n+            updated_scores = logits_processor(ngrams, fixed_logits)\n+            updated_softmaxes += torch.nn.functional.softmax(updated_scores, dim=1).cpu().numpy()\n+\n+        updated_softmaxes = np.mean(updated_softmaxes, axis=0) / num_keys\n+        is_close = torch.all(\n+            torch.isclose(\n+                torch.tensor(updated_softmaxes, device=torch_device),\n+                torch.nn.Softmax()(fixed_logits[0]),  # Take any batch entry, all are same.\n+                atol=1e-3,\n+                rtol=0,\n+            )\n+        )\n+        self.assertTrue(is_close)\n+\n+    @parameterized.expand([(2, 10, 1, 0.01), (100, 5, 1, 0.01), (100, 10, 2, 0.02)])\n+    def test_synthidtext_watermark_processor_bias_test(self, vocab_size, ngram_len, num_layers, atol):\n+        \"\"\"Test SynthID watermarking bias matches theoretical value.\"\"\"\n+        batch_size = 20000\n+        generator = torch.Generator(device=torch_device).manual_seed(0)\n+        np.random.seed(0)\n+\n+        keys = [np.random.randint(0, 10**9) for _ in range(num_layers)]\n+        # Use 10**9 rather than vocab_size to ensure variety in (n-1)-grams.\n+        context = torch.randint(\n+            low=0,\n+            high=10**9,\n+            size=(batch_size, ngram_len - 1),\n+            dtype=torch.int64,\n+            generator=generator,\n+            device=torch_device,\n+        )\n+\n+        context_history_size = 1024\n+        logits_processor = SynthIDTextWatermarkLogitsProcessor(\n+            ngram_len=ngram_len,\n+            keys=keys,\n+            sampling_table_size=2**16,\n+            sampling_table_seed=0,\n+            context_history_size=context_history_size,\n+            device=torch_device,\n+        )\n+\n+        scores = torch.ones(\n+            (batch_size, vocab_size),\n+            dtype=torch.float64,\n+            device=torch_device,\n+        )\n+        # Init state of the logits processor.\n+        logits_processor(context, scores)\n+        # insert context into the state.\n+        for idx in range(1, ngram_len - 1):\n+            _ = logits_processor(context[:, :idx], scores)\n+\n+        updated_scores = logits_processor(context, scores)\n+\n+        probs = torch.nn.functional.softmax(updated_scores, dim=1)\n+        generator = torch.Generator(device=torch_device).manual_seed(0)\n+        next_tokens = torch.multinomial(\n+            probs,\n+            num_samples=1,\n+            generator=generator,\n+        )\n+\n+        ngrams = torch.concat((context, next_tokens), dim=1)\n+        g_values = logits_processor.compute_g_values(ngrams)\n+        mean_g_values = g_values.mean(dtype=torch.float64, dim=(0, 1))\n+\n+        expected_mean_g_value = logits_processor.expected_mean_g_value(\n+            vocab_size=vocab_size,\n+        )\n+        is_close = torch.all(\n+            torch.isclose(\n+                mean_g_values,\n+                torch.tensor(expected_mean_g_value, dtype=torch.float64, device=torch_device),\n+                atol=atol,\n+                rtol=0,\n+            )\n+        )\n+        self.assertTrue(is_close)"
        },
        {
            "sha": "4e5d8f30265995fd271bf9ff89f2abc23300bb9c",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 59,
            "deletions": 3,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0f0c61899019d316db17a493023828aa44db06d/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0f0c61899019d316db17a493023828aa44db06d/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=b0f0c61899019d316db17a493023828aa44db06d",
            "patch": "@@ -84,6 +84,7 @@\n         SampleEncoderDecoderOutput,\n         StoppingCriteria,\n         StoppingCriteriaList,\n+        SynthIDTextWatermarkingConfig,\n         WatermarkDetector,\n         WatermarkingConfig,\n     )\n@@ -2517,9 +2518,9 @@ def test_beam_search_low_memory(self):\n         self.assertListEqual(low_output.tolist(), high_output.tolist())\n \n     @slow\n-    def test_watermark_generation(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n-        model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").to(torch_device)\n+    def test_green_red_watermark_generation(self):\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n         tokenizer.pad_token_id = tokenizer.eos_token_id\n         model_inputs = tokenizer(\"I will be\", return_tensors=\"pt\").to(torch_device)\n         input_len = model_inputs[\"input_ids\"].shape[-1]\n@@ -2548,6 +2549,61 @@ def test_watermark_generation(self):\n         self.assertListEqual(detection_out_watermarked.prediction.tolist(), [True])\n         self.assertListEqual(detection_out.prediction.tolist(), [False])\n \n+    \"\"\"Check the mean bias inserted by the watermarking algorithm.\"\"\"\n+\n+    @slow\n+    def test_synthid_text_watermark_generation_mean_expected_bias(self):\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n+        tokenizer.pad_token_id = tokenizer.eos_token_id\n+        model_inputs = tokenizer(\"I will be\", return_tensors=\"pt\").to(torch_device)\n+        input_len = 5\n+        batch_size = 200\n+\n+        # generation should work with both input types: WatermarkingConfig or Dict, so let's check it here :)\n+        watermark_config = SynthIDTextWatermarkingConfig(keys=[10, 20], ngram_len=5, debug_mode=True)\n+        logits_processor = watermark_config.construct_processor(model.config.vocab_size, torch_device)\n+        mean_g_values_repeats = []\n+        for _ in range(40):\n+            input_ids = torch.zeros(\n+                (batch_size, input_len),\n+                dtype=torch.int64,\n+                device=torch_device,\n+            )\n+            model_inputs = {\n+                \"input_ids\": input_ids,\n+                \"attention_mask\": torch.ones_like(input_ids, device=torch_device),\n+            }\n+            output = model.generate(\n+                **model_inputs, watermarking_config=watermark_config, do_sample=True, max_length=500, top_k=1000\n+            )\n+            g_values = logits_processor.compute_g_values(input_ids=output[:, input_len:])\n+            context_repetition_mask = logits_processor.compute_context_repetition_mask(\n+                input_ids=output[:, input_len:],\n+            ).unsqueeze(dim=2)\n+\n+            mean_g_values = torch.masked.mean(\n+                g_values,\n+                mask=context_repetition_mask,\n+                dim=0,\n+                keepdim=True,\n+                dtype=torch.float64,\n+            )\n+            mean_g_values_repeats.append(mean_g_values)\n+\n+        mean_g_values = torch.concat(mean_g_values_repeats, dim=0).mean(dim=0)\n+        expected_mean_g_value = logits_processor.expected_mean_g_value(\n+            vocab_size=model.config.vocab_size,\n+        )\n+        atol = 0.03\n+        is_close = torch.isclose(\n+            mean_g_values,\n+            torch.tensor(expected_mean_g_value, dtype=torch.float64),\n+            atol=atol,\n+            rtol=0,\n+        )\n+        self.assertTrue(torch.all(is_close))\n+\n     @slow\n     def test_beam_search_example_integration(self):\n         # PT-only test: TF doesn't have a BeamSearchScorer"
        }
    ],
    "stats": {
        "total": 2318,
        "additions": 2238,
        "deletions": 80
    }
}