{
    "author": "Cyrilvallez",
    "message": "Rework the Cache documentation (#40373)\n\n* start working the doc\n\n* remove gemma2\n\n* review",
    "sha": "29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa",
    "files": [
        {
            "sha": "f58d4a995e80fcb1fc1b639dc22b804498ca245d",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 61,
            "deletions": 119,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa",
            "patch": "@@ -22,20 +22,19 @@ A KV *cache* stores these calculations so they can be reused without recomputing\n \n Transformers offers several [`Cache`] classes that implement different caching mechanisms. Some of these [`Cache`] classes are optimized to save memory while others are designed to maximize generation speed. Refer to the table below to compare cache types and use it to help you select the best cache for your use case.\n \n-| Cache Type             | Memory Efficient  | Supports torch.compile() | Initialization Recommended | Latency | Long Context Generation |\n-|------------------------|------------------|--------------------------|----------------------------|---------|-------------------------|\n-| Dynamic Cache          | No               | No                       | No                         | Mid     | No                      |\n-| Static Cache           | No               | Yes                      | Yes                        | High    | No                      |\n-| Offloaded Cache         | Yes              | No                       | No                         | Low     | Yes                     |\n-| Offloaded Static Cache  | No               | Yes                      | Yes                        | High    | Yes                     |\n-| Quantized Cache        | Yes              | No                       | No                         | Low     | Yes                     |\n-| Sliding Window Cache   | No               | Yes                      | Yes                        | High    | No                      |\n+| Cache Type             | Supports sliding layers  | Supports offloading | Supports torch.compile() | Expected memory usage |\n+|------------------------|--------------------------|---------------------|--------------------------|-----------------------|\n+| Dynamic Cache          |           Yes            |          Yes        |           No             |         Medium        |\n+| Static Cache           |           Yes            |          Yes        |           Yes            |         High          |\n+| Quantized Cache        |           No             |          No         |           No             |         Low           |\n \n This guide introduces you to the different [`Cache`] classes and shows you how to use them for generation.\n \n ## Default cache\n \n-The [`DynamicCache`] is the default cache class for most models. It allows the cache size to grow dynamically in order to store an increasing number of keys and values as generation progresses.\n+The [`DynamicCache`] is the default cache class for all models. It allows the cache size to grow dynamically in order to store an increasing number of keys and values as generation progresses.\n+\n+Note that for models using sliding window attention (Mistral, Gemma2,...) or chunked attention (Llama4), the cache will stop growing when the layers using these types of attention have reached their maximum size (the sliding window or chunk size).\n \n Disable the cache by configuring `use_cache=False` in [`~GenerationMixin.generate`].\n \n@@ -50,9 +49,9 @@ inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.de\n model.generate(**inputs, do_sample=False, max_new_tokens=20, use_cache=False)\n ```\n \n-Cache classes can also be initialized first before calling and passing it to the models [past_key_values](https://hf.co/docs/transformers/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput.past_key_values) parameter. This cache initialization strategy is only recommended for some cache types.\n+Cache classes can also be initialized first before calling and passing it to the models [past_key_values](https://hf.co/docs/transformers/internal/generation_utils#transformers.generation.GenerateDecoderOnlyOutput.past_key_values) parameter. This can be useful for more fine-grained control, or more advanced usage such as context caching.\n \n-In most other cases, it's easier to define the cache strategy in the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) parameter.\n+In most cases, it's easier to define the cache strategy in the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) parameter.\n \n ```py\n import torch\n@@ -62,24 +61,50 @@ tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n-past_key_values = DynamicCache()\n+past_key_values = DynamicCache(config=model.config)\n out = model.generate(**inputs, do_sample=False, max_new_tokens=20, past_key_values=past_key_values)\n ```\n \n-## Memory efficient caches\n+## Fixed-size cache\n \n-The KV cache can occupy a significant portion of memory and become a [bottleneck](https://hf.co/blog/llama31#inference-memory-requirements) for long-context generation. Memory efficient caches focus on trading off speed for reduced memory usage. This is especially important for large language models (LLMs) and if your hardware is memory constrained.\n+The default [`DynamicCache`] prevents you from taking advantage of most just-in-time (JIT) optimizations because the cache size isn't fixed. JIT optimizations enable you to maximize latency at the expense of memory usage. All of the following cache types are compatible with JIT optimizations like [torch.compile](./llm_optims#static-kv-cache-and-torchcompile) to accelerate generation. \n+\n+A fixed-size cache ([`StaticCache`]) pre-allocates a specific maximum cache size for the kv pairs. You can generate up to the maximum cache size without needing to modify it. However, having a fixed (usually large) size for the key/value states means that while generating, a lot of tokens will actually be masked as they should not take part in the attention. So this trick allows to easily `compile` the decoding stage, but it incurs a waste of tokens in the attention computation. As all things, it's then a trade-off which should be very good if you generate with several sequence of more or less the same lengths, but may be sub-optimal if you have for example 1 very large sequence, and then only short sequences (as the fix cache size would be large, a lot would be wasted for the short sequences). Make sure you understand the impact if you use it!\n+\n+As for [`DynamicCache`], note that for models using sliding window attention (Mistral, Gemma2,...) or chunked attention (Llama4), the cache will never be larger than the sliding window/chunk size on layers using these types of attention, even if the maximum length specified is larger.\n+\n+You can enable [`StaticCache`] by configuring `cache_implementation=\"static\"` in [`~GenerationMixin.generate`]. This will also turn on automatic `compilation` of the decoding stage for greedy and sample decoding strategies.\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n+inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n+\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"static\")\n+tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n+\"Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of\"\n+```\n+\n+## Cache offloading\n \n-### Offloaded cache\n+The KV cache can occupy a significant portion of memory and become a [bottleneck](https://hf.co/blog/llama31#inference-memory-requirements) for long-context generation. Memory efficient caches focus on trading off speed for reduced memory usage. This is especially important for large language models (LLMs) and if your hardware is memory constrained.\n \n-The [`OffloadedCache`] saves GPU memory by moving the KV cache for most model layers to the CPU. Only the current layer cache is maintained on the GPU during a models `forward` iteration over the layers. [`OffloadedCache`] asynchronously prefetches the next layer cache and sends the previous layer cache back to the CPU.\n+Offloading the cache saves GPU memory by moving the KV cache for model layers except one to the CPU. Only the current layer cache is maintained on the GPU during a models `forward` iteration over the layers. It will asynchronously prefetch the next layer's cache, and send back the current layer's cache back to the CPU after attention computation.\n \n-This cache strategy always generates the same result as [`DynamicCache`] and works as a drop-in replacement or fallback. You may want to use [`OffloadedCache`] if you have a GPU and you're getting out-of-memory (OOM) errors.\n+You may want to consider offloading if you have a small GPU and you're getting out-of-memory (OOM) errors.\n \n > [!WARNING]\n-> You may notice a small degradation in generation throughput compared to [`DynamicCache`] depending on your model and generation choices (context size, number of generated tokens, number of beams, etc.).\n+> You may notice a small degradation in generation throughput compared to a full on-device cache, depending on your model and generation choices (context size, number of generated tokens, number of beams, etc.). This is because moving the key/value states back and forth requires some work.\n \n-Enable [`OffloadedCache`] by configuring `cache_implementation=\"offloaded\"` in either [`GenerationConfig`] or [`~GenerationMixin.generate`].\n+Offloading is available for both [`DynamicCache`] and [`StaticCache`]. You can enable it by configuring `cache_implementation=\"offloaded\"` for the dynamic version, or `cache_implementation=\"offloaded_static\"` for the static version, in either [`GenerationConfig`] or [`~GenerationMixin.generate`].\n+Additionally, you can also instantiate your own [`DynamicCache`] or [`StaticCache`] with the `offloading=True` option, and pass this cache in `generate` or your model's `forward` (for example, `past_key_values=DynamicCache(config=model.config, offloading=True)` for a dynamic cache).\n+\n+Note that the 2 [`Cache`] classes mentionned above have an additional option when instantiating them directly, `offload_only_non_sliding`.\n+This additional argument decides if the layers using sliding window/chunk attention (if any), will be offloaded as well. Since\n+these layers are usually short anyway, it may be better to avoid offloading them, as offloading may incur a speed penalty. By default, this option is `False` for [`DynamicCache`], and `True` for [`StaticCache`].\n \n ```py\n import torch\n@@ -95,7 +120,7 @@ print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n Fun fact: The shortest war in history was between Britain and Zanzibar on August 27, 1896.\n ```\n \n-The example below shows how you can fallback on [`OffloadedCache`] if you run out of memory.\n+The example below shows how you can fallback to an offloaded cache if you run out of memory:\n \n ```py\n import torch\n@@ -126,44 +151,40 @@ out = resilient_generate(model, **inputs, **beams)\n responses = tokenizer.batch_decode(out[:,-28:], skip_special_tokens=True)\n ```\n \n-### Quantized cache\n+## Quantized cache\n \n-The [`QuantizedCache`] reduces memory requirements by quantizing the KV values to a lower precision. [`QuantizedCache`] currently supports two quantization backends.\n+The [`QuantizedCache`] reduces memory requirements by quantizing the KV values to a lower precision. [`QuantizedCache`] currently supports two quantization backends:\n \n-- [`HQQQuantizedCache`] supports int2, int4, and int8 datatypes.\n-- [`QuantoQuantizedCache`] supports int2 and int4 datatypes. This is the default quantization backend.\n+- `hqq` supports int2, int4, and int8 datatypes.\n+- `quanto` supports int2 and int4 datatypes. This is the default quantization backend.\n \n > [!WARNING]\n > Quantizing the cache can harm latency if the context length is short and there is enough GPU memory available for generation without enabling cache quantization. Try to find a balance between memory efficiency and latency.\n \n Enable [`QuantizedCache`] by configuring `cache_implementation=\"quantized\"` in [`GenerationConfig`], and the quantization backend, as well as any additional quantization related parameters should also be passed either as a dict. You should use the default values for these additional parameters unless you're running out-of-memory. In that case, consider decreasing the residual length.\n \n <hfoptions id=\"quantized-cache\">\n-<hfoption id=\"HQQQuantizedCache\">\n \n-For [`HQQQuantizedCache`], we recommend setting the `axis-key` and `axis-value` parameters to `1`.\n+For the `hqq` backend, we recommend setting the `axis-key` and `axis-value` parameters to `1`.\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, HQQQuantizedCache\n+from transformers import AutoTokenizer, AutoModelForCausalLM, QuantizedCache\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n-out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"backend\": \"HQQ\"})\n+out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"backend\": \"hqq\"})\n print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n I like rock music because it's loud and energetic. It's a great way to express myself and rel\n ```\n \n-</hfoption>\n-<hfoption id=\"Quanto\">\n-\n-For [`QuantoQuantizedCache`], we recommend setting the `axis-key` and `axis-value` parameters to `0`.\n+For `quanto` backend, we recommend setting the `axis-key` and `axis-value` parameters to `0`.\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, QuantoQuantizedCache\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n@@ -174,92 +195,19 @@ print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n I like rock music because it's loud and energetic. It's a great way to express myself and rel\n ```\n \n-</hfoption>\n-</hfoptions>\n-\n-## Speed optimized caches\n-\n-The default [`DynamicCache`] prevents you from taking advantage of just-in-time (JIT) optimizations because the cache size isn't fixed. JIT optimizations enable you to maximize latency at the expense of memory usage. All of the following cache types are compatible with JIT optimizations like [torch.compile](./llm_optims#static-kv-cache-and-torchcompile) to accelerate generation.\n-\n-### Static cache\n-\n-A [`StaticCache`] pre-allocates a specific maximum cache size for the kv pairs. You can generate up to the maximum cache size without needing to modify it.\n-\n-Enable [`StaticCache`] by configuring `cache_implementation=\"static\"` in [`~GenerationMixin.generate`].\n-\n-```py\n-import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map=\"auto\")\n-inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n-\n-out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"static\")\n-tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n-\"Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of\"\n-```\n-\n-### Offloaded static cache\n-\n-The [`OffloadedStaticCache`] is very similar to the [OffloadedCache](#offloaded-cache) except the cache size is set to a maximum cache size. Otherwise, [`OffloadedStaticCache`] only keeps the current layer cache on the GPU and the rest are moved to the CPU.\n-\n-Enable [`OffloadedStaticCache`] by configuring `cache_implementation=\"offloaded_static\"` in [`~GenerationMixin.generate`].\n-\n-```py\n-import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", dtype=torch.float16, device_map={\"\": 0})\n-inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n-\n-out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"offloaded_static\")\n-tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n-\"Hello, my name is [Your Name], and I am a [Your Profession] with [Number of Years] of\"\n-```\n-Cache offloading requires a CUDA GPU or Intel XPU.\n-\n-### Sliding window cache\n-\n-[`SlidingWindowCache`] implements a sliding window over the previous kv pairs, and only keeps the last `sliding_window` tokens. This cache type is designed to only work with models that support *sliding window attention*, such as [Mistral](./model_doc/mistral). Older kv states are discarded and replaced by new kv states.\n-\n-Enable [`SlidingWindowCache`] by configuring `cache_implementation=\"sliding_window\"` in [`~GenerationMixin.generate`].\n-\n-```py\n-import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n-model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", dtype=torch.float16, device_map=\"auto\")\n-inputs = tokenizer(\"Yesterday I was on a rock concert and.\", return_tensors=\"pt\").to(model.device)\n-\n-out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation=\"sliding_window\")\n-tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n-```\n-\n-## Model caches\n-\n-Some model types, like encoder-decoder models or [Gemma2](./model_doc/gemma2) and [Mamba](./model_doc/mamba), have dedicated cache classes.\n-\n-### Encoder-decoder cache\n+## Encoder-decoder cache\n \n [`EncoderDecoderCache`] is designed for encoder-decoder models. It manages both the self-attention and cross-attention caches to ensure storage and retrieval of previous kv pairs. It is possible to individually set a different cache type for the encoder and decoder.\n \n-This cache type doesn't require any setup. It can be used when calling [`~GenerationMixin.generate`] or a models `forward` method.\n+This cache type doesn't require any setup. It is a simple wrapper around 2 [`Cache`]s as described above, that will be used independently directly by the model.\n \n-> [!TIP]\n-> The [`EncoderDecoderCache`] currently only supports [Whisper](./model_doc/whisper).\n-\n-### Model-specific caches\n+## Model-specific caches\n \n Some models have a unique way of storing past kv pairs or states that is not compatible with any other cache classes.\n \n-[Gemma2](./model_doc/gemma2) requires [`HybridCache`], which uses a combination of [`SlidingWindowCache`] for sliding window attention and [`StaticCache`] for global attention under the hood.\n-\n-[Mamba](./model_doc/mamba) requires [`MambaCache`] because the model doesn't have an attention mechanism or kv states.\n+Mamba models, such as [Mamba](./model_doc/mamba), require a specific cache because the model doesn't have an attention mechanism or kv states. Thus, they are not compatible with the above [`Cache`] classes.\n \n-## Iterative generation\n+# Iterative generation\n \n A cache can also work in iterative generation settings where there is back-and-forth interaction with a model (chatbots). Like regular generation, iterative generation with a cache allows a model to efficiently handle ongoing conversations without recomputing the entire context at each step.\n \n@@ -271,13 +219,7 @@ For example, some models use special `<think> ... </think>` tokens during reason\n \n ```py\n import torch\n-from transformers import AutoTokenizer,AutoModelForCausalLM\n-from transformers.cache_utils import (\n-    DynamicCache,\n-    StaticCache,\n-    SlidingWindowCache,\n-    QuantoQuantizedCache,\n-)\n+from transformers import AutoTokenizer,AutoModelForCausalLM, DynamicCache, StaticCache\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map='auto')\n@@ -297,7 +239,7 @@ for prompt in user_prompts:\n     messages.append({\"role\": \"assistant\", \"content\": completion})\n ```\n \n-## Prefill a cache\n+## Prefill a cache (prefix caching)\n \n In some situations, you may want to fill a [`Cache`] with kv pairs for a certain prefix prompt and reuse it to generate different sequences.\n "
        },
        {
            "sha": "680de41d0380c582622621714bc537ea03217c8f",
            "filename": "docs/source/en/model_doc/gemma2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md?ref=29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa",
            "patch": "@@ -124,22 +124,6 @@ visualizer(\"You are an assistant. Make sure you print me\")\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/gemma-2-attn-mask.png\"/>\n </div>\n \n-## Notes\n-\n-- Use a [`HybridCache`] instance to enable caching in Gemma 2. Gemma 2 doesn't support kv-caching strategies like [`DynamicCache`] or tuples of tensors because it uses sliding window attention every second layer.\n-\n-    ```python\n-    from transformers import AutoTokenizer, AutoModelForCausalLM, HybridCache\n-\n-    model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\n-    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n-\n-    inputs = tokenizer(text=\"My name is Gemma\", return_tensors=\"pt\")\n-    max_generated_length = inputs.input_ids.shape[1] + 10\n-    past_key_values = HybridCache(config=model.config, max_cache_len=max_generated_length)\n-    outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-    ```\n-\n ## Gemma2Config\n \n [[autodoc]] Gemma2Config"
        },
        {
            "sha": "a779986801d2779784a9b1d6d8f346d2db197dcc",
            "filename": "docs/source/ko/model_doc/gemma2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma2.md?ref=29ddcacea3ad9d3cdf6c5d8e51d1d39cbc5e7dfa",
            "patch": "@@ -30,12 +30,6 @@ Gemma2 모델은 Google의 Gemma2 팀이 작성한 [Gemma2: Open Models Based on\n \n - 원본 체크포인트는 변환 스크립트 `src/transformers/models/Gemma2/convert_Gemma2_weights_to_hf.py`를 사용하여 변환할 수 있습니다.\n \n-<Tip warning={true}>\n-\n-- Gemma2는 매 두 번째 레이어마다 슬라이딩 윈도우 어텐션을 사용하므로 [`~DynamicCache`] 또는 텐서의 튜플과 같은 일반적인 kv 캐싱에는 적합하지 않습니다. Gemma2의 forward 호출에서 캐싱을 활성화하려면 [`~HybridCache`] 인스턴스를 초기화하고 이를 `past_key_values`로 forward 호출에 전달해야 합니다. 또한 `past_key_values`에 이미 이전의 키와 값이 포함되어 있다면 `cache_position`도 준비해야 합니다.\n-\n-</Tip>\n-\n 이 모델은 [Arthur Zucker](https://huggingface.co/ArthurZ), [Pedro Cuenca](https://huggingface.co/pcuenq), [Tom Arsen]()이 기여했습니다.\n \n ## Gemma2Config [[transformers.Gemma2Config]]"
        }
    ],
    "stats": {
        "total": 202,
        "additions": 61,
        "deletions": 141
    }
}