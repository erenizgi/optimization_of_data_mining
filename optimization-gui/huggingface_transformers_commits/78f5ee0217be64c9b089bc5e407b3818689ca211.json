{
    "author": "NielsRogge",
    "message": "Add LlavaImageProcessor (#33191)\n\n* First draft\r\n\r\n* Add equivalence test\r\n\r\n* Update docstrings\r\n\r\n* Add tests\r\n\r\n* Use numpy\r\n\r\n* Fix tests\r\n\r\n* Improve variable names\r\n\r\n* Improve docstring\r\n\r\n* Add link\r\n\r\n* Remove script\r\n\r\n* Add copied from\r\n\r\n* Address comment\r\n\r\n* Add note in docs\r\n\r\n* Add docstring, data format\r\n\r\n* Improve test\r\n\r\n* Add test\r\n\r\n* update\r\n\r\n* Update src/transformers/models/llava/image_processing_llava.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/models/llava/image_processing_llava.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* loop once only\r\n\r\n---------\r\n\r\nCo-authored-by: raushan <raushan@huggingface.co>\r\nCo-authored-by: Raushan Turganbay <raushan.turganbay@alumni.nu.edu.kz>\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "78f5ee0217be64c9b089bc5e407b3818689ca211",
    "files": [
        {
            "sha": "a3afc216b7762b04fda332410c6a8eccb7df476f",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f5ee0217be64c9b089bc5e407b3818689ca211/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f5ee0217be64c9b089bc5e407b3818689ca211/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=78f5ee0217be64c9b089bc5e407b3818689ca211",
            "patch": "@@ -162,6 +162,16 @@ For multiple turns conversation:\n \"USER: <image>\\n<prompt1> ASSISTANT: <answer1></s>USER: <prompt2> ASSISTANT: <answer2></s>USER: <prompt3> ASSISTANT:\"\n ```\n \n+## Note regarding reproducing original implementation\n+\n+In order to match the logits of the [original implementation](https://github.com/haotian-liu/LLaVA/tree/main), one needs to additionally specify `do_pad=True` when instantiating `LLavaImageProcessor`:\n+\n+```python\n+from transformers import LLavaImageProcessor\n+\n+image_processor = LLavaImageProcessor.from_pretrained(\"https://huggingface.co/llava-hf/llava-1.5-7b-hf\", do_pad=True)\n+```\n+\n ### Using Flash Attention 2\n \n Flash Attention 2 is an even faster, optimized version of the previous optimization, please refer to the [Flash Attention 2 section of performance docs](https://huggingface.co/docs/transformers/perf_infer_gpu_one).\n@@ -180,6 +190,11 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n \n [[autodoc]] LlavaConfig\n \n+## LlavaImageProcessor\n+\n+[[autodoc]] LlavaImageProcessor\n+    - preprocess\n+\n ## LlavaProcessor\n \n [[autodoc]] LlavaProcessor"
        },
        {
            "sha": "8b089276666ac10b9925af5350530dcfb01156aa",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=78f5ee0217be64c9b089bc5e407b3818689ca211",
            "patch": "@@ -1243,6 +1243,7 @@\n     _import_structure[\"models.layoutlmv2\"].extend([\"LayoutLMv2FeatureExtractor\", \"LayoutLMv2ImageProcessor\"])\n     _import_structure[\"models.layoutlmv3\"].extend([\"LayoutLMv3FeatureExtractor\", \"LayoutLMv3ImageProcessor\"])\n     _import_structure[\"models.levit\"].extend([\"LevitFeatureExtractor\", \"LevitImageProcessor\"])\n+    _import_structure[\"models.llava\"].append(\"LlavaImageProcessor\")\n     _import_structure[\"models.llava_next\"].append(\"LlavaNextImageProcessor\")\n     _import_structure[\"models.llava_next_video\"].append(\"LlavaNextVideoImageProcessor\")\n     _import_structure[\"models.llava_onevision\"].extend(\n@@ -6334,6 +6335,7 @@\n             LayoutLMv3ImageProcessor,\n         )\n         from .models.levit import LevitFeatureExtractor, LevitImageProcessor\n+        from .models.llava import LlavaImageProcessor\n         from .models.llava_next import LlavaNextImageProcessor\n         from .models.llava_next_video import LlavaNextVideoImageProcessor\n         from .models.llava_onevision import LlavaOnevisionImageProcessor, LlavaOnevisionVideoProcessor"
        },
        {
            "sha": "39df65d80457fc9044a3d42dea04c3950cd035d0",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=78f5ee0217be64c9b089bc5e407b3818689ca211",
            "patch": "@@ -101,7 +101,7 @@\n             (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\",)),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\",)),\n             (\"levit\", (\"LevitImageProcessor\",)),\n-            (\"llava\", (\"CLIPImageProcessor\",)),\n+            (\"llava\", (\"LlavaImageProcessor\",)),\n             (\"llava_next\", (\"LlavaNextImageProcessor\",)),\n             (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\",)),\n             (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\",)),"
        },
        {
            "sha": "a94f3e70bcad03558860239732915aa044e15508",
            "filename": "src/transformers/models/llava/image_processing_llava.py",
            "status": "added",
            "additions": 436,
            "deletions": 0,
            "changes": 436,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py?ref=78f5ee0217be64c9b089bc5e407b3818689ca211",
            "patch": "@@ -0,0 +1,436 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for LLaVa.\"\"\"\n+\n+from typing import Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import (\n+    convert_to_rgb,\n+    get_resize_output_image_size,\n+    resize,\n+    to_channel_dimension_format,\n+)\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_kwargs,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, is_vision_available, logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+class LlavaImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a LLaVa image processor.\n+\n+    Args:\n+        do_pad (`bool`, *optional*, defaults to `False`):\n+            Whether to pad the image to a square based on the longest edge.\n+            The padding value is determined by the `image_mean` parameter.\n+            Can be overridden by `do_pad` in the `preprocess` method.\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n+            `do_resize` in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n+            Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n+            the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n+            method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n+        do_center_crop (`bool`, *optional*, defaults to `True`):\n+            Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n+            `preprocess` method.\n+        crop_size (`Dict[str, int]` *optional*, defaults to 224):\n+            Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n+            method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_pad: bool = False,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_center_crop: bool = True,\n+        crop_size: Dict[str, int] = None,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"shortest_edge\": 224}\n+        size = get_size_dict(size, default_to_square=False)\n+        crop_size = crop_size if crop_size is not None else {\"height\": 224, \"width\": 224}\n+        crop_size = get_size_dict(crop_size, default_to_square=True, param_name=\"crop_size\")\n+\n+        self.do_pad = do_pad\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.do_convert_rgb = do_convert_rgb\n+        self._valid_processor_keys = [\n+            \"images\",\n+            \"do_pad\",\n+            \"do_resize\",\n+            \"size\",\n+            \"resample\",\n+            \"do_center_crop\",\n+            \"crop_size\",\n+            \"do_rescale\",\n+            \"rescale_factor\",\n+            \"do_normalize\",\n+            \"image_mean\",\n+            \"image_std\",\n+            \"do_convert_rgb\",\n+            \"return_tensors\",\n+            \"data_format\",\n+            \"input_data_format\",\n+        ]\n+\n+    def pad_to_square(\n+        self,\n+        image: np.ndarray,\n+        background_color: Union[int, Tuple[int, int, int]] = 0,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                The image to pad.\n+            background_color (`int` or `Tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+\n+        Returns:\n+            `np.ndarray`: The padded image.\n+        \"\"\"\n+        height, width = get_image_size(image, input_data_format)\n+        num_channels = image.shape[0] if input_data_format == ChannelDimension.FIRST else image.shape[-1]\n+\n+        if height == width:\n+            image = (\n+                to_channel_dimension_format(image, data_format, input_data_format)\n+                if data_format is not None\n+                else image\n+            )\n+            return image\n+\n+        max_dim = max(height, width)\n+\n+        # Ensure background_color is the correct shape\n+        if isinstance(background_color, int):\n+            background_color = [background_color]\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        if input_data_format == ChannelDimension.FIRST:\n+            result = np.zeros((num_channels, max_dim, max_dim), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[i, :, :] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[:, start : start + height, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, :, start : start + width] = image\n+        else:\n+            result = np.zeros((max_dim, max_dim, num_channels), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[:, :, i] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[start : start + height, :, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, start : start + width, :] = image\n+\n+        image = (\n+            to_channel_dimension_format(result, data_format, input_data_format) if data_format is not None else result\n+        )\n+        return image\n+\n+    # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Dict[str, int],\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image. The shortest edge of the image is resized to size[\"shortest_edge\"], with the longest edge\n+        resized to keep the input aspect ratio.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Size of the output image.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                Resampling filter to use when resiizing the image.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format of the image. If not provided, it will be the same as the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+        \"\"\"\n+        default_to_square = True\n+        if \"shortest_edge\" in size:\n+            size = size[\"shortest_edge\"]\n+            default_to_square = False\n+        elif \"height\" in size and \"width\" in size:\n+            size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\"Size must contain either 'shortest_edge' or 'height' and 'width'.\")\n+\n+        output_size = get_resize_output_image_size(\n+            image,\n+            size=size,\n+            default_to_square=default_to_square,\n+            input_data_format=input_data_format,\n+        )\n+        return resize(\n+            image,\n+            size=output_size,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_pad: bool = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        do_center_crop: Optional[bool] = None,\n+        crop_size: Optional[int] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n+                Whether to pad the image to a square based on the longest edge.\n+                The padding value is determined by the `image_mean` parameter.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n+                Whether to center crop the image.\n+            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n+                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        do_pad = do_pad if do_pad is not None else self.do_pad\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, param_name=\"size\", default_to_square=False)\n+        resample = resample if resample is not None else self.resample\n+        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n+        crop_size = crop_size if crop_size is not None else self.crop_size\n+        crop_size = get_size_dict(crop_size, param_name=\"crop_size\", default_to_square=True)\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n+\n+        images = make_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+        # we don't pass `do_pad` here since LLaVa uses a custom padding to a square\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_center_crop=do_center_crop,\n+            crop_size=crop_size,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        processed_images = []\n+        for image in images:\n+            if do_pad:\n+                image = self.pad_to_square(\n+                    image=image,\n+                    background_color=tuple(int(x * 255) for x in self.image_mean),\n+                    input_data_format=input_data_format,\n+                )\n+\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                images = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            processed_images.append(image)\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"LlavaImageProcessor\"]"
        },
        {
            "sha": "b8138e25818598c5060158443b055a06cd84b439",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=78f5ee0217be64c9b089bc5e407b3818689ca211",
            "patch": "@@ -39,13 +39,13 @@ class LlavaProcessorKwargs(ProcessingKwargs, total=False):\n \n class LlavaProcessor(ProcessorMixin):\n     r\"\"\"\n-    Constructs a Llava processor which wraps a Llava image processor and a Llava tokenizer into a single processor.\n+    Constructs a LLaVa processor which wraps a LLaVa image processor and a LLaMa tokenizer into a single processor.\n \n-    [`LlavaProcessor`] offers all the functionalities of [`CLIPImageProcessor`] and [`LlamaTokenizerFast`]. See the\n+    [`LlavaProcessor`] offers all the functionalities of [`LlavaImageProcessor`] and [`LlamaTokenizerFast`]. See the\n     [`~LlavaProcessor.__call__`] and [`~LlavaProcessor.decode`] for more information.\n \n     Args:\n-        image_processor ([`CLIPImageProcessor`], *optional*):\n+        image_processor ([`LlavaImageProcessor`], *optional*):\n             The image processor is a required input.\n         tokenizer ([`LlamaTokenizerFast`], *optional*):\n             The tokenizer is a required input."
        },
        {
            "sha": "cf343341cfd7568343c606a59dc6908fe3ad83cf",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f5ee0217be64c9b089bc5e407b3818689ca211/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=78f5ee0217be64c9b089bc5e407b3818689ca211",
            "patch": "@@ -380,6 +380,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class LlavaImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class LlavaNextImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "75570c50d90a2205365699d0343e151c4a35b096",
            "filename": "tests/models/llava/test_image_processing_llava.py",
            "status": "added",
            "additions": 203,
            "deletions": 0,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/78f5ee0217be64c9b089bc5e407b3818689ca211/tests%2Fmodels%2Fllava%2Ftest_image_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78f5ee0217be64c9b089bc5e407b3818689ca211/tests%2Fmodels%2Fllava%2Ftest_image_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_image_processing_llava.py?ref=78f5ee0217be64c9b089bc5e407b3818689ca211",
            "patch": "@@ -0,0 +1,203 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+from typing import Tuple, Union\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import LlavaImageProcessor\n+\n+\n+class LlavaImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_pad=True,\n+        do_resize=True,\n+        size=None,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_normalize=True,\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n+        do_convert_rgb=True,\n+    ):\n+        size = size if size is not None else {\"shortest_edge\": 20}\n+        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_pad = do_pad\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_pad\": self.do_pad,\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTester.expected_output_image_shape\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.crop_size[\"height\"], self.crop_size[\"width\"]\n+\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTester.prepare_image_inputs\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+# Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTest with CLIP->Llava\n+class LlavaImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = LlavaImageProcessor if is_vision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = LlavaImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    # Ignore copy\n+    def test_image_processor_properties(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(image_processing, \"size\"))\n+        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n+        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+\n+    # Ignore copy\n+    def test_padding(self):\n+        \"\"\"\n+        LLaVA needs to pad images to square size before processing as per orig implementation.\n+        Checks that image processor pads images correctly given different background colors.\n+        \"\"\"\n+\n+        # taken from original implementation: https://github.com/haotian-liu/LLaVA/blob/c121f0432da27facab705978f83c4ada465e46fd/llava/mm_utils.py#L152\n+        def pad_to_square_original(\n+            image: Image.Image, background_color: Union[int, Tuple[int, int, int]] = 0\n+        ) -> Image.Image:\n+            width, height = image.size\n+            if width == height:\n+                return image\n+            elif width > height:\n+                result = Image.new(image.mode, (width, width), background_color)\n+                result.paste(image, (0, (width - height) // 2))\n+                return result\n+            else:\n+                result = Image.new(image.mode, (height, height), background_color)\n+                result.paste(image, ((height - width) // 2, 0))\n+                return result\n+\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+\n+        # test with images in channel-last and channel-first format\n+        for image in image_inputs:\n+            padded_image = image_processor.pad_to_square(image)\n+            padded_image_original = pad_to_square_original(Image.fromarray(image))\n+            padded_image_original = np.array(padded_image_original)\n+\n+            np.testing.assert_allclose(padded_image, padded_image_original)\n+\n+            padded_image = image_processor.pad_to_square(image.transpose(2, 0, 1), input_data_format=\"channels_first\")\n+            padded_image = padded_image.transpose(1, 2, 0)\n+\n+            np.testing.assert_allclose(padded_image, padded_image_original)\n+\n+        # test background color\n+        background_color = (122, 116, 104)\n+        for image in image_inputs:\n+            padded_image = image_processor.pad_to_square(image, background_color=background_color)\n+            padded_image_original = pad_to_square_original(Image.fromarray(image), background_color=background_color)\n+            padded_image_original = np.array(padded_image_original)\n+\n+            np.testing.assert_allclose(padded_image, padded_image_original)\n+\n+        background_color = 122\n+        for image in image_inputs:\n+            padded_image = image_processor.pad_to_square(image, background_color=background_color)\n+            padded_image_original = pad_to_square_original(Image.fromarray(image), background_color=background_color)\n+            padded_image_original = np.array(padded_image_original)\n+\n+            np.testing.assert_allclose(padded_image, padded_image_original)\n+\n+        # background color length should match channel length\n+        with self.assertRaises(ValueError):\n+            padded_image = image_processor.pad_to_square(image_inputs[0], background_color=(122, 104))\n+\n+        with self.assertRaises(ValueError):\n+            padded_image = image_processor.pad_to_square(image_inputs[0], background_color=(122, 104, 0, 0))\n+\n+    @unittest.skip(reason=\"LLaVa does not support 4 channel images yet\")\n+    # Ignore copy\n+    def test_call_numpy_4_channels(self):\n+        pass"
        }
    ],
    "stats": {
        "total": 671,
        "additions": 667,
        "deletions": 4
    }
}