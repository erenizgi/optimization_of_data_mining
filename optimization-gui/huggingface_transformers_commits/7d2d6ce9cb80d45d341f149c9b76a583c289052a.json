{
    "author": "zucchini-nlp",
    "message": "VLM: fixes after refactor (#32907)\n\n* leave only half of the changes\r\n\r\n* fix tests\r\n\r\n* [run-slow] llava, llava_next, llava_next_video, vipllava, video_llava\r\n\r\n* fix tests, first try\r\n\r\n* [run-slow] llava, llava_next, llava_next_video, vipllava, video_llava\r\n\r\n* fix, second try\r\n\r\n* [run-slow] llava, llava_next, llava_next_video, vipllava, video_llava\r\n\r\n* fix\r\n\r\n* [run-slow] llava, llava_next, llava_next_video, vipllava, video_llava",
    "sha": "7d2d6ce9cb80d45d341f149c9b76a583c289052a",
    "files": [
        {
            "sha": "94388af99ec17fbe39336ea75c0989f400642a93",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -476,6 +476,7 @@ def forward(\n                     inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\n                         image_features, inputs_embeds, input_ids, attention_mask, labels\n                     )\n+                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n                 else:\n                     # Retrieve the first layer to inspect the logits and mask out the hidden states\n                     # that are set to 0\n@@ -506,6 +507,9 @@ def forward(\n \n                     attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n                     position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[\n+                        -target_length:\n+                    ]\n \n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n@@ -585,9 +589,7 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        if legacy_processing:\n-            model_inputs[\"pixel_values\"] = pixel_values\n-        elif cache_position[0] == 0:\n+        if legacy_processing or cache_position[0] == 0:\n             # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n             # Otherwise we need pixel values to be passed to model\n             model_inputs[\"pixel_values\"] = pixel_values"
        },
        {
            "sha": "678724ae95be41c23fde2cfcbe7935e55b196421",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -136,6 +136,7 @@ def __call__(\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n         # try to expand inputs in processing if we have the necessary parts\n+        prompt_strings = text\n         if image_inputs.get(\"pixel_values\") is not None:\n             if self.patch_size is not None and self.vision_feature_select_strategy is not None:\n                 # Replace the image token with the expanded image token sequence\n@@ -150,7 +151,6 @@ def __call__(\n                     sample = sample.replace(self.image_token, self.image_token * num_image_tokens)\n                     prompt_strings.append(sample)\n             else:\n-                prompt_strings = text\n                 logger.warning_once(\n                     \"Expanding inputs for image tokens in LLaVa should be done in processing. \"\n                     \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \""
        },
        {
            "sha": "18a17c6dcd06b9dd0912a28edddfe0bdd56b12f1",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -848,6 +848,7 @@ def forward(\n                         position_ids,\n                         labels=labels,\n                     )\n+                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n                 else:\n                     # Retrieve the first layer to inspect the logits and mask out the hidden states\n                     # that are set to 0\n@@ -877,6 +878,9 @@ def forward(\n                     extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n                     attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n                     position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[\n+                        -target_length:\n+                    ]\n \n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n@@ -956,12 +960,9 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        if legacy_processing:\n-            model_inputs[\"pixel_values\"] = pixel_values\n-            model_inputs[\"image_sizes\"] = image_sizes\n-        elif cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+        # Otherwise we need pixel values to be passed to model\n+        if legacy_processing or cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"image_sizes\"] = image_sizes\n "
        },
        {
            "sha": "2a2df041283ed3471cb56a6c633539d207add01c",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 23,
            "deletions": 24,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -140,30 +140,29 @@ def __call__(\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-        if self.patch_size is None or self.vision_feature_select_strategy is None:\n-            prompt_strings = text\n-            logger.warning_once(\n-                \"Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. \"\n-                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n-            )\n-        # cannot infer image expansion length if no images are found\n-        elif not image_inputs:\n-            prompt_strings = text\n-        else:\n-            image_sizes = image_inputs[\"image_sizes\"]\n-            height, width = get_image_size(to_numpy_array(image_inputs[\"pixel_values\"][0][0]))\n-            prompt_strings = []\n-            for image_size, sample in zip(image_sizes, text):\n-                # Replace the image token with the expanded image token sequence\n-                orig_height, orig_width = image_size\n-                num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n-                if self.vision_feature_select_strategy == \"default\":\n-                    num_image_tokens -= 1\n-\n-                sample = sample.replace(self.image_token, self.image_token * num_image_tokens)\n-                prompt_strings.append(sample)\n+        prompt_strings = text\n+        if image_inputs:\n+            if self.patch_size is None or self.vision_feature_select_strategy is None:\n+                logger.warning_once(\n+                    \"Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. \"\n+                    \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n+                    \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n+                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                )\n+            else:\n+                image_sizes = iter(image_inputs[\"image_sizes\"])\n+                height, width = get_image_size(to_numpy_array(image_inputs[\"pixel_values\"][0][0]))\n+                prompt_strings = []\n+                for sample in text:\n+                    while self.image_token in sample:\n+                        image_size = next(image_sizes)\n+                        orig_height, orig_width = image_size\n+                        num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n+                        if self.vision_feature_select_strategy == \"default\":\n+                            num_image_tokens -= 1\n+                        sample = sample.replace(self.image_token, \"<placeholder>\" * num_image_tokens, 1)\n+                    prompt_strings.append(sample)\n+                prompt_strings = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n \n         text_inputs = self.tokenizer(\n             prompt_strings,"
        },
        {
            "sha": "e765dfb95cc335792c785fe0f585dccd38bda763",
            "filename": "src/transformers/models/llava_next_video/diff_llava_next_video.py",
            "status": "modified",
            "additions": 106,
            "deletions": 125,
            "changes": 231,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fdiff_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fdiff_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fdiff_llava_next_video.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -29,7 +29,6 @@\n     image_size_to_num_patches,\n )\n \n-from ...cache_utils import Cache\n from ...utils import (\n     logging,\n     replace_return_docstrings,\n@@ -389,13 +388,17 @@ def forward(\n \n             # if the number of image/video tokens is more than image embeddings seq length, then prob we expanded it in processing\n             # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\n-            img_token_count = (input_ids == self.config.image_token_index).sum(1).max()\n-            video_token_count = (input_ids == self.config.video_token_index).sum(1).max()\n-            inputs_expanded = (\n-                img_token_count < self.config.image_seq_length and video_token_count < self.config.video_seq_length\n+            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n+                1\n+            ).max() < self.config.image_seq_length\n+            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n+                1\n+            ).max() < self.config.video_seq_length\n+            inputs_not_expanded = (img_token_not_enough and pixel_values is not None) or (\n+                video_token_not_enough and pixel_values_videos is not None\n             )\n-            pixels_present = input_ids.shape[-1] == 1 and pixel_values is not None and pixel_values_videos is not None\n-            legacy_processing = inputs_expanded or pixels_present\n+            pixels_present = input_ids.shape[-1] == 1 and (pixel_values is not None or pixel_values_videos is not None)\n+            legacy_processing = inputs_not_expanded or pixels_present\n \n         image_features = feature_lens = None\n         if pixel_values is not None and pixel_values.size(0) > 0:\n@@ -414,75 +417,76 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-            if legacy_processing:\n-                logger.warning_once(\n-                    \"Expanding inputs for image.video tokens in LLaVa-NeXT-Video should be done in processing. \"\n-                    \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                    \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+        if legacy_processing:\n+            logger.warning_once(\n+                \"Expanding inputs for image.video tokens in LLaVa-NeXT-Video should be done in processing. \"\n+                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n+                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+            )\n+            if input_ids.shape[1] != 1:\n+                iterator = (\n+                    (image_features, feature_lens, self.config.image_token_index),\n+                    (video_features, video_feature_lens, self.config.video_token_index),\n                 )\n-                if input_ids.shape[1] != 1:\n-                    iterator = (\n-                        (image_features, feature_lens, self.config.image_token_index),\n-                        (video_features, video_feature_lens, self.config.video_token_index),\n-                    )\n-                    for features, lens, special_token in zip(iterator):\n-                        if features is not None:\n-                            (\n-                                inputs_embeds,\n-                                attention_mask,\n-                                position_ids,\n-                                labels,\n-                                input_ids,\n-                            ) = self._merge_input_ids_with_image_features(\n-                                features,\n-                                lens,\n-                                inputs_embeds,\n-                                input_ids,\n-                                attention_mask,\n-                                position_ids,\n-                                labels=labels,\n-                                image_token_index=special_token,\n-                            )\n-                else:\n-                    # Retrieve the first layer to inspect the logits and mask out the hidden states that are set to 0\n-                    first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-                    # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                    batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-                    # Get the target length\n-                    target_length = input_ids.shape[1]\n-                    past_length = first_layer_past_key_value.shape[-1]\n-                    extended_attention_mask = torch.ones(\n-                        (attention_mask.shape[0], past_length),\n-                        dtype=attention_mask.dtype,\n-                        device=attention_mask.device,\n-                    )\n-                    # Filter out only the tokens that can be un-attended, this can happen\n-                    # if one uses Llava + Fused modules where the cache on the\n-                    # first iteration is already big enough, or if one passes custom cache\n-                    valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                    new_batch_index = batch_index[valid_indices]\n-                    new_non_attended_tokens = non_attended_tokens[valid_indices]\n-                    # Zero-out the places where we don't need to attend\n-                    extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-                    attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                    position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-\n-            # TODO: @raushan retain only the new behavior after v4.47\n+                for features, lens, special_token in iterator:\n+                    if features is not None:\n+                        (\n+                            inputs_embeds,\n+                            attention_mask,\n+                            position_ids,\n+                            labels,\n+                            input_ids,\n+                        ) = self._merge_input_ids_with_image_features(\n+                            features,\n+                            lens,\n+                            inputs_embeds,\n+                            input_ids,\n+                            attention_mask,\n+                            position_ids,\n+                            labels=labels,\n+                            image_token_index=special_token,\n+                        )\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n             else:\n-                if image_features is not None:\n-                    special_image_mask = (\n-                        (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n-                    )\n-                    image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                    inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-                if video_features is not None:\n-                    special_image_mask = (\n-                        (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n-                    )\n-                    video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                    inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+                # Retrieve the first layer to inspect the logits and mask out the hidden states that are set to 0\n+                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n+                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n+                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n+                # Get the target length\n+                target_length = input_ids.shape[1]\n+                past_length = first_layer_past_key_value.shape[-1]\n+                extended_attention_mask = torch.ones(\n+                    (attention_mask.shape[0], past_length),\n+                    dtype=attention_mask.dtype,\n+                    device=attention_mask.device,\n+                )\n+                # Filter out only the tokens that can be un-attended, this can happen\n+                # if one uses Llava + Fused modules where the cache on the\n+                # first iteration is already big enough, or if one passes custom cache\n+                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n+                new_batch_index = batch_index[valid_indices]\n+                new_non_attended_tokens = non_attended_tokens[valid_indices]\n+                # Zero-out the places where we don't need to attend\n+                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n+                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n+                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n+\n+        # TODO: @raushan retain only the new behavior after v4.47\n+        else:\n+            if image_features is not None:\n+                special_image_mask = (\n+                    (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                )\n+                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+            if video_features is not None:\n+                special_image_mask = (\n+                    (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                )\n+                video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,\n@@ -493,6 +497,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = outputs[0]\n@@ -534,58 +539,34 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_sizes=None,\n         attention_mask=None,\n+        cache_position=None,\n         **kwargs,\n     ):\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, Cache):\n-                cache_length = past_key_values.get_seq_length()\n-                past_length = past_key_values.seen_tokens\n-            else:\n-                cache_length = past_length = past_key_values[0][0].shape[2]\n-\n-            # Keep only the unprocessed tokens:\n-            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n-            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n-            # input)\n-            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n-                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n-            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n-            # input_ids based on the past_length.\n-            elif past_length < input_ids.shape[1]:\n-                input_ids = input_ids[:, past_length:]\n-\n-            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n-            elif self.config.image_token_index in input_ids or self.config.video_token_index in input_ids:\n-                input_ids = input_ids[:, input_ids.shape[1] - 1 :]\n-\n-            # If the cache has seen more tokens than it can hold, then the cache has a size limit. Let's discard the\n-            # older attention values, as their corresponding values are not part of the input.\n-            if cache_length < past_length and attention_mask is not None:\n-                attention_mask = attention_mask[:, -(cache_length + input_ids.shape[1]) :]\n-\n-        position_ids = kwargs.get(\"position_ids\", None)\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_key_values is None:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-                \"pixel_values\": pixel_values,\n-                \"pixel_values_videos\": pixel_values_videos,\n-                \"image_sizes\": image_sizes,\n-            }\n+        if input_ids is not None:\n+            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n+                1\n+            ).max() < self.config.image_seq_length\n+            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n+                1\n+            ).max() < self.config.video_seq_length\n+            legacy_processing = (img_token_not_enough and pixel_values is not None) or (\n+                video_token_not_enough and pixel_values_videos is not None\n+            )\n+\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            **kwargs,\n         )\n+\n+        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+        # Otherwise we need pixel values to be passed to model\n+        if legacy_processing or cache_position[0] == 0:\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n+            model_inputs[\"image_sizes\"] = image_sizes\n+\n         return model_inputs"
        },
        {
            "sha": "7d6776738c39fde19af65aec5519389dfcadeb1c",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 108,
            "deletions": 128,
            "changes": 236,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -31,7 +31,6 @@\n \n from ... import PreTrainedModel\n from ...activations import ACT2FN\n-from ...cache_utils import Cache\n from ...image_processing_utils import select_best_resolution\n from ...modeling_outputs import ModelOutput\n from ...utils import (\n@@ -767,6 +766,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n     ) -> Union[Tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -874,13 +874,17 @@ def forward(\n \n             # if the number of image/video tokens is more than image embeddings seq length, then prob we expanded it in processing\n             # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\n-            img_token_count = (input_ids == self.config.image_token_index).sum(1).max()\n-            video_token_count = (input_ids == self.config.video_token_index).sum(1).max()\n-            inputs_expanded = (\n-                img_token_count < self.config.image_seq_length and video_token_count < self.config.video_seq_length\n+            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n+                1\n+            ).max() < self.config.image_seq_length\n+            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n+                1\n+            ).max() < self.config.video_seq_length\n+            inputs_not_expanded = (img_token_not_enough and pixel_values is not None) or (\n+                video_token_not_enough and pixel_values_videos is not None\n             )\n-            pixels_present = input_ids.shape[-1] == 1 and pixel_values is not None and pixel_values_videos is not None\n-            legacy_processing = inputs_expanded or pixels_present\n+            pixels_present = input_ids.shape[-1] == 1 and (pixel_values is not None or pixel_values_videos is not None)\n+            legacy_processing = inputs_not_expanded or pixels_present\n \n         image_features = feature_lens = None\n         if pixel_values is not None and pixel_values.size(0) > 0:\n@@ -899,75 +903,76 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-            if legacy_processing:\n-                logger.warning_once(\n-                    \"Expanding inputs for image.video tokens in LLaVa-NeXT-Video should be done in processing. \"\n-                    \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                    \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+        if legacy_processing:\n+            logger.warning_once(\n+                \"Expanding inputs for image.video tokens in LLaVa-NeXT-Video should be done in processing. \"\n+                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n+                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+            )\n+            if input_ids.shape[1] != 1:\n+                iterator = (\n+                    (image_features, feature_lens, self.config.image_token_index),\n+                    (video_features, video_feature_lens, self.config.video_token_index),\n                 )\n-                if input_ids.shape[1] != 1:\n-                    iterator = (\n-                        (image_features, feature_lens, self.config.image_token_index),\n-                        (video_features, video_feature_lens, self.config.video_token_index),\n-                    )\n-                    for features, lens, special_token in iterator:\n-                        if features is not None:\n-                            (\n-                                inputs_embeds,\n-                                attention_mask,\n-                                position_ids,\n-                                labels,\n-                                input_ids,\n-                            ) = self._merge_input_ids_with_image_features(\n-                                features,\n-                                lens,\n-                                inputs_embeds,\n-                                input_ids,\n-                                attention_mask,\n-                                position_ids,\n-                                labels=labels,\n-                                image_token_index=special_token,\n-                            )\n-                else:\n-                    # Retrieve the first layer to inspect the logits and mask out the hidden states that are set to 0\n-                    first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-                    # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                    batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-                    # Get the target length\n-                    target_length = input_ids.shape[1]\n-                    past_length = first_layer_past_key_value.shape[-1]\n-                    extended_attention_mask = torch.ones(\n-                        (attention_mask.shape[0], past_length),\n-                        dtype=attention_mask.dtype,\n-                        device=attention_mask.device,\n-                    )\n-                    # Filter out only the tokens that can be un-attended, this can happen\n-                    # if one uses Llava + Fused modules where the cache on the\n-                    # first iteration is already big enough, or if one passes custom cache\n-                    valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                    new_batch_index = batch_index[valid_indices]\n-                    new_non_attended_tokens = non_attended_tokens[valid_indices]\n-                    # Zero-out the places where we don't need to attend\n-                    extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n-                    attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                    position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-\n-            # TODO: @raushan retain only the new behavior after v4.47\n+                for features, lens, special_token in iterator:\n+                    if features is not None:\n+                        (\n+                            inputs_embeds,\n+                            attention_mask,\n+                            position_ids,\n+                            labels,\n+                            input_ids,\n+                        ) = self._merge_input_ids_with_image_features(\n+                            features,\n+                            lens,\n+                            inputs_embeds,\n+                            input_ids,\n+                            attention_mask,\n+                            position_ids,\n+                            labels=labels,\n+                            image_token_index=special_token,\n+                        )\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n             else:\n-                if image_features is not None:\n-                    special_image_mask = (\n-                        (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n-                    )\n-                    image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                    inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-                if video_features is not None:\n-                    special_image_mask = (\n-                        (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n-                    )\n-                    video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                    inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+                # Retrieve the first layer to inspect the logits and mask out the hidden states that are set to 0\n+                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n+                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n+                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n+                # Get the target length\n+                target_length = input_ids.shape[1]\n+                past_length = first_layer_past_key_value.shape[-1]\n+                extended_attention_mask = torch.ones(\n+                    (attention_mask.shape[0], past_length),\n+                    dtype=attention_mask.dtype,\n+                    device=attention_mask.device,\n+                )\n+                # Filter out only the tokens that can be un-attended, this can happen\n+                # if one uses Llava + Fused modules where the cache on the\n+                # first iteration is already big enough, or if one passes custom cache\n+                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n+                new_batch_index = batch_index[valid_indices]\n+                new_non_attended_tokens = non_attended_tokens[valid_indices]\n+                # Zero-out the places where we don't need to attend\n+                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n+                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n+                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n+\n+        # TODO: @raushan retain only the new behavior after v4.47\n+        else:\n+            if image_features is not None:\n+                special_image_mask = (\n+                    (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                )\n+                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+            if video_features is not None:\n+                special_image_mask = (\n+                    (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+                )\n+                video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,\n@@ -978,6 +983,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n             num_logits_to_keep=num_logits_to_keep,\n         )\n \n@@ -1020,64 +1026,38 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_sizes=None,\n         attention_mask=None,\n+        cache_position=None,\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, Cache):\n-                cache_length = past_key_values.get_seq_length()\n-                past_length = past_key_values.seen_tokens\n-            else:\n-                cache_length = past_length = past_key_values[0][0].shape[2]\n-\n-            # Keep only the unprocessed tokens:\n-            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n-            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n-            # input)\n-            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n-                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n-            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n-            # input_ids based on the past_length.\n-            elif past_length < input_ids.shape[1]:\n-                input_ids = input_ids[:, past_length:]\n-\n-            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n-            elif self.config.image_token_index in input_ids or self.config.video_token_index in input_ids:\n-                input_ids = input_ids[:, input_ids.shape[1] - 1 :]\n-\n-            # If the cache has seen more tokens than it can hold, then the cache has a size limit. Let's discard the\n-            # older attention values, as their corresponding values are not part of the input.\n-            if cache_length < past_length and attention_mask is not None:\n-                attention_mask = attention_mask[:, -(cache_length + input_ids.shape[1]) :]\n-\n-        position_ids = kwargs.get(\"position_ids\", None)\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_key_values is None:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n-\n-        if \"num_logits_to_keep\" != None:\n-            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-                \"pixel_values\": pixel_values,\n-                \"pixel_values_videos\": pixel_values_videos,\n-                \"image_sizes\": image_sizes,\n-            }\n+        if input_ids is not None:\n+            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n+                1\n+            ).max() < self.config.image_seq_length\n+            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n+                1\n+            ).max() < self.config.video_seq_length\n+            legacy_processing = (img_token_not_enough and pixel_values is not None) or (\n+                video_token_not_enough and pixel_values_videos is not None\n+            )\n+\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            num_logits_to_keep=num_logits_to_keep,\n+            **kwargs,\n         )\n+\n+        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+        # Otherwise we need pixel values to be passed to model\n+        if legacy_processing or cache_position[0] == 0:\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n+            model_inputs[\"image_sizes\"] = image_sizes\n+\n         return model_inputs\n \n     def _get_image_features(self, pixel_values, image_sizes):"
        },
        {
            "sha": "e0e4534e42b565ef0bb6aa62277f4cab8aa50558",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 57,
            "deletions": 20,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -19,6 +19,7 @@\n from typing import TYPE_CHECKING, List, Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n+from ...image_processing_utils import select_best_resolution\n from ...image_utils import ImageInput, VideoInput, get_image_size, to_numpy_array\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n@@ -160,35 +161,29 @@ def __call__(\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-        print(self.patch_size, self.vision_feature_select_strategy, image_inputs, videos_inputs.keys())\n-\n         if self.patch_size is None or self.vision_feature_select_strategy is None:\n-            prompt_strings = text\n             logger.warning_once(\n                 \"Expanding inputs for image/video tokens in LLaVa-NeXT-Video should be done in processing. \"\n                 \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n                 \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n                 \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n             )\n-        # cannot infer image expansion length if no images/videos are found\n-        elif not image_inputs and not videos_inputs:\n-            prompt_strings = text\n         else:\n             # images expand taking into account num_of_patches in each image\n             if image_inputs:\n-                image_sizes = image_inputs[\"image_sizes\"]\n+                image_sizes = iter(image_inputs[\"image_sizes\"])\n                 height, width = get_image_size(to_numpy_array(image_inputs[\"pixel_values\"][0][0]))\n                 prompt_strings = []\n-                for image_size, sample in zip(image_sizes, text):\n-                    # Replace the image token with the expanded image token sequence\n-                    orig_height, orig_width = image_size\n-                    num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n-                    if self.vision_feature_select_strategy == \"default\":\n-                        num_image_tokens -= 1\n-\n-                    sample = sample.replace(self.image_token, self.image_token * num_image_tokens)\n+                for sample in text:\n+                    while self.image_token in sample:\n+                        image_size = next(image_sizes)\n+                        orig_height, orig_width = image_size\n+                        num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n+                        if self.vision_feature_select_strategy == \"default\":\n+                            num_image_tokens -= 1\n+                        sample = sample.replace(self.image_token, \"<placeholder>\" * num_image_tokens, 1)\n                     prompt_strings.append(sample)\n-                text = prompt_strings\n+                text = [sample.replace(\"<placeholder>\", self.image_token) for sample in prompt_strings]\n \n             # videos are easier, simply get frames and multiply\n             if videos_inputs:\n@@ -197,23 +192,65 @@ def __call__(\n                 num_frames = one_video.shape[0]  # frame dim is always after batch dim\n                 num_image_tokens = (height // self.patch_size) * (width // self.patch_size)\n                 num_video_tokens = num_image_tokens // 4 * num_frames  # divide by 4 needed for avg pooling layer\n-\n                 prompt_strings = []\n                 for sample in text:\n                     sample = sample.replace(self.video_token, self.video_token * num_video_tokens)\n                     prompt_strings.append(sample)\n+                text = prompt_strings\n \n         text_inputs = self.tokenizer(\n-            prompt_strings,\n+            text,\n             return_tensors=return_tensors,\n             padding=padding,\n             truncation=truncation,\n             max_length=max_length,\n         )\n-        print(text_inputs.keys())\n-\n         return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs})\n \n+    # Copied from transformers.models.llava_next.processing_llava_next.LlavaNextProcessor._get_number_of_features\n+    def _get_number_of_features(self, orig_height: int, orig_width: int, height: int, width: int) -> int:\n+        image_grid_pinpoints = self.image_processor.image_grid_pinpoints\n+\n+        height_best_resolution, width_best_resolution = select_best_resolution(\n+            [orig_height, orig_width], image_grid_pinpoints\n+        )\n+        scale_height, scale_width = height_best_resolution // height, width_best_resolution // width\n+\n+        patches_height = height // self.patch_size\n+        patches_width = width // self.patch_size\n+        unpadded_features, newline_features = self._get_unpadded_features(\n+            orig_height, orig_width, patches_height, patches_width, scale_height, scale_width\n+        )\n+        # The base patch covers the entire image (+1 for the CLS)\n+        base_features = patches_height * patches_width + 1\n+        num_image_tokens = unpadded_features + newline_features + base_features\n+        return num_image_tokens\n+\n+    # Copied from transformers.models.llava_next.processing_llava_next.LlavaNextProcessor._get_unpadded_features\n+    def _get_unpadded_features(self, height, width, patches_height, patches_width, scale_height, scale_width):\n+        \"\"\"\n+        Get number of features for a given image with height/width. LLaVA-NeXT is different from LLaVA\n+        because it divided each image into patches depending on its resolution. Therefore we need to calculate how many\n+        patches an image is divided into and get the number of features from that.\n+        \"\"\"\n+        current_height = patches_height * scale_height\n+        current_width = patches_width * scale_width\n+\n+        original_aspect_ratio = width / height\n+        current_aspect_ratio = current_width / current_height\n+        if original_aspect_ratio > current_aspect_ratio:\n+            new_height = (height * current_width) // width\n+            padding = (current_height - new_height) // 2\n+            current_height -= padding * 2\n+        else:\n+            new_width = (width * current_height) // height\n+            padding = (current_width - new_width) // 2\n+            current_width -= padding * 2\n+\n+        unpadded_features = current_height * current_width\n+        newline_features = current_height\n+        return (unpadded_features, newline_features)\n+\n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "08e02d9a702acbfe51a5f31aa161bca93db66f4b",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 26,
            "deletions": 17,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -529,15 +529,19 @@ def forward(\n \n             # if the number of image/video tokens is more than image embeddings seq length, then prob we expanded it in processing\n             # not very reliable, but we don't expect one to actually pass 500+ images for one prompt\n-            img_token_count = (input_ids == self.config.image_token_index).sum(1).max()\n-            video_token_count = (input_ids == self.config.video_token_index).sum(1).max()\n-            inputs_expanded = (\n-                img_token_count < self.config.image_seq_length and video_token_count < self.config.video_seq_length\n+            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n+                1\n+            ).max() < self.config.image_seq_length\n+            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n+                1\n+            ).max() < self.config.video_seq_length\n+            inputs_not_expanded = (img_token_not_enough and pixel_values_images is not None) or (\n+                video_token_not_enough and pixel_values_videos is not None\n             )\n-            pixels_present = (\n-                input_ids.shape[-1] == 1 and pixel_values_images is not None and pixel_values_videos is not None\n+            pixels_present = input_ids.shape[-1] == 1 and (\n+                pixel_values_images is not None or pixel_values_videos is not None\n             )\n-            legacy_processing = inputs_expanded or pixels_present\n+            legacy_processing = inputs_not_expanded or pixels_present\n \n         if pixel_values_images is not None or pixel_values_videos is not None:\n             image_outputs, video_outputs, num_frames = self._get_vision_features(\n@@ -577,6 +581,7 @@ def forward(\n                                 labels,\n                                 num_frames=frames,\n                             )\n+                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n                 else:\n                     # Retrieve the first layer to inspect the logits and mask out the hidden states\n                     # that are set to 0\n@@ -606,6 +611,9 @@ def forward(\n \n                     attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n                     position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[\n+                        -target_length:\n+                    ]\n \n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n@@ -678,11 +686,16 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n-        # Trigger the new behavior if we have more than image embeddings seq length tokens for images\n-        legacy_processing = input_ids is not None and (\n-            (input_ids == self.config.image_token_index).sum(1).max() < self.config.image_seq_length\n-            and (input_ids == self.config.video_token_index).sum(1).max() < self.config.video_seq_length\n-        )\n+        if input_ids is not None:\n+            img_token_not_enough = (input_ids == self.config.image_token_index).sum(\n+                1\n+            ).max() < self.config.image_seq_length\n+            video_token_not_enough = (input_ids == self.config.video_token_index).sum(\n+                1\n+            ).max() < self.config.video_seq_length\n+            legacy_processing = (img_token_not_enough and pixel_values_images is not None) or (\n+                video_token_not_enough and pixel_values_videos is not None\n+            )\n \n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n@@ -694,11 +707,7 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        if legacy_processing:\n-            model_inputs[\"pixel_values_images\"] = pixel_values_images\n-            model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n-\n-        elif cache_position[0] == 0:\n+        if legacy_processing or cache_position[0] == 0:\n             # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n             # Otherwise we need pixel values to be passed to model\n             model_inputs[\"pixel_values_images\"] = pixel_values_images"
        },
        {
            "sha": "bd6f91270965bbed6a81f4ccbe4aed24bad373a4",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -145,24 +145,28 @@ def __call__(\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-        if encoded_images is not None and self.patch_size is None or self.vision_feature_select_strategy is None:\n-            prompt_strings = text\n+        prompt_strings = text\n+        if encoded_images is not None and (self.patch_size is None or self.vision_feature_select_strategy is None):\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in Video-LLaVa should be done in processing. \"\n                 \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n                 \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n                 \"Using processors without these attributes in the config is deprecated and will throw an error in v4.44.\"\n             )\n+        # Replace the image/video tokens with the expanded token sequence\n         elif encoded_images is not None:\n-            # Replace the image token with the expanded image token sequence\n-            if \"pixel_values\" in encoded_images:\n-                height, width = get_image_size(to_numpy_array(encoded_images.get(\"pixel_values\")[0]))\n+            if \"pixel_values_images\" in encoded_images.keys():\n+                height, width = get_image_size(to_numpy_array(encoded_images.get(\"pixel_values_images\")[0]))\n                 num_frames = 1\n-            else:\n+\n+            if \"pixel_values_videos\" in encoded_images.keys():\n                 one_video = to_numpy_array(encoded_images.get(\"pixel_values_videos\")[0])\n                 height, width = get_image_size(one_video[0])\n                 num_frames = one_video.shape[0]  # frame dim is always after batch dim\n \n+            num_image_tokens = (height // self.patch_size) * (width // self.patch_size) + 1\n+            num_video_tokens = num_image_tokens * num_frames\n+\n             num_image_tokens = (height // self.patch_size) * (width // self.patch_size) + 1\n             num_video_tokens = num_image_tokens * num_frames\n             if self.vision_feature_select_strategy == \"default\":"
        },
        {
            "sha": "5367b1e088d2aa89c55e5a33989d87f834fcb49c",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -471,6 +471,7 @@ def forward(\n                     inputs_embeds, attention_mask, labels, position_ids = self._merge_input_ids_with_image_features(\n                         image_features, inputs_embeds, input_ids, attention_mask, labels\n                     )\n+                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n                 else:\n                     # Retrieve the first layer to inspect the logits and mask out the hidden states\n                     # that are set to 0\n@@ -500,6 +501,9 @@ def forward(\n \n                     attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n                     position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[\n+                        -target_length:\n+                    ]\n \n             # TODO: @raushan retain only the new behavior after v4.47\n             else:\n@@ -579,9 +583,7 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        if legacy_processing:\n-            model_inputs[\"pixel_values\"] = pixel_values\n-        elif cache_position[0] == 0:\n+        if legacy_processing or cache_position[0] == 0:\n             # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n             # Otherwise we need pixel values to be passed to model\n             model_inputs[\"pixel_values\"] = pixel_values"
        },
        {
            "sha": "2fed802b5a2fb319cd2dc1582f81ce51e44ae86d",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 37,
            "deletions": 12,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -302,7 +302,7 @@ def test_small_model_integration_test_llama_single(self):\n         inputs = processor(prompt, raw_image, return_tensors=\"pt\").to(torch_device, torch.float16)\n \n         output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n-        EXPECTED_DECODED_TEXT = \"USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as sudden changes in weather can make the pier unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could cause accidents or injuries. Additionally, be cautious of the tides and currents, as they can change rapidly and pose a risk to swimmers or those who venture too close to the edge of the pier. Lastly, be respectful of the environment and other visitors, as the pier is a shared space where people can enjoy the view, relax, or engage in recreational activities.\"  # fmt: skip\n+        EXPECTED_DECODED_TEXT = \"USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as sudden changes in weather can make the pier unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could cause accidents or injuries. Additionally, be cautious of the tides and currents, as they can change rapidly and pose a risk to swimmers or those who venture too close to the edge of the pier. Finally, be respectful of the environment and other visitors, and follow any posted rules or guidelines for the area.\"  # fmt: skip\n \n         self.assertEqual(\n             processor.decode(output[0], skip_special_tokens=True),\n@@ -353,7 +353,10 @@ def test_small_model_integration_test_batch(self):\n \n         output = model.generate(**inputs, max_new_tokens=20)\n \n-        EXPECTED_DECODED_TEXT = ['USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, there are a few things to be cautious about and items to bring along', 'USER:  \\nWhat is this?\\nASSISTANT: Cats']  # fmt: skip\n+        EXPECTED_DECODED_TEXT = [\n+            'USER:  \\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nASSISTANT: When visiting this place, there are a few things to be cautious about and items to bring.',\n+            'USER:  \\nWhat is this?\\nASSISTANT: Cats'\n+        ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n@@ -393,7 +396,7 @@ def test_small_model_integration_test_llama_batched_regression(self):\n     @require_torch\n     @require_vision\n     def test_batched_generation(self):\n-        model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\").to(torch_device)\n+        model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", load_in_4bit=True)\n \n         processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n \n@@ -415,9 +418,9 @@ def test_batched_generation(self):\n         model = model.eval()\n \n         EXPECTED_OUTPUT = [\n-            \"\\n \\nUSER: What's the the difference of two images?\\nASSISTANT: In the two images, the primary difference is the presence of a small dog in one and a ll\",\n-            \"\\nUSER: Describe the image.\\nASSISTANT: The image features a small, fluffy dog sitting on a sidewalk. The dog is holding\",\n-            \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone, adult llama standing on a grassy hill. The llama\",\n+            \"\\n \\nUSER: What's the the difference of two images?\\nASSISTANT: The difference between the two images is that one shows a dog standing on a grassy field, while\",\n+            \"\\nUSER: Describe the image.\\nASSISTANT: The image features a brown and white dog sitting on a sidewalk. The dog is holding a small\",\n+            \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone llama standing on a grassy hill. The llama is the\",\n         ]\n \n         generate_ids = model.generate(**inputs, max_new_tokens=20)\n@@ -451,26 +454,23 @@ def test_llava_index_error_bug(self):\n     def test_llava_merge_inputs_error_bug(self):\n         # This is a reproducer of https://github.com/huggingface/transformers/pull/28333 and makes sure it does not happen anymore\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n-        model = LlavaForConditionalGeneration.from_pretrained(\n-            model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True\n-        ).to(torch_device)\n+        model = LlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n \n         # Simulate some user inputs\n         pixel_values = torch.randn(\n-            (2, 3, 336, 336),\n+            (1, 3, 336, 336),\n             dtype=torch.float,\n             device=torch_device,\n         )\n         input_ids = torch.tensor(\n             [\n                 [32001, 32001, 1, 15043, 7084, 32000, 29871, 13, 7900],\n-                [1, 15043, 7084, 29901, 29871, 32000, 29871, 13, 7900],\n             ],\n             dtype=torch.long,\n             device=torch_device,\n         )\n         attention_mask = torch.tensor(\n-            [[0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]],\n+            [[0, 0, 1, 1, 1, 1, 1, 1, 1]],\n             dtype=torch.long,\n             device=torch_device,\n         )\n@@ -515,6 +515,31 @@ def test_generation_no_images(self):\n         # Make sure that `generate` works\n         _ = model.generate(**inputs, max_new_tokens=20)\n \n+    @slow\n+    @require_bitsandbytes\n+    def test_generation_siglip_backbone(self):\n+        model_id = \"llava-hf/llava-interleave-qwen-0.5b-hf\"\n+        model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=\"float16\", device_map=torch_device)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        # check processing with expansion of inputs (w/o expansion should work with any backbone)\n+        processor.vision_feature_select_strategy = \"default\"\n+        processor.patch_size = 14\n+\n+        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        inputs = processor(\n+            text=\"<|im_start|>user\\n<image>\\nWhat are these?<|im_end|>\\n<|im_start|>assistant\",\n+            images=raw_image,\n+            return_tensors=\"pt\",\n+        ).to(torch_device, torch.float16)\n+\n+        # Make sure that `generate` works\n+        output = model.generate(**inputs, max_new_tokens=30)\n+\n+        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\"\n+        self.assertTrue(processor.batch_decode(output, skip_special_tokens=True)[0] == EXPECTED_DECODED_TEXT)\n+\n     @slow\n     @require_bitsandbytes\n     def test_expansion_in_processing(self):"
        },
        {
            "sha": "3120db216ea4bbc3db7fb67e5347a4e544de596f",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 53,
            "deletions": 29,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -363,11 +363,7 @@ def test_small_model_integration_test(self):\n             output = model(**inputs)\n \n         expected_slice = torch.tensor(\n-            [\n-                [-4.7695, -4.5664, -0.2786],\n-                [-10.6250, -10.8906, -2.5254],\n-                [-6.7383, -7.2461, -0.6787],\n-            ],\n+            [[-4.7695, -4.5664, -0.2788], [-10.6172, -10.8828, -2.5273], [-6.7383, -7.2422, -0.6694]],\n             dtype=torch.float32,\n             device=torch_device,\n         )\n@@ -471,16 +467,16 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n             output = model(**inputs)\n \n         expected_slice = torch.tensor(\n-            [[-0.0308, -0.0313, -0.0314], [-0.3064, -0.3013, -0.2986], [-0.1226, -0.1246, -0.1210]],\n+            [[-0.1287, -0.1294, -0.1284], [-0.2744, -0.2698, -0.2671], [-0.1071, -0.1091, -0.1056]],\n             dtype=torch.float32,\n             device=torch_device,\n         )\n         assert torch.allclose(output.logits[0, -3:, -3:], expected_slice, atol=1e-3)\n-        assert torch.allclose(output.loss, torch.tensor(6.8619, device=torch_device))\n+        assert torch.allclose(output.loss, torch.tensor(7.0206, device=torch_device), atol=1e-3)\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=50)\n-        EXPECTED_DECODED_TEXT = '[INST]  \\nWhat is shown in this image? [/INST] The image shows a forested area with a misty or foggy atmosphere. In the foreground, there is a grassy field with a few deer grazing. The deer are partially obscured by the fog, and the trees in the background'  # fmt: skip\n+        EXPECTED_DECODED_TEXT = '[INST]  \\nWhat is shown in this image? [/INST] The image shows two deer, likely fawns, in a grassy area with trees in the background. The setting appears to be a forest or woodland, and the photo is taken during what seems to be either dawn or dusk, given'  # fmt: skip\n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n@@ -534,38 +530,66 @@ def test_padding_side_when_merging_inputs(self):\n \n         # model is in eval mode by default so we should get pad on the left side\n         # we can check the first hidden-states (aka inputs embeds)\n-        # the first element was lo-res image and we expect the first 1414 tokens to be all pads\n-        output_eval = model(**inputs_batched, output_hidden_states=True)\n-        self.assertTrue((output_eval.hidden_states[0][0, :1414, ...] == 0).all().item())\n-\n-        # otherwise padding is on the right side, so it's last 1414 tokens\n-        self.processor.padding_side = \"right\"\n-        inputs_batched = self.processor(\n-            [self.prompt, self.prompt], images=[lowres_img, cats_image], return_tensors=\"pt\", padding=True\n-        ).to(torch_device)\n-\n-        model.train()\n+        # the first element was lo-res image and we expect the first 732 tokens to be all pads\n         with torch.no_grad():\n-            output_train = model(**inputs_batched, output_hidden_states=True)\n-        self.assertTrue((output_train.hidden_states[0][0, -1414:, ...] == 0).all().item())\n+            output_eval = model(**inputs_batched, output_hidden_states=True)\n+        self.assertTrue((output_eval.hidden_states[0][0, :732, ...] == 0).all().item())\n \n         with self.assertLogs(\"transformers\", level=\"WARNING\") as logs:\n             model.padding_side = \"left\"\n             model.train()\n-            model(**inputs_batched, output_hidden_states=True)\n+            with torch.no_grad():\n+                model(**inputs_batched, output_hidden_states=True)\n \n-            self.assertIn(\n-                \"Padding side is set to 'left' but the model is in training mode. For training\", logs.output[0]\n-            )\n+            self.assertIn(\"Padding side is set to 'left' but the model is in training mode. For training\", logs)\n \n         with self.assertLogs(\"transformers\", level=\"WARNING\") as logs:\n             model.padding_side = \"right\"\n             model.eval()\n-            model(**inputs_batched, output_hidden_states=True)\n+            with torch.no_grad():\n+                model(**inputs_batched, output_hidden_states=True)\n \n-            self.assertIn(\n-                \"Padding side is set to 'right' but the model is in inference mode. For correct\", logs.output[0]\n-            )\n+            self.assertIn(\"Padding side is set to 'right' but the model is in inference mode. For correct\", logs)\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_expansion_in_processing_multiimage(self):\n+        model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n+        model = LlavaNextForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        prompt = \"USER: <image><image>\\nDescribe the similarity between the two images:\\nASSISTANT:\"\n+        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        deer_image = Image.open(\n+            requests.get(\n+                \"https://4.img-dpreview.com/files/p/TS560x560~forums/56876524/03975b28741443319e9a94615e35667e\",\n+                stream=True,\n+            ).raw\n+        )\n+\n+        # check processing with expansion of inputs\n+        processor.vision_feature_select_strategy = \"default\"\n+        processor.patch_size = 14\n+        inputs_expanded = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n+            torch_device, torch.float16\n+        )\n+        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 3969)\n+\n+        # check processing without expansion of inputs (legacy behavior)\n+        processor.vision_feature_select_strategy = None\n+        processor.patch_size = None\n+        inputs = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n+            torch_device, torch.float16\n+        )\n+        self.assertTrue(inputs.input_ids.shape[-1] == 23)\n+\n+        # generate exactly 20 tokens\n+        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n+        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n+\n+        # check that both inputs are handled correctly and generate the same output\n+        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "35df4085df0563df22026e9e624d619cbe930ec7",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 85,
            "deletions": 50,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -18,6 +18,7 @@\n import unittest\n \n import numpy as np\n+import requests\n from huggingface_hub import hf_hub_download\n \n from transformers import (\n@@ -363,37 +364,14 @@ def test_small_model_integration_test(self):\n         )\n \n         inputs = self.processor(self.prompt_video, videos=self.video, return_tensors=\"pt\")\n-        expected_input_ids = [\n-            1,\n-            3148,\n-            1001,\n-            29901,\n-            29871,\n-            32000,\n-            13,\n-            11008,\n-            338,\n-            445,\n-            4863,\n-            2090,\n-            1460,\n-            29973,\n-            319,\n-            1799,\n-            9047,\n-            13566,\n-            29901,\n-        ]\n-        self.assertListEqual(expected_input_ids, inputs.input_ids[0].tolist())\n-\n         # verify single forward pass\n         inputs = inputs.to(torch_device)\n         with torch.no_grad():\n             output = model(**inputs)\n \n         # verify generation\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=40)\n-        EXPECTED_DECODED_TEXT = 'USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and exaggerated reactions of the child to the book. The child appears to be reading a book, but instead of a calm and focused reading experience'  # fmt: skip\n+        EXPECTED_DECODED_TEXT = 'USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a book while another child is attempting to read the same book. The child who is reading the book seems'  # fmt: skip\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n@@ -416,7 +394,10 @@ def test_small_model_integration_test_batch(self):\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n \n-        EXPECTED_DECODED_TEXT = ['USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and exaggerated reactions of the child to the', 'USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and exaggerated reactions of the child to the']  # fmt: skip\n+        EXPECTED_DECODED_TEXT = [\n+            'USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a',\n+            'USER: \\nWhy is this video funny? ASSISTANT: The humor in this video comes from the unexpected and somewhat comical situation of a young child reading a'\n+        ]  # fmt: skip\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n@@ -447,7 +428,7 @@ def test_small_model_integration_test_batch_different_vision_types(self):\n \n         # verify generation\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=50)\n-        EXPECTED_DECODED_TEXT = 'USER: \\nWhat is shown in this image? ASSISTANT: The image appears to be a graphical representation of a benchmark test for a machine learning model. It shows the performance of various models on a task, with the x-axis representing the number of parameters (measured in millions) and the y'  # fmt: skip\n+        EXPECTED_DECODED_TEXT = 'USER: \\nWhat is shown in this image? ASSISTANT: The image appears to be a graphical representation of a machine learning model\\'s performance on a task, likely related to natural language processing or text understanding. It shows a scatter plot with two axes, one labeled \"BLIP-2\"'  # fmt: skip\n         self.assertEqual(self.processor.decode(output[0], skip_special_tokens=True), EXPECTED_DECODED_TEXT)\n \n     @slow\n@@ -493,41 +474,25 @@ def test_padding_side_when_merging_inputs(self):\n         # model is in eval mode by default so we should get pad on the left side\n         # we can check the first hidden-states (aka inputs embeds)\n         # the first element was lo-res image and we expect the first 1482 tokens to be all pads\n-        output_eval = model(**inputs_batched, output_hidden_states=True)\n-        self.assertTrue((output_eval.hidden_states[0][0, :1482, ...] == 0).all().item())\n-\n-        # otherwise padding is on the right side, so it's last 1482 tokens\n-        self.processor.padding_side = \"right\"\n-        inputs_batched = self.processor(\n-            [self.prompt_video, self.prompt_image],\n-            images=[self.image],\n-            videos=[self.video],\n-            return_tensors=\"pt\",\n-            padding=True,\n-        ).to(torch_device)\n-\n-        model.train()\n         with torch.no_grad():\n-            output_train = model(**inputs_batched, output_hidden_states=True)\n-        self.assertTrue((output_train.hidden_states[0][0, -1482:, ...] == 0).all().item())\n+            output_eval = model(**inputs_batched, output_hidden_states=True)\n+        self.assertTrue((output_eval.hidden_states[0][0, :1482, ...] == 0).all().item())\n \n         with self.assertLogs(\"transformers\", level=\"WARNING\") as logs:\n             model.padding_side = \"left\"\n             model.train()\n-            model(**inputs_batched, output_hidden_states=True)\n+            with torch.no_grad():\n+                model(**inputs_batched, output_hidden_states=True)\n \n-            self.assertIn(\n-                \"Padding side is set to 'left' but the model is in training mode. For training\", logs.output[0]\n-            )\n+            self.assertIn(\"Padding side is set to 'left' but the model is in training mode. For training\", logs)\n \n         with self.assertLogs(\"transformers\", level=\"WARNING\") as logs:\n             model.padding_side = \"right\"\n             model.eval()\n-            model(**inputs_batched, output_hidden_states=True)\n+            with torch.no_grad():\n+                model(**inputs_batched, output_hidden_states=True)\n \n-            self.assertIn(\n-                \"Padding side is set to 'right' but the model is in inference mode. For correct\", logs.output[0]\n-            )\n+            self.assertIn(\"Padding side is set to 'right' but the model is in inference mode. For correct\", logs)\n \n     @slow\n     @require_bitsandbytes\n@@ -556,3 +521,73 @@ def test_expansion_in_processing(self):\n \n         # check that both inputs are handled correctly and generate the same output\n         self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_expansion_in_processing_images(self):\n+        model_id = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n+        model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n+            \"llava-hf/LLaVA-NeXT-Video-7B-hf\", load_in_4bit=True\n+        )\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        # check processing with expansion of inputs\n+        processor.vision_feature_select_strategy = \"default\"\n+        processor.patch_size = 14\n+        inputs_expanded = processor(self.prompt_image, images=[self.image], return_tensors=\"pt\").to(torch_device)\n+        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2652)\n+\n+        # check processing without expansion of inputs (legacy behavior)\n+        processor.vision_feature_select_strategy = None\n+        processor.patch_size = None\n+        inputs = processor(self.prompt_image, images=[self.image], return_tensors=\"pt\").to(torch_device)\n+        self.assertTrue(inputs.input_ids.shape[-1] == 19)\n+\n+        # generate exactly 20 tokens\n+        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n+        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n+\n+        # check that both inputs are handled correctly and generate the same output\n+        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_expansion_in_processing_multiimage(self):\n+        model_id = \"llava-hf/LLaVA-NeXT-Video-7B-hf\"\n+        model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n+            \"llava-hf/LLaVA-NeXT-Video-7B-hf\", load_in_4bit=True\n+        )\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        prompt = \"USER: <image><image>\\nDescribe the similarity between the two images:\\nASSISTANT:\"\n+        image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        deer_image = Image.open(\n+            requests.get(\n+                \"https://4.img-dpreview.com/files/p/TS560x560~forums/56876524/03975b28741443319e9a94615e35667e\",\n+                stream=True,\n+            ).raw\n+        )\n+\n+        # check processing with expansion of inputs\n+        processor.vision_feature_select_strategy = \"default\"\n+        processor.patch_size = 14\n+        inputs_expanded = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n+            torch_device, torch.float16\n+        )\n+        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 3968)\n+\n+        # check processing without expansion of inputs (legacy behavior)\n+        processor.vision_feature_select_strategy = None\n+        processor.patch_size = None\n+        inputs = processor(text=prompt, images=[raw_image, deer_image], return_tensors=\"pt\").to(\n+            torch_device, torch.float16\n+        )\n+        self.assertTrue(inputs.input_ids.shape[-1] == 22)\n+\n+        # generate exactly 20 tokens\n+        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n+        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n+\n+        # check that both inputs are handled correctly and generate the same output\n+        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())"
        },
        {
            "sha": "a8b2229a02f5f24b2ccbb12f1934f76223867545",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 55,
            "deletions": 74,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -383,18 +383,19 @@ def test_small_model_integration_test(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n         model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n \n-        prompt = \"USER: <video>Why is this video funny? ASSISTANT:\"\n+        prompt = \"USER: <video>\\nWhy is this video funny? ASSISTANT:\"\n         video_file = hf_hub_download(\n             repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n         )\n         video_file = np.load(video_file)\n         inputs = self.processor(prompt, videos=video_file, return_tensors=\"pt\")\n \n-        EXPECTED_INPUT_IDS = torch.tensor([[1,  3148, 1001, 29901, 29871, 32001, 3750, 338, 445, 4863, 2090, 1460, 29973, 319, 1799, 9047, 13566, 29901]])  # fmt: skip\n+        EXPECTED_INPUT_IDS = torch.tensor([[1,  3148, 1001, 29901, 29871, 32001, 13, 11008, 338, 445, 4863, 2090, 1460, 29973, 319, 1799, 9047, 13566, 29901]])  # fmt: skip\n+\n         self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n \n         output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n-        EXPECTED_DECODED_TEXT = \"USER:  Why is this video funny? ASSISTANT: The video is funny because the baby is playing with a Wii remote while sitting on a bed\"  # fmt: skip\n+        EXPECTED_DECODED_TEXT = \"USER: \\nWhy is this video funny? ASSISTANT: The video is funny because it shows a baby sitting on a bed and reading a book, which\"  # fmt: skip\n \n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=True),\n@@ -404,12 +405,11 @@ def test_small_model_integration_test(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_mixed_inputs(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n         model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n \n         prompts = [\n-            \"USER: <image>What are the cats in the image doing? ASSISTANT:\",\n-            \"USER: <video>Why is this video funny? ASSISTANT:\",\n+            \"USER: <image>\\nWhat are the cats in the image doing? ASSISTANT:\",\n+            \"USER: <video>\\nWhy is this video funny? ASSISTANT:\",\n         ]\n         video_file = hf_hub_download(\n             repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n@@ -422,8 +422,8 @@ def test_small_model_integration_test_mixed_inputs(self):\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n \n         EXPECTED_DECODED_TEXT = [\n-            'USER:  What are the cats in the image doing? ASSISTANT: The cats in the image are lying down on a red couch, possibly sleeping or rest',\n-            'USER:  Why is this video funny? ASSISTANT: The video is funny because the baby is playing with a Wii remote while sitting on a bed'\n+            'USER: \\nWhat are the cats in the image doing? ASSISTANT: The cats in the image are sleeping or resting on a couch.',\n+            'USER: \\nWhy is this video funny? ASSISTANT: The video is funny because it shows a baby sitting on a bed and reading a book. The'\n         ]  # fmt: skip\n \n         self.assertEqual(\n@@ -434,24 +434,22 @@ def test_small_model_integration_test_mixed_inputs(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n-\n         model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n         processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n \n-        prompt = \"USER: <video>Describe the video in details. ASSISTANT:\"\n+        prompt = \"USER: <video>\\nDescribe the video in details. ASSISTANT:\"\n         video_file = hf_hub_download(\n             repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n         )\n         video_file = np.load(video_file)\n         inputs = self.processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n \n         output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n-        EXPECTED_DECODED_TEXT = \"USER:  Describe the video in details. ASSISTANT: The video features a young child sitting on a bed, holding a book and reading it. \" \\\n-            \"The child appears to be enjoying the book, as they are fully engaged in the reading process. The bed is located in a bedroom, and there is a chair nearby. \" \\\n-            \"The child is wearing a light blue shirt and pink pants, and they have glasses on. The room is well-lit, and there is a clock on the wall. The child seems \" \\\n-            \"to be in a comfortable and relaxed environment, which is conducive to reading and learning. Overall, the video captures a heartwarming moment of a child \" \\\n-            \"engaging in a simple yet essential activity, which is reading.\"  # fmt: skip\n+        EXPECTED_DECODED_TEXT = \"USER: \\nDescribe the video in details. ASSISTANT: The video features a young child sitting on a bed, holding a book and reading it. \" \\\n+            \"The child appears to be enjoying the book, as they are fully engaged in the activity. The bed is located in a bedroom, and there is a chair nearby. The \" \\\n+            \"child is wearing a blue shirt and glasses, which suggests that they might have a visual impairment. The room is well-lit, and there is a clock on the wall, \" \\\n+            \"indicating the time. The child's focus on the book indicates that they are interested in the content and are actively participating in the reading process. \" \\\n+            \"Overall, the video captures a heartwarming moment of a child engaging in a simple yet essential activity, which is reading.\"  # fmt: skip\n \n         self.assertEqual(\n             processor.decode(output[0], skip_special_tokens=True),\n@@ -461,15 +459,13 @@ def test_small_model_integration_test_llama(self):\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n-\n         model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n         processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n         processor.tokenizer.padding_side = \"left\"\n \n         prompts = [\n-            \"USER: <video>What is the baby doing? ASSISTANT:\",\n-            \"USER: <video>Who is sitting next to the woman? ASSISTANT:\",\n+            \"USER: <video>\\nWhat is the baby doing? ASSISTANT:\",\n+            \"USER: <video>\\nWho is sitting next to the woman? ASSISTANT:\",\n         ]\n         video_1 = np.load(\n             hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\")\n@@ -483,48 +479,12 @@ def test_small_model_integration_test_llama_batched(self):\n         output = model.generate(**inputs, max_new_tokens=20)\n \n         EXPECTED_DECODED_TEXT = [\n-            'USER:  What is the baby doing? ASSISTANT: The baby is sitting on a bed and reading a book.',\n-            'USER:  Who is sitting next to the woman? ASSISTANT: A small dog is sitting next to the woman.'\n+            'USER: \\nWhat is the baby doing? ASSISTANT: The baby is sitting on a bed and reading a book.',\n+            'USER: \\nWho is sitting next to the woman? ASSISTANT: A small dog is sitting next to the woman.'\n         ]  # fmt: skip\n \n         self.assertEqual(processor.batch_decode(output, skip_special_tokens=True), EXPECTED_DECODED_TEXT)\n \n-    @slow\n-    @require_bitsandbytes\n-    def test_small_model_integration_test_llama_batched_regression(self):\n-        # Let' s make sure we test the preprocessing to replace what is used\n-\n-        # Multi-image & multi-prompt (e.g. 3 images and 2 prompts now fails with SDPA, this tests if \"eager\" works as before)\n-        model = VideoLlavaForConditionalGeneration.from_pretrained(\n-            \"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True, attn_implementation=\"eager\"\n-        )\n-        processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", pad_token=\"<pad>\")\n-        processor.tokenizer.padding_side = \"left\"\n-\n-        prompts = [\n-            \"USER: <video>What is the baby doing? ASSISTANT:\",\n-            \"USER: <video>Who is sitting next to the woman? ASSISTANT: A small dog is sitting next to the woman. USER: <video>What about this video? ASSITANT:\",\n-        ]\n-        video_1 = np.load(\n-            hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\")\n-        )\n-        video_2 = np.load(\n-            hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo_2.npy\", repo_type=\"dataset\")\n-        )\n-\n-        inputs = processor(prompts, videos=[video_1, video_2, video_1], return_tensors=\"pt\", padding=True)\n-\n-        output = model.generate(**inputs, max_new_tokens=20)\n-\n-        # fmt: off\n-        EXPECTED_DECODED_TEXT = [\n-            'USER:  What is the baby doing? ASSISTANT: The baby is sitting on a bed and reading a book.',\n-            'USER:  Who is sitting next to the woman? ASSISTANT: A small dog is sitting next to the woman. USER:  What about this video? ASSITANT: The video shows a baby sitting on a bed, reading a book. The baby is wearing glass'\n-        ]\n-        # fmt: on\n-\n-        self.assertEqual(processor.batch_decode(output, skip_special_tokens=True), EXPECTED_DECODED_TEXT)\n-\n     @slow\n     @require_bitsandbytes\n     def test_video_llava_index_error_bug(self):\n@@ -552,32 +512,23 @@ def test_video_llava_index_error_bug(self):\n     @require_torch_gpu\n     def test_video_llava_merge_inputs_error_bug(self):\n         # This is a reproducer of https://github.com/huggingface/transformers/pull/28333 and makes sure it does not happen anymore\n-        model = VideoLlavaForConditionalGeneration.from_pretrained(\n-            \"LanguageBind/Video-LLaVA-7B-hf\", torch_dtype=torch.float16, low_cpu_mem_usage=True\n-        ).to(torch_device)\n+        model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\", load_in_4bit=True)\n \n         # Simulate some user inputs\n         pixel_values_videos = torch.randn(\n-            (2, 8, 3, 224, 224),\n+            (1, 8, 3, 224, 224),\n             dtype=torch.float,\n             device=torch_device,\n         )\n         # fmt: off\n         input_ids = torch.tensor(\n-            [\n-                [\n-                    32001, 32001, 1, 15043, 7084, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 29871, 13, 7900\n-                ],\n-                [\n-                    1, 15043, 7084, 29901, 29871, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 29871, 13, 7900\n-                ],\n-            ],\n+            [[32002, 32002, 1, 15043, 7084, 32001, 29871, 13, 7900]],\n             dtype=torch.long,\n             device=torch_device,\n         )\n         # fmt: on\n         attention_mask = torch.tensor(\n-            [[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n+            [[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n             dtype=torch.long,\n             device=torch_device,\n         )\n@@ -591,14 +542,44 @@ def test_video_llava_merge_inputs_error_bug(self):\n         ).loss\n         loss.backward()\n \n+    @slow\n+    @require_bitsandbytes\n+    def test_expansion_in_processing_images(self):\n+        model_id = \"LanguageBind/Video-LLaVA-7B-hf\"\n+        model = VideoLlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n+        processor = VideoLlavaProcessor.from_pretrained(model_id)\n+\n+        prompt = \"USER: <image>\\nDescribe the image in details. ASSISTANT:\"\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+\n+        # check processing with expansion of inputs\n+        processor.vision_feature_select_strategy = \"default\"\n+        processor.patch_size = 14\n+        inputs_expanded = processor(prompt, images=image, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 274)\n+\n+        # check processing without expansion of inputs (legacy behavior)\n+        processor.vision_feature_select_strategy = None\n+        processor.patch_size = None\n+        inputs = processor(prompt, images=image, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        self.assertTrue(inputs.input_ids.shape[-1] == 19)\n+\n+        # generate exactly 20 tokens\n+        output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)\n+        output_expanded = model.generate(**inputs_expanded, min_new_tokens=20, max_new_tokens=20)\n+\n+        # check that both inputs are handled correctly and generate the same output\n+        self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n+\n     @slow\n     @require_bitsandbytes\n     def test_expansion_in_processing(self):\n         model_id = \"LanguageBind/Video-LLaVA-7B-hf\"\n         model = VideoLlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n         processor = VideoLlavaProcessor.from_pretrained(model_id)\n \n-        prompt = \"USER: <video>Describe the video in details. ASSISTANT:\"\n+        prompt = \"USER: <video>\\nDescribe the video in details. ASSISTANT:\"\n         video_file = hf_hub_download(\n             repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n         )\n@@ -608,13 +589,13 @@ def test_expansion_in_processing(self):\n         processor.vision_feature_select_strategy = \"default\"\n         processor.patch_size = 14\n         inputs_expanded = processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2073)\n+        self.assertTrue(inputs_expanded.input_ids.shape[-1] == 2074)\n \n         # check processing without expansion of inputs (legacy behavior)\n         processor.vision_feature_select_strategy = None\n         processor.patch_size = None\n         inputs = processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n-        self.assertTrue(inputs.input_ids.shape[-1] == 18)\n+        self.assertTrue(inputs.input_ids.shape[-1] == 19)\n \n         # generate exactly 20 tokens\n         output = model.generate(**inputs, min_new_tokens=20, max_new_tokens=20)"
        },
        {
            "sha": "c5a363da9f5dd5d0d132dd81fa1f5fbf09243300",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2d6ce9cb80d45d341f149c9b76a583c289052a/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=7d2d6ce9cb80d45d341f149c9b76a583c289052a",
            "patch": "@@ -271,26 +271,23 @@ def test_small_model_integration_test(self):\n     def test_vipllava_merge_inputs_error_bug(self):\n         # This is a reproducer of https://github.com/huggingface/transformers/pull/28333 and makes sure it does not happen anymore\n         model_id = \"llava-hf/vip-llava-7b-hf\"\n-        model = VipLlavaForConditionalGeneration.from_pretrained(\n-            model_id, torch_dtype=torch.float16, low_cpu_mem_usage=True\n-        ).to(torch_device)\n+        model = VipLlavaForConditionalGeneration.from_pretrained(model_id, load_in_4bit=True)\n \n         # Simulate some user inputs\n         pixel_values = torch.randn(\n-            (2, 3, 336, 336),\n+            (1, 3, 336, 336),\n             dtype=torch.float,\n             device=torch_device,\n         )\n         input_ids = torch.tensor(\n             [\n                 [32001, 32001, 1, 15043, 7084, 32000, 29871, 13, 7900],\n-                [1, 15043, 7084, 29901, 29871, 32000, 29871, 13, 7900],\n             ],\n             dtype=torch.long,\n             device=torch_device,\n         )\n         attention_mask = torch.tensor(\n-            [[0, 0, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]],\n+            [[0, 0, 1, 1, 1, 1, 1, 1, 1]],\n             dtype=torch.long,\n             device=torch_device,\n         )"
        }
    ],
    "stats": {
        "total": 1085,
        "additions": 581,
        "deletions": 504
    }
}