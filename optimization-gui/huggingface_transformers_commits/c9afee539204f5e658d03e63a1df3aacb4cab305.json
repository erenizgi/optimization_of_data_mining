{
    "author": "VladOS95-cyber",
    "message": "Add gguf support for gpt2 (#34044)\n\n* add gpt2 gguf support\r\n\r\n* add doc change\r\n\r\n* small refactoring",
    "sha": "c9afee539204f5e658d03e63a1df3aacb4cab305",
    "files": [
        {
            "sha": "7418bbc497e66098e96e4d295b5a74b43ced4b03",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9afee539204f5e658d03e63a1df3aacb4cab305/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9afee539204f5e658d03e63a1df3aacb4cab305/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=c9afee539204f5e658d03e63a1df3aacb4cab305",
            "patch": "@@ -83,6 +83,7 @@ For now the supported model architectures are the architectures that have been v\n - Bloom\n - Falcon\n - StableLM\n+- GPT2\n \n ## Example usage\n "
        },
        {
            "sha": "cc317b18b052ed68807a92260eaeb131f45ab5df",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9afee539204f5e658d03e63a1df3aacb4cab305/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9afee539204f5e658d03e63a1df3aacb4cab305/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=c9afee539204f5e658d03e63a1df3aacb4cab305",
            "patch": "@@ -163,6 +163,19 @@\n         \"output.weight\": \"lm_head.weight\",\n         \"output_norm\": \"model.norm\",\n     },\n+    \"gpt2\": {\n+        \"token_embd\": \"transformer.wte\",\n+        \"blk\": \"transformer.h\",\n+        \"position_embd\": \"transformer.wpe\",\n+        \"output_norm\": \"transformer.ln_f\",\n+        \"attn_norm\": \"ln_1\",\n+        \"attn_qkv\": \"attn.c_attn\",\n+        \"attn_output.weight\": \"attn.c_proj.weight\",\n+        \"attn_output.bias\": \"attn.c_proj.bias\",\n+        \"ffn_norm\": \"ln_2\",\n+        \"ffn_up\": \"mlp.c_fc\",\n+        \"ffn_down\": \"mlp.c_proj\",\n+    },\n }\n \n \n@@ -271,6 +284,14 @@\n         \"attention.layer_norm_epsilon\": \"layer_norm_eps\",\n         \"vocab_size\": \"vocab_size\",\n     },\n+    \"gpt2\": {\n+        \"block_count\": \"n_layer\",\n+        \"context_length\": \"n_ctx\",\n+        \"embedding_length\": \"n_embd\",\n+        \"feed_forward_length\": \"feed_forward_length\",\n+        \"attention.head_count\": \"n_head\",\n+        \"attention.layer_norm_epsilon\": \"layer_norm_epsilon\",\n+    },\n }\n \n GGUF_TOKENIZER_MAPPING = {\n@@ -600,6 +621,7 @@ def converted(self) -> Tokenizer:\n     \"bloom\": GGUFGPTConverter,\n     \"falcon\": GGUFGPTConverter,\n     \"stablelm\": GGUFGPTConverter,\n+    \"gpt2\": GGUFGPTConverter,\n }\n \n "
        },
        {
            "sha": "b1d7b8960854766df6ad0bd7801b4a535211f7e5",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9afee539204f5e658d03e63a1df3aacb4cab305/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9afee539204f5e658d03e63a1df3aacb4cab305/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=c9afee539204f5e658d03e63a1df3aacb4cab305",
            "patch": "@@ -191,6 +191,23 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n                 else:\n                     weights = reverse_reshape_bias(weights, num_heads, n_embed)\n \n+            if architecture == \"gpt2\":\n+                if (\n+                    \"attn_qkv.weight\" in name\n+                    or \"ffn_down.weight\" in name\n+                    or \"ffn_up.weight\" in name\n+                    or \"attn_output.weight\" in name\n+                ):\n+                    # Original transpose implementation\n+                    # https://github.com/ggerganov/llama.cpp/blob/a38b884c6c4b0c256583acfaaabdf556c62fabea/convert_hf_to_gguf.py#L2060-L2061\n+                    weights = weights.T\n+                if name == \"output.weight\":\n+                    # output.weight has conflicts with attn_output.weight in name checking\n+                    # we have to explicitly check that name is exactly output.weight\n+                    name = \"lm_head.weight\"\n+                    parsed_parameters[\"tensors\"][name] = torch.from_numpy(np.copy(weights))\n+                    continue\n+\n             for tensor_name in tensor_key_mapping:\n                 if tensor_name in name:\n                     name = name.replace(tensor_name, tensor_key_mapping[tensor_name])"
        },
        {
            "sha": "795b5ce067298f580ebd43953b844dc8c12d935e",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9afee539204f5e658d03e63a1df3aacb4cab305/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9afee539204f5e658d03e63a1df3aacb4cab305/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py?ref=c9afee539204f5e658d03e63a1df3aacb4cab305",
            "patch": "@@ -97,8 +97,8 @@ def __init__(\n         **kwargs,\n     ):\n         super().__init__(\n-            vocab_file,\n-            merges_file,\n+            vocab_file=vocab_file,\n+            merges_file=merges_file,\n             tokenizer_file=tokenizer_file,\n             unk_token=unk_token,\n             bos_token=bos_token,"
        },
        {
            "sha": "3074a19828d25b1779461c2c09369ce11461f840",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9afee539204f5e658d03e63a1df3aacb4cab305/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9afee539204f5e658d03e63a1df3aacb4cab305/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=c9afee539204f5e658d03e63a1df3aacb4cab305",
            "patch": "@@ -51,6 +51,9 @@ class GgufIntegrationTests(unittest.TestCase):\n     stablelm_model_id = \"afrideva/stablelm-3b-4e1t-GGUF\"\n     stablelm2_model_id = \"afrideva/stablelm-2-1_6b-GGUF\"\n     original_stablelm2_model_id = \"stabilityai/stablelm-2-1_6b\"\n+    gpt2_model_id = \"mradermacher/gpt2-GGUF\"\n+    gpt2_original_model_id = \"openai-community/gpt2\"\n+    gpt2_xl_model_id = \"RichardErkhov/openai-community_-_gpt2-xl-gguf\"\n \n     # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n@@ -87,6 +90,9 @@ class GgufIntegrationTests(unittest.TestCase):\n     fp16_falcon7b_model_id = \"falcon-7b-fp16.gguf\"\n     q2_k_falcon40b_model_id = \"tiiuae-falcon-40b-Q2_K.gguf\"\n     fp16_qwen2moe_model_id = \"Qwen1.5-MoE-A2.7B.gguf\"\n+    fp16_gpt2_model_id = \"gpt2.f16.gguf\"\n+    q8_gpt2_model_id = \"gpt2.Q8_0.gguf\"\n+    q6_k_gpt2_xl_model_id = \"gpt2-xl.Q6_K.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -476,6 +482,53 @@ def test_bloom_weights_conversion_fp16(self):\n                 self.assertTrue(quantized_param.shape == original_param.shape)\n                 torch.testing.assert_close(quantized_param, original_param)\n \n+    def test_gpt2_q8(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.gpt2_model_id, gguf_file=self.q8_gpt2_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.gpt2_model_id,\n+            gguf_file=self.q8_gpt2_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I'm sorry. I'm sorry. I\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_gpt2_weights_conversion_fp16(self):\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.gpt2_model_id,\n+            gguf_file=self.fp16_gpt2_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.gpt2_original_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        quantized_state_dict = quantized_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in quantized_state_dict:\n+                self.assertTrue(original_params.shape == quantized_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, quantized_state_dict[layer_name])\n+\n+    def test_gpt2_xl_Q6_K(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.gpt2_xl_model_id, gguf_file=self.q6_k_gpt2_xl_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.gpt2_xl_model_id,\n+            gguf_file=self.q6_k_gpt2_xl_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello, I'm a newbie to the world of\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n     @unittest.skip(reason=\"Heavy memory\")\n     def test_falcon40b_q2_k(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.falcon40b_model_id, gguf_file=self.q2_k_falcon40b_model_id)"
        }
    ],
    "stats": {
        "total": 97,
        "additions": 95,
        "deletions": 2
    }
}