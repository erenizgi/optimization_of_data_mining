{
    "author": "alex-jw-brooks",
    "message": "Expose blip2qformer (#37254)\n\n* Expose blip2qformer\n\n* Add missing args to blip2 config",
    "sha": "2515a5a29045fcec3935be25fd4e6f0475759b53",
    "files": [
        {
            "sha": "8e73953cb2ccbbc8c4ea9cc177448f30a1251d65",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2515a5a29045fcec3935be25fd4e6f0475759b53/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2515a5a29045fcec3935be25fd4e6f0475759b53/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=2515a5a29045fcec3935be25fd4e6f0475759b53",
            "patch": "@@ -54,6 +54,7 @@\n         (\"blenderbot-small\", \"BlenderbotSmallConfig\"),\n         (\"blip\", \"BlipConfig\"),\n         (\"blip-2\", \"Blip2Config\"),\n+        (\"blip_2_qformer\", \"Blip2QFormerConfig\"),\n         (\"bloom\", \"BloomConfig\"),\n         (\"bridgetower\", \"BridgeTowerConfig\"),\n         (\"bros\", \"BrosConfig\"),\n@@ -391,6 +392,7 @@\n         (\"blenderbot-small\", \"BlenderbotSmall\"),\n         (\"blip\", \"BLIP\"),\n         (\"blip-2\", \"BLIP-2\"),\n+        (\"blip_2_qformer\", \"BLIP-2 QFormer\"),\n         (\"bloom\", \"BLOOM\"),\n         (\"bort\", \"BORT\"),\n         (\"bridgetower\", \"BridgeTower\"),\n@@ -781,6 +783,7 @@\n         (\"granitevision\", \"llava_next\"),\n         (\"sam_vision_model\", \"sam\"),\n         (\"llama4_text\", \"llama4\"),\n+        (\"blip_2_qformer\", \"blip_2\"),\n     ]\n )\n "
        },
        {
            "sha": "36a75f48e1a8721bb521b41161d059fd29abbf73",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2515a5a29045fcec3935be25fd4e6f0475759b53/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2515a5a29045fcec3935be25fd4e6f0475759b53/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=2515a5a29045fcec3935be25fd4e6f0475759b53",
            "patch": "@@ -53,6 +53,7 @@\n         (\"blenderbot-small\", \"BlenderbotSmallModel\"),\n         (\"blip\", \"BlipModel\"),\n         (\"blip-2\", \"Blip2Model\"),\n+        (\"blip_2_qformer\", \"Blip2QFormerModel\"),\n         (\"bloom\", \"BloomModel\"),\n         (\"bridgetower\", \"BridgeTowerModel\"),\n         (\"bros\", \"BrosModel\"),"
        },
        {
            "sha": "fa186633c435624155536422bad2ab55644913fd",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2515a5a29045fcec3935be25fd4e6f0475759b53/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2515a5a29045fcec3935be25fd4e6f0475759b53/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=2515a5a29045fcec3935be25fd4e6f0475759b53",
            "patch": "@@ -144,6 +144,8 @@ class Blip2QFormerConfig(PretrainedConfig):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n             The epsilon used by the layer normalization layers.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Index to be used for padding token.\n         position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n             Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n             positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to"
        },
        {
            "sha": "424fbf0a5a980bec477fd0f3ed1d19764a0e99c1",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 80,
            "deletions": 21,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/2515a5a29045fcec3935be25fd4e6f0475759b53/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2515a5a29045fcec3935be25fd4e6f0475759b53/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=2515a5a29045fcec3935be25fd4e6f0475759b53",
            "patch": "@@ -456,6 +456,21 @@ def _init_weights(self, module):\n             configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n+BLIP_2_QFORMER_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Blip2QFormerConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n BLIP_2_VISION_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n@@ -621,6 +636,60 @@ def _init_weights(self, module):\n \"\"\"\n \n \n+BLIP2_QFORMER_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        query_embeds (`torch.FloatTensor`  of shape `(batch_size, sequence_length, hidden_size)`):\n+            Hidden states to be used in the attention computation. If cross-attention,\n+            will be used for the query (i.e., key and value will use the encoder_hidden_states).\n+\n+        query_length (`int`, *optional*):\n+            Length of the query, usually based on the number of query tokens.\n+            If no value is provided, query_length will be inferred by the query_embeds.\n+\n+        attention_mask (`torch.FloatTensor`, *optional*):\n+            Attention mask of size `(batch, sequence_length)` where padding elements\n+            are indicated by 0.\n+\n+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+\n+        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, `optional`):\n+            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n+            the model is configured as a decoder.\n+\n+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, `optional`):\n+            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n+            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of:\n+            shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n+            value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n+            used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key\n+            value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape\n+            `(batch_size, sequence_length)`.\n+\n+        use_cache (`bool`, `optional`):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->Blip2\n class Blip2Encoder(nn.Module):\n     \"\"\"\n@@ -1248,11 +1317,13 @@ def forward(\n         return embeddings\n \n \n-class Blip2QFormerModel(Blip2PreTrainedModel):\n-    \"\"\"\n-    Querying Transformer (Q-Former), used in BLIP-2.\n+@add_start_docstrings(\n     \"\"\"\n-\n+    BLIP-2 Querying Transformer (Q-Former).\n+    \"\"\",\n+    BLIP_2_QFORMER_START_DOCSTRING,\n+)\n+class Blip2QFormerModel(Blip2PreTrainedModel):\n     def __init__(self, config: Blip2QFormerConfig):\n         super().__init__(config)\n         self.config = config\n@@ -1323,6 +1394,10 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n+    @add_start_docstrings_to_model_forward(BLIP2_QFORMER_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(\n+        output_type=BaseModelOutputWithPoolingAndCrossAttentions, config_class=Blip2QFormerConfig\n+    )\n     def forward(\n         self,\n         query_embeds: torch.FloatTensor,\n@@ -1338,23 +1413,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, `optional`):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, `optional`):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of:\n-            shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n-            value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n-            used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key\n-            value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape\n-            `(batch_size, sequence_length)`.\n-        use_cache (`bool`, `optional`):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n+        Returns:\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "bc274da53e55990d2f2e55cce9ba06685fd8b4d9",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2515a5a29045fcec3935be25fd4e6f0475759b53/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2515a5a29045fcec3935be25fd4e6f0475759b53/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=2515a5a29045fcec3935be25fd4e6f0475759b53",
            "patch": "@@ -106,7 +106,6 @@\n     \"BlenderbotSmallConfig\",\n     \"BlenderbotSmallTokenizerFast\",\n     \"BlenderbotTokenizerFast\",\n-    \"Blip2QFormerConfig\",\n     \"Blip2VisionConfig\",\n     \"BlipTextConfig\",\n     \"BlipVisionConfig\","
        },
        {
            "sha": "dc594c590e2f9a11f517520a7d18be927e0714f4",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/2515a5a29045fcec3935be25fd4e6f0475759b53/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2515a5a29045fcec3935be25fd4e6f0475759b53/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=2515a5a29045fcec3935be25fd4e6f0475759b53",
            "patch": "@@ -187,7 +187,6 @@\n     \"ClapAudioModelWithProjection\",\n     \"Blip2TextModelWithProjection\",\n     \"Blip2VisionModelWithProjection\",\n-    \"Blip2QFormerModel\",\n     \"Blip2VisionModel\",\n     \"ErnieMForInformationExtraction\",\n     \"FastSpeech2ConformerHifiGan\","
        }
    ],
    "stats": {
        "total": 109,
        "additions": 86,
        "deletions": 23
    }
}