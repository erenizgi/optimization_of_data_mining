{
    "author": "zucchini-nlp",
    "message": "ðŸ”´ VLM: compile compatibility (#35724)\n\n* llavas\r\n\r\n* add mroe models\r\n\r\n* fix `compile_forward` test for all models\r\n\r\n* fix copies\r\n\r\n* make style\r\n\r\n* also doesn't support cache class\r\n\r\n* fix some tests\r\n\r\n* not copied from\r\n\r\n* ci green?\r\n\r\n* fix tests\r\n\r\n* fix copies\r\n\r\n* fix tests\r\n\r\n* check with `numel` and remove `item`\r\n\r\n* fix copies\r\n\r\n* fix copies\r\n\r\n* Update src/transformers/models/cohere2/modeling_cohere2.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* opt remove cross attn\r\n\r\n* gemma2\r\n\r\n* fixup\r\n\r\n* fixup\r\n\r\n* fix newly added test\r\n\r\n* maybe fixed?\r\n\r\n* green please?\r\n\r\n---------\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
    "files": [
        {
            "sha": "916631da7e8f908bdd7260e1dd7dea55205bc178",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -2016,6 +2016,9 @@ def forward(\n class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     config_class = Blip2Config\n     main_input_name = \"pixel_values\"\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)"
        },
        {
            "sha": "65322e236ca0a59a803806cbea582c6208246841",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -1284,13 +1284,13 @@ def forward(\n \n         if pixel_values is not None:\n             image_tokens = self.get_image_tokens(pixel_values)\n-            n_image_tokens_in_text = (input_ids == self.vocabulary_mapping.image_token_id).sum().item()\n-            n_image_features = image_tokens.shape[0] * image_tokens.shape[1]\n-            if n_image_tokens_in_text != n_image_features:\n+            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_tokens.numel():\n+                n_image_tokens_in_text = (input_ids == self.vocabulary_mapping.image_token_id).sum()\n+                n_image_features = image_tokens.shape[0] * image_tokens.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens_in_text}, features {n_image_features}\"\n                 )\n-            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n             image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n             input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n "
        },
        {
            "sha": "75144c65ecfff81bf385ff951a16f077440351d9",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -25,7 +25,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, HybridCache\n+from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -701,7 +701,7 @@ def _update_causal_mask(\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n-        if isinstance(past_key_values, HybridCache):\n+        if isinstance(past_key_values, (HybridCache, StaticCache)):\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]"
        },
        {
            "sha": "c977f873dc8cb3568ce77c6782a55067621f3925",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -25,7 +25,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, HybridCache\n+from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -713,7 +713,7 @@ def _update_causal_mask(\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n-        if isinstance(past_key_values, HybridCache):\n+        if isinstance(past_key_values, (HybridCache, StaticCache)):\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]"
        },
        {
            "sha": "805e6ba0d2a3eec166f9fd958749b5084b0c35e3",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -20,7 +20,7 @@\n import torch.utils.checkpoint\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, HybridCache\n+from ...cache_utils import Cache, HybridCache, StaticCache\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n@@ -550,7 +550,7 @@ def _update_causal_mask(\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n         sequence_length = input_tensor.shape[1]\n-        if isinstance(past_key_values, HybridCache):\n+        if isinstance(past_key_values, (HybridCache, StaticCache)):\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]"
        },
        {
            "sha": "fb9a1fb68889ca1013c46da8fc04a7d301e7e059",
            "filename": "src/transformers/models/got_ocr2/configuration_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -132,8 +132,6 @@ class GotOcr2Config(PretrainedConfig):\n             The config object or dictionary of the vision backbone.\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n             The config object or dictionary of the text backbone.\n-        ignore_index (`int`, *optional*, defaults to -100):\n-            The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 151859):\n             The image token index to encode the image prompt.\n         image_seq_length (`int`, *optional*, defaults to 576):\n@@ -161,13 +159,11 @@ def __init__(\n         self,\n         vision_config=None,\n         text_config=None,\n-        ignore_index=-100,\n         image_token_index=151859,\n         image_seq_length=576,\n         pad_token_id=-1,\n         **kwargs,\n     ):\n-        self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.image_seq_length = image_seq_length\n         self.pad_token_id = pad_token_id"
        },
        {
            "sha": "86598ac08965f6f07773b7f6d18396ff51bffb6f",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 83,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -594,6 +594,8 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         # important: this ported version of GotOcr2 isn't meant for training from scratch - only\n@@ -748,89 +750,6 @@ def get_image_features(\n         image_outputs = self.vision_tower(pixel_values).last_hidden_state\n         return self.multi_modal_projector(image_outputs)\n \n-    def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, input_ids, attention_mask, labels):\n-        num_images, num_image_patches, embed_dim = image_features.shape\n-        batch_size, sequence_length = input_ids.shape\n-        left_padding = not torch.sum(input_ids[:, -1] == torch.tensor(self.pad_token_id))\n-        # 1. Create a mask to know where special image tokens are\n-        special_image_token_mask = input_ids == self.config.image_token_index\n-        num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n-        # Compute the maximum embed dimension\n-        max_embed_dim = (num_special_image_tokens.max() * (num_image_patches - 1)) + sequence_length\n-        batch_indices, non_image_indices = torch.where(input_ids != self.config.image_token_index)\n-\n-        # 2. Compute the positions where text should be written\n-        # Calculate new positions for text tokens in merged image-text sequence.\n-        # `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images - 1` text tokens.\n-        # `torch.cumsum` computes how each image token shifts subsequent text token positions.\n-        # - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.\n-        new_token_positions = torch.cumsum((special_image_token_mask * (num_image_patches - 1) + 1), -1) - 1\n-        nb_image_pad = max_embed_dim - 1 - new_token_positions[:, -1]\n-        if left_padding:\n-            new_token_positions += nb_image_pad[:, None]  # offset for left padding\n-        text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n-\n-        # 3. Create the full embedding, already padded to the maximum position\n-        final_embedding = torch.zeros(\n-            batch_size, max_embed_dim, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device\n-        )\n-        final_attention_mask = torch.zeros(\n-            batch_size, max_embed_dim, dtype=attention_mask.dtype, device=inputs_embeds.device\n-        )\n-        if labels is not None:\n-            final_labels = torch.full(\n-                (batch_size, max_embed_dim), self.config.ignore_index, dtype=input_ids.dtype, device=input_ids.device\n-            )\n-        # In case the Vision model or the Language model has been offloaded to CPU, we need to manually\n-        # set the corresponding tensors into their correct target device.\n-        target_device = inputs_embeds.device\n-        batch_indices, non_image_indices, text_to_overwrite = (\n-            batch_indices.to(target_device),\n-            non_image_indices.to(target_device),\n-            text_to_overwrite.to(target_device),\n-        )\n-        attention_mask = attention_mask.to(target_device)\n-\n-        # 4. Fill the embeddings based on the mask. If we have [\"hey\" \"<image>\", \"how\", \"are\"]\n-        # we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features\n-        final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_image_indices]\n-        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_image_indices]\n-        if labels is not None:\n-            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_image_indices]\n-\n-        # 5. Fill the embeddings corresponding to the images. Anything that is not `text_positions` needs filling (#29835)\n-        image_to_overwrite = torch.full(\n-            (batch_size, max_embed_dim), True, dtype=torch.bool, device=inputs_embeds.device\n-        )\n-        image_to_overwrite[batch_indices, text_to_overwrite] = False\n-        if left_padding:\n-            image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n-        else:\n-            mask = torch.ones_like(image_to_overwrite, dtype=torch.bool).cumsum(-1) - 1\n-            padding_mask = mask <= new_token_positions[:, -1:].to(target_device)\n-            image_to_overwrite &= padding_mask\n-\n-        if image_to_overwrite.sum() != image_features.shape[:-1].numel():\n-            raise ValueError(\n-                f\"The input provided to the model are wrong. The number of image tokens is {torch.sum(special_image_token_mask)} while\"\n-                f\" the number of image given to the model is {num_images}. This prevents correct indexing and breaks batch generation.\"\n-            )\n-\n-        final_embedding[image_to_overwrite] = image_features.contiguous().reshape(-1, embed_dim).to(target_device)\n-        final_attention_mask |= image_to_overwrite\n-        position_ids = (final_attention_mask.cumsum(-1) - 1).masked_fill_((final_attention_mask == 0), 1)\n-\n-        # 6. Mask out the embedding at padding positions, as we later use the past_key_value value to determine the non-attended tokens.\n-        batch_indices, pad_indices = torch.where(input_ids == self.pad_token_id)\n-        indices_to_mask = new_token_positions[batch_indices, pad_indices]\n-\n-        final_embedding[batch_indices, indices_to_mask] = 0\n-\n-        if labels is None:\n-            final_labels = None\n-\n-        return final_embedding, final_attention_mask, final_labels, position_ids\n-\n     @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=GotOcr2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "fff434ead2e9c06a1ffc0d8a60da1e560cb01420",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -170,8 +170,6 @@ class GotOcr2Config(PretrainedConfig):\n             The config object or dictionary of the vision backbone.\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n             The config object or dictionary of the text backbone.\n-        ignore_index (`int`, *optional*, defaults to -100):\n-            The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 151859):\n             The image token index to encode the image prompt.\n         image_seq_length (`int`, *optional*, defaults to 576):\n@@ -199,13 +197,11 @@ def __init__(\n         self,\n         vision_config=None,\n         text_config=None,\n-        ignore_index=-100,\n         image_token_index=151859,\n         image_seq_length=576,\n         pad_token_id=-1,\n         **kwargs,\n     ):\n-        self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.image_seq_length = image_seq_length\n         self.pad_token_id = pad_token_id"
        },
        {
            "sha": "10b6efbc594368657ffe5c2002b1d65b907ed6b3",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -51,7 +51,7 @@ class GPTNeoXJapanesePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = False  # TODO (fix me): compilation fails due to a stide error?\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -129,8 +129,8 @@ def forward(\n \n         cos, sin = position_embeddings\n         query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n-        query = torch.cat((query, query_pass), dim=-1)\n-        key = torch.cat((key, key_pass), dim=-1)\n+        query = torch.cat((query, query_pass), dim=-1).contiguous()\n+        key = torch.cat((key, key_pass), dim=-1).contiguous()\n \n         # Cache QKV values\n         if layer_past is not None:"
        },
        {
            "sha": "546e78eac1482cd821de3c1be98fc4ab29ce85a8",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 26,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -1108,6 +1108,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1116,13 +1117,8 @@ def _update_causal_mask(\n         past_key_values: Cache,\n         output_attentions: bool,\n     ):\n-        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n-        # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n-        # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n-        # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n-\n         if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n \n@@ -1143,7 +1139,6 @@ def _update_causal_mask(\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n-        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -1154,25 +1149,17 @@ def _update_causal_mask(\n                 else past_seen_tokens + sequence_length + 1\n             )\n \n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n-            causal_mask = attention_mask\n-        else:\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n@@ -1182,6 +1169,7 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask"
        },
        {
            "sha": "ea42d65b845c5fee2cfac3834fd377fd082968e7",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -1290,6 +1290,9 @@ def forward(\n class InstructBlipForConditionalGeneration(InstructBlipPreTrainedModel, GenerationMixin):\n     config_class = InstructBlipConfig\n     main_input_name = \"pixel_values\"\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n     def __init__(self, config: InstructBlipConfig):\n         super().__init__(config)"
        },
        {
            "sha": "5183a3c22fafc8bc0e91fd6659684618f8167eb2",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -1284,6 +1284,9 @@ def forward(\n class InstructBlipVideoForConditionalGeneration(InstructBlipVideoPreTrainedModel, GenerationMixin):\n     config_class = InstructBlipVideoConfig\n     main_input_name = \"pixel_values\"\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n     def __init__(self, config: InstructBlipVideoConfig):\n         super().__init__(config)"
        },
        {
            "sha": "f476591b2eb6ff50b1e47cb7e22ca8b7d804466a",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -37,8 +37,6 @@ class LlavaConfig(PretrainedConfig):\n             The config object or dictionary of the vision backbone.\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n             The config object or dictionary of the text backbone.\n-        ignore_index (`int`, *optional*, defaults to -100):\n-            The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 32000):\n             The image token index to encode the image prompt.\n         projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n@@ -83,7 +81,6 @@ def __init__(\n         self,\n         vision_config=None,\n         text_config=None,\n-        ignore_index=-100,\n         image_token_index=32000,\n         projector_hidden_act=\"gelu\",\n         vision_feature_select_strategy=\"default\",\n@@ -92,7 +89,6 @@ def __init__(\n         multimodal_projector_bias=True,\n         **kwargs,\n     ):\n-        self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.projector_hidden_act = projector_hidden_act\n         self.image_seq_length = image_seq_length"
        },
        {
            "sha": "610ab417d92bce35b91028fc27fbde9307b1ff23",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 8,
            "deletions": 88,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -28,6 +28,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -136,6 +137,8 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Llava isn't meant for training from scratch - only\n@@ -321,89 +324,6 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n-    def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, input_ids, attention_mask, labels):\n-        num_images, num_image_patches, embed_dim = image_features.shape\n-        batch_size, sequence_length = input_ids.shape\n-        left_padding = not torch.sum(input_ids[:, -1] == torch.tensor(self.pad_token_id))\n-        # 1. Create a mask to know where special image tokens are\n-        special_image_token_mask = input_ids == self.config.image_token_index\n-        num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n-        # Compute the maximum embed dimension\n-        max_embed_dim = (num_special_image_tokens.max() * (num_image_patches - 1)) + sequence_length\n-        batch_indices, non_image_indices = torch.where(input_ids != self.config.image_token_index)\n-\n-        # 2. Compute the positions where text should be written\n-        # Calculate new positions for text tokens in merged image-text sequence.\n-        # `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images - 1` text tokens.\n-        # `torch.cumsum` computes how each image token shifts subsequent text token positions.\n-        # - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.\n-        new_token_positions = torch.cumsum((special_image_token_mask * (num_image_patches - 1) + 1), -1) - 1\n-        nb_image_pad = max_embed_dim - 1 - new_token_positions[:, -1]\n-        if left_padding:\n-            new_token_positions += nb_image_pad[:, None]  # offset for left padding\n-        text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n-\n-        # 3. Create the full embedding, already padded to the maximum position\n-        final_embedding = torch.zeros(\n-            batch_size, max_embed_dim, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device\n-        )\n-        final_attention_mask = torch.zeros(\n-            batch_size, max_embed_dim, dtype=attention_mask.dtype, device=inputs_embeds.device\n-        )\n-        if labels is not None:\n-            final_labels = torch.full(\n-                (batch_size, max_embed_dim), self.config.ignore_index, dtype=input_ids.dtype, device=input_ids.device\n-            )\n-        # In case the Vision model or the Language model has been offloaded to CPU, we need to manually\n-        # set the corresponding tensors into their correct target device.\n-        target_device = inputs_embeds.device\n-        batch_indices, non_image_indices, text_to_overwrite = (\n-            batch_indices.to(target_device),\n-            non_image_indices.to(target_device),\n-            text_to_overwrite.to(target_device),\n-        )\n-        attention_mask = attention_mask.to(target_device)\n-\n-        # 4. Fill the embeddings based on the mask. If we have [\"hey\" \"<image>\", \"how\", \"are\"]\n-        # we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features\n-        final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_image_indices]\n-        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_image_indices]\n-        if labels is not None:\n-            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_image_indices]\n-\n-        # 5. Fill the embeddings corresponding to the images. Anything that is not `text_positions` needs filling (#29835)\n-        image_to_overwrite = torch.full(\n-            (batch_size, max_embed_dim), True, dtype=torch.bool, device=inputs_embeds.device\n-        )\n-        image_to_overwrite[batch_indices, text_to_overwrite] = False\n-        if left_padding:\n-            image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n-        else:\n-            mask = torch.ones_like(image_to_overwrite, dtype=torch.bool).cumsum(-1) - 1\n-            padding_mask = mask <= new_token_positions[:, -1:].to(target_device)\n-            image_to_overwrite &= padding_mask\n-\n-        if image_to_overwrite.sum() != image_features.shape[:-1].numel():\n-            raise ValueError(\n-                f\"The input provided to the model are wrong. The number of image tokens is {torch.sum(special_image_token_mask)} while\"\n-                f\" the number of image given to the model is {num_images}. This prevents correct indexing and breaks batch generation.\"\n-            )\n-\n-        final_embedding[image_to_overwrite] = image_features.contiguous().reshape(-1, embed_dim).to(target_device)\n-        final_attention_mask |= image_to_overwrite\n-        position_ids = (final_attention_mask.cumsum(-1) - 1).masked_fill_((final_attention_mask == 0), 1)\n-\n-        # 6. Mask out the embedding at padding positions, as we later use the past_key_value value to determine the non-attended tokens.\n-        batch_indices, pad_indices = torch.where(input_ids == self.pad_token_id)\n-        indices_to_mask = new_token_positions[batch_indices, pad_indices]\n-\n-        final_embedding[batch_indices, indices_to_mask] = 0\n-\n-        if labels is None:\n-            final_labels = None\n-\n-        return final_embedding, final_attention_mask, final_labels, position_ids\n-\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(LLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -499,14 +419,14 @@ def forward(\n                 image_sizes=image_sizes,\n             )\n \n-            n_image_tokens = (input_ids == self.config.image_token_index).sum()\n-            n_image_features = image_features.shape[0] * image_features.shape[1]\n-            if n_image_tokens != n_image_features:\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n "
        },
        {
            "sha": "3836dbf71cd29335955e5a0d261504b7b3058cea",
            "filename": "src/transformers/models/llava_next/configuration_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -36,8 +36,6 @@ class LlavaNextConfig(PretrainedConfig):\n             The config object or dictionary of the vision backbone.\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n             The config object or dictionary of the text backbone.\n-        ignore_index (`int`, *optional*, defaults to -100):\n-            The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 32000):\n             The image token index to encode the image prompt.\n         projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n@@ -88,7 +86,6 @@ def __init__(\n         self,\n         vision_config=None,\n         text_config=None,\n-        ignore_index=-100,\n         image_token_index=32000,\n         projector_hidden_act=\"gelu\",\n         vision_feature_select_strategy=\"default\",\n@@ -99,7 +96,6 @@ def __init__(\n         multimodal_projector_bias=True,\n         **kwargs,\n     ):\n-        self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.projector_hidden_act = projector_hidden_act\n         self.image_seq_length = image_seq_length"
        },
        {
            "sha": "3cdf1b348404f360568aa048a310bd5053f9a676",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 8,
            "deletions": 244,
            "changes": 252,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -31,6 +31,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -245,6 +246,8 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         # important: this ported version of LlavaNext isn't meant for training from scratch - only\n@@ -405,245 +408,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def _merge_input_ids_with_image_features(\n-        self,\n-        image_features,\n-        feature_lens,\n-        inputs_embeds,\n-        input_ids,\n-        attention_mask,\n-        position_ids=None,\n-        labels=None,\n-        image_token_index=None,\n-        ignore_index=-100,\n-    ):\n-        \"\"\"\n-        Merge input_ids with with image features into final embeddings\n-\n-        Args:\n-            image_features (`torch.Tensor` of shape `(all_feature_lens, embed_dim)`):\n-                All vision vectors of all images in the batch\n-            feature_lens (`torch.LongTensor` of shape `(num_images)`):\n-                The length of visual embeddings of each image as stacked in `image_features`\n-            inputs_embeds (`torch.Tensor` of shape `(batch_size, sequence_length, embed_dim)`):\n-                Token embeddings before merging with visual embeddings\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Input_ids of tokens, possibly filled with image token\n-            attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Mask to avoid performing attention on padding token indices.\n-            position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-                config.n_positions - 1]`.\n-            labels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*)\n-                :abels need to be recalculated to support training (if provided)\n-            image_token_index (`int`, *optional*)\n-                Token id used to indicate the special \"image\" token. Defaults to `config.image_token_index`\n-            ignore_index (`int`, *optional*)\n-                Value that is used to pad `labels` and will be ignored when calculated loss. Default: -100.\n-        Returns:\n-            final_embedding, final_attention_mask, position_ids, final_labels\n-\n-        Explanation:\n-            each image has variable length embeddings, with length specified by feature_lens\n-            image_features is concatenation of all visual embed vectors\n-            task: fill each <image> with the correct number of visual embeddings\n-            Example:\n-                X (5 patches), Y (3 patches), Z (8)\n-                X, Y are in the same sequence (in-context learning)\n-            if right padding\n-                input_ids: [\n-                    a b c d e f X g h i j k Y l m\n-                    o p q r Z s t u v _ _ _ _ _ _\n-                ]\n-                input_ids should be: [\n-                    a b c d e f X X X X X g h i j k Y Y Y l m\n-                    o p q r Z Z Z Z Z Z Z Z s t u v _ _ _ _ _\n-                ]\n-                labels should be: [\n-                    a b c d e f _ _ _ _ _ g h i j k _ _ _ l m\n-                    o p q r _ _ _ _ _ _ _ _ s t u v _ _ _ _ _\n-                ]\n-            elif left padding\n-                input_ids: [\n-                    a b c d e f X g h i j k Y l m\n-                    _ _ _ _ _ _ o p q r Z s t u v\n-                ]\n-                input_ids should be: [\n-                    a b c d e f X X X X X g h i j k Y Y Y l m\n-                    _ _ _ _ _ o p q r Z Z Z Z Z Z Z Z s t u v\n-                ]\n-                labels should be: [\n-                    a b c d e f _ _ _ _ _ g h i j k _ _ _ l m\n-                    _ _ _ _ _ o p q r _ _ _ _ _ _ _ _ s t u v\n-                ]\n-            Edge cases:\n-                * If tokens are same but image token sizes are different, then cannot infer left or right padding\n-                ```python\n-                cat_img = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n-                chart_img = Image.open(requests.get(\"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\", stream=True).raw)\n-                prompts = [\n-                    \"[INST] <image>\\nWhat is shown in this image? [/INST]\",\n-                    \"[INST] <image>\\nWhat is shown in this image? [/INST]\",\n-                ]\n-                inputs = processor(prompts, [chart_img, cat_img], return_tensors='pt', padding=True).to(\"cuda\")\n-                    chart_img has 2634 tokens, while cat_img has 2340 tokens\n-                ```\n-\n-                input_ids: [\n-                    a b c d X g h\n-                    i j Y k l m n\n-                ]\n-                where X is 3 tokens while Y is 5, this mean after merge\n-                if left-padding (batched generation)\n-                    input_ids should be: [\n-                        _ _ a b c d X X X g h\n-                        i j Y Y Y Y Y k l m n\n-                    ]\n-                elif (right padding) (training)\n-                    input_ids should be: [\n-                        a b c d X X X g h _ _\n-                        i j Y Y Y Y Y k l m n\n-                    ]\n-        \"\"\"\n-        image_token_index = image_token_index if image_token_index is not None else self.config.image_token_index\n-        ignore_index = ignore_index if ignore_index is not None else self.config.ignore_index\n-\n-        if self.training and self.padding_side == \"left\":\n-            logger.warning_once(\n-                \"Padding side is set to 'left' but the model is in training mode. For training \"\n-                \"it is recommended to set `model.padding_side='right' and `processor.tokenizer.padding_side='right'`. \"\n-                \"If that's intended, ignore this warning\"\n-            )\n-        if not self.training and self.padding_side == \"right\":\n-            logger.warning_once(\n-                \"Padding side is set to 'right' but the model is in inference mode. For correct \"\n-                \"generation results, please set `model.padding_side='left'` and `processor.tokenizer.padding_side='left'`. \"\n-                \"If that's intended, ignore this warning\"\n-            )\n-\n-        with torch.no_grad():\n-            # ! in llava 1.6, number of patches is variable\n-            num_images = feature_lens.size(0)\n-            num_image_features, embed_dim = image_features.shape\n-            if feature_lens.sum() != num_image_features:\n-                raise ValueError(f\"{feature_lens=} / {feature_lens.sum()} != {image_features.shape=}\")\n-            batch_size = input_ids.shape[0]\n-            _left_padding = torch.any(attention_mask[:, 0] == 0)\n-            _right_padding = torch.any(attention_mask[:, -1] == 0)\n-\n-            left_padding = self.padding_side == \"left\"\n-            if batch_size > 1:\n-                if _left_padding and _right_padding:\n-                    raise ValueError(f\"both side of attention_mask has zero, invalid. {attention_mask}\")\n-                elif _right_padding and left_padding:\n-                    left_padding = False\n-                elif _left_padding and not left_padding:\n-                    left_padding = True\n-\n-            # Whether to turn off right padding\n-            # 1. Create a mask to know where special image tokens are\n-            special_image_token_mask = input_ids == image_token_index\n-            # special_image_token_mask: [bsz, seqlen]\n-            num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n-            # num_special_image_tokens: [bsz]\n-            # Reserve for padding of num_images\n-            total_num_special_image_tokens = torch.sum(special_image_token_mask)\n-            if total_num_special_image_tokens != num_images:\n-                raise ValueError(\n-                    f\"Number of image tokens in input_ids ({total_num_special_image_tokens}) different from num_images ({num_images}).\"\n-                )\n-            # Compute the maximum embed dimension\n-            # max_image_feature_lens is max_feature_lens per batch\n-            feature_lens = feature_lens.to(input_ids.device)\n-            feature_lens_batch = feature_lens.split(num_special_image_tokens.tolist(), dim=0)\n-            feature_lens_batch_sum = torch.tensor([x.sum() for x in feature_lens_batch], device=input_ids.device)\n-            embed_sequence_lengths = (\n-                (attention_mask == 1).long().sum(-1) - num_special_image_tokens + feature_lens_batch_sum\n-            )\n-            max_embed_dim = embed_sequence_lengths.max()\n-\n-            batch_indices, non_image_indices = torch.where((input_ids != image_token_index) & (attention_mask == 1))\n-            # 2. Compute the positions where text should be written\n-            # Calculate new positions for text tokens in merged image-text sequence.\n-            # `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images` text tokens.\n-            # `torch.cumsum` computes how each image token shifts subsequent text token positions.\n-            # - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.\n-            # ! instead of special_image_token_mask * (num_image_patches - 1)\n-            #   special_image_token_mask * (num_feature_len - 1)\n-            special_image_token_mask = special_image_token_mask.long()\n-            special_image_token_mask[special_image_token_mask == 1] = feature_lens - 1\n-            new_token_positions = torch.cumsum((special_image_token_mask + 1), -1) - 1\n-            if left_padding:\n-                # shift right token positions so that they are ending at the same number\n-                # the below here was incorrect? new_token_positions += new_token_positions[:, -1].max() - new_token_positions[:, -1:]\n-                new_token_positions += max_embed_dim - 1 - new_token_positions[:, -1:]\n-\n-            text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n-\n-        # 3. Create the full embedding, already padded to the maximum position\n-        final_embedding = torch.zeros(\n-            batch_size, max_embed_dim, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device\n-        )\n-        final_attention_mask = torch.zeros(\n-            batch_size, max_embed_dim, dtype=attention_mask.dtype, device=inputs_embeds.device\n-        )\n-        final_input_ids = torch.full(\n-            (batch_size, max_embed_dim), self.pad_token_id, dtype=input_ids.dtype, device=inputs_embeds.device\n-        )\n-        # In case the Vision model or the Language model has been offloaded to CPU, we need to manually\n-        # set the corresponding tensors into their correct target device.\n-        target_device = inputs_embeds.device\n-        batch_indices, non_image_indices, text_to_overwrite = (\n-            batch_indices.to(target_device),\n-            non_image_indices.to(target_device),\n-            text_to_overwrite.to(target_device),\n-        )\n-        attention_mask = attention_mask.to(target_device)\n-        input_ids = input_ids.to(target_device)\n-\n-        # 4. Fill the embeddings based on the mask. If we have [\"hey\" \"<image>\", \"how\", \"are\"]\n-        # we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features\n-        final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_image_indices]\n-        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_image_indices]\n-        final_input_ids[batch_indices, text_to_overwrite] = input_ids[batch_indices, non_image_indices]\n-        final_labels = None\n-        if labels is not None:\n-            labels = labels.to(target_device)\n-            final_labels = torch.full_like(final_attention_mask, ignore_index).to(torch.long)\n-            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_image_indices]\n-\n-        # 5. Fill the embeddings corresponding to the images. Anything that is not `text_positions` needs filling (#29835)\n-        with torch.no_grad():\n-            image_to_overwrite = torch.full(\n-                (batch_size, max_embed_dim), True, dtype=torch.bool, device=inputs_embeds.device\n-            )\n-            image_to_overwrite[batch_indices, text_to_overwrite] = False\n-            embed_indices = torch.arange(max_embed_dim).unsqueeze(0).to(target_device)\n-            embed_indices = embed_indices.expand(batch_size, max_embed_dim)\n-            embed_seq_lens = embed_sequence_lengths[:, None].to(target_device)\n-\n-            if left_padding:\n-                # exclude padding on the left\n-                max_embed_dim = max_embed_dim.to(target_device)\n-                val = (max_embed_dim - embed_indices) <= embed_seq_lens\n-            else:\n-                # exclude padding on the right\n-                val = embed_indices < embed_seq_lens\n-            image_to_overwrite &= val\n-\n-            if image_to_overwrite.sum() != num_image_features:\n-                raise ValueError(\n-                    f\"{image_to_overwrite.sum()=} != {num_image_features=} The input provided to the model are wrong. \"\n-                    f\"The number of image tokens is {torch.sum(special_image_token_mask)} while\"\n-                    f\" the number of image given to the model is {num_images}. \"\n-                    f\"This prevents correct indexing and breaks batch generation.\"\n-                )\n-        final_embedding[image_to_overwrite] = image_features.contiguous().reshape(-1, embed_dim).to(target_device)\n-        final_attention_mask |= image_to_overwrite\n-        position_ids = (final_attention_mask.cumsum(-1) - 1).masked_fill_((final_attention_mask == 0), 1)\n-\n-        return final_embedding, final_attention_mask, position_ids, final_labels, final_input_ids\n-\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -875,14 +639,14 @@ def forward(\n                 image_newline=self.image_newline,\n             )\n \n-            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-            n_image_features = image_features.shape[0]\n-            if n_image_tokens != n_image_features:\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n "
        },
        {
            "sha": "01450f6b587c8ebcf2fec108a45934a5c25612d8",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -38,8 +38,6 @@ class LlavaNextVideoConfig(PretrainedConfig):\n             The config object or dictionary of the vision backbone.\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n             The config object or dictionary of the text backbone.\n-        ignore_index (`int`, *optional*, defaults to -100):\n-            The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 32001):\n             The image token index to encode the image prompt.\n         projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n@@ -96,7 +94,6 @@ def __init__(\n         self,\n         vision_config=None,\n         text_config=None,\n-        ignore_index=-100,\n         image_token_index=32001,\n         projector_hidden_act=\"gelu\",\n         multimodal_projector_bias=True,\n@@ -116,7 +113,6 @@ def __init__(\n         self.spatial_pool_stride = spatial_pool_stride\n         self.image_seq_length = image_seq_length\n         self.video_seq_length = video_seq_length\n-        self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.projector_hidden_act = projector_hidden_act\n         self.multimodal_projector_bias = multimodal_projector_bias"
        },
        {
            "sha": "9ce88c541231dc828e56e71bf089b2775d44e499",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 19,
            "deletions": 250,
            "changes": 269,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -32,7 +32,13 @@\n from ...image_processing_utils import select_best_resolution\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n+    logging,\n+    replace_return_docstrings,\n+)\n from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava_next_video import LlavaNextVideoConfig\n@@ -153,6 +159,8 @@ class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         # important: this ported version of LlavaNextVideo isn't meant for training from scratch - only\n@@ -440,245 +448,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def _merge_input_ids_with_image_features(\n-        self,\n-        image_features,\n-        feature_lens,\n-        inputs_embeds,\n-        input_ids,\n-        attention_mask,\n-        position_ids=None,\n-        labels=None,\n-        image_token_index=None,\n-        ignore_index=-100,\n-    ):\n-        \"\"\"\n-        Merge input_ids with with image features into final embeddings\n-\n-        Args:\n-            image_features (`torch.Tensor` of shape `(all_feature_lens, embed_dim)`):\n-                All vision vectors of all images in the batch\n-            feature_lens (`torch.LongTensor` of shape `(num_images)`):\n-                The length of visual embeddings of each image as stacked in `image_features`\n-            inputs_embeds (`torch.Tensor` of shape `(batch_size, sequence_length, embed_dim)`):\n-                Token embeddings before merging with visual embeddings\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Input_ids of tokens, possibly filled with image token\n-            attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Mask to avoid performing attention on padding token indices.\n-            position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-                config.n_positions - 1]`.\n-            labels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*)\n-                :abels need to be recalculated to support training (if provided)\n-            image_token_index (`int`, *optional*)\n-                Token id used to indicate the special \"image\" token. Defaults to `config.image_token_index`\n-            ignore_index (`int`, *optional*)\n-                Value that is used to pad `labels` and will be ignored when calculated loss. Default: -100.\n-        Returns:\n-            final_embedding, final_attention_mask, position_ids, final_labels\n-\n-        Explanation:\n-            each image has variable length embeddings, with length specified by feature_lens\n-            image_features is concatenation of all visual embed vectors\n-            task: fill each <image> with the correct number of visual embeddings\n-            Example:\n-                X (5 patches), Y (3 patches), Z (8)\n-                X, Y are in the same sequence (in-context learning)\n-            if right padding\n-                input_ids: [\n-                    a b c d e f X g h i j k Y l m\n-                    o p q r Z s t u v _ _ _ _ _ _\n-                ]\n-                input_ids should be: [\n-                    a b c d e f X X X X X g h i j k Y Y Y l m\n-                    o p q r Z Z Z Z Z Z Z Z s t u v _ _ _ _ _\n-                ]\n-                labels should be: [\n-                    a b c d e f _ _ _ _ _ g h i j k _ _ _ l m\n-                    o p q r _ _ _ _ _ _ _ _ s t u v _ _ _ _ _\n-                ]\n-            elif left padding\n-                input_ids: [\n-                    a b c d e f X g h i j k Y l m\n-                    _ _ _ _ _ _ o p q r Z s t u v\n-                ]\n-                input_ids should be: [\n-                    a b c d e f X X X X X g h i j k Y Y Y l m\n-                    _ _ _ _ _ o p q r Z Z Z Z Z Z Z Z s t u v\n-                ]\n-                labels should be: [\n-                    a b c d e f _ _ _ _ _ g h i j k _ _ _ l m\n-                    _ _ _ _ _ o p q r _ _ _ _ _ _ _ _ s t u v\n-                ]\n-            Edge cases:\n-                * If tokens are same but image token sizes are different, then cannot infer left or right padding\n-                ```python\n-                cat_img = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n-                chart_img = Image.open(requests.get(\"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\", stream=True).raw)\n-                prompts = [\n-                    \"[INST] <image>\\nWhat is shown in this image? [/INST]\",\n-                    \"[INST] <image>\\nWhat is shown in this image? [/INST]\",\n-                ]\n-                inputs = processor(prompts, [chart_img, cat_img], return_tensors='pt', padding=True).to(\"cuda\")\n-                    chart_img has 2634 tokens, while cat_img has 2340 tokens\n-                ```\n-\n-                input_ids: [\n-                    a b c d X g h\n-                    i j Y k l m n\n-                ]\n-                where X is 3 tokens while Y is 5, this mean after merge\n-                if left-padding (batched generation)\n-                    input_ids should be: [\n-                        _ _ a b c d X X X g h\n-                        i j Y Y Y Y Y k l m n\n-                    ]\n-                elif (right padding) (training)\n-                    input_ids should be: [\n-                        a b c d X X X g h _ _\n-                        i j Y Y Y Y Y k l m n\n-                    ]\n-        \"\"\"\n-        image_token_index = image_token_index if image_token_index is not None else self.config.image_token_index\n-        ignore_index = ignore_index if ignore_index is not None else self.config.ignore_index\n-\n-        if self.training and self.padding_side == \"left\":\n-            logger.warning_once(\n-                \"Padding side is set to 'left' but the model is in training mode. For training \"\n-                \"it is recommended to set `model.padding_side='right' and `processor.tokenizer.padding_side='right'`. \"\n-                \"If that's intended, ignore this warning\"\n-            )\n-        if not self.training and self.padding_side == \"right\":\n-            logger.warning_once(\n-                \"Padding side is set to 'right' but the model is in inference mode. For correct \"\n-                \"generation results, please set `model.padding_side='left'` and `processor.tokenizer.padding_side='left'`. \"\n-                \"If that's intended, ignore this warning\"\n-            )\n-\n-        with torch.no_grad():\n-            # ! in llava 1.6, number of patches is variable\n-            num_images = feature_lens.size(0)\n-            num_image_features, embed_dim = image_features.shape\n-            if feature_lens.sum() != num_image_features:\n-                raise ValueError(f\"{feature_lens=} / {feature_lens.sum()} != {image_features.shape=}\")\n-            batch_size = input_ids.shape[0]\n-            _left_padding = torch.any(attention_mask[:, 0] == 0)\n-            _right_padding = torch.any(attention_mask[:, -1] == 0)\n-\n-            left_padding = self.padding_side == \"left\"\n-            if batch_size > 1:\n-                if _left_padding and _right_padding:\n-                    raise ValueError(f\"both side of attention_mask has zero, invalid. {attention_mask}\")\n-                elif _right_padding and left_padding:\n-                    left_padding = False\n-                elif _left_padding and not left_padding:\n-                    left_padding = True\n-\n-            # Whether to turn off right padding\n-            # 1. Create a mask to know where special image tokens are\n-            special_image_token_mask = input_ids == image_token_index\n-            # special_image_token_mask: [bsz, seqlen]\n-            num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n-            # num_special_image_tokens: [bsz]\n-            # Reserve for padding of num_images\n-            total_num_special_image_tokens = torch.sum(special_image_token_mask)\n-            if total_num_special_image_tokens != num_images:\n-                raise ValueError(\n-                    f\"Number of image tokens in input_ids ({total_num_special_image_tokens}) different from num_images ({num_images}).\"\n-                )\n-            # Compute the maximum embed dimension\n-            # max_image_feature_lens is max_feature_lens per batch\n-            feature_lens = feature_lens.to(input_ids.device)\n-            feature_lens_batch = feature_lens.split(num_special_image_tokens.tolist(), dim=0)\n-            feature_lens_batch_sum = torch.tensor([x.sum() for x in feature_lens_batch], device=input_ids.device)\n-            embed_sequence_lengths = (\n-                (attention_mask == 1).long().sum(-1) - num_special_image_tokens + feature_lens_batch_sum\n-            )\n-            max_embed_dim = embed_sequence_lengths.max()\n-\n-            batch_indices, non_image_indices = torch.where((input_ids != image_token_index) & (attention_mask == 1))\n-            # 2. Compute the positions where text should be written\n-            # Calculate new positions for text tokens in merged image-text sequence.\n-            # `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images` text tokens.\n-            # `torch.cumsum` computes how each image token shifts subsequent text token positions.\n-            # - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.\n-            # ! instead of special_image_token_mask * (num_image_patches - 1)\n-            #   special_image_token_mask * (num_feature_len - 1)\n-            special_image_token_mask = special_image_token_mask.long()\n-            special_image_token_mask[special_image_token_mask == 1] = feature_lens - 1\n-            new_token_positions = torch.cumsum((special_image_token_mask + 1), -1) - 1\n-            if left_padding:\n-                # shift right token positions so that they are ending at the same number\n-                # the below here was incorrect? new_token_positions += new_token_positions[:, -1].max() - new_token_positions[:, -1:]\n-                new_token_positions += max_embed_dim - 1 - new_token_positions[:, -1:]\n-\n-            text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n-\n-        # 3. Create the full embedding, already padded to the maximum position\n-        final_embedding = torch.zeros(\n-            batch_size, max_embed_dim, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device\n-        )\n-        final_attention_mask = torch.zeros(\n-            batch_size, max_embed_dim, dtype=attention_mask.dtype, device=inputs_embeds.device\n-        )\n-        final_input_ids = torch.full(\n-            (batch_size, max_embed_dim), self.pad_token_id, dtype=input_ids.dtype, device=inputs_embeds.device\n-        )\n-        # In case the Vision model or the Language model has been offloaded to CPU, we need to manually\n-        # set the corresponding tensors into their correct target device.\n-        target_device = inputs_embeds.device\n-        batch_indices, non_image_indices, text_to_overwrite = (\n-            batch_indices.to(target_device),\n-            non_image_indices.to(target_device),\n-            text_to_overwrite.to(target_device),\n-        )\n-        attention_mask = attention_mask.to(target_device)\n-        input_ids = input_ids.to(target_device)\n-\n-        # 4. Fill the embeddings based on the mask. If we have [\"hey\" \"<image>\", \"how\", \"are\"]\n-        # we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features\n-        final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_image_indices]\n-        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_image_indices]\n-        final_input_ids[batch_indices, text_to_overwrite] = input_ids[batch_indices, non_image_indices]\n-        final_labels = None\n-        if labels is not None:\n-            labels = labels.to(target_device)\n-            final_labels = torch.full_like(final_attention_mask, ignore_index).to(torch.long)\n-            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_image_indices]\n-\n-        # 5. Fill the embeddings corresponding to the images. Anything that is not `text_positions` needs filling (#29835)\n-        with torch.no_grad():\n-            image_to_overwrite = torch.full(\n-                (batch_size, max_embed_dim), True, dtype=torch.bool, device=inputs_embeds.device\n-            )\n-            image_to_overwrite[batch_indices, text_to_overwrite] = False\n-            embed_indices = torch.arange(max_embed_dim).unsqueeze(0).to(target_device)\n-            embed_indices = embed_indices.expand(batch_size, max_embed_dim)\n-            embed_seq_lens = embed_sequence_lengths[:, None].to(target_device)\n-\n-            if left_padding:\n-                # exclude padding on the left\n-                max_embed_dim = max_embed_dim.to(target_device)\n-                val = (max_embed_dim - embed_indices) <= embed_seq_lens\n-            else:\n-                # exclude padding on the right\n-                val = embed_indices < embed_seq_lens\n-            image_to_overwrite &= val\n-\n-            if image_to_overwrite.sum() != num_image_features:\n-                raise ValueError(\n-                    f\"{image_to_overwrite.sum()=} != {num_image_features=} The input provided to the model are wrong. \"\n-                    f\"The number of image tokens is {torch.sum(special_image_token_mask)} while\"\n-                    f\" the number of image given to the model is {num_images}. \"\n-                    f\"This prevents correct indexing and breaks batch generation.\"\n-                )\n-        final_embedding[image_to_overwrite] = image_features.contiguous().reshape(-1, embed_dim).to(target_device)\n-        final_attention_mask |= image_to_overwrite\n-        position_ids = (final_attention_mask.cumsum(-1) - 1).masked_fill_((final_attention_mask == 0), 1)\n-\n-        return final_embedding, final_attention_mask, position_ids, final_labels, final_input_ids\n-\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -948,14 +717,14 @@ def forward(\n                 image_newline=self.image_newline,\n             )\n \n-            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-            n_image_features = image_features.shape[0]\n-            if n_image_tokens != n_image_features:\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n@@ -970,14 +739,14 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-            n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n-            n_video_features = video_features.shape[0]\n-            if n_video_tokens != n_video_features:\n+            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n+                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+                n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n "
        },
        {
            "sha": "8769f8db4131f59c8a5cf913f96ecc071c186454",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 11,
            "deletions": 14,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -30,6 +30,7 @@\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import (\n+    is_torchdynamo_compiling,\n     logging,\n )\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -52,8 +53,6 @@ class LlavaNextVideoConfig(PretrainedConfig):\n             The config object or dictionary of the vision backbone.\n         text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `LlamaConfig`):\n             The config object or dictionary of the text backbone.\n-        ignore_index (`int`, *optional*, defaults to -100):\n-            The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 32001):\n             The image token index to encode the image prompt.\n         projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n@@ -110,7 +109,6 @@ def __init__(\n         self,\n         vision_config=None,\n         text_config=None,\n-        ignore_index=-100,\n         image_token_index=32001,\n         projector_hidden_act=\"gelu\",\n         multimodal_projector_bias=True,\n@@ -130,7 +128,6 @@ def __init__(\n         self.spatial_pool_stride = spatial_pool_stride\n         self.image_seq_length = image_seq_length\n         self.video_seq_length = video_seq_length\n-        self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.projector_hidden_act = projector_hidden_act\n         self.multimodal_projector_bias = multimodal_projector_bias\n@@ -479,14 +476,14 @@ def forward(\n                 image_newline=self.image_newline,\n             )\n \n-            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-            n_image_features = image_features.shape[0]\n-            if n_image_tokens != n_image_features:\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n@@ -501,14 +498,14 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-            n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n-            n_video_features = video_features.shape[0]\n-            if n_video_tokens != n_video_features:\n+            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n+                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+                n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n "
        },
        {
            "sha": "e86ce394e13de4dc7afd420236c36ad890458cca",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 12,
            "deletions": 19,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -30,6 +30,7 @@\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n+    is_torchdynamo_compiling,\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n@@ -250,7 +251,7 @@ class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_cache_class = True\n-    _supports_static_cache = False  # Qwen2 doesn't but llava has no reasons to not support\n+    _supports_static_cache = True\n     _supports_quantized_cache = True\n     _supports_sdpa = True\n \n@@ -712,19 +713,15 @@ def forward(\n                 image_newline=self.image_newline,\n                 vision_aspect_ratio=vision_aspect_ratio,\n             )\n-            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-            n_image_features = image_features.shape[0]\n \n-            if n_image_tokens != n_image_features:\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (\n-                (input_ids == self.config.image_token_index)\n-                .unsqueeze(-1)\n-                .expand_as(inputs_embeds)\n-                .to(inputs_embeds.device)\n-            )\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n@@ -741,18 +738,14 @@ def forward(\n             video_features = torch.cat((video_features, image_newline), dim=1)\n             video_features = video_features.flatten(0, 1)\n \n-            n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n-            n_video_features = video_features.shape[0]\n-            if n_video_tokens != n_video_features:\n+            special_video_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n+                n_video_tokens = (input_ids == self.config.video_token_index).sum()\n+                n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                 )\n-            special_video_mask = (\n-                (input_ids == self.config.video_token_index)\n-                .unsqueeze(-1)\n-                .expand_as(inputs_embeds)\n-                .to(inputs_embeds.device)\n-            )\n             video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n "
        },
        {
            "sha": "f1f1ef1821c77eb1a92825cc8600f028cee0a345",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 251,
            "deletions": 167,
            "changes": 418,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -22,10 +22,10 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n-    _prepare_4d_causal_attention_mask,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n+    AttentionMaskConverter,\n )\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -98,6 +98,7 @@ class OPTAttention(nn.Module):\n     def __init__(\n         self,\n         config: OPTConfig,\n+        layer_idx: int = None,\n         **kwargs,\n     ):\n         super().__init__()\n@@ -106,6 +107,13 @@ def __init__(\n         self.num_heads = config.num_attention_heads\n         self.dropout = config.attention_dropout\n         self.enable_bias = config.enable_bias\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n \n         self.head_dim = self.embed_dim // self.num_heads\n         self.is_causal = True\n@@ -122,9 +130,6 @@ def __init__(\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n \n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int) -> torch.Tensor:\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -134,92 +139,53 @@ def forward(\n         output_attentions: bool = False,\n         # isn't needed in normal attention, but needed in flash attention so to keep the signature same\n         position_ids: Optional[torch.Tensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        cache_position: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         bsz, tgt_len, _ = hidden_states.size()\n \n         # get query proj\n         query_states = self.q_proj(hidden_states) * self.scaling\n-        # get key, value proj\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-        if past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n-\n-        past_key_value = (key_states, value_states)\n+        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n-        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n-        key_states = key_states.view(*proj_shape)\n-        value_states = value_states.view(*proj_shape)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        src_len = key_states.size(1)\n-        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n-\n-        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n-            raise ValueError(\n-                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n-                f\" {attn_weights.size()}\"\n+        if past_key_value is not None:\n+            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+            key_states, value_states = past_key_value.update(\n+                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n \n+        attn_weights = torch.matmul(query_states, key_states.transpose(3, 2))\n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n-                )\n-            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n-            attn_weights = torch.max(\n-                attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min, device=attn_weights.device)\n-            )\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            attn_weights = attn_weights + causal_mask\n \n         # upcast to fp32 if the weights are in fp16. Please see https://github.com/huggingface/transformers/pull/17437\n-        if attn_weights.dtype == torch.float16:\n-            attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)\n-        else:\n-            attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n \n         if layer_head_mask is not None:\n             if layer_head_mask.size() != (self.num_heads,):\n                 raise ValueError(\n                     f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is\"\n                     f\" {layer_head_mask.size()}\"\n                 )\n-            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n-\n-        if output_attentions:\n-            # this operation is a bit awkward, but it's required to\n-            # make sure that attn_weights keeps its gradient.\n-            # In order to do so, attn_weights have to be reshaped\n-            # twice and have to be reused in the following\n-            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n-            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n-        else:\n-            attn_weights_reshaped = None\n+            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights\n \n         attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+        attn_output = torch.matmul(attn_probs, value_states)\n \n-        attn_output = torch.bmm(attn_probs, value_states)\n-\n-        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n-        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n \n         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n         # partitioned aross GPUs when using tensor-parallelism.\n         attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n-\n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, attn_weights_reshaped, past_key_value\n+        return attn_output, attn_probs, past_key_value\n \n \n class OptFlashAttention2(OPTAttention):\n@@ -245,33 +211,33 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         position_ids: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-        bsz, _, _ = hidden_states.size()\n \n-        # get query proj\n-        query_states = self.q_proj(hidden_states)\n-        # get key, value proj\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-        if past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+        bsz, query_length, _ = hidden_states.size()\n \n-        past_key_value = (key_states, value_states)\n+        query_states = self.q_proj(hidden_states)\n+        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim)\n \n-        query_length = query_states.shape[1]\n-        tgt_len = key_states.shape[-2]\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        query_states = query_states.view(bsz, query_length, self.num_heads, self.head_dim)\n-        key_states = key_states.transpose(1, 2).view(bsz, tgt_len, self.num_heads, self.head_dim)\n-        value_states = value_states.transpose(1, 2).view(bsz, tgt_len, self.num_heads, self.head_dim)\n+        if past_key_value is not None:\n+            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+            key_states, value_states = past_key_value.update(\n+                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+            )\n \n         attn_dropout = self.dropout if self.training else 0.0\n \n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in float16 just to be sure everything works as expected.\n@@ -331,6 +297,7 @@ def forward(\n         layer_head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         position_ids: Optional[torch.Tensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions or layer_head_mask is not None:\n             logger.warning_once(\n@@ -344,24 +311,24 @@ def forward(\n                 layer_head_mask=layer_head_mask,\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n-            )  # TODO after merge add position_ids=position_ids\n+                cache_position=cache_position,\n+            )\n \n         bsz, q_len, _ = hidden_states.size()\n \n-        query_states = self.q_proj(hidden_states) * self.scaling\n-        query_states = self._shape(query_states, -1, bsz)\n-\n-        # get key, value proj\n-        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n-        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n-        if past_key_value is not None:\n-            # reuse k, v, self_attention\n-            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n-            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+        query_states = self.q_proj(hidden_states)\n+        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        past_key_value = (key_states, value_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n \n-        # shape now is (bsz, num_heads, seq_len, head_dim), all are continuous\n+        if past_key_value is not None:\n+            # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+            key_states, value_states = past_key_value.update(\n+                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+            )\n \n         causal_mask = attention_mask\n         if attention_mask is not None:\n@@ -378,10 +345,6 @@ def forward(\n             attn_mask=causal_mask,\n             dropout_p=self.dropout if self.training else 0.0,\n             is_causal=is_causal,\n-            # this model uses the scaling factor in the query projection for some reason, but not in Q@K^T\n-            # so we need to scale to remove scaling in SDPA to have similar results with eager.\n-            # Maybe needs a change in the model to remove scaling in query projection\n-            scale=1.0,\n         )\n \n         attn_output = attn_output.transpose(1, 2).contiguous()\n@@ -399,11 +362,11 @@ def forward(\n \n \n class OPTDecoderLayer(nn.Module):\n-    def __init__(self, config: OPTConfig):\n+    def __init__(self, config: OPTConfig, layer_idx: int = None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n-        self.self_attn = OPT_ATTENTION_CLASSES[config._attn_implementation](config=config)\n+        self.self_attn = OPT_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n \n         self.do_layer_norm_before = config.do_layer_norm_before\n         self.dropout = config.dropout\n@@ -425,6 +388,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         position_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -440,6 +404,8 @@ def forward(\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n             past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence..\n         \"\"\"\n \n         residual = hidden_states\n@@ -456,6 +422,7 @@ def forward(\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n             output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n@@ -524,6 +491,9 @@ class OPTPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"OPTDecoderLayer\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -601,6 +571,10 @@ def _init_weights(self, module):\n             config.n_positions - 1]`. for padding use -1.\n \n             [What are position IDs?](../glossary#position-ids)\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n \"\"\"\n \n \n@@ -643,9 +617,7 @@ def __init__(self, config: OPTConfig):\n         else:\n             self.final_layer_norm = None\n \n-        self.layers = nn.ModuleList([OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n-        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n-        self._use_sdpa = config._attn_implementation == \"sdpa\"\n+        self.layers = nn.ModuleList([OPTDecoderLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -657,48 +629,130 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n-        inputs_embeds: torch.Tensor,\n-        input_shape: Tuple[int, int],\n-        past_key_values_length: int,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n     ):\n-        \"\"\"\n-        Updates the causal mask for the decoder.\n-        \"\"\"\n-        batch_size, seq_length = input_shape\n-        mask_seq_length = past_key_values_length + seq_length\n-        if self._use_flash_attention_2:\n-            # 2d mask is passed through the layers\n-            causal_attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-            attention_mask = (\n-                torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-                if attention_mask is None\n-                else attention_mask\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and (attention_mask == 0.0).any():\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n             )\n \n-            return causal_attention_mask, attention_mask\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n-        elif attention_mask.shape[1] != mask_seq_length:\n-            raise ValueError(\n-                f\"The provided attention mask has length {attention_mask.shape[1]}, but its length should be \"\n-                f\"{mask_seq_length} (sum of the lengths of current and past inputs)\"\n-            )\n-        if self._use_sdpa and not output_attentions and head_mask is None:\n-            causal_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n-            )\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n         else:\n-            causal_attention_mask = _prepare_4d_causal_attention_mask(\n-                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n \n-        return causal_attention_mask, attention_mask\n+        return causal_mask\n \n     def forward(\n         self,\n@@ -712,6 +766,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -764,6 +819,10 @@ def forward(\n                 config.n_positions - 1]`. for padding use -1.\n \n                 [What are position IDs?](../glossary#position-ids)\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+                this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+                the complete sequence length.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -773,51 +832,65 @@ def forward(\n \n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if input_ids is not None:\n+            input_ids = input_ids.view(-1, input_ids.shape[-1])\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            if past_key_values is None:\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n+                    \"You should pass an instance of `DynamicCache` instead, e.g. \"\n+                    \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if attention_mask is None:\n+            seq_length = past_seen_tokens + inputs_embeds.shape[1]\n+            attention_mask = torch.ones(inputs_embeds.shape[0], seq_length, device=inputs_embeds.device)\n \n-        causal_attention_mask, attention_mask = self._update_causal_mask(\n-            inputs_embeds, input_shape, past_key_values_length, attention_mask, head_mask, output_attentions\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n         )\n-        # embed positions\n \n+        # embed positions\n         if position_ids is None:\n+            # position_ids = cache_position.unsqueeze(0)\n             position_ids = torch.cumsum(attention_mask, dim=1)\n             position_ids = (position_ids * attention_mask - 1).long()\n-            # cut positions if `past_key_values_length` is > 0\n-            position_ids = position_ids[:, past_key_values_length:]\n+            # cut positions if `past_seen_tokens` is > 0\n+            position_ids = position_ids[:, past_seen_tokens:]\n \n-        pos_embeds = self.embed_positions(attention_mask, past_key_values_length, position_ids=position_ids)\n+        pos_embeds = self.embed_positions(attention_mask, past_seen_tokens, position_ids=position_ids)\n \n         if self.project_in is not None:\n             inputs_embeds = self.project_in(inputs_embeds)\n \n         hidden_states = inputs_embeds + pos_embeds.to(inputs_embeds.device)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = () if use_cache else None\n+        next_decoder_cache = None\n \n         # check if head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask], [\"head_mask\"]):\n@@ -838,34 +911,34 @@ def forward(\n                 if dropout_probability < self.layerdrop:\n                     continue\n \n-            past_key_value = past_key_values[idx] if past_key_values is not None else None\n-\n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_attention_mask,\n+                    causal_mask,\n                     head_mask[idx] if head_mask is not None else None,\n                     None,\n                     output_attentions,\n                     use_cache,\n                     position_ids,\n+                    cache_position,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_attention_mask,\n+                    attention_mask=causal_mask,\n                     position_ids=position_ids,\n                     layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n-                    past_key_value=past_key_value,\n+                    past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n+                    cache_position=cache_position,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n             if use_cache:\n-                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n \n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n@@ -881,6 +954,9 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n         if not return_dict:\n             return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n         return BaseModelOutputWithPast(\n@@ -930,6 +1006,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -950,6 +1027,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         if not return_dict:\n@@ -1008,6 +1086,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1069,6 +1148,10 @@ def forward(\n                 config.n_positions - 1]`. for padding use -1.\n \n                 [What are position IDs?](../glossary#position-ids)\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+                this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+                the complete sequence length.\n \n         Returns:\n \n@@ -1107,6 +1190,7 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         logits = self.lm_head(outputs[0]).contiguous()"
        },
        {
            "sha": "35ad047a00dd3ccb86d82b94936a2b82e103d2d8",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -29,6 +29,7 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -508,7 +509,7 @@ def forward(\n \n             special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n                 image_tokens_in_text = torch.sum(input_ids == self.config.image_token_index)\n                 raise ValueError(\n                     f\"Number of images does not match number of special image tokens in the input text. \""
        },
        {
            "sha": "e761481d82593377987dd5325aa64f226b26f484",
            "filename": "src/transformers/models/video_llava/configuration_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -38,8 +38,6 @@ class VideoLlavaConfig(PretrainedConfig):\n         text_config (`Union[AutoConfig, dict]`, *optional*):\n             The config object of the text backbone. Can be any of `LlamaConfig` or `MistralConfig`.\n             Defaults to `LlamaConfig` if not indicated.\n-        ignore_index (`int`, *optional*, defaults to -100):\n-            The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 32000):\n             The image token index to encode the image prompt.\n         video_token_index (`int`, *optional*, defaults to 32001):\n@@ -88,7 +86,6 @@ def __init__(\n         self,\n         vision_config=None,\n         text_config=None,\n-        ignore_index=-100,\n         image_token_index=32000,\n         video_token_index=32001,\n         projector_hidden_act=\"gelu\",\n@@ -99,7 +96,6 @@ def __init__(\n         multimodal_projector_bias=True,\n         **kwargs,\n     ):\n-        self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.video_token_index = video_token_index\n         self.projector_hidden_act = projector_hidden_act"
        },
        {
            "sha": "ba4de65374420cbacfabe14bb48c7a136289054c",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 13,
            "deletions": 96,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -28,6 +28,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -137,6 +138,8 @@ class VideoLlavaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = (\n@@ -276,92 +279,6 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n-    def _merge_input_ids_with_visual_features(\n-        self, visual_features, inputs_embeds, input_ids, attention_mask, labels, num_frames=1\n-    ):\n-        num_images, num_image_patches, embed_dim = visual_features.shape\n-        batch_size, sequence_length = input_ids.shape\n-        left_padding = not torch.sum(input_ids[:, -1] == torch.tensor(self.pad_token_id))\n-        special_vision_token = self.config.video_token_index if num_frames > 1 else self.config.image_token_index\n-\n-        # 1. Create a mask to know where special image tokens are\n-        special_image_token_mask = input_ids == special_vision_token\n-        num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n-        # Compute the maximum embed dimension\n-        max_seq_len = (num_special_image_tokens.max() * (num_image_patches * num_frames - 1)) + sequence_length\n-        batch_indices, non_image_indices = torch.where(input_ids != special_vision_token)\n-\n-        # 2. Compute the positions where text should be written\n-        # Calculate new positions for text tokens in merged image-text sequence.\n-        # `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images - 1` text tokens.\n-        # `torch.cumsum` computes how each image token shifts subsequent text token positions.\n-        # - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.\n-        new_token_positions = (\n-            torch.cumsum((special_image_token_mask * (num_image_patches * num_frames - 1) + 1), dim=-1) - 1\n-        )\n-        nb_image_pad = max_seq_len - 1 - new_token_positions[:, -1]\n-        if left_padding:\n-            new_token_positions += nb_image_pad[:, None]  # offset for left padding\n-        text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n-\n-        # 3. Create the full embedding, already padded to the maximum position\n-        # expand input ids so that the second \"merge\" with videos does not fail\n-        final_embedding = torch.zeros(\n-            batch_size, max_seq_len, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device\n-        )\n-        final_attention_mask = torch.zeros(\n-            batch_size, max_seq_len, dtype=attention_mask.dtype, device=inputs_embeds.device\n-        )\n-        final_input_ids = torch.full(\n-            (batch_size, max_seq_len), self.pad_token_id, dtype=input_ids.dtype, device=inputs_embeds.device\n-        )\n-        # In case the Vision model or the Language model has been offloaded to CPU, we need to manually\n-        # set the corresponding tensors into their correct target device.\n-        target_device = inputs_embeds.device\n-        batch_indices, non_image_indices, text_to_overwrite = (\n-            batch_indices.to(target_device),\n-            non_image_indices.to(target_device),\n-            text_to_overwrite.to(target_device),\n-        )\n-        attention_mask = attention_mask.to(target_device)\n-\n-        # 4. Fill the embeddings based on the mask. If we have [\"hey\" \"<image>\", \"how\", \"are\"]\n-        # we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features\n-        final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_image_indices]\n-        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_image_indices]\n-        final_input_ids[batch_indices, text_to_overwrite] = input_ids[batch_indices, non_image_indices]\n-        if labels is not None:\n-            final_labels = torch.full(\n-                (batch_size, max_seq_len), self.config.ignore_index, dtype=input_ids.dtype, device=input_ids.device\n-            )\n-            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_image_indices]\n-        else:\n-            final_labels = None\n-\n-        # 5. Fill the embeddings corresponding to the images. Anything that is still zeros needs filling\n-        image_to_overwrite = torch.full((batch_size, max_seq_len), True, dtype=torch.bool, device=inputs_embeds.device)\n-        image_to_overwrite[batch_indices, text_to_overwrite] = False\n-        if left_padding:\n-            image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n-        else:\n-            mask = torch.ones_like(image_to_overwrite, dtype=torch.bool).cumsum(-1) - 1\n-            padding_mask = mask <= new_token_positions[:, -1:].to(target_device)\n-            image_to_overwrite &= padding_mask\n-\n-        if image_to_overwrite.sum() != visual_features.shape[:-1].numel():\n-            visual_type = \"videos\" if num_frames == 8 else \"images\"\n-            num_images //= num_frames\n-            raise ValueError(\n-                f\"The input provided to the model are wrong. The number of {visual_type} tokens is {torch.sum(special_image_token_mask)} while\"\n-                f\" the number of {visual_type} given to the model is {num_images}. This prevents correct indexing and breaks batch generation.\"\n-            )\n-\n-        final_embedding[image_to_overwrite] = visual_features.contiguous().reshape(-1, embed_dim).to(target_device)\n-        final_attention_mask |= image_to_overwrite\n-        position_ids = (final_attention_mask.cumsum(-1) - 1).masked_fill_((final_attention_mask == 0), 1)\n-\n-        return final_embedding, final_attention_mask, final_labels, position_ids, final_input_ids\n-\n     def get_image_features(\n         self,\n         pixel_values_images: torch.FloatTensor,\n@@ -579,14 +496,14 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-            n_image_features = image_features.shape[0] * image_features.shape[1]\n-            if n_image_tokens != n_image_features:\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n@@ -595,14 +512,14 @@ def forward(\n                 pixel_values_videos=pixel_values_videos, vision_feature_layer=vision_feature_layer\n             )\n \n-            n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n-            n_video_features = video_features.shape[0] * video_features.shape[1]\n-            if n_video_tokens != n_video_features:\n+            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n+                n_video_tokens = (input_ids == self.config.video_token_index).sum()\n+                n_video_features = video_features.shape[0] * video_features.shape[1]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n "
        },
        {
            "sha": "ac24cce24129eb783359a345fc40bdd3016e1d0c",
            "filename": "src/transformers/models/vipllava/configuration_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -37,8 +37,6 @@ class VipLlavaConfig(PretrainedConfig):\n             Custom vision config or dict\n         text_config (`Union[AutoConfig, dict]`, *optional*):\n             The config object of the text backbone. Can be any of `LlamaConfig` or `MistralConfig`.\n-        ignore_index (`int`, *optional*, defaults to -100):\n-            The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 32000):\n             The image token index to encode the image prompt.\n         projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n@@ -78,15 +76,13 @@ def __init__(\n         self,\n         vision_config=None,\n         text_config=None,\n-        ignore_index=-100,\n         image_token_index=32000,\n         projector_hidden_act=\"gelu\",\n         projector_layernorm_eps=1e-5,\n         vision_feature_layers=[-2, -5, -8, -11, 6],\n         image_seq_length=576,\n         **kwargs,\n     ):\n-        self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.projector_hidden_act = projector_hidden_act\n         self.projector_layernorm_eps = projector_layernorm_eps"
        },
        {
            "sha": "ef4b3bff3958b242f51b5c49fb486395ea8d0453",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 8,
            "deletions": 88,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -28,6 +28,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -137,6 +138,8 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         # important: this ported version of VipLlava isn't meant for training from scratch - only\n@@ -297,89 +300,6 @@ def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_lay\n         image_features = self.multi_modal_projector(image_features)\n         return image_features\n \n-    def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, input_ids, attention_mask, labels):\n-        num_images, num_image_patches, embed_dim = image_features.shape\n-        batch_size, sequence_length = input_ids.shape\n-        left_padding = not torch.sum(input_ids[:, -1] == torch.tensor(self.pad_token_id))\n-        # 1. Create a mask to know where special image tokens are\n-        special_image_token_mask = input_ids == self.config.image_token_index\n-        num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n-        # Compute the maximum embed dimension\n-        max_embed_dim = (num_special_image_tokens.max() * (num_image_patches - 1)) + sequence_length\n-        batch_indices, non_image_indices = torch.where(input_ids != self.config.image_token_index)\n-\n-        # 2. Compute the positions where text should be written\n-        # Calculate new positions for text tokens in merged image-text sequence.\n-        # `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images - 1` text tokens.\n-        # `torch.cumsum` computes how each image token shifts subsequent text token positions.\n-        # - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.\n-        new_token_positions = torch.cumsum((special_image_token_mask * (num_image_patches - 1) + 1), -1) - 1\n-        nb_image_pad = max_embed_dim - 1 - new_token_positions[:, -1]\n-        if left_padding:\n-            new_token_positions += nb_image_pad[:, None]  # offset for left padding\n-        text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n-\n-        # 3. Create the full embedding, already padded to the maximum position\n-        final_embedding = torch.zeros(\n-            batch_size, max_embed_dim, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device\n-        )\n-        final_attention_mask = torch.zeros(\n-            batch_size, max_embed_dim, dtype=attention_mask.dtype, device=inputs_embeds.device\n-        )\n-        if labels is not None:\n-            final_labels = torch.full(\n-                (batch_size, max_embed_dim), self.config.ignore_index, dtype=input_ids.dtype, device=input_ids.device\n-            )\n-        # In case the Vision model or the Language model has been offloaded to CPU, we need to manually\n-        # set the corresponding tensors into their correct target device.\n-        target_device = inputs_embeds.device\n-        batch_indices, non_image_indices, text_to_overwrite = (\n-            batch_indices.to(target_device),\n-            non_image_indices.to(target_device),\n-            text_to_overwrite.to(target_device),\n-        )\n-        attention_mask = attention_mask.to(target_device)\n-\n-        # 4. Fill the embeddings based on the mask. If we have [\"hey\" \"<image>\", \"how\", \"are\"]\n-        # we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features\n-        final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_image_indices]\n-        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_image_indices]\n-        if labels is not None:\n-            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_image_indices]\n-\n-        # 5. Fill the embeddings corresponding to the images. Anything that is not `text_positions` needs filling (#29835)\n-        image_to_overwrite = torch.full(\n-            (batch_size, max_embed_dim), True, dtype=torch.bool, device=inputs_embeds.device\n-        )\n-        image_to_overwrite[batch_indices, text_to_overwrite] = False\n-        if left_padding:\n-            image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n-        else:\n-            mask = torch.ones_like(image_to_overwrite, dtype=torch.bool).cumsum(-1) - 1\n-            padding_mask = mask <= new_token_positions[:, -1:].to(target_device)\n-            image_to_overwrite &= padding_mask\n-\n-        if image_to_overwrite.sum() != image_features.shape[:-1].numel():\n-            raise ValueError(\n-                f\"The input provided to the model are wrong. The number of image tokens is {torch.sum(special_image_token_mask)} while\"\n-                f\" the number of image given to the model is {num_images}. This prevents correct indexing and breaks batch generation.\"\n-            )\n-\n-        final_embedding[image_to_overwrite] = image_features.contiguous().reshape(-1, embed_dim).to(target_device)\n-        final_attention_mask |= image_to_overwrite\n-        position_ids = (final_attention_mask.cumsum(-1) - 1).masked_fill_((final_attention_mask == 0), 1)\n-\n-        # 6. Mask out the embedding at padding positions, as we later use the past_key_value value to determine the non-attended tokens.\n-        batch_indices, pad_indices = torch.where(input_ids == self.pad_token_id)\n-        indices_to_mask = new_token_positions[batch_indices, pad_indices]\n-\n-        final_embedding[batch_indices, indices_to_mask] = 0\n-\n-        if labels is None:\n-            final_labels = None\n-\n-        return final_embedding, final_attention_mask, final_labels, position_ids\n-\n     @deprecate_kwarg(\"num_logits_to_keep\", version=\"4.50\", new_name=\"logits_to_keep\")\n     @add_start_docstrings_to_model_forward(VIPLLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=VipLlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -469,14 +389,14 @@ def forward(\n                 pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n             )\n \n-            n_image_tokens = (input_ids == self.config.image_token_index).sum().item()\n-            n_image_features = image_features.shape[0] * image_features.shape[1]\n-            if n_image_tokens != n_image_features:\n+            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n "
        },
        {
            "sha": "3b9700dc20c99fabbbb30c8b307cedaee7274341",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 42,
            "deletions": 14,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -1783,12 +1783,12 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             model.config.use_cache = True\n             model.config.is_decoder = True\n             batch_size = input_ids.shape[0]\n-            max_length = 30\n+            max_new_tokens = 10\n \n             # here we force to not stop at eos and go until max-length\n             model.generation_config.eos_token_id = model.config.get_text_config().eos_token_id = -1\n             generation_kwargs = {\n-                \"max_length\": max_length,\n+                \"max_new_tokens\": max_new_tokens,\n                 \"cache_implementation\": \"static\",\n                 \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n             }\n@@ -1811,10 +1811,11 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n \n             # we should get `max_length - 1` in shape, not `max_length - embeds_length`.\n             # -1 because the last generated token isn't yet in the cache.\n-            cache_shape = (batch_size, num_key_value_heads, max_length - 1, head_dim)\n-            self.assertTrue(isinstance(outputs.past_key_values, StaticCache))\n-            self.assertTrue(len(outputs.past_key_values.key_cache) == num_hidden_layers)\n-            self.assertTrue(outputs.past_key_values.key_cache[0].shape == cache_shape)\n+            max_length = max_new_tokens + inputs_embeds.shape[1] - 1\n+            cache_shape = [batch_size, num_key_value_heads, max_length, head_dim]\n+            self.assertIsInstance(outputs.past_key_values, StaticCache)\n+            self.assertEqual(len(outputs.past_key_values.key_cache), num_hidden_layers)\n+            self.assertListEqual(list(outputs.past_key_values.key_cache[0].shape), cache_shape)\n \n     @pytest.mark.generate\n     def test_generate_continue_from_past_key_values(self):\n@@ -2022,7 +2023,7 @@ def test_generate_with_static_cache(self):\n \n             config.is_decoder = True\n             batch_size = main_input.shape[0]\n-            seq_length = main_input.shape[-1]\n+            seq_length = self.model_tester.seq_length\n             max_new_tokens = 20\n \n             for dtype in (torch.float32, torch.float16):\n@@ -2134,7 +2135,15 @@ def test_generate_compile_model_forward(self):\n             # compilation-specific setup\n             torch.compiler.reset()  # prevent cached compilation from being used in the test\n             has_defined_cache_implementation = model.generation_config.cache_implementation is not None\n-            model.generation_config.compile_config._compile_all_devices = True  # force compilation (e.g. fast CI, CPU)\n+\n+            # BLIP is the only exception with custom generate which call `self.lm.generate()`\n+            # We should avoid such calls in all subsequent multimodal models and try to make `generate()`\n+            # compatible with multimodality\n+            if \"blip\" in model.__class__.__name__.lower():\n+                model.language_model.generation_config.compile_config._compile_all_devices = True\n+            else:\n+                # force compilation (e.g. fast CI, CPU\n+                model.generation_config.compile_config._compile_all_devices = True\n \n             generation_kwargs = {\n                 \"do_sample\": False,\n@@ -2175,7 +2184,14 @@ def test_generate_compile_model_forward(self):\n                 )\n                 self.assertFalse(isinstance(decoder_cache, DynamicCache))\n                 self.assertTrue(decoder_cache.is_compileable)\n-                self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n+\n+                # BLIP is the only exception with custom generate which call `self.lm.generate()`\n+                # We should avoid such calls in all subsequent multimodal models and try to make `generate()`\n+                # compatible with multimodality\n+                if \"blip\" in model.__class__.__name__.lower():\n+                    self.assertTrue(hasattr(model.language_model, \"_compiled_call\"))\n+                else:\n+                    self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n \n             for dynamic_result, compiled_result in zip(dynamic_outputs, compiled_outputs):\n                 self._check_similar_generate_outputs(dynamic_result, compiled_result)\n@@ -2198,9 +2214,19 @@ def test_generate_compilation_all_outputs(self):\n             # compilation-specific setup\n             torch.compiler.reset()  # prevent cached compilation from being used in the test\n             has_defined_cache_implementation = model.generation_config.cache_implementation is not None\n-            model.generation_config.compile_config._compile_all_devices = True  # force compilation (e.g. fast CI, CPU)\n-            if not has_defined_cache_implementation:\n-                model.generation_config.cache_implementation = \"static\"\n+\n+            # BLIP is the only exception with custom generate which call `self.lm.generate()`\n+            # We should avoid such calls in all subsequent multimodal models and try to make `generate()`\n+            # compatible with multimodality\n+            if \"blip\" in model.__class__.__name__.lower():\n+                model.language_model.generation_config.compile_config._compile_all_devices = True\n+                if not has_defined_cache_implementation:\n+                    model.language_model.generation_config.cache_implementation = \"static\"\n+            else:\n+                # force compilation (e.g. fast CI, CPU)\n+                model.generation_config.compile_config._compile_all_devices = True\n+                if not has_defined_cache_implementation:\n+                    model.generation_config.cache_implementation = \"static\"\n \n             logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n             output_generate = model.generate(\n@@ -2218,8 +2244,10 @@ def test_generate_compilation_all_outputs(self):\n                 **inputs_dict,\n             )\n \n-            # Sanity check: compilation has happened\n-            self.assertTrue(hasattr(model, \"_compiled_call\"))\n+            if \"blip\" in model.__class__.__name__.lower():\n+                self.assertTrue(hasattr(model.language_model, \"_compiled_call\"))\n+            else:\n+                self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n \n             if model.config.is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)"
        },
        {
            "sha": "8b5e62de14c7b70787bf7fe13e2e3b8ae31bf491",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -286,10 +286,18 @@ def test_generate_from_inputs_embeds_0_greedy(self):\n     def test_generate_from_inputs_embeds_1_beam_search(self):\n         pass\n \n-    @unittest.skip(reason=\"Unsupported\")\n+    @unittest.skip(reason=\"Dynamic control flow due to MoE\")\n     def test_generate_with_static_cache(self):\n         pass\n \n+    @unittest.skip(reason=\"Dynamic control flow due to MoE\")\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Dynamic control flow due to MoE\")\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n \n @require_torch\n class AriaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "a405a1f97fb30ad618c24e20d5cc90455c74d3e3",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -816,6 +816,10 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n     def test_generate_from_inputs_embeds(self, _, num_beams):\n         pass\n \n+    @unittest.skip(\"BLIP2 cannot generate only from input ids, and requires pixel values in all cases to be present\")\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n \n # this class is based on `T5ModelTester` found in tests/models/t5/test_modeling_t5.py\n class Blip2TextModelTester:"
        },
        {
            "sha": "491fd9f9ec4f851cf71695a778cd8e9c0db18fae",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -386,10 +386,6 @@ def test_disk_offload_bin(self):\n     def test_cpu_offload(self):\n         pass\n \n-    @unittest.skip(\"Doesn't work, tensors are not almost same\")  # TODO raushan fixme\n-    def test_custom_4d_attention_mask(self):\n-        pass\n-\n     @unittest.skip(\"VQ-VAE module doesn't initialize weights properly\")\n     def test_initialization(self):\n         pass"
        },
        {
            "sha": "178bec98ac62d4bfcbde979ae3a611c499c5ba20",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -256,12 +256,6 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_past_key_values_format(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"GotOcr2 needs a dynamic control flow to pass pixel values to the forward function only in the first generation step\"\n-    )\n-    def test_generate_compile_1_end_to_end(self):\n-        pass\n-\n     @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n     def test_flash_attn_2_fp32_ln(self):\n         pass"
        },
        {
            "sha": "32c45d6e71f7b56157b6bcb9f03ef4f3746fb004",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -838,6 +838,14 @@ def test_contrastive_generate_low_memory(self):\n     def test_custom_4d_attention_mask(self):\n         pass\n \n+    @unittest.skip(reason=\"IDEFICS cannot compile due to dynamic control flow when checking inputs\")\n+    def test_generate_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(reason=\"IDEFICS cannot compile due to dynamic control flow when checking inputs\")\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n     @unittest.skip(reason=\"We only test the model that takes in multiple images\")\n     def test_model(self):\n         pass"
        },
        {
            "sha": "bbf877289040fc57e503414efb31dc84916894fa",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -530,6 +530,12 @@ def test_save_load_fast_init_from_base(self):\n     def test_save_load_fast_init_to_base(self):\n         pass\n \n+    @unittest.skip(\n+        \"InstructBLIP cannot generate only from input ids, and requires pixel values in all cases to be present\"\n+    )\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "351dea3d6fae2bdffdc7cf2a6e10b3b5a8edd407",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -546,6 +546,12 @@ def test_save_load_fast_init_from_base(self):\n     def test_save_load_fast_init_to_base(self):\n         pass\n \n+    @unittest.skip(\n+        \"InstructBLIPVideo cannot generate only from input ids, and requires pixel values in all cases to be present\"\n+    )\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "b47423a02ec787470643d4dbe925d0ca338cff50",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -316,14 +316,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n-    def test_sdpa_can_dispatch_on_flash(self):\n-        pass\n-\n     @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n     def test_flash_attn_2_fp32_ln(self):\n         pass"
        },
        {
            "sha": "0c75df53c1bbe322db7cc00060b8db5bf8c161f9",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 16,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -365,22 +365,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"Feedforward chunking is not yet supported\")\n-    def test_feed_forward_chunking(self):\n-        pass\n-\n-    @unittest.skip(reason=\"CPU offload is not yet supported\")\n-    def test_cpu_offload(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n-    def test_sdpa_can_dispatch_on_flash(self):\n-        pass\n-\n     @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n     def test_flash_attn_2_fp32_ln(self):\n         pass\n@@ -391,6 +375,10 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n+    @unittest.skip(\"LLaVA Next has dynamic control flow in unpadding\")\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n \n @require_torch\n class LlavaNextForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "6d4df92f5c22879d3006bcda0d46ae18d13a0aa1",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 20,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -382,26 +382,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"Feedforward chunking is not yet supported\")\n-    def test_feed_forward_chunking(self):\n-        pass\n-\n-    @unittest.skip(reason=\"CPU offload is not yet supported\")\n-    def test_cpu_offload(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"Compile not yet supported because in LLava models (https://github.com/huggingface/transformers/issues/29891)\"\n-    )\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"Compile not yet supported because in LLava models (https://github.com/huggingface/transformers/issues/29891)\"\n-    )\n-    def test_sdpa_can_dispatch_on_flash(self):\n-        pass\n-\n     @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n     def test_flash_attn_2_fp32_ln(self):\n         pass\n@@ -412,6 +392,10 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n+    @unittest.skip(\"LLaVA Next Video has dynamic control flow in unpadding\")\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n \n @require_torch\n class LlavaNextVideoForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "c9bb448278e70ece6fdb39e63f1af60e0aff7748",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -346,6 +346,10 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n+    @unittest.skip(\"LLaVA OneVision has dynamic control flow in unpadding\")\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n \n @require_torch\n class LlavaOnevisionForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "994d88444809025b3c0f4a2e7f39b9b77da5903b",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -540,7 +540,6 @@ def prepare_config_and_inputs_for_common(self):\n             \"attention_mask\": attention_mask,\n             \"decoder_input_ids\": decoder_input_ids,\n             \"decoder_attention_mask\": decoder_attention_mask,\n-            \"use_cache\": False,\n         }\n         return config, inputs_dict\n "
        },
        {
            "sha": "dad740cde721503e59e59eee429e056fe9528712",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -81,15 +81,14 @@ def __init__(\n         hidden_act=\"gelu\",\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=20,\n+        max_position_embeddings=50,\n         eos_token_id=2,\n         pad_token_id=1,\n         bos_token_id=0,\n         embed_dim=16,\n         num_labels=3,\n         word_embed_proj_dim=16,\n         type_sequence_label_size=2,\n-        attn_implementation=\"eager\",\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -113,7 +112,6 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.word_embed_proj_dim = word_embed_proj_dim\n         self.is_encoder_decoder = False\n-        self.attn_implementation = attn_implementation\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size).clamp(\n@@ -143,7 +141,6 @@ def get_config(self):\n             embed_dim=self.embed_dim,\n             is_encoder_decoder=False,\n             word_embed_proj_dim=self.word_embed_proj_dim,\n-            attn_implementation=self.attn_implementation,\n         )\n \n     def get_pipeline_config(self):"
        },
        {
            "sha": "a0439550f8f0eb34a9e75c521e51485337b0cb7c",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -545,7 +545,6 @@ def prepare_config_and_inputs_for_common(self):\n             \"attention_mask\": attention_mask,\n             \"decoder_input_ids\": decoder_input_ids,\n             \"decoder_attention_mask\": decoder_attention_mask,\n-            \"use_cache\": False,\n         }\n         return config, inputs_dict\n "
        },
        {
            "sha": "528f125693f776fcbbab28b61363b94b0faccae2",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -226,14 +226,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"Pass because video-LLava requires `attention_mask is not None`\")\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Pass because video-LLava requires `attention_mask is not None`\")\n-    def test_sdpa_can_dispatch_on_flash(self):\n-        pass\n-\n     @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n     def test_flash_attn_2_fp32_ln(self):\n         pass"
        },
        {
            "sha": "24f99d4b0b18ba9bc80da8ba8eaf06553f8f1751",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -306,14 +306,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"Compile not yet supported because it is not yet supported in LLava\")\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n-    def test_sdpa_can_dispatch_on_flash(self):\n-        pass\n-\n     @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n     def test_flash_attn_2_fp32_ln(self):\n         pass"
        },
        {
            "sha": "a707b25a3110e7d6d11d93dcd0db8d8ebffb444c",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=0c78ef6cd3907c6f67b073ee59beb4d6cbc57ea4",
            "patch": "@@ -4324,10 +4324,6 @@ def test_sdpa_can_dispatch_on_flash(self):\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            if config.model_type in [\"llava\", \"llava_next\", \"vipllava\", \"video_llava\"]:\n-                self.skipTest(\n-                    reason=\"Llava-like models currently (transformers==4.39.1) requires an attention_mask input\"\n-                )\n             if config.model_type in [\"paligemma\"]:\n                 self.skipTest(\n                     \"PaliGemma-like models currently (transformers==4.41.0) requires an attention_mask input\"\n@@ -4778,6 +4774,9 @@ def test_custom_4d_attention_mask(self):\n             model = model_class(config).to(device=torch_device, dtype=torch.float32)\n             set_model_for_less_flaky_test(model)\n \n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                continue  # this model doesn't accept position ids as input\n+\n             (\n                 input_ids,\n                 position_ids,"
        }
    ],
    "stats": {
        "total": 1673,
        "additions": 461,
        "deletions": 1212
    }
}