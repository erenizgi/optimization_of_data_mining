{
    "author": "manueldeprada",
    "message": "Allow custom args in `custom_generate` Callables and unify generation args structure (#40586)\n\n* Squashed commit of the following:\n\ncommit beb2b5f7a04ea9e12876696db66f3589fbae10c5\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 16:03:25 2025 +0200\n\n    also standardize _get_stopping_criteria\n\ncommit 15c25663fa991e0a215a7f3cdcf13a9d3a989faa\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 15:48:38 2025 +0200\n\n    watch super.generate() usages\n\ncommit 67dd845be2202d191a54b2872f1cb3f71b74b7d6\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 14:44:32 2025 +0200\n\n    ops\n\ncommit 4655dfa28fd59d5dc083a41d8396de042d99858c\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 14:41:36 2025 +0200\n\n    wrong merge\n\ncommit 46478143994e7b27d51c972a7881e0fea3cb6e3c\nMerge: a72c2c4b2f 8564e210ca\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 14:36:15 2025 +0200\n\n    Merge branch 'main' of github.com:huggingface/transformers into fix-custom-gen-from-function2\n\ncommit a72c2c4b2f9c0e09fe6ec7992d4d02bfa279da2a\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 14:04:59 2025 +0200\n\n    ops5\n\ncommit e72f91411b961979bb3d271810f57905cee5b577\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 12:06:19 2025 +0200\n\n    ops4\n\ncommit 12ca97b1078a42167143e0243036f6ef87d5fdac\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 11:58:59 2025 +0200\n\n    ops3\n\ncommit 8cac6c60a318dd381793d4bf1ef3775823f3c95b\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 11:43:03 2025 +0200\n\n    ops2\n\ncommit 4681a7d5dc6c8b96a515d9d79f06380c096b9a9f\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 11:40:51 2025 +0200\n\n    ops\n\ncommit 0d72aa6cbd99a5933c5a95a39bea9088ee21e50f\nMerge: e0d47e980e 5bb6186b8e\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 11:37:28 2025 +0200\n\n    Merge branch 'remove-constrained-bs' into fix-custom-gen-from-function2\n\ncommit 5bb6186b8efbd5fdb8e3464a22f958343b9c450c\nMerge: 44973dac7d b0db5a02f3\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 11:36:30 2025 +0200\n\n    Merge branch 'main' into remove-constrained-bs\n\ncommit 44973dac7df4b4e2111c71f5fac918be21f3de52\nMerge: 1ddab4bee1 893d89e5e6\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 11:29:48 2025 +0200\n\n    Merge commit '893d89e5e6fac7279fe4292bfa3b027172287162' into remove-constrained-bs\n\ncommit e0d47e980e26d32b028c2b402ccb71262637a7a7\nMerge: 88128e4563 1ddab4bee1\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 10:52:50 2025 +0200\n\n    Merge branch 'remove-constrained-bs' into fix-custom-gen-from-function2\n\ncommit 88128e4563c0be583728e1d3c639bc93143c4029\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Mon Sep 1 10:44:38 2025 +0200\n\n    fix custom generate args, refactor gen mode args\n\ncommit 1ddab4bee159f6c20722e7ff5cd41d5041fab0aa\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Sun Aug 31 21:03:53 2025 +0200\n\n    fix\n\ncommit 6095fdda677ef7fbeb06c05f4f914a11b45257b4\nMerge: 4a8b6d2ce1 04addbc9ec\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 17:49:16 2025 +0200\n\n    Merge branch 'remove-constrained-bs' of github.com:manueldeprada/transformers into remove-constrained-bs\n\ncommit 4a8b6d2ce18b3a8b52c5261fea427e2416f65187\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 17:48:25 2025 +0200\n\n    restore and deprecate beam obkects\n\ncommit 04addbc9ec62dd4f59d15128e8cd9499e2cda3bb\nMerge: e800c7841e becab2c601\nAuthor: Manuel de Prada Corral <6536835+manueldeprada@users.noreply.github.com>\nDate:   Thu Aug 28 14:38:29 2025 +0200\n\n    Merge branch 'main' into remove-constrained-bs\n\ncommit e800c7841e5c46ce5698fc9be309d0808f85d23c\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 14:38:10 2025 +0200\n\n    tests gone after green\n\ncommit 33971d21ac40aef76a7e1122f4a98ef28beadbe8\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 14:07:11 2025 +0200\n\n    tests green, changed handling of deprecated methods\n\ncommit ab303835c184d0a87789da7aed7d8de5ba85d867\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 12:58:01 2025 +0200\n\n    tests fix\n\ncommit ec74274ca52a6aa0b5f300374fda838609680506\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 12:32:05 2025 +0200\n\n    ops\n\ncommit 0fb19004ccd285dcad485fce0865b355ce5493e0\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 11:45:16 2025 +0200\n\n    whoops\n\ncommit c946bea5e45aea021c8878c57fcabc2a13f06fe5\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 11:35:36 2025 +0200\n\n    testing...\n\ncommit 924c0dec6d9ea6b4890644fe7f711dc778f820bb\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 11:22:46 2025 +0200\n\n    sweeep ready for tests\n\ncommit b05aa771d3994b07cd460cda74b274c9e4f315e6\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Thu Aug 28 11:13:01 2025 +0200\n\n    restore and deprecate constraints\n\ncommit 9c7962d10efa7178b69d3c99e69663756e1cd979\nMerge: fceeb383f9 c17bf304d5\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Wed Aug 27 20:44:21 2025 +0200\n\n    Merge branch 'remove-group-bs' into remove-constrained-bs\n\ncommit c17bf304d5cf33af7f34f9f6057915d5f5821dae\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Wed Aug 27 17:00:50 2025 +0200\n\n    fix test\n\ncommit d579aeec6706b77fcc24c1f6806cd7277d7db56e\nMerge: 822efd8c3c ed5dd2999c\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Wed Aug 27 16:04:31 2025 +0200\n\n    Merge branch 'main' of github.com:huggingface/transformers into remove-group-bs\n\ncommit 822efd8c3cf475d079e64293aa06e4ab59740fd7\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Wed Aug 27 15:59:51 2025 +0200\n\n    aaand remove tests after all green!!\n\ncommit 62cb274a4acb9f24201902242f1b0dc4e46daac1\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Wed Aug 27 11:48:19 2025 +0200\n\n    fix\n\ncommit c89c892e7b24a7d71831f2b35264456005030925\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Wed Aug 27 11:45:20 2025 +0200\n\n    testing that hub works the same\n\ncommit fceeb383f99e4a836679d67b1d2a8520152eaf49\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Tue Aug 26 20:06:59 2025 +0200\n\n    draft\n\ncommit 6a9b384078f3798587ba865ac7ddfefc9a79e41c\nMerge: 8af3af13ab 58cebc848b\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Tue Aug 26 15:00:05 2025 +0200\n\n    Merge branch 'main' of github.com:huggingface/transformers into remove-group-bs\n\ncommit 8af3af13abb85ca60e795d0390832f398a56c34f\nAuthor: Manuel de Prada Corral <manueldeprada@gmail.com>\nDate:   Tue Aug 26 11:55:45 2025 +0200\n\n    Squashed commit remove-constrastive-search\n\n* ops\n\n* fix\n\n* ops\n\n* review\n\n* fix\n\n* fix dia\n\n* review",
    "sha": "3f40ebf6209c724acea11a43625c7fe10cd0cba7",
    "files": [
        {
            "sha": "2716c79c702f49dcc6fc191ade24fffcdc15519d",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 115,
            "deletions": 92,
            "changes": 207,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=3f40ebf6209c724acea11a43625c7fe10cd0cba7",
            "patch": "@@ -999,11 +999,11 @@ def _get_candidate_generator(\n         generation_config: GenerationConfig,\n         input_ids: torch.LongTensor,\n         inputs_tensor: torch.Tensor,\n-        assistant_model: \"PreTrainedModel\",\n         logits_processor: LogitsProcessorList,\n-        target_tokenizer: \"PreTrainedTokenizerBase\",\n-        assistant_tokenizer: \"PreTrainedTokenizerBase\",\n         model_kwargs: dict,\n+        assistant_model: Optional[\"PreTrainedModel\"] = None,\n+        target_tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n+        assistant_tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n     ) -> CandidateGenerator:\n         \"\"\"\n         Returns the candidate generator to be used in `assisted_generation`\n@@ -1300,7 +1300,6 @@ def _get_stopping_criteria(\n         generation_config: GenerationConfig,\n         stopping_criteria: Optional[StoppingCriteriaList],\n         tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n-        **kwargs,\n     ) -> StoppingCriteriaList:\n         criteria = StoppingCriteriaList()\n         if generation_config.max_length is not None:\n@@ -1493,35 +1492,38 @@ def compute_transition_scores(\n \n         return transition_scores\n \n-    def _validate_assistant(self, assistant_model, tokenizer, assistant_tokenizer):\n-        if assistant_model is None:\n-            return\n-\n-        if self.config.is_encoder_decoder and not assistant_model.config.is_encoder_decoder:\n-            attributes_to_check = [\"encoder_attention_heads\", \"encoder_ffn_dim\", \"encoder_layers\"]\n-            attributes_to_check = [attr for attr in dir(assistant_model.config) if attr in attributes_to_check]\n-            are_equal = all(\n-                getattr(self.config, attr) == getattr(assistant_model.config, attr) for attr in attributes_to_check\n+    def _validate_generation_mode(self, generation_mode, generation_mode_kwargs):\n+        if generation_mode == GenerationMode.BEAM_SEARCH and \"streamer\" in generation_mode_kwargs:\n+            raise ValueError(\n+                \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n             )\n-            if not are_equal:\n-                raise ValueError(\n-                    \"The main model and the assistant don't have compatible encoder-dependent input shapes. \"\n-                    \"Ensure you load the assistant with the correct encoder-decoder class, e.g. `AutoModelForSpeechSeq2Seq` for Whisper.\"\n-                )\n \n-        doc_reference = (\n-            \"(see https://huggingface.co/docs/transformers/en/generation_strategies#universal-assisted-decoding)\"\n-        )\n-        if self.config.get_text_config().vocab_size == assistant_model.config.get_text_config().vocab_size:\n-            if assistant_tokenizer is not None:\n-                raise ValueError(\n-                    f\"`assistant_tokenizer` is not required when the main and assistant models use the same tokenizer. Please omit `assistant_tokenizer` from `generate()` {doc_reference}.\"\n-                )\n-        else:\n-            if tokenizer is None or assistant_tokenizer is None:\n-                raise ValueError(\n-                    f\"The main and assistant models have different tokenizers. Please provide `tokenizer` and `assistant_tokenizer` to `generate()` {doc_reference}.\"\n+        if (assistant_model := generation_mode_kwargs.get(\"assistant_model\")) is not None:\n+            if self.config.is_encoder_decoder and not assistant_model.config.is_encoder_decoder:\n+                attributes_to_check = [\"encoder_attention_heads\", \"encoder_ffn_dim\", \"encoder_layers\"]\n+                attributes_to_check = [attr for attr in dir(assistant_model.config) if attr in attributes_to_check]\n+                are_equal = all(\n+                    getattr(self.config, attr) == getattr(assistant_model.config, attr) for attr in attributes_to_check\n                 )\n+                if not are_equal:\n+                    raise ValueError(\n+                        \"The main model and the assistant don't have compatible encoder-dependent input shapes. \"\n+                        \"Ensure you load the assistant with the correct encoder-decoder class, e.g. `AutoModelForSpeechSeq2Seq` for Whisper.\"\n+                    )\n+\n+            doc_reference = (\n+                \"(see https://huggingface.co/docs/transformers/en/generation_strategies#universal-assisted-decoding)\"\n+            )\n+            if self.config.get_text_config().vocab_size == assistant_model.config.get_text_config().vocab_size:\n+                if \"assistant_tokenizer\" in generation_mode_kwargs:\n+                    raise ValueError(\n+                        f\"`assistant_tokenizer` is not required when the main and assistant models use the same tokenizer. Please omit `assistant_tokenizer` from `generate()` {doc_reference}.\"\n+                    )\n+            else:\n+                if \"tokenizer\" not in generation_mode_kwargs or \"assistant_tokenizer\" not in generation_mode_kwargs:\n+                    raise ValueError(\n+                        f\"The main and assistant models have different tokenizers. Please provide `tokenizer` and `assistant_tokenizer` to `generate()` {doc_reference}.\"\n+                    )\n \n     def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):\n         \"\"\"Validates model kwargs for generation. Generate argument typos will also be caught here.\"\"\"\n@@ -1869,7 +1871,7 @@ def _prepare_cache_for_generation(\n         self,\n         generation_config: GenerationConfig,\n         model_kwargs: dict,\n-        assistant_model: \"PreTrainedModel\",\n+        generation_mode: GenerationMode,\n         batch_size: int,\n         max_cache_length: int,\n     ) -> bool:\n@@ -1923,7 +1925,10 @@ def _prepare_cache_for_generation(\n \n         # TODO(joao): support static caches in assisted generation. assisted generation needs to roll back caches,\n         # which is only supported in dynamic caches atm\n-        if assistant_model is not None and generation_config.cache_implementation is not None:\n+        if (\n+            generation_mode == GenerationMode.ASSISTED_GENERATION\n+            and generation_config.cache_implementation is not None\n+        ):\n             logger.warning_once(\n                 \"An assistant model is provided, using a dynamic cache instead of a cache of type=\"\n                 f\"'{generation_config.cache_implementation}'.\"\n@@ -1933,7 +1938,6 @@ def _prepare_cache_for_generation(\n         # Assisted decoding and contrastive search require cache rollback, which is incompatible with sliding layers.\n         # To handle this, we skip passing the model config to DynamicCache (forcing a full-layer cache).\n         # The \"dynamic_full\" option is a shortcut for generate() users to avoid sliding layers on their own.\n-        generation_mode = generation_config.get_generation_mode(assistant_model)\n         if (\n             generation_mode in (GenerationMode.ASSISTED_GENERATION, GenerationMode.CONTRASTIVE_SEARCH)\n             or generation_config.cache_implementation == \"dynamic_full\"\n@@ -2125,15 +2129,13 @@ def _valid_auto_compile_criteria(self, model_kwargs: dict, generation_config: Ge\n \n     def _get_deprecated_gen_repo(\n         self,\n-        generation_config: GenerationConfig,\n+        generation_mode: GenerationMode,\n         trust_remote_code: bool,\n         custom_generate: Optional[str] = None,\n-        assistant_model: Optional[\"PreTrainedModel\"] = None,\n     ) -> Optional[str]:\n         \"\"\"\n-        Returns the Hub repo for a deprecated generation strategy, if any.\n+        Returns the Hub repo for a deprecated generation mode, if any.\n         \"\"\"\n-        generation_mode = generation_config.get_generation_mode(assistant_model)\n         moved_to_hub_modes = {\n             GenerationMode.DOLA_GENERATION: \"transformers-community/dola\",\n             GenerationMode.CONTRASTIVE_SEARCH: \"transformers-community/contrastive-search\",\n@@ -2156,6 +2158,37 @@ def _get_deprecated_gen_repo(\n             )\n         return repo\n \n+    def _extract_generation_mode_kwargs(\n+        self,\n+        custom_generate,\n+        kwargs,\n+        synced_gpus,\n+        assistant_model,\n+        streamer,\n+    ) -> dict[str, Any]:\n+        \"\"\"\n+        Extracts and returns the generation mode related keyword arguments from the provided kwargs.\n+        \"\"\"\n+        generation_mode_kwargs = {\n+            \"tokenizer\": kwargs.pop(\"tokenizer\", None),\n+            \"assistant_tokenizer\": kwargs.pop(\"assistant_tokenizer\", None),\n+            \"assistant_model\": assistant_model,\n+            \"streamer\": streamer,\n+        }\n+        if synced_gpus is not None:\n+            generation_mode_kwargs[\"synced_gpus\"] = (\n+                is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n+            ) and dist.get_world_size() > 1\n+        generation_mode_kwargs = {k: v for k, v in generation_mode_kwargs.items() if v is not None}\n+        # Custom_generate callables can have their own set of arguments\n+        # To extract them, we compare the signature with the standard _sample method\n+        if isinstance(custom_generate, Callable):\n+            usual_mode_kwargs = inspect.signature(GenerationMixin._sample).parameters.keys()\n+            custom_generate_kwargs = inspect.signature(custom_generate).parameters.keys()\n+            new_custom_keys = custom_generate_kwargs - usual_mode_kwargs\n+            generation_mode_kwargs = {k: kwargs.pop(k) for k in new_custom_keys if k in kwargs}\n+        return generation_mode_kwargs\n+\n     @torch.no_grad()\n     def generate(\n         self,\n@@ -2292,47 +2325,46 @@ def generate(\n             )\n             return custom_generate_function(model=self, **generate_arguments)\n \n-        # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n-        tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n-        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n+        # 1. Handle kwargs, `generation_config`, validate them and obtain generation mode\n+        generation_mode_kwargs = self._extract_generation_mode_kwargs(\n+            custom_generate,\n+            kwargs,\n+            synced_gpus,\n+            assistant_model,\n+            streamer,\n+        )\n \n         generation_config, model_kwargs = self._prepare_generation_config(\n             generation_config, use_model_defaults, **kwargs\n         )\n+        generation_mode = generation_config.get_generation_mode(assistant_model)\n+\n         self._validate_model_kwargs(model_kwargs.copy())\n-        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n+        self._validate_generation_mode(generation_mode, generation_mode_kwargs)\n \n         # Deprecation-related step: set Hub repo for deprecated strategies.\n         # NOTE: This must come after initializing generation_config, since we need it to determine if this is a deprecated mode.\n         # It must also be before any preparation steps, since Hub repos expect to be loaded before preparation steps.\n         # TODO joao, manuel: remove this in v4.62.0\n-        if deprecate_mode_repo := self._get_deprecated_gen_repo(\n-            generation_config, trust_remote_code, custom_generate, assistant_model\n-        ):\n+        if deprecate_mode_repo := self._get_deprecated_gen_repo(generation_mode, trust_remote_code, custom_generate):\n             return GenerationMixin.generate(\n                 self,\n-                inputs,\n-                generation_config,\n-                logits_processor,\n-                stopping_criteria,\n-                prefix_allowed_tokens_fn,\n-                synced_gpus,\n-                assistant_model,\n-                streamer,\n-                negative_prompt_ids,\n-                negative_prompt_attention_mask,\n-                use_model_defaults,\n+                inputs=inputs,\n+                generation_config=generation_config,\n+                logits_processor=logits_processor,\n+                stopping_criteria=stopping_criteria,\n+                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n+                assistant_model=assistant_model,\n+                negative_prompt_ids=negative_prompt_ids,\n+                negative_prompt_attention_mask=negative_prompt_attention_mask,\n+                use_model_defaults=use_model_defaults,\n                 custom_generate=deprecate_mode_repo,\n                 trust_remote_code=trust_remote_code,\n-                tokenizer=tokenizer,\n-                assistant_tokenizer=assistant_tokenizer,\n+                **generation_mode_kwargs,\n                 **kwargs,\n             )\n \n         # 2. Set generation parameters if not already defined\n-        if synced_gpus is None:\n-            synced_gpus = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1\n-\n         logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n         stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n \n@@ -2406,7 +2438,7 @@ def generate(\n         )\n \n         if generation_config.token_healing:\n-            input_ids = self.heal_tokens(input_ids, tokenizer)\n+            input_ids = self.heal_tokens(input_ids, generation_mode_kwargs.get(\"tokenizer\"))\n \n         if streamer is not None:\n             streamer.put(input_ids.cpu())\n@@ -2444,17 +2476,9 @@ def generate(\n         ):\n             max_cache_length += inputs_tensor.shape[1]\n         self._prepare_cache_for_generation(\n-            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length\n+            generation_config, model_kwargs, generation_mode, batch_size, max_cache_length\n         )\n \n-        # 8. determine generation mode\n-        generation_mode = generation_config.get_generation_mode(assistant_model)\n-\n-        if streamer is not None and (generation_config.num_beams > 1):\n-            raise ValueError(\n-                \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n-            )\n-\n         if self.device.type != input_ids.device.type:\n             warnings.warn(\n                 \"You are calling .generate() with the `input_ids` being on a device type different\"\n@@ -2466,7 +2490,7 @@ def generate(\n                 UserWarning,\n             )\n \n-        # 9. prepare logits processors and stopping criteria\n+        # 8. prepare logits processors and stopping criteria\n         prepared_logits_processor = self._get_logits_processor(\n             generation_config=generation_config,\n             input_ids_seq_length=input_ids_length,\n@@ -2479,22 +2503,23 @@ def generate(\n             negative_prompt_attention_mask=negative_prompt_attention_mask,\n         )\n         prepared_stopping_criteria = self._get_stopping_criteria(\n-            generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n+            generation_config=generation_config,\n+            stopping_criteria=stopping_criteria,\n+            tokenizer=generation_mode_kwargs.get(\"tokenizer\"),\n         )\n \n         # Set model_kwargs `use_cache` so we can use it later in forward runs\n         model_kwargs[\"use_cache\"] = generation_config.use_cache\n \n-        # 10. go into different generation modes\n+        # 9. go into different generation modes\n         if isinstance(custom_generate, Callable):\n             result = custom_generate(\n                 self,\n                 input_ids,\n                 logits_processor=prepared_logits_processor,\n                 stopping_criteria=prepared_stopping_criteria,\n                 generation_config=generation_config,\n-                synced_gpus=synced_gpus,\n-                streamer=streamer,\n+                **generation_mode_kwargs,\n                 **model_kwargs,\n             )\n         elif generation_mode == GenerationMode.ASSISTED_GENERATION:\n@@ -2516,50 +2541,48 @@ def generate(\n                     f\"assisted generation is not supported with stateful models, such as {self.__class__.__name__}\"\n                 )\n \n-            # 11. Get the candidate generator, given the parameterization\n+            # 10. Get the candidate generator, given the parameterization\n             candidate_generator = self._get_candidate_generator(\n                 generation_config=generation_config,\n                 input_ids=input_ids,\n                 inputs_tensor=inputs_tensor,\n-                assistant_model=assistant_model,\n+                assistant_model=generation_mode_kwargs.pop(\"assistant_model\", None),\n                 logits_processor=logits_processor,\n-                target_tokenizer=tokenizer,\n-                assistant_tokenizer=assistant_tokenizer,\n+                target_tokenizer=generation_mode_kwargs.pop(\"tokenizer\", None),\n+                assistant_tokenizer=generation_mode_kwargs.pop(\"assistant_tokenizer\", None),\n                 model_kwargs=model_kwargs,\n             )\n \n-            # 12. run assisted generate\n+            # 11. run assisted generate\n             result = self._assisted_decoding(\n                 input_ids,\n                 candidate_generator=candidate_generator,\n                 logits_processor=prepared_logits_processor,\n                 stopping_criteria=prepared_stopping_criteria,\n                 generation_config=generation_config,\n-                synced_gpus=synced_gpus,\n-                streamer=streamer,\n+                **generation_mode_kwargs,\n                 **model_kwargs,\n             )\n \n         elif generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n-            # 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n+            # 10. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n             result = self._sample(\n                 input_ids,\n                 logits_processor=prepared_logits_processor,\n                 stopping_criteria=prepared_stopping_criteria,\n                 generation_config=generation_config,\n-                synced_gpus=synced_gpus,\n-                streamer=streamer,\n+                **generation_mode_kwargs,\n                 **model_kwargs,\n             )\n \n         elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n-            # 11. run beam sample\n+            # 10. run beam sample\n             result = self._beam_search(\n                 input_ids,\n                 logits_processor=prepared_logits_processor,\n                 stopping_criteria=prepared_stopping_criteria,\n                 generation_config=generation_config,\n-                synced_gpus=synced_gpus,\n+                **generation_mode_kwargs,\n                 **model_kwargs,\n             )\n \n@@ -2681,8 +2704,8 @@ def _sample(\n         logits_processor: LogitsProcessorList,\n         stopping_criteria: StoppingCriteriaList,\n         generation_config: GenerationConfig,\n-        synced_gpus: bool,\n-        streamer: Optional[\"BaseStreamer\"],\n+        synced_gpus: bool = False,\n+        streamer: Optional[\"BaseStreamer\"] = None,\n         **model_kwargs,\n     ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n         r\"\"\"\n@@ -3110,7 +3133,7 @@ def _beam_search(\n         logits_processor: LogitsProcessorList,\n         stopping_criteria: StoppingCriteriaList,\n         generation_config: GenerationConfig,\n-        synced_gpus: bool,\n+        synced_gpus: bool = False,\n         **model_kwargs,\n     ) -> Union[GenerateBeamOutput, torch.LongTensor]:\n         r\"\"\"\n@@ -3447,8 +3470,8 @@ def _assisted_decoding(\n         logits_processor: LogitsProcessorList,\n         stopping_criteria: StoppingCriteriaList,\n         generation_config: GenerationConfig,\n-        synced_gpus: bool,\n-        streamer: Optional[\"BaseStreamer\"],\n+        synced_gpus: bool = False,\n+        streamer: Optional[\"BaseStreamer\"] = None,\n         **model_kwargs,\n     ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n         r\"\"\""
        },
        {
            "sha": "b14f353685c23326003aa48af1c8c9a75b8be710",
            "filename": "src/transformers/models/csm/generation_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py?ref=3f40ebf6209c724acea11a43625c7fe10cd0cba7",
            "patch": "@@ -153,8 +153,8 @@ def _sample(\n         logits_processor: LogitsProcessorList,\n         stopping_criteria: StoppingCriteriaList,\n         generation_config: GenerationConfig,\n-        synced_gpus: bool,\n-        streamer: Optional[\"BaseStreamer\"],\n+        synced_gpus: bool = False,\n+        streamer: Optional[\"BaseStreamer\"] = None,\n         **model_kwargs,\n     ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n         \"\"\""
        },
        {
            "sha": "439b498b0988ef3874d6e185d98e2cab9b416e93",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=3f40ebf6209c724acea11a43625c7fe10cd0cba7",
            "patch": "@@ -265,14 +265,20 @@ def _main_generate_loop(\n     ):\n         # ********** mostly taken from main generate function up to calling the different methods (see NOTE) **********\n         # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n-        tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n-        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n-\n+        generation_mode_kwargs = self._extract_generation_mode_kwargs(\n+            custom_generate,\n+            kwargs,\n+            synced_gpus,\n+            assistant_model,\n+            streamer,\n+        )\n         generation_config, model_kwargs = self._prepare_generation_config(\n             generation_config, use_model_defaults, **kwargs\n         )\n+        generation_mode = generation_config.get_generation_mode(assistant_model)\n+\n         self._validate_model_kwargs(model_kwargs.copy())\n-        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n+        self._validate_generation_mode(generation_mode, generation_mode_kwargs)\n \n         # 2. Set generation parameters if not already defined\n         if synced_gpus is None:\n@@ -308,7 +314,7 @@ def _main_generate_loop(\n         )\n \n         if generation_config.token_healing:\n-            input_ids = self.heal_tokens(input_ids, tokenizer)\n+            input_ids = self.heal_tokens(input_ids, generation_mode_kwargs.get(\"tokenizer\"))\n \n         if streamer is not None:\n             streamer.put(input_ids.cpu())\n@@ -347,18 +353,10 @@ def _main_generate_loop(\n         ):\n             max_cache_length += inputs_tensor.shape[1]\n         self._prepare_cache_for_generation(\n-            generation_config, model_kwargs, assistant_model, batch_size, max_cache_length\n+            generation_config, model_kwargs, generation_mode, batch_size, max_cache_length\n         )\n \n-        # 8. determine generation mode\n-        generation_mode = generation_config.get_generation_mode(assistant_model)\n-\n-        if streamer is not None and (generation_config.num_beams > 1):\n-            raise ValueError(\n-                \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n-            )\n-\n-        # 9. prepare logits processors and stopping criteria\n+        # 8. prepare logits processors and stopping criteria\n         prepared_logits_processor = self._get_logits_processor(\n             generation_config=generation_config,\n             input_ids_seq_length=input_ids_length,\n@@ -371,7 +369,9 @@ def _main_generate_loop(\n             negative_prompt_attention_mask=negative_prompt_attention_mask,\n         )\n         prepared_stopping_criteria = self._get_stopping_criteria(\n-            generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=tokenizer, **kwargs\n+            generation_config=generation_config,\n+            stopping_criteria=stopping_criteria,\n+            tokenizer=generation_mode_kwargs.get(\"tokenizer\"),\n         )\n \n         # Set model_kwargs `use_cache` so we can use it later in forward runs\n@@ -393,8 +393,7 @@ def _main_generate_loop(\n                 logits_processor=prepared_logits_processor,\n                 stopping_criteria=prepared_stopping_criteria,\n                 generation_config=generation_config,\n-                synced_gpus=synced_gpus,\n-                streamer=streamer,\n+                **generation_mode_kwargs,\n                 **model_kwargs,\n             )\n         else:"
        },
        {
            "sha": "c10a0f80acf12f37d0d1fe89f643f273ad39dda6",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=3f40ebf6209c724acea11a43625c7fe10cd0cba7",
            "patch": "@@ -1222,7 +1222,7 @@ def _prepare_model_inputs(\n         self.codec_model._prepare_cache_for_generation(\n             generation_config=self.codec_model.generation_config,\n             model_kwargs=temporary_model_kwargs,\n-            assistant_model=None,\n+            generation_mode=None,\n             batch_size=batch_size,\n             max_cache_length=self.config.codec_config.sliding_window,\n         )"
        },
        {
            "sha": "8541a911e9475c0542f7837b1d14acab007d074b",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=3f40ebf6209c724acea11a43625c7fe10cd0cba7",
            "patch": "@@ -357,7 +357,7 @@ def _prepare_model_inputs(\n         self.codec_model._prepare_cache_for_generation(\n             generation_config=self.codec_model.generation_config,\n             model_kwargs=temporary_model_kwargs,\n-            assistant_model=None,\n+            generation_mode=None,\n             batch_size=batch_size,\n             max_cache_length=self.config.codec_config.sliding_window,\n         )"
        },
        {
            "sha": "8a66f9e13912b17b681e61e4c38be67ac6cffdc8",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=3f40ebf6209c724acea11a43625c7fe10cd0cba7",
            "patch": "@@ -1258,7 +1258,7 @@ def generate(\n         self._prepare_cache_for_generation(\n             generation_config,\n             model_kwargs,\n-            assistant_model=None,\n+            generation_mode=None,\n             batch_size=batch_size,\n             max_cache_length=max_cache_length,\n         )"
        },
        {
            "sha": "58c012a1cfb93ffc48d1d53b3609e365882b22db",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=3f40ebf6209c724acea11a43625c7fe10cd0cba7",
            "patch": "@@ -2173,7 +2173,7 @@ def generate(\n         self._prepare_cache_for_generation(\n             generation_config,\n             model_kwargs,\n-            assistant_model=None,\n+            generation_mode=None,\n             batch_size=batch_size,\n             max_cache_length=max_cache_length,\n         )"
        },
        {
            "sha": "f3932137a082968a07b6b8393bedcb559f283ab8",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f40ebf6209c724acea11a43625c7fe10cd0cba7/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=3f40ebf6209c724acea11a43625c7fe10cd0cba7",
            "patch": "@@ -1566,7 +1566,7 @@ def extend_enc_output(tensor, num_beams=None):\n         self._prepare_cache_for_generation(\n             generation_config,\n             model_kwargs,\n-            assistant_model=None,\n+            generation_mode=None,\n             batch_size=input_ids.shape[0],\n             max_cache_length=generation_config.max_length - 1,\n         )"
        }
    ],
    "stats": {
        "total": 256,
        "additions": 139,
        "deletions": 117
    }
}