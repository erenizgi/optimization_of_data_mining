{
    "author": "marcndo",
    "message": "fix chunked prefill implementation issue-43082 (#43132)\n\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>",
    "sha": "5828e655311b2d8f7cca2d2992264f04df843956",
    "files": [
        {
            "sha": "55d24d152c39b8d9939d0518a81a3c1d55b655d2",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5828e655311b2d8f7cca2d2992264f04df843956/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5828e655311b2d8f7cca2d2992264f04df843956/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=5828e655311b2d8f7cca2d2992264f04df843956",
            "patch": "@@ -3860,7 +3860,6 @@ def _prefill(self, input_ids: torch.LongTensor, generation_config: GenerationCon\n                 model_kwargs[\"cache_position\"] = torch.arange(\n                     past_length, current_length, dtype=torch.long, device=input_chunk.device\n                 )\n-                model_kwargs[\"position_ids\"] = model_kwargs[\"cache_position\"].unsqueeze(0)\n                 model_inputs = self.prepare_inputs_for_generation(input_chunk, **model_kwargs)\n \n                 outputs = model_forward(**model_inputs, return_dict=True)"
        }
    ],
    "stats": {
        "total": 1,
        "additions": 0,
        "deletions": 1
    }
}