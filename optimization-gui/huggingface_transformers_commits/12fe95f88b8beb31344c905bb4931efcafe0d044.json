{
    "author": "stevhliu",
    "message": "[docs] Update shard size (#42749)\n\n* shard size\n\n* feedback\n\n* add link\n\n* clarify\n\n* update link",
    "sha": "12fe95f88b8beb31344c905bb4931efcafe0d044",
    "files": [
        {
            "sha": "23db536bd6eac4bba3469b754dae401308934280",
            "filename": "docs/source/en/models.md",
            "status": "modified",
            "additions": 8,
            "deletions": 38,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/12fe95f88b8beb31344c905bb4931efcafe0d044/docs%2Fsource%2Fen%2Fmodels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/12fe95f88b8beb31344c905bb4931efcafe0d044/docs%2Fsource%2Fen%2Fmodels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodels.md?ref=12fe95f88b8beb31344c905bb4931efcafe0d044",
            "patch": "@@ -112,64 +112,34 @@ Transformers reduces some of these memory-related challenges with fast initializ\n \n ### Sharded checkpoints\n \n-The [`~PreTrainedModel.save_pretrained`] method automatically shards checkpoints larger than 10GB.\n+[`~PreTrainedModel.save_pretrained`] automatically shards checkpoints larger than 50GB. This keeps shard counts low for large models and simplifies file management.\n \n-Each shard is loaded sequentially after the previous shard is loaded, limiting memory usage to only the model size and the largest shard size.\n+Parameters load in parallel and peak memory only depends on model size. Use `max_shard_size` in [`~PreTrainedModel.save_pretrained`] to set the maximum checkpoint size before sharding.\n \n-The `max_shard_size` parameter defaults to 5GB for each shard because it is easier to run on free-tier GPU instances without running out of memory.\n+> [!NOTE]\n+> Memory usage for models requiring dynamic weight conversion depends on the model size and the size of the largest parameters in a single conversion. This typically applies to mixture-of-experts (MoE) models where the memory usage is the model size plus the number of experts on one layer. Refer to the [dynamic weight loader](./weightconverter#fast-and-efficient-model-loading) guide to learn more about how models are loaded.\n \n-For example, create some shards checkpoints for [BioMistral/BioMistral-7B](https://hf.co/BioMistral/BioMistral-7B) in [`~PreTrainedModel.save_pretrained`].\n-\n-```py\n-from transformers import AutoModel\n-import tempfile\n-import os\n-\n-model = AutoModel.from_pretrained(\"biomistral/biomistral-7b\")\n-with tempfile.TemporaryDirectory() as tmp_dir:\n-    model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n-    print(sorted(os.listdir(tmp_dir)))\n-```\n-\n-Reload the sharded checkpoint with [`~PreTrainedModel.from_pretrained`].\n-\n-```py\n-with tempfile.TemporaryDirectory() as tmp_dir:\n-    model.save_pretrained(tmp_dir)\n-    new_model = AutoModel.from_pretrained(tmp_dir)\n-```\n-\n-Sharded checkpoints can also be directly loaded with [`~transformers.trainer_utils.load_sharded_checkpoint`].\n-\n-```py\n-from transformers.trainer_utils import load_sharded_checkpoint\n-\n-with tempfile.TemporaryDirectory() as tmp_dir:\n-    model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n-    load_sharded_checkpoint(model, tmp_dir)\n-```\n-\n-The [`~PreTrainedModel.save_pretrained`] method creates an index file that maps parameter names to the files they're stored in. The index file has two keys, `metadata` and `weight_map`.\n+[`~PreTrainedModel.save_pretrained`] also creates an index file mapping parameter names to their shard files. The index contains two keys, `metadata` and `weight_map`.\n \n ```py\n import json\n \n with tempfile.TemporaryDirectory() as tmp_dir:\n-    model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")\n+    model.save_pretrained(tmp_dir, max_shard_size=\"50GB\")\n     with open(os.path.join(tmp_dir, \"model.safetensors.index.json\"), \"r\") as f:\n         index = json.load(f)\n \n print(index.keys())\n ```\n \n-The `metadata` key provides the total model size.\n+`metadata` stores the total model size.\n \n ```py\n index[\"metadata\"]\n {'total_size': 28966928384}\n ```\n \n-The `weight_map` key maps each parameter to the shard it's stored in.\n+`weight_map` maps each parameter to its shard file.\n \n ```py\n index[\"weight_map\"]"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 8,
        "deletions": 38
    }
}