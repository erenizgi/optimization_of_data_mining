{
    "author": "stevhliu",
    "message": "[docs] Fix image link (#36869)\n\n* fix image link\n\n* fix\n\n* update\n\n* fix",
    "sha": "a844297088d046b3d16d3726ca96822b479e884c",
    "files": [
        {
            "sha": "7bd6be74f5b11ed9dc756ae1373d7bacd6cdc037",
            "filename": "docs/source/en/processors.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a844297088d046b3d16d3726ca96822b479e884c/docs%2Fsource%2Fen%2Fprocessors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a844297088d046b3d16d3726ca96822b479e884c/docs%2Fsource%2Fen%2Fprocessors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fprocessors.md?ref=a844297088d046b3d16d3726ca96822b479e884c",
            "patch": "@@ -29,8 +29,8 @@ import requests\n \n processor = AutoProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\n \n-prompt = \"answer en Where is the cow standing?\"\n-url = \"https://huggingface.co/gv-hf/PaliGemma-test-224px-hf/resolve/main/cow_beach_1.png\"\n+prompt = \"answer en Where is the cat standing?\"\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n image = Image.open(requests.get(url, stream=True).raw)\n \n inputs = processor(text=prompt, images=image, return_tensors=\"pt\")"
        },
        {
            "sha": "ac06bdeb823f35c4134d3bd53cae0a230ff64308",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 29,
            "deletions": 12,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/a844297088d046b3d16d3726ca96822b479e884c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a844297088d046b3d16d3726ca96822b479e884c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=a844297088d046b3d16d3726ca96822b479e884c",
            "patch": "@@ -1272,20 +1272,37 @@ def forward(\n         >>> import requests\n         >>> from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n \n-        >>> model = Gemma3ForConditionalGeneration.from_pretrained(\"google/Gemma3-test-224px-hf\")\n-        >>> processor = AutoProcessor.from_pretrained(\"google/Gemma3-test-224px-hf\")\n-\n-        >>> prompt = \"answer en Where is the cow standing?\"\n-        >>> url = \"https://huggingface.co/gv-hf/Gemma3-test-224px-hf/resolve/main/cow_beach_1.png\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n-        >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n-\n+        >>> model = Gemma3ForConditionalGeneration.from_pretrained(\"google/gemma-3-4b-it\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n+\n+        >>> messages = [\n+        ...     {\n+        ...         \"role\": \"system\",\n+        ...         \"content\": [\n+        ...             {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n+        ...         ]\n+        ...     },\n+        ...     {\n+        ...         \"role\": \"user\", \"content\": [\n+        ...             {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"},\n+        ...             {\"type\": \"text\", \"text\": \"Where is the cat standing?\"},\n+        ...         ]\n+        ...     },\n+        ... ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+        ...     messages,\n+        ...     tokenizer=True,\n+        ...     return_dict=True,\n+        ...     return_tensors=\"pt\",\n+        ...     add_generation_prompt=True\n+        ... )\n         >>> # Generate\n-        >>> generate_ids = model.generate(**inputs, max_length=30)\n+        >>> generate_ids = model.generate(**inputs)\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"answer en Where is the cow standing?\\nbeach\"\n-        ```\"\"\"\n+        \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhere is the cat standing?\\nmodel\\nBased on the image, the cat is standing in a snowy area, likely outdoors. It appears to\"\n+        ```\n+        \"\"\"\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "e9baaf1c52913b1dc15a599bd22d37e12cad603f",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 29,
            "deletions": 12,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/a844297088d046b3d16d3726ca96822b479e884c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a844297088d046b3d16d3726ca96822b479e884c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=a844297088d046b3d16d3726ca96822b479e884c",
            "patch": "@@ -883,20 +883,37 @@ def forward(\n         >>> import requests\n         >>> from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n \n-        >>> model = Gemma3ForConditionalGeneration.from_pretrained(\"google/Gemma3-test-224px-hf\")\n-        >>> processor = AutoProcessor.from_pretrained(\"google/Gemma3-test-224px-hf\")\n-\n-        >>> prompt = \"answer en Where is the cow standing?\"\n-        >>> url = \"https://huggingface.co/gv-hf/Gemma3-test-224px-hf/resolve/main/cow_beach_1.png\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n-        >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n-\n+        >>> model = Gemma3ForConditionalGeneration.from_pretrained(\"google/gemma-3-4b-it\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n+\n+        >>> messages = [\n+        ...     {\n+        ...         \"role\": \"system\",\n+        ...         \"content\": [\n+        ...             {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n+        ...         ]\n+        ...     },\n+        ...     {\n+        ...         \"role\": \"user\", \"content\": [\n+        ...             {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"},\n+        ...             {\"type\": \"text\", \"text\": \"Where is the cat standing?\"},\n+        ...         ]\n+        ...     },\n+        ... ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+        ...     messages,\n+        ...     tokenizer=True,\n+        ...     return_dict=True,\n+        ...     return_tensors=\"pt\",\n+        ...     add_generation_prompt=True\n+        ... )\n         >>> # Generate\n-        >>> generate_ids = model.generate(**inputs, max_length=30)\n+        >>> generate_ids = model.generate(**inputs)\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"answer en Where is the cow standing?\\nbeach\"\n-        ```\"\"\"\n+        \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhere is the cat standing?\\nmodel\\nBased on the image, the cat is standing in a snowy area, likely outdoors. It appears to\"\n+        ```\n+        \"\"\"\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")"
        },
        {
            "sha": "238658add5f701fb7c5309f3b01ee260ef473469",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a844297088d046b3d16d3726ca96822b479e884c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a844297088d046b3d16d3726ca96822b479e884c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=a844297088d046b3d16d3726ca96822b479e884c",
            "patch": "@@ -464,19 +464,19 @@ def forward(\n         >>> import requests\n         >>> from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n \n-        >>> model = PaliGemmaForConditionalGeneration.from_pretrained(\"google/PaliGemma-test-224px-hf\")\n-        >>> processor = AutoProcessor.from_pretrained(\"google/PaliGemma-test-224px-hf\")\n+        >>> model = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma2-3b-mix-224\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/paligemma2-3b-mix-224\")\n \n-        >>> prompt = \"answer en Where is the cow standing?\"\n-        >>> url = \"https://huggingface.co/gv-hf/PaliGemma-test-224px-hf/resolve/main/cow_beach_1.png\"\n+        >>> prompt = \"Where is the cat standing?\"\n+        >>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n         >>> image = Image.open(requests.get(url, stream=True).raw)\n \n         >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n \n         >>> # Generate\n-        >>> generate_ids = model.generate(**inputs, max_length=30)\n+        >>> generate_ids = model.generate(**inputs,)\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"answer en Where is the cow standing?\\nbeach\"\n+        \"Where is the cat standing?\\nsnow\"\n         ```\"\"\"\n \n         if (input_ids is None) ^ (inputs_embeds is not None):"
        }
    ],
    "stats": {
        "total": 98,
        "additions": 66,
        "deletions": 32
    }
}