{
    "author": "gante",
    "message": "[generate] shape checks in tests compatible with fixed-length caches (+ some minor fixes) (#35993)\n\n* shape checks compatible with static cache\r\n\r\n* add test\r\n\r\n* tmp\r\n\r\n* manually turn on eager attn when we want to output attn\r\n\r\n* typo\r\n\r\n* generalize to encoder-decoder models\r\n\r\n* force compilation on cpu\r\n\r\n* tmp commit\r\n\r\n* fix static cache shape checks\r\n\r\n* models with odd caches\r\n\r\n* fix copies\r\n\r\n* shorter cache search loop\r\n\r\n* use decoder_past_key_values everywhere\r\n\r\n* better test variable names and comments\r\n\r\n* signature\r\n\r\n* rename _check_outputs into _check_generate_outputs\r\n\r\n* add comments\r\n\r\n* HybridCache future test note",
    "sha": "be2ac0916a7902e1683d708805270142257a254a",
    "files": [
        {
            "sha": "a773c4a1d96b622c6c003ff6a01d0c7e3009f714",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 21,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -116,6 +116,16 @@\n     from accelerate.hooks import AlignDevicesHook, add_hook_to_module\n \n \n+# Variable names used to hold the cache at generation time\n+ALL_CACHE_NAMES = [\n+    \"past_key_values\",  # default\n+    \"cache_params\",  # mamba-based models\n+    \"state\",  # rwkv\n+    \"mems\",  # xlnet\n+    \"past_buckets_states\",  # reformer\n+]\n+\n+\n @dataclass\n class GenerateDecoderOnlyOutput(ModelOutput):\n     \"\"\"\n@@ -756,21 +766,6 @@ def _expand_dict_for_generation(dict_to_expand):\n \n         return input_ids, model_kwargs\n \n-    def _extract_past_from_model_output(self, outputs: ModelOutput):\n-        past_key_values = None\n-        cache_name = \"past_key_values\"\n-        if \"past_key_values\" in outputs:\n-            past_key_values = outputs.past_key_values\n-        elif \"mems\" in outputs:\n-            past_key_values = outputs.mems\n-        elif \"past_buckets_states\" in outputs:\n-            past_key_values = outputs.past_buckets_states\n-        elif \"cache_params\" in outputs:\n-            past_key_values = outputs.cache_params\n-            cache_name = \"cache_params\"\n-\n-        return cache_name, past_key_values\n-\n     def _update_model_kwargs_for_generation(\n         self,\n         outputs: ModelOutput,\n@@ -779,10 +774,15 @@ def _update_model_kwargs_for_generation(\n         num_new_tokens: int = 1,\n     ) -> Dict[str, Any]:\n         # update past_key_values keeping its naming used in model code\n-        cache_name, cache = self._extract_past_from_model_output(outputs)\n-        model_kwargs[cache_name] = cache\n-        if getattr(outputs, \"state\", None) is not None:\n-            model_kwargs[\"state\"] = outputs.state\n+        for possible_cache_name in ALL_CACHE_NAMES:\n+            if possible_cache_name in outputs:\n+                # TODO (joao): remove output/input mismatch when these old models (xlnet, reformer) are deprecated\n+                if possible_cache_name in (\"past_buckets_states\", \"mems\"):\n+                    cache_name = \"past_key_values\"\n+                else:\n+                    cache_name = possible_cache_name\n+                model_kwargs[cache_name] = getattr(outputs, possible_cache_name)\n+                break\n \n         # update token_type_ids with last value\n         if \"token_type_ids\" in model_kwargs:\n@@ -2087,7 +2087,7 @@ def generate(\n         # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n         # - different models have a different cache name expected by the model (default = \"past_key_values\")\n         # - `max_length`, prepared above, is used to determine the maximum cache length\n-        max_cache_length = generation_config.max_length\n+        max_cache_length = generation_config.max_length - 1\n         if (\n             inputs_tensor.shape[1] != input_ids_length\n             and model_input_name == \"inputs_embeds\"\n@@ -2994,7 +2994,9 @@ def _contrastive_search(\n                 next_past_key_values = selected_outputs[\"past_key_values\"]\n \n             else:\n-                _, next_past_key_values = self._extract_past_from_model_output(outputs)\n+                next_past_key_values = None\n+                for possible_cache_name in ALL_CACHE_NAMES:\n+                    next_past_key_values = next_past_key_values or getattr(outputs, possible_cache_name, None)\n                 # Do it in-place layer per layer to save memory\n                 if isinstance(next_past_key_values, DynamicCache) or (\n                     isinstance(next_past_key_values, EncoderDecoderCache)"
        },
        {
            "sha": "279a7c046c4dc98b49c36d73877bb3b21d2495d1",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -2557,32 +2557,3 @@ def generate(\n             return outputs\n         else:\n             return output_values\n-\n-    def _update_model_kwargs_for_generation(\n-        self,\n-        outputs: ModelOutput,\n-        model_kwargs: Dict[str, Any],\n-        is_encoder_decoder: bool = False,\n-        model_inputs: Optional[Dict[str, Any]] = None,\n-    ) -> Dict[str, Any]:\n-        # update past_key_values\n-        cache_name, cache = self._extract_past_from_model_output(outputs)\n-        model_kwargs[cache_name] = cache\n-\n-        if getattr(outputs, \"state\", None) is not None:\n-            model_kwargs[\"state\"] = outputs.state\n-\n-        # update token_type_ids with last value\n-        if \"token_type_ids\" in model_kwargs:\n-            token_type_ids = model_kwargs[\"token_type_ids\"]\n-            model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n-\n-        # update decoder attention mask\n-        if \"decoder_attention_mask\" in model_kwargs:\n-            decoder_attention_mask = model_kwargs[\"decoder_attention_mask\"]\n-            model_kwargs[\"decoder_attention_mask\"] = torch.cat(\n-                [decoder_attention_mask, decoder_attention_mask.new_ones((decoder_attention_mask.shape[0], 1))],\n-                dim=-1,\n-            )\n-\n-        return model_kwargs"
        },
        {
            "sha": "320d2093133fe0cc51c2b76eacb68c2431d4e118",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 49,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -1329,54 +1329,6 @@ def prepare_inputs_for_generation(\n         )\n         return model_inputs\n \n-    def _update_model_kwargs_for_generation(\n-        self,\n-        outputs: ModelOutput,\n-        model_kwargs: Dict[str, Any],\n-        is_encoder_decoder: bool = False,\n-        num_new_tokens: int = 1,\n-    ) -> Dict[str, Any]:\n-        # update past_key_values keeping its naming used in model code\n-        cache_name, cache = self._extract_past_from_model_output(outputs)\n-        model_kwargs[cache_name] = cache\n-        if getattr(outputs, \"state\", None) is not None:\n-            model_kwargs[\"state\"] = outputs.state\n-\n-        # update attention_mask\n-        if getattr(outputs, \"attention_mask\", None) is not None:\n-            model_kwargs[\"attention_mask\"] = outputs.attention_mask\n-\n-        # update token_type_ids with last value\n-        if \"token_type_ids\" in model_kwargs:\n-            token_type_ids = model_kwargs[\"token_type_ids\"]\n-            model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n-\n-        if not is_encoder_decoder:\n-            # update attention mask\n-            if \"attention_mask\" in model_kwargs:\n-                attention_mask = model_kwargs[\"attention_mask\"]\n-                model_kwargs[\"attention_mask\"] = torch.cat(\n-                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n-                )\n-        else:\n-            # update decoder attention mask\n-            if \"decoder_attention_mask\" in model_kwargs:\n-                decoder_attention_mask = model_kwargs[\"decoder_attention_mask\"]\n-                model_kwargs[\"decoder_attention_mask\"] = torch.cat(\n-                    [decoder_attention_mask, decoder_attention_mask.new_ones((decoder_attention_mask.shape[0], 1))],\n-                    dim=-1,\n-                )\n-\n-        if model_kwargs.get(\"use_cache\", True):\n-            model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + num_new_tokens\n-        else:\n-            past_positions = model_kwargs.pop(\"cache_position\")\n-            new_positions = torch.arange(\n-                past_positions[-1] + 1, past_positions[-1] + num_new_tokens + 1, dtype=past_positions.dtype\n-            ).to(past_positions.device)\n-            model_kwargs[\"cache_position\"] = torch.cat((past_positions, new_positions))\n-        return model_kwargs\n-\n     def _reorder_cache(self, *args, **kwargs):\n         return self.language_model._reorder_cache(*args, **kwargs)\n "
        },
        {
            "sha": "6833fd476ef2b1d376892a8c76199c86380fafde",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 204,
            "deletions": 103,
            "changes": 307,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -72,7 +72,14 @@\n         SpeechEncoderDecoderModel,\n         T5ForConditionalGeneration,\n     )\n-    from transformers.cache_utils import Cache, DynamicCache, EncoderDecoderCache, QuantoQuantizedCache, StaticCache\n+    from transformers.cache_utils import (\n+        Cache,\n+        DynamicCache,\n+        EncoderDecoderCache,\n+        HybridCache,\n+        QuantoQuantizedCache,\n+        StaticCache,\n+    )\n     from transformers.generation import (\n         BeamSampleDecoderOnlyOutput,\n         BeamSampleEncoderDecoderOutput,\n@@ -473,6 +480,8 @@ def test_greedy_generate(self):\n     def test_greedy_generate_dict_outputs(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(\n@@ -499,12 +508,14 @@ def test_greedy_generate_dict_outputs(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, GreedySearchDecoderOnlyOutput)\n \n-            self._check_outputs(output_generate, model.config)\n+            self._check_generate_outputs(output_generate, model.config)\n \n     @pytest.mark.generate\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n@@ -531,7 +542,7 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n                     output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                 )\n \n-            self._check_outputs(output_generate, model.config, use_cache=True)\n+            self._check_generate_outputs(output_generate, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_sample_generate(self):\n@@ -550,6 +561,8 @@ def test_sample_generate(self):\n     def test_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._sample_generate(\n@@ -577,7 +590,7 @@ def test_sample_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, SampleDecoderOnlyOutput)\n \n-            self._check_outputs(output_generate, model.config, num_return_sequences=2)\n+            self._check_generate_outputs(output_generate, model.config, num_return_sequences=2)\n \n     @pytest.mark.generate\n     def test_beam_search_generate(self):\n@@ -598,6 +611,8 @@ def test_beam_search_generate(self):\n     def test_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n@@ -625,7 +640,7 @@ def test_beam_search_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n \n-            self._check_outputs(\n+            self._check_generate_outputs(\n                 output_generate,\n                 model.config,\n                 num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n@@ -642,6 +657,8 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n             if any(model_name in model_class.__name__.lower() for model_name in [\"rwkv\"]):\n                 self.skipTest(reason=\"Won't fix: model with non-standard dictionary output shapes\")\n \n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n \n@@ -666,7 +683,7 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n                     output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                 )\n \n-            self._check_outputs(\n+            self._check_generate_outputs(\n                 output_generate,\n                 model.config,\n                 use_cache=True,\n@@ -721,6 +738,8 @@ def test_beam_sample_generate(self):\n     def test_beam_sample_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_beam_kwargs()\n@@ -750,7 +769,7 @@ def test_beam_sample_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSampleDecoderOnlyOutput)\n \n-            self._check_outputs(\n+            self._check_generate_outputs(\n                 output_generate,\n                 model.config,\n                 num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n@@ -813,6 +832,8 @@ def test_group_beam_search_generate(self):\n     def test_group_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n             model = model_class(config).to(torch_device).eval()\n             beam_kwargs = self._get_diverse_beam_kwargs()\n@@ -840,7 +861,7 @@ def test_group_beam_search_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n \n-            self._check_outputs(\n+            self._check_generate_outputs(\n                 output_generate,\n                 model.config,\n                 num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n@@ -909,6 +930,8 @@ def test_constrained_beam_search_generate(self):\n     def test_constrained_beam_search_generate_dict_output(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n             model = model_class(config).to(torch_device).eval()\n \n@@ -947,7 +970,7 @@ def test_constrained_beam_search_generate_dict_output(self):\n                 # Retrocompatibility check\n                 self.assertIsInstance(output_generate, BeamSearchDecoderOnlyOutput)\n \n-            self._check_outputs(\n+            self._check_generate_outputs(\n                 output_generate,\n                 model.config,\n                 num_return_sequences=beam_kwargs[\"num_return_sequences\"],\n@@ -999,6 +1022,8 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n             config.is_decoder = True\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._contrastive_generate(\n@@ -1019,7 +1044,7 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n                     output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n                 )\n \n-            self._check_outputs(output_generate, model.config, use_cache=True)\n+            self._check_generate_outputs(output_generate, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_contrastive_generate_low_memory(self):\n@@ -1205,7 +1230,7 @@ def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n             # The two outputs must match and their shape must be as expected\n             self._check_similar_generate_outputs(output_greedy, output_assisted)\n             for output in (output_greedy, output_assisted):\n-                self._check_outputs(output, model.config, use_cache=True)\n+                self._check_generate_outputs(output, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n@@ -1270,7 +1295,7 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n             # The two outputs must match and their shape must be as expected\n             self._check_similar_generate_outputs(output_greedy, output_prompt_lookup)\n             for output in (output_greedy, output_prompt_lookup):\n-                self._check_outputs(output, model.config, use_cache=True)\n+                self._check_generate_outputs(output, model.config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_dola_decoding_sample(self):\n@@ -1320,7 +1345,7 @@ def test_dola_decoding_sample(self):\n                 \"dola_layers\": \"low\",\n             }\n             output_dola = model.generate(**generation_kwargs, **logits_processor_kwargs, **inputs_dict)\n-            self._check_outputs(output_dola, model.config, use_cache=getattr(config, \"use_cache\", False))\n+            self._check_generate_outputs(output_dola, model.config, use_cache=getattr(config, \"use_cache\", False))\n \n     @pytest.mark.generate\n     def test_assisted_decoding_sample(self):\n@@ -1381,7 +1406,7 @@ def test_assisted_decoding_sample(self):\n             }\n             output_assisted = model.generate(**generation_kwargs, **inputs_dict)\n \n-            self._check_outputs(output_assisted, config, use_cache=True)\n+            self._check_generate_outputs(output_assisted, config, use_cache=True)\n \n     @pytest.mark.generate\n     def test_prompt_lookup_decoding_stops_at_eos(self):\n@@ -1419,6 +1444,8 @@ def test_generate_with_head_masking(self):\n         for model_class in self.all_generative_model_classes:\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n             text_config = config.get_text_config()\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n \n             # We want to test only encoder-decoder models\n             if not text_config.is_encoder_decoder:\n@@ -1765,7 +1792,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             num_hidden_layers = text_config.num_hidden_layers\n \n             inputs_embeds = model.get_input_embeddings()(input_ids)\n-            max_cache_len += inputs_embeds.shape[1]\n+            max_cache_len += inputs_embeds.shape[1] - 1  # the last generated token has no cache\n             outputs = model.generate(inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict)\n \n             # we should get `max_length` in shape, not `max_length - embeds_length`\n@@ -2003,7 +2030,7 @@ def test_generate_with_static_cache(self):\n                 )\n \n                 # Check 1: The cache shapes must match the expected shapes\n-                max_cache_len = seq_length + max_new_tokens\n+                max_cache_len = seq_length + max_new_tokens - 1  # cache len = gen len - 1, the last token has no cache\n                 text_config = config.text_config if hasattr(config, \"text_config\") else config\n                 head_dim = (\n                     text_config.head_dim\n@@ -2138,6 +2165,58 @@ def test_generate_compile_model_forward(self):\n             for dynamic_result, compiled_result in zip(dynamic_outputs, compiled_outputs):\n                 self._check_similar_generate_outputs(dynamic_result, compiled_result)\n \n+    @pytest.mark.generate\n+    def test_generate_compilation_all_outputs(self):\n+        \"\"\"\n+        Tests that all optional outputs are behaving as expected when compilation is triggered.\n+        In essence, it's the same as `test_greedy_generate_dict_outputs`, but with automatic compilation triggered.\n+        \"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_static_cache:\n+                self.skipTest(\"This model doesn't support static cache (= no expectations of compilation support)\")\n+\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            if self.has_attentions:\n+                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n+            model = model_class(config).to(torch_device).eval()\n+\n+            # compilation-specific setup\n+            torch.compiler.reset()  # prevent cached compilation from being used in the test\n+            has_defined_cache_implementation = model.generation_config.cache_implementation is not None\n+            model.generation_config.compile_config._compile_all_devices = True  # force compilation (e.g. fast CI, CPU)\n+            if not has_defined_cache_implementation:\n+                model.generation_config.cache_implementation = \"static\"\n+\n+            logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n+            output_generate = model.generate(\n+                do_sample=False,\n+                num_beams=1,\n+                max_new_tokens=self.max_new_tokens,\n+                min_new_tokens=self.max_new_tokens,\n+                output_attentions=True,\n+                output_hidden_states=True,\n+                output_scores=True,\n+                output_logits=True,\n+                return_dict_in_generate=True,\n+                use_cache=True,\n+                **logits_processor_kwargs,\n+                **inputs_dict,\n+            )\n+\n+            # Sanity check: compilation has happened\n+            self.assertTrue(hasattr(model, \"_compiled_call\"))\n+\n+            if model.config.is_encoder_decoder:\n+                self.assertTrue(output_generate.sequences.shape[-1] == self.max_new_tokens + 1)\n+                self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n+            else:\n+                self.assertTrue(\n+                    output_generate.sequences.shape[-1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[-1]\n+                )\n+                self.assertIsInstance(output_generate, GenerateDecoderOnlyOutput)\n+\n+            self._check_generate_outputs(output_generate, model.config, use_cache=True)\n+\n     @pytest.mark.generate\n     def test_generate_methods_with_logits_to_keep(self):\n         for model_class in self.all_generative_model_classes:\n@@ -2290,86 +2369,90 @@ def test_eager_matches_fa2_generate(self):\n         # check whether we still need the overwrites\n         self._test_attention_implementation(\"flash_attention_2\")\n \n-    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n+    def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n         input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n         internal_batch_size = (\n             input_batch_size * num_beams if num_beams > 1 else input_batch_size * num_return_sequences\n         )\n \n-        seq_length = getattr(self.model_tester, \"seq_length\", None)\n-        seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_length)\n-        seq_length = getattr(self.model_tester, \"text_seq_length\", seq_length)\n+        prompt_length = getattr(self.model_tester, \"seq_length\", None)\n+        prompt_length = getattr(self.model_tester, \"encoder_seq_length\", prompt_length)\n+        prompt_length = getattr(self.model_tester, \"text_seq_length\", prompt_length)\n \n         config = config.text_config if hasattr(config, \"text_config\") else config\n \n-        gen_len = (\n-            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n+        generated_length = (\n+            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - prompt_length\n         )\n+        decoder_past_key_values = getattr(output, \"past_key_values\", None)\n+        if config.is_encoder_decoder and isinstance(decoder_past_key_values, EncoderDecoderCache):\n+            decoder_past_key_values = decoder_past_key_values.self_attention_cache\n \n         # in some models we subsample the sequence length in inner layers\n         if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n-            seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n+            prompt_length = self.model_tester.get_subsampled_output_lengths(prompt_length)\n \n         # scores\n-        self._check_scores(internal_batch_size, output.scores, length=gen_len, config=config)\n+        self._check_scores(\n+            batch_size=internal_batch_size, scores=output.scores, generated_length=generated_length, config=config\n+        )\n \n         # unprocessed logits\n-        self._check_logits(internal_batch_size, output.logits, config=config)\n+        self._check_logits(batch_size=internal_batch_size, logits=output.logits, config=config)\n \n         # Attentions\n         if self.has_attentions:\n             if config.is_encoder_decoder:\n                 # encoder\n                 self._check_encoder_attention_for_generate(\n-                    output.encoder_attentions, input_batch_size, config, seq_length\n+                    attentions=output.encoder_attentions,\n+                    batch_size=input_batch_size,\n+                    config=config,\n+                    prompt_length=prompt_length,\n                 )\n                 # decoder\n                 self._check_attentions_for_generate(\n-                    internal_batch_size,\n-                    output.decoder_attentions,\n-                    min_length=1,\n-                    max_length=output.sequences.shape[-1],\n+                    batch_size=internal_batch_size,\n+                    attentions=output.decoder_attentions,\n+                    prompt_length=1,  # the BOS token\n+                    output_length=output.sequences.shape[-1],\n                     config=config,\n-                    use_cache=use_cache,\n+                    decoder_past_key_values=decoder_past_key_values,\n                 )\n             else:\n-                # if use_cache first input is equal to no use_cache, so skip here\n-                attentions = output.attentions if not use_cache else output.attentions[1:]\n-                min_length = seq_length if not use_cache else seq_length + 1\n                 self._check_attentions_for_generate(\n-                    internal_batch_size,\n-                    attentions=attentions,\n-                    min_length=min_length,\n-                    max_length=output.sequences.shape[-1],\n+                    batch_size=internal_batch_size,\n+                    attentions=output.attentions,\n+                    prompt_length=prompt_length,\n+                    output_length=output.sequences.shape[-1],\n                     config=config,\n-                    use_cache=use_cache,\n+                    decoder_past_key_values=decoder_past_key_values,\n                 )\n \n         # Hidden States\n         if config.is_encoder_decoder:\n             # encoder\n             self._check_encoder_hidden_states_for_generate(\n-                output.encoder_hidden_states, input_batch_size, config, seq_length\n+                hidden_states=output.encoder_hidden_states,\n+                batch_size=input_batch_size,\n+                config=config,\n+                prompt_length=prompt_length,\n             )\n-\n             # decoder\n             self._check_hidden_states_for_generate(\n-                internal_batch_size,\n-                output.decoder_hidden_states,\n-                min_length=1,\n-                max_length=output.sequences.shape[-1],\n+                batch_size=internal_batch_size,\n+                hidden_states=output.decoder_hidden_states,\n+                prompt_length=1,  # the BOS token\n+                output_length=output.sequences.shape[-1],\n                 config=config,\n                 use_cache=use_cache,\n             )\n         else:\n-            # if use_cache first input is equal to no use_cache, so skip here\n-            hidden_states = output.hidden_states if not use_cache else output.hidden_states[1:]\n-            min_length = seq_length if not use_cache else seq_length + 1\n             self._check_hidden_states_for_generate(\n-                internal_batch_size,\n-                hidden_states,\n-                min_length=min_length,\n-                max_length=output.sequences.shape[-1],\n+                batch_size=internal_batch_size,\n+                hidden_states=output.hidden_states,\n+                prompt_length=prompt_length,\n+                output_length=output.sequences.shape[-1],\n                 config=config,\n                 use_cache=use_cache,\n             )\n@@ -2396,131 +2479,149 @@ def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1\n         )\n         if has_standard_cache:\n             if use_cache:\n-                past_key_values = output.past_key_values\n-                past_sequence_length = output.sequences.shape[-1] - 1\n+                cache_length = output.sequences.shape[-1] - 1\n                 self._check_past_key_values_for_generate(\n-                    internal_batch_size,\n-                    past_key_values,\n-                    seq_length=past_sequence_length,\n+                    batch_size=internal_batch_size,\n+                    decoder_past_key_values=decoder_past_key_values,\n+                    cache_length=cache_length,\n                     config=config,\n                 )\n             elif use_cache is False:\n-                self.assertTrue(output.past_key_values is None)\n+                self.assertTrue(decoder_past_key_values is None)\n \n-    def _check_scores(self, batch_size, scores, length, config):\n+    def _check_scores(self, batch_size, scores, generated_length, config):\n         vocab_size = config.get_text_config(decoder=True).vocab_size\n         expected_shape = (batch_size, vocab_size)\n         self.assertIsInstance(scores, tuple)\n-        self.assertEqual(len(scores), length)\n+        self.assertEqual(len(scores), generated_length)\n         self.assertListEqual([iter_scores.shape for iter_scores in scores], [expected_shape] * len(scores))\n \n-    def _check_logits(self, batch_size, scores, config):\n+    def _check_logits(self, batch_size, logits, config):\n         vocab_size = config.get_text_config(decoder=True).vocab_size\n-        self.assertIsInstance(scores, tuple)\n-        self.assertListEqual([iter_scores.shape[0] for iter_scores in scores], [batch_size] * len(scores))\n+        self.assertIsInstance(logits, tuple)\n+        self.assertListEqual([iter_logits.shape[0] for iter_logits in logits], [batch_size] * len(logits))\n         # vocabulary difference equal to one (imagegptmodel?) or zero (all other models)\n-        vocab_diff = vocab_size - scores[0].shape[-1]\n+        vocab_diff = vocab_size - logits[0].shape[-1]\n         self.assertTrue(vocab_diff in [0, 1])\n-        self.assertListEqual([vocab_size - score.shape[-1] for score in scores], [vocab_diff] * len(scores))\n+        self.assertListEqual([vocab_size - score.shape[-1] for score in logits], [vocab_diff] * len(logits))\n \n     def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n         self.assertIsInstance(attentions, tuple)\n         self.assertListEqual(\n             [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n         )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_attentions in enumerate(attentions):\n-            tgt_len = min_length + idx if not use_cache else 1\n-            src_len = min_length + idx\n+        self.assertEqual(len(attentions), (output_length - prompt_length))\n+\n+        use_cache = decoder_past_key_values is not None\n+        has_static_cache = isinstance(decoder_past_key_values, (StaticCache, HybridCache))\n+\n+        # When `output_attentions=True`, each iteration of generate appends the attentions corresponding to the new\n+        # token(s)\n+        # NOTE: `HybridCache` may have different lengths on different layers, if this test starts failing add more\n+        # elaborate checks\n+        for generated_length, iter_attentions in enumerate(attentions):\n+            # regardless of using cache, the first forward pass will have the full prompt as input\n+            if use_cache and generated_length > 0:\n+                model_input_length = 1\n+            else:\n+                model_input_length = prompt_length + generated_length\n+            query_length = (\n+                prompt_length + generated_length\n+                if not has_static_cache\n+                else decoder_past_key_values.get_max_cache_shape()\n+            )\n \n             expected_shape = (\n-                batch_size * num_beam_groups,\n+                batch_size,\n                 config.num_attention_heads,\n-                tgt_len,\n-                src_len,\n+                model_input_length,\n+                query_length,\n             )\n             # check attn size\n             self.assertListEqual(\n                 [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n             )\n \n-    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, seq_length):\n-        encoder_expected_shape = (batch_size, config.num_attention_heads, seq_length, seq_length)\n+    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, prompt_length):\n+        encoder_expected_shape = (batch_size, config.num_attention_heads, prompt_length, prompt_length)\n         self.assertIsInstance(attentions, tuple)\n         self.assertListEqual(\n             [layer_attentions.shape for layer_attentions in attentions],\n             [encoder_expected_shape] * len(attentions),\n         )\n \n     def _check_hidden_states_for_generate(\n-        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n     ):\n         self.assertIsInstance(hidden_states, tuple)\n         self.assertListEqual(\n             [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n             [True] * len(hidden_states),\n         )\n-        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_hidden_states in enumerate(hidden_states):\n-            seq_len = min_length + idx if not use_cache else 1\n-            expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n+        self.assertEqual(len(hidden_states), (output_length - prompt_length))\n+\n+        # When `output_hidden_states=True`, each iteration of generate appends the hidden states corresponding to the\n+        # new token(s)\n+        # NOTE: `HybridCache` may have different lengths on different layers, if this test starts failing add more\n+        # elaborate checks\n+        for generated_length, iter_hidden_states in enumerate(hidden_states):\n+            # regardless of using cache, the first forward pass will have the full prompt as input\n+            if use_cache and generated_length > 0:\n+                model_input_length = 1\n+            else:\n+                model_input_length = prompt_length + generated_length\n+            expected_shape = (batch_size, model_input_length, config.hidden_size)\n             # check hidden size\n             self.assertListEqual(\n                 [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n                 [expected_shape] * len(iter_hidden_states),\n             )\n \n-    def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, config, seq_length):\n-        encoder_expected_shape = (batch_size, seq_length, config.hidden_size)\n+    def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, config, prompt_length):\n+        encoder_expected_shape = (batch_size, prompt_length, config.hidden_size)\n         self.assertIsInstance(hidden_states, tuple)\n         self.assertListEqual(\n             [layer_hidden_states.shape for layer_hidden_states in hidden_states],\n             [encoder_expected_shape] * len(hidden_states),\n         )\n \n-    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config, num_beam_groups=1):\n-        self.assertIsInstance(past_key_values, (tuple, Cache))\n-\n-        # Encoder-decoder models: pull and verify the decoder cache\n-        if isinstance(past_key_values, EncoderDecoderCache):\n-            past_key_values = past_key_values.self_attention_cache\n+    def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n+        self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n \n         # (batch, head, seq_length, head_features)\n         expected_shape = (\n-            batch_size * num_beam_groups,\n+            batch_size,\n             config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n-            seq_length,\n+            cache_length,\n             config.hidden_size // config.num_attention_heads,\n         )\n \n-        if isinstance(past_key_values, Cache):\n+        if isinstance(decoder_past_key_values, Cache):\n             self.assertListEqual(\n-                [key_tensor.shape for key_tensor in past_key_values.key_cache],\n-                [expected_shape] * len(past_key_values.key_cache),\n+                [key_tensor.shape for key_tensor in decoder_past_key_values.key_cache],\n+                [expected_shape] * len(decoder_past_key_values.key_cache),\n             )\n             self.assertListEqual(\n-                [value_tensor.shape for value_tensor in past_key_values.value_cache],\n-                [expected_shape] * len(past_key_values.value_cache),\n+                [value_tensor.shape for value_tensor in decoder_past_key_values.value_cache],\n+                [expected_shape] * len(decoder_past_key_values.value_cache),\n             )\n \n         # Legacy cache format checks. This branch should be removed when all models use `Cache` by default\n         else:\n             self.assertListEqual(\n-                [isinstance(iter_past_key_values, tuple) for iter_past_key_values in past_key_values],\n-                [True] * len(past_key_values),\n+                [isinstance(iter_past_key_values, tuple) for iter_past_key_values in decoder_past_key_values],\n+                [True] * len(decoder_past_key_values),\n             )\n             # check shape key, value\n             self.assertListEqual(\n-                [layer_past_key_values[0].shape for layer_past_key_values in past_key_values],\n-                [expected_shape] * len(past_key_values),\n+                [layer_past_key_values[0].shape for layer_past_key_values in decoder_past_key_values],\n+                [expected_shape] * len(decoder_past_key_values),\n             )\n             self.assertListEqual(\n-                [layer_past_key_values[1].shape for layer_past_key_values in past_key_values],\n-                [expected_shape] * len(past_key_values),\n+                [layer_past_key_values[1].shape for layer_past_key_values in decoder_past_key_values],\n+                [expected_shape] * len(decoder_past_key_values),\n             )\n \n     def _check_sequence_inside_sequence(self, tensor_1, tensor_2):"
        },
        {
            "sha": "775971cf280f1a8009afd93de87d67b0d87b7615",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 94,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -723,103 +723,12 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     # overwrite because BLIP internally calls LM.generate() with embeds thus it cannot operate in no cache format\n-    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n+    def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n         use_cache = True  # force this to be True in case False is passed\n-\n-        input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n-        internal_batch_size = (\n-            input_batch_size * num_beams if num_beams > 1 else input_batch_size * num_return_sequences\n-        )\n-\n-        seq_length = getattr(self.model_tester, \"seq_length\", None)\n-        seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_length)\n-        seq_length = getattr(self.model_tester, \"text_seq_length\", seq_length)\n-\n-        config = config.text_config if hasattr(config, \"text_config\") else config\n-\n-        gen_len = (\n-            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n+        super()._check_generate_outputs(\n+            output, config, use_cache=use_cache, num_return_sequences=num_return_sequences, num_beams=num_beams\n         )\n \n-        # in some models we subsample the sequence length in inner layers\n-        if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n-            seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n-\n-        # scores\n-        self._check_scores(internal_batch_size, output.scores, length=gen_len, config=config)\n-\n-        # unprocessed logits\n-        self._check_logits(internal_batch_size, output.logits, config=config)\n-\n-        # Attentions\n-        if self.has_attentions:\n-            if config.is_encoder_decoder:\n-                # encoder\n-                self._check_encoder_attention_for_generate(\n-                    output.encoder_attentions, input_batch_size, config, seq_length\n-                )\n-                # decoder\n-                self._check_attentions_for_generate(\n-                    internal_batch_size,\n-                    output.decoder_attentions,\n-                    min_length=1,\n-                    max_length=output.sequences.shape[-1],\n-                    config=config,\n-                    use_cache=use_cache,\n-                )\n-            else:\n-                # if use_cache first input is equal to no use_cache, so skip here\n-                attentions = output.attentions if not use_cache else output.attentions[1:]\n-                min_length = seq_length if not use_cache else seq_length + 1\n-                self._check_attentions_for_generate(\n-                    internal_batch_size,\n-                    attentions=attentions,\n-                    min_length=min_length,\n-                    max_length=output.sequences.shape[-1],\n-                    config=config,\n-                    use_cache=use_cache,\n-                )\n-\n-        # Hidden States\n-        if config.is_encoder_decoder:\n-            # encoder\n-            self._check_encoder_hidden_states_for_generate(\n-                output.encoder_hidden_states, input_batch_size, config, seq_length\n-            )\n-\n-            # decoder\n-            self._check_hidden_states_for_generate(\n-                internal_batch_size,\n-                output.decoder_hidden_states,\n-                min_length=1,\n-                max_length=output.sequences.shape[-1],\n-                config=config,\n-                use_cache=use_cache,\n-            )\n-        else:\n-            # if use_cache first input is equal to no use_cache, so skip here\n-            hidden_states = output.hidden_states if not use_cache else output.hidden_states[1:]\n-            min_length = seq_length if not use_cache else seq_length + 1\n-            self._check_hidden_states_for_generate(\n-                internal_batch_size,\n-                hidden_states,\n-                min_length=min_length,\n-                max_length=output.sequences.shape[-1],\n-                config=config,\n-                use_cache=use_cache,\n-            )\n-\n-        # Past Key Value States\n-        if use_cache:\n-            past_key_values = output.past_key_values\n-            past_sequence_length = output.sequences.shape[-1] - 1\n-            self._check_past_key_values_for_generate(\n-                internal_batch_size,\n-                past_key_values,\n-                seq_length=past_sequence_length,\n-                config=config,\n-            )\n-\n     # overwrite because BLIP2 cannot generate only from input ids, and requires pixel values in all cases to be present\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):"
        },
        {
            "sha": "881856ea70da6810c01cef6d7ca2d60a937f1d7c",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 46,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -20,7 +20,7 @@\n from parameterized import parameterized\n from pytest import mark\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, Cohere2Config, HybridCache, is_torch_available, pipeline\n+from transformers import AutoModelForCausalLM, AutoTokenizer, Cohere2Config, is_torch_available, pipeline\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     require_flash_attn,\n@@ -135,51 +135,6 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_generate_continue_from_inputs_embeds(self):\n         pass\n \n-    # overwrite because HybridCache has fixed length for key/values\n-    def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n-    ):\n-        self.assertIsInstance(attentions, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n-        )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_attentions in enumerate(attentions):\n-            tgt_len = min_length + idx if not use_cache else 1\n-            src_len = min_length + idx if not use_cache else max_length\n-\n-            expected_shape = (\n-                batch_size * num_beam_groups,\n-                config.num_attention_heads,\n-                tgt_len,\n-                src_len,\n-            )\n-            # check attn size\n-            self.assertListEqual(\n-                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n-            )\n-\n-    # overwrite because HybridCache has fixed length for key/values\n-    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config, num_beam_groups=1):\n-        self.assertIsInstance(past_key_values, HybridCache)\n-\n-        # check shape key, value (batch, head, max_seq_length, head_features)\n-        head_dim = config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n-        num_key_value_heads = (\n-            config.num_attention_heads\n-            if getattr(config, \"num_key_value_heads\", None) is None\n-            else config.num_key_value_heads\n-        )\n-        num_hidden_layers = config.num_hidden_layers\n-\n-        # we should get `max_length` in shape, not `max_length - embeds_length`\n-        # `+1` because the test in Mixin subtracts 1 which is needed for tuple cache\n-        static_cache_shape = (batch_size, num_key_value_heads, seq_length + 1, head_dim)\n-        static_layers = [layer_idx for layer_idx, boolean in enumerate(past_key_values.is_sliding) if not boolean]\n-        self.assertTrue(len(past_key_values.key_cache) == num_hidden_layers)\n-        self.assertTrue(past_key_values.key_cache[static_layers[0]].shape == static_cache_shape)\n-\n     @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n     def test_sdpa_equivalence(self):\n         pass"
        },
        {
            "sha": "8d02565b4b5a963d5e6d7dff87dab9f0c375d29e",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 46,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -20,7 +20,7 @@\n from parameterized import parameterized\n from pytest import mark\n \n-from transformers import AutoModelForCausalLM, AutoTokenizer, Gemma2Config, HybridCache, is_torch_available, pipeline\n+from transformers import AutoModelForCausalLM, AutoTokenizer, Gemma2Config, is_torch_available, pipeline\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     require_flash_attn,\n@@ -150,51 +150,6 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_generate_continue_from_inputs_embeds(self):\n         pass\n \n-    # overwrite because HybridCache has fixed length for key/values\n-    def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n-    ):\n-        self.assertIsInstance(attentions, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n-        )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_attentions in enumerate(attentions):\n-            tgt_len = min_length + idx if not use_cache else 1\n-            src_len = min_length + idx if not use_cache else max_length\n-\n-            expected_shape = (\n-                batch_size * num_beam_groups,\n-                config.num_attention_heads,\n-                tgt_len,\n-                src_len,\n-            )\n-            # check attn size\n-            self.assertListEqual(\n-                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n-            )\n-\n-    # overwrite because HybridCache has fixed length for key/values\n-    def _check_past_key_values_for_generate(self, batch_size, past_key_values, seq_length, config, num_beam_groups=1):\n-        self.assertIsInstance(past_key_values, HybridCache)\n-\n-        # check shape key, value (batch, head, max_seq_length, head_features)\n-        head_dim = config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n-        num_key_value_heads = (\n-            config.num_attention_heads\n-            if getattr(config, \"num_key_value_heads\", None) is None\n-            else config.num_key_value_heads\n-        )\n-        num_hidden_layers = config.num_hidden_layers\n-\n-        # we should get `max_length` in shape, not `max_length - embeds_length`\n-        # `+1` because the test in Mixin subtracts 1 which is needed for tuple cache\n-        static_cache_shape = (batch_size, num_key_value_heads, seq_length + 1, head_dim)\n-        static_layers = [layer_idx for layer_idx, boolean in enumerate(past_key_values.is_sliding) if not boolean]\n-        self.assertTrue(len(past_key_values.key_cache) == num_hidden_layers)\n-        self.assertTrue(past_key_values.key_cache[static_layers[0]].shape == static_cache_shape)\n-\n     @unittest.skip(\"Gemma2's eager attn/sdpa attn outputs are expected to be different\")\n     def test_sdpa_equivalence(self):\n         pass"
        },
        {
            "sha": "e4251c700ed66e0b8ac07e194cc909f81fd2038b",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 12,
            "deletions": 37,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -456,51 +456,26 @@ def test_model_various_embeddings(self):\n             self.model_tester.create_and_check_model(*config_and_inputs)\n \n     def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n         # GIT attention shape depends on image inputs, overwrite\n-        self.assertIsInstance(attentions, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n-        )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n         image_length = int((config.vision_config.image_size / config.vision_config.patch_size) ** 2 + 1)\n-\n-        for idx, iter_attentions in enumerate(attentions):\n-            tgt_len = min_length + idx + image_length if not use_cache else 1\n-            src_len = min_length + idx + image_length\n-\n-            expected_shape = (\n-                batch_size * num_beam_groups,\n-                config.num_attention_heads,\n-                tgt_len,\n-                src_len,\n-            )\n-            # check attn size\n-            self.assertListEqual(\n-                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n-            )\n+        prompt_length += image_length\n+        output_length += image_length\n+        super()._check_attentions_for_generate(\n+            batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n+        )\n \n     def _check_hidden_states_for_generate(\n-        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n     ):\n         # GIT attention shape depends on image inputs, overwrite\n-        self.assertIsInstance(hidden_states, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n-            [True] * len(hidden_states),\n-        )\n-        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n         image_length = int((config.vision_config.image_size / config.vision_config.patch_size) ** 2 + 1)\n-\n-        for idx, iter_hidden_states in enumerate(hidden_states):\n-            seq_len = min_length + idx + image_length if not use_cache else 1\n-            expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n-            # check hidden size\n-            self.assertListEqual(\n-                [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n-                [expected_shape] * len(iter_hidden_states),\n-            )\n+        prompt_length += image_length\n+        output_length += image_length\n+        super()._check_hidden_states_for_generate(\n+            batch_size, hidden_states, prompt_length, output_length, config, use_cache=use_cache\n+        )\n \n     @slow\n     def test_model_from_pretrained(self):"
        },
        {
            "sha": "1306dc50d9748719e4bfdef14de3b5345b616a3b",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -815,7 +815,7 @@ def test_generate_continue_from_inputs_embeds(self):\n                     )\n \n     def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n         \"\"\"\n         Overwrite from generation tests because Idefics has only SDPA layers."
        },
        {
            "sha": "6d96c444c36d0948d03927521793a68788685c90",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -251,10 +251,10 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         return inputs_dict\n \n     # we overwrite the _check_scores method of GenerationTesterMixin, as ImageGPTForCausalImageModeling doesn't have tied input- and output embeddings\n-    def _check_scores(self, batch_size, scores, length, config):\n+    def _check_scores(self, batch_size, scores, generated_length, config):\n         expected_shape = (batch_size, config.vocab_size - 1)\n         self.assertIsInstance(scores, tuple)\n-        self.assertEqual(len(scores), length)\n+        self.assertEqual(len(scores), generated_length)\n         self.assertListEqual([iter_scores.shape for iter_scores in scores], [expected_shape] * len(scores))\n \n     @run_test_using_subprocess"
        },
        {
            "sha": "434784f05a85c17317e943a80fd6dad341c4ea64",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 94,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -565,103 +565,12 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     # overwrite because InstructBLIP internally calls LM.generate() with embeds thus it cannot operate in no cache format\n-    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n+    def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n         use_cache = True  # force this to be True in case False is passed\n-\n-        input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n-        internal_batch_size = (\n-            input_batch_size * num_beams if num_beams > 1 else input_batch_size * num_return_sequences\n-        )\n-\n-        seq_length = getattr(self.model_tester, \"seq_length\", None)\n-        seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_length)\n-        seq_length = getattr(self.model_tester, \"text_seq_length\", seq_length)\n-\n-        config = config.text_config if hasattr(config, \"text_config\") else config\n-\n-        gen_len = (\n-            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n+        super()._check_generate_outputs(\n+            output, config, use_cache=use_cache, num_return_sequences=num_return_sequences, num_beams=num_beams\n         )\n \n-        # in some models we subsample the sequence length in inner layers\n-        if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n-            seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n-\n-        # scores\n-        self._check_scores(internal_batch_size, output.scores, length=gen_len, config=config)\n-\n-        # unprocessed logits\n-        self._check_logits(internal_batch_size, output.logits, config=config)\n-\n-        # Attentions\n-        if self.has_attentions:\n-            if config.is_encoder_decoder:\n-                # encoder\n-                self._check_encoder_attention_for_generate(\n-                    output.encoder_attentions, input_batch_size, config, seq_length\n-                )\n-                # decoder\n-                self._check_attentions_for_generate(\n-                    internal_batch_size,\n-                    output.decoder_attentions,\n-                    min_length=1,\n-                    max_length=output.sequences.shape[-1],\n-                    config=config,\n-                    use_cache=use_cache,\n-                )\n-            else:\n-                # if use_cache first input is equal to no use_cache, so skip here\n-                attentions = output.attentions if not use_cache else output.attentions[1:]\n-                min_length = seq_length if not use_cache else seq_length + 1\n-                self._check_attentions_for_generate(\n-                    internal_batch_size,\n-                    attentions=attentions,\n-                    min_length=min_length,\n-                    max_length=output.sequences.shape[-1],\n-                    config=config,\n-                    use_cache=use_cache,\n-                )\n-\n-        # Hidden States\n-        if config.is_encoder_decoder:\n-            # encoder\n-            self._check_encoder_hidden_states_for_generate(\n-                output.encoder_hidden_states, input_batch_size, config, seq_length\n-            )\n-\n-            # decoder\n-            self._check_hidden_states_for_generate(\n-                internal_batch_size,\n-                output.decoder_hidden_states,\n-                min_length=1,\n-                max_length=output.sequences.shape[-1],\n-                config=config,\n-                use_cache=use_cache,\n-            )\n-        else:\n-            # if use_cache first input is equal to no use_cache, so skip here\n-            hidden_states = output.hidden_states if not use_cache else output.hidden_states[1:]\n-            min_length = seq_length if not use_cache else seq_length + 1\n-            self._check_hidden_states_for_generate(\n-                internal_batch_size,\n-                hidden_states,\n-                min_length=min_length,\n-                max_length=output.sequences.shape[-1],\n-                config=config,\n-                use_cache=use_cache,\n-            )\n-\n-        # Past Key Value States\n-        if use_cache:\n-            past_key_values = output.past_key_values\n-            past_sequence_length = output.sequences.shape[-1] - 1\n-            self._check_past_key_values_for_generate(\n-                internal_batch_size,\n-                past_key_values,\n-                seq_length=past_sequence_length,\n-                config=config,\n-            )\n-\n     # overwrite because InstructBLIP cannot generate only from input ids, and requires `pixel` values and `qformer_input_ids` in all cases to be present\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):"
        },
        {
            "sha": "e8ed52b72338fcc78d50b1f9943b1249929f2463",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 94,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -581,103 +581,12 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     # overwrite because InstructBLIPVideo internally calls LM.generate() with embeds thus it cannot operate in no cache format\n-    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n+    def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n         use_cache = True  # force this to be True in case False is passed\n-\n-        input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n-        internal_batch_size = (\n-            input_batch_size * num_beams if num_beams > 1 else input_batch_size * num_return_sequences\n-        )\n-\n-        seq_length = getattr(self.model_tester, \"seq_length\", None)\n-        seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_length)\n-        seq_length = getattr(self.model_tester, \"text_seq_length\", seq_length)\n-\n-        config = config.text_config if hasattr(config, \"text_config\") else config\n-\n-        gen_len = (\n-            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n+        super()._check_generate_outputs(\n+            output, config, use_cache=use_cache, num_return_sequences=num_return_sequences, num_beams=num_beams\n         )\n \n-        # in some models we subsample the sequence length in inner layers\n-        if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n-            seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n-\n-        # scores\n-        self._check_scores(internal_batch_size, output.scores, length=gen_len, config=config)\n-\n-        # unprocessed logits\n-        self._check_logits(internal_batch_size, output.logits, config=config)\n-\n-        # Attentions\n-        if self.has_attentions:\n-            if config.is_encoder_decoder:\n-                # encoder\n-                self._check_encoder_attention_for_generate(\n-                    output.encoder_attentions, input_batch_size, config, seq_length\n-                )\n-                # decoder\n-                self._check_attentions_for_generate(\n-                    internal_batch_size,\n-                    output.decoder_attentions,\n-                    min_length=1,\n-                    max_length=output.sequences.shape[-1],\n-                    config=config,\n-                    use_cache=use_cache,\n-                )\n-            else:\n-                # if use_cache first input is equal to no use_cache, so skip here\n-                attentions = output.attentions if not use_cache else output.attentions[1:]\n-                min_length = seq_length if not use_cache else seq_length + 1\n-                self._check_attentions_for_generate(\n-                    internal_batch_size,\n-                    attentions=attentions,\n-                    min_length=min_length,\n-                    max_length=output.sequences.shape[-1],\n-                    config=config,\n-                    use_cache=use_cache,\n-                )\n-\n-        # Hidden States\n-        if config.is_encoder_decoder:\n-            # encoder\n-            self._check_encoder_hidden_states_for_generate(\n-                output.encoder_hidden_states, input_batch_size, config, seq_length\n-            )\n-\n-            # decoder\n-            self._check_hidden_states_for_generate(\n-                internal_batch_size,\n-                output.decoder_hidden_states,\n-                min_length=1,\n-                max_length=output.sequences.shape[-1],\n-                config=config,\n-                use_cache=use_cache,\n-            )\n-        else:\n-            # if use_cache first input is equal to no use_cache, so skip here\n-            hidden_states = output.hidden_states if not use_cache else output.hidden_states[1:]\n-            min_length = seq_length if not use_cache else seq_length + 1\n-            self._check_hidden_states_for_generate(\n-                internal_batch_size,\n-                hidden_states,\n-                min_length=min_length,\n-                max_length=output.sequences.shape[-1],\n-                config=config,\n-                use_cache=use_cache,\n-            )\n-\n-        # Past Key Value States\n-        if use_cache:\n-            past_key_values = output.past_key_values\n-            past_sequence_length = output.sequences.shape[-1] - 1\n-            self._check_past_key_values_for_generate(\n-                internal_batch_size,\n-                past_key_values,\n-                seq_length=past_sequence_length,\n-                config=config,\n-            )\n-\n     # overwrite because InstructBLIPVideo cannot generate only from input ids, and requires `pixel` values and `qformer_input_ids` in all cases to be present\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):"
        },
        {
            "sha": "b6a3db5c943af3a93931793e56f3d0b5f846e0aa",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -468,12 +468,12 @@ def test_attention_outputs(self):\n                 ],\n             )\n \n-    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, seq_length):\n+    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, prompt_length):\n         # overwrite because LED does not have (bs, num_heads, seq_len, seq_len) shape\n         encoder_expected_shape = (\n             batch_size,\n             config.num_attention_heads,\n-            seq_length,\n+            prompt_length,\n             self.model_tester.attention_window // 2 * 2 + 1,\n         )\n         self.assertIsInstance(attentions, tuple)"
        },
        {
            "sha": "a166a6dab70975a2539f24187f95ab9072ba7fe3",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -785,7 +785,7 @@ def test_attention_outputs(self):\n                     [self.model_tester.num_attention_heads, block_len, 3 * block_len],\n                 )\n \n-    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, seq_length):\n+    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, prompt_length):\n         block_len = getattr(self.model_tester, \"block_len\", None)\n         encoder_expected_shape = (batch_size, 2, config.num_attention_heads, block_len, 3 * block_len)\n         self.assertIsInstance(attentions, tuple)\n@@ -920,10 +920,10 @@ def test_attention_outputs(self):\n                     [self.model_tester.num_attention_heads, block_len, 3 * block_len + global_seq_len],\n                 )\n \n-    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, seq_length):\n+    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, prompt_length):\n         block_len = getattr(self.model_tester, \"block_len\", None)\n         global_block_size = getattr(self.model_tester, \"global_block_size\", None)\n-        global_seq_length = seq_length // global_block_size\n+        global_seq_length = prompt_length // global_block_size\n         encoder_expected_shape = (\n             batch_size,\n             2,"
        },
        {
            "sha": "4e4c4636b7db122528503f97beb721c55f1bee77",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -323,32 +323,37 @@ def test_inputs_embeds_matches_input_ids(self):\n             torch.testing.assert_close(out_embeds, out_ids)\n \n     def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n         # Mllama has cross attention layers and those have a different shape than normal attention layers\n         self.assertIsInstance(attentions, tuple)\n         self.assertListEqual(\n             [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n         )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n+        self.assertEqual(len(attentions), (output_length - prompt_length))\n \n         cross_attention_layers = self.model_tester.text_config[\"cross_attention_layers\"]\n+        use_cache = decoder_past_key_values is not None\n \n-        for idx, iter_attentions in enumerate(attentions):\n-            tgt_len = min_length + idx if not use_cache else 1\n-            src_len = min_length + idx\n+        for generated_length, iter_attentions in enumerate(attentions):\n+            # regardless of using cache, the first forward pass will have the full prompt as input\n+            if use_cache and generated_length > 0:\n+                model_input_length = 1\n+            else:\n+                model_input_length = prompt_length + generated_length\n+            query_length = prompt_length + generated_length\n \n             expected_shape = (\n-                batch_size * num_beam_groups,\n+                batch_size,\n                 config.num_attention_heads,\n-                tgt_len,\n-                src_len,\n+                model_input_length,\n+                query_length,\n             )\n \n             expected_shape_cross = (\n-                batch_size * num_beam_groups,\n+                batch_size,\n                 config.num_attention_heads,\n-                tgt_len,\n+                model_input_length,\n                 self.model_tester.image_length,\n             )\n "
        },
        {
            "sha": "9eb0eaa4d44349b76e47da687f90fd0a086fa65b",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 67,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -575,77 +575,12 @@ def prepare_config_and_inputs_for_generate(self, batch_size=2):\n \n         return config, filtered_inputs_dict\n \n-    def _check_hidden_states_for_generate(\n-        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n-    ):\n-        # Overwrite because the generate method actually alway uses `inputs_embeds` so `use_cache` is always `True`\n-        self.assertIsInstance(hidden_states, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n-            [True] * len(hidden_states),\n-        )\n-        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_hidden_states in enumerate(hidden_states):\n-            seq_len = min_length if idx == 0 else 1\n-            expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n-            # check hidden size\n-            self.assertListEqual(\n-                [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n-                [expected_shape] * len(iter_hidden_states),\n-            )\n-\n-    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n+    def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n         # Overwrite because the generate method actually alway uses `inputs_embeds` so `use_cache` is always `True`\n-        super()._check_outputs(\n+        super()._check_generate_outputs(\n             output, config, use_cache=True, num_return_sequences=num_return_sequences, num_beams=num_beams\n         )\n \n-    def _check_hidden_states_for_generate(\n-        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n-    ):\n-        # Overwrite because the generate method actually alway uses `inputs_embeds` so `use_cache` is always `True`\n-        self.assertIsInstance(hidden_states, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n-            [True] * len(hidden_states),\n-        )\n-        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_hidden_states in enumerate(hidden_states):\n-            seq_len = 1\n-            expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n-            # check hidden size\n-            self.assertListEqual(\n-                [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n-                [expected_shape] * len(iter_hidden_states),\n-            )\n-\n-    def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n-    ):\n-        # Overwrite because the generate method actually alway uses `inputs_embeds` so `use_cache` is always `True`\n-        self.assertIsInstance(attentions, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n-        )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_attentions in enumerate(attentions):\n-            tgt_len = 1\n-            src_len = min_length + idx\n-\n-            expected_shape = (\n-                batch_size * num_beam_groups,\n-                config.num_attention_heads,\n-                tgt_len,\n-                src_len,\n-            )\n-            # check attn size\n-            self.assertListEqual(\n-                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n-            )\n-\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "2c3b74edd17a8f94541799046eda5917c30802a3",
            "filename": "tests/models/pegasus_x/test_modeling_pegasus_x.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -399,11 +399,11 @@ def test_attention_outputs(self):\n                 ],\n             )\n \n-    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, seq_length):\n+    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, prompt_length):\n         encoder_expected_shape = (\n             batch_size,\n             config.num_attention_heads,\n-            math.ceil(seq_length / config.block_size),\n+            math.ceil(prompt_length / config.block_size),\n             config.block_size,\n             config.block_size + config.num_global_tokens,\n         )\n@@ -413,8 +413,8 @@ def _check_encoder_attention_for_generate(self, attentions, batch_size, config,\n             [encoder_expected_shape] * len(attentions),\n         )\n \n-    def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, config, seq_length):\n-        encoder_expected_shape = (batch_size, self.round_up(seq_length, config.block_size), config.hidden_size)\n+    def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, config, prompt_length):\n+        encoder_expected_shape = (batch_size, self.round_up(prompt_length, config.block_size), config.hidden_size)\n         self.assertIsInstance(hidden_states, tuple)\n         # Only the last layer will have the hidden states truncated back to token level\n         self.assertListEqual(\n@@ -424,7 +424,7 @@ def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, c\n         # Only the last layer will have the hidden states truncated back to token level\n         self.assertEqual(\n             hidden_states[-1][0].shape,\n-            (batch_size, seq_length, config.hidden_size),\n+            (batch_size, prompt_length, config.hidden_size),\n         )\n \n     def test_hidden_states_output(self):"
        },
        {
            "sha": "3b051db37c912bc34be3b9239c1cacfca3938a37",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -753,20 +753,20 @@ def test_load_vision_text_config(self):\n             text_config = Pix2StructTextConfig.from_pretrained(tmp_dir_name)\n             self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n \n-    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, seq_length):\n+    def _check_encoder_attention_for_generate(self, attentions, batch_size, config, prompt_length):\n         # overwrite because # pix2struct seq length depends on image inputs\n-        seq_length = self.model_tester.max_patches\n-        encoder_expected_shape = (batch_size, config.num_attention_heads, seq_length, seq_length)\n+        prompt_length = self.model_tester.max_patches\n+        encoder_expected_shape = (batch_size, config.num_attention_heads, prompt_length, prompt_length)\n         self.assertIsInstance(attentions, tuple)\n         self.assertListEqual(\n             [layer_attentions.shape for layer_attentions in attentions],\n             [encoder_expected_shape] * len(attentions),\n         )\n \n-    def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, config, seq_length):\n+    def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, config, prompt_length):\n         # overwrite because # pix2struct seq length depends on image inputs\n-        seq_length = self.model_tester.max_patches\n-        encoder_expected_shape = (batch_size, seq_length, config.hidden_size)\n+        prompt_length = self.model_tester.max_patches\n+        encoder_expected_shape = (batch_size, prompt_length, config.hidden_size)\n         self.assertIsInstance(hidden_states, tuple)\n         self.assertListEqual(\n             [layer_hidden_states.shape for layer_hidden_states in hidden_states],"
        },
        {
            "sha": "a7cfc2a04f58681f6df51b448cf729bde81e0025",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -367,9 +367,6 @@ def test_contrastive_generate(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    def _check_attentions_for_generate(self, *args, **kwargs):\n-        return True  # Model does not return attention\n-\n     @unittest.skip(reason=\"Past key values are not returned\")\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         pass\n@@ -382,9 +379,6 @@ def test_model_parallelism(self):\n     def test_model_parallel_beam_search(self):\n         pass\n \n-    def _check_past_key_values_for_generate(self, *args, **kwargs):\n-        return True\n-\n     @unittest.skip(reason=\"Rely on `past_key_values` to crop the assistant pkv. Not supported\")\n     def test_assisted_decoding_matches_greedy_search(self):\n         pass\n@@ -397,25 +391,6 @@ def test_left_padding_compatibility(self):\n     def test_assisted_decoding_sample(self):\n         pass\n \n-    def _check_hidden_states_for_generate(\n-        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n-    ):\n-        self.assertIsInstance(hidden_states, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n-            [True] * len(hidden_states),\n-        )\n-        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_hidden_states in enumerate(hidden_states):\n-            seq_len = min_length + idx if not use_cache else 1\n-            expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n-            # check hidden size\n-            self.assertListEqual(\n-                [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n-                [expected_shape] * len(iter_hidden_states),\n-            )\n-\n     @unittest.skip(reason=\"TODO @arthurzucker not super important and failing.\")\n     def test_initialization(self):\n         pass"
        },
        {
            "sha": "24b59b2f1b876c996eef3f24239dfabe56bf1300",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 64,
            "deletions": 45,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -620,62 +620,72 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n+        # NOTE (joao): this function is substancially different from the original, the attention has different\n+        # *number* of shapes in certain conditions\n         self.assertIsInstance(attentions, tuple)\n         self.assertListEqual(\n             [isinstance(iter_attentions, list) for iter_attentions in attentions], [True] * len(attentions)\n         )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n+        self.assertEqual(len(attentions), (output_length - prompt_length))\n \n-        for idx, iter_attentions in enumerate(attentions):\n-            tgt_len = min_length + idx if not use_cache else 1\n-            num_chunks = tgt_len // config.local_attn_chunk_length + (tgt_len % config.local_attn_chunk_length != 0)\n-            tgt_chunk_len = config.local_attn_chunk_length\n-            src_chunk_len = config.local_attn_chunk_length * (\n+        for generated_length, iter_attentions in enumerate(attentions):\n+            use_cache = decoder_past_key_values is not None and generated_length > 0\n+\n+            model_input_length = prompt_length + generated_length if not use_cache else 1\n+            num_chunks = model_input_length // config.local_attn_chunk_length + (\n+                model_input_length % config.local_attn_chunk_length != 0\n+            )\n+            model_input_chunk_len = config.local_attn_chunk_length\n+            query_chunk_len = config.local_attn_chunk_length * (\n                 1 + config.local_num_chunks_after + config.local_num_chunks_before\n             )\n \n             if use_cache:\n                 expected_shape = (\n-                    batch_size * num_beam_groups,\n+                    batch_size,\n                     config.num_attention_heads,\n-                    tgt_len,\n-                    min_length // config.local_attn_chunk_length + 1 + idx,\n+                    model_input_length,\n+                    prompt_length // config.local_attn_chunk_length + generated_length,\n                 )\n             else:\n                 expected_shape = (\n-                    batch_size * num_beam_groups,\n+                    batch_size,\n                     config.num_attention_heads,\n                     num_chunks,\n-                    tgt_chunk_len,\n-                    src_chunk_len,\n+                    model_input_chunk_len,\n+                    query_chunk_len,\n                 )\n             # check attn size\n             self.assertListEqual(\n                 [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n             )\n \n     def _check_hidden_states_for_generate(\n-        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n     ):\n+        # NOTE (joao): this function is substancially different from the original, the hidden states have different\n+        # length in certain conditions\n         self.assertIsInstance(hidden_states, tuple)\n         self.assertListEqual(\n             [isinstance(iter_hidden_states, list) for iter_hidden_states in hidden_states],\n             [True] * len(hidden_states),\n         )\n-        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_hidden_states in enumerate(hidden_states):\n-            seq_len = min_length + idx\n-            seq_len = config.local_attn_chunk_length * (\n-                seq_len // config.local_attn_chunk_length + (seq_len % config.local_attn_chunk_length != 0)\n+        self.assertEqual(len(hidden_states), (output_length - prompt_length))\n+\n+        for generation_length, iter_hidden_states in enumerate(hidden_states):\n+            use_cache_this_iter = use_cache and generation_length > 0\n+            model_input_length = prompt_length + generation_length\n+            model_output_length = config.local_attn_chunk_length * (\n+                model_input_length // config.local_attn_chunk_length\n+                + (model_input_length % config.local_attn_chunk_length != 0)\n             )\n \n-            if use_cache:\n-                seq_len = 1\n+            if use_cache_this_iter:\n+                model_output_length = 1\n \n-            expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n+            expected_shape = (batch_size, model_output_length, config.hidden_size)\n             # check hidden size\n             self.assertListEqual(\n                 [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n@@ -789,63 +799,72 @@ def setUp(self):\n         self.config_tester = ConfigTester(self, config_class=ReformerConfig, hidden_size=37)\n \n     def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n+        # NOTE (joao): this function is substancially different from the original, the attention has different\n+        # *number* of shapes in certain conditions\n         self.assertIsInstance(attentions, tuple)\n         self.assertListEqual(\n             [isinstance(iter_attentions, list) for iter_attentions in attentions], [True] * len(attentions)\n         )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n+        self.assertEqual(len(attentions), (output_length - prompt_length))\n \n-        for idx, iter_attentions in enumerate(attentions):\n-            tgt_len = min_length + idx if not use_cache else 1\n-            num_chunks = tgt_len // config.lsh_attn_chunk_length + (tgt_len % config.lsh_attn_chunk_length != 0)\n-            tgt_chunk_len = config.lsh_attn_chunk_length\n-            src_chunk_len = config.lsh_attn_chunk_length * (\n+        for generated_length, iter_attentions in enumerate(attentions):\n+            use_cache = decoder_past_key_values is not None and generated_length > 0\n+            model_input_len = prompt_length + generated_length if not use_cache else 1\n+            num_chunks = model_input_len // config.lsh_attn_chunk_length + (\n+                model_input_len % config.lsh_attn_chunk_length != 0\n+            )\n+            model_input_chunk_len = config.lsh_attn_chunk_length\n+            query_chunk_len = config.lsh_attn_chunk_length * (\n                 1 + config.lsh_num_chunks_after + config.lsh_num_chunks_before\n             )\n \n             if use_cache:\n                 expected_shape = (\n-                    batch_size * num_beam_groups,\n+                    batch_size,\n                     config.num_attention_heads,\n                     config.num_hashes,\n-                    tgt_len,\n+                    model_input_len,\n                     config.num_hashes * (1 + config.lsh_num_chunks_after + config.lsh_num_chunks_before),\n                 )\n             else:\n                 expected_shape = (\n-                    batch_size * num_beam_groups,\n+                    batch_size,\n                     config.num_attention_heads,\n                     num_chunks * config.num_hashes,\n-                    tgt_chunk_len,\n-                    src_chunk_len,\n+                    model_input_chunk_len,\n+                    query_chunk_len,\n                 )\n             # check attn size\n             self.assertListEqual(\n                 [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n             )\n \n     def _check_hidden_states_for_generate(\n-        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n     ):\n+        # NOTE (joao): this function is substancially different from the original, the hidden states have different\n+        # length in certain conditions\n         self.assertIsInstance(hidden_states, tuple)\n         self.assertListEqual(\n             [isinstance(iter_hidden_states, list) for iter_hidden_states in hidden_states],\n             [True] * len(hidden_states),\n         )\n-        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_hidden_states in enumerate(hidden_states):\n-            seq_len = min_length + idx if not use_cache else 1\n-            seq_len = config.lsh_attn_chunk_length * (\n-                seq_len // config.lsh_attn_chunk_length + (seq_len % config.lsh_attn_chunk_length != 0)\n+        self.assertEqual(len(hidden_states), (output_length - prompt_length))\n+\n+        for generation_length, iter_hidden_states in enumerate(hidden_states):\n+            use_cache_this_iter = use_cache and generation_length > 0\n+            model_input_length = prompt_length + generation_length\n+            model_output_length = config.local_attn_chunk_length * (\n+                model_input_length // config.local_attn_chunk_length\n+                + (model_input_length % config.local_attn_chunk_length != 0)\n             )\n \n-            if use_cache:\n-                seq_len = 1\n+            if use_cache_this_iter:\n+                model_output_length = 1\n \n-            expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n+            expected_shape = (batch_size, model_output_length, config.hidden_size)\n             # check hidden size\n             self.assertListEqual(\n                 [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],"
        },
        {
            "sha": "ef9a2b33bc242271b05a9f1fe852598860dfe488",
            "filename": "tests/models/speech_to_text/test_modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -416,48 +416,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n-        batch_size, seq_length = input_ids.shape[:2]\n-        subsampled_seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n-        num_sequences_in_output = batch_size * num_return_sequences\n-        gen_len = (\n-            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n-        )\n-\n-        # scores\n-        self._check_scores(num_sequences_in_output, output.scores, length=gen_len, config=config)\n-\n-        # Attentions\n-        # encoder\n-        self._check_encoder_attention_for_generate(\n-            output.encoder_attentions, batch_size, config, subsampled_seq_length\n-        )\n-        # decoder\n-        self._check_attentions_for_generate(\n-            num_sequences_in_output,\n-            output.decoder_attentions,\n-            min_length=1,\n-            max_length=output.sequences.shape[-1],\n-            config=config,\n-            use_cache=use_cache,\n-        )\n-\n-        # Hidden States\n-        # encoder\n-        self._check_encoder_hidden_states_for_generate(\n-            output.encoder_hidden_states, batch_size, config, subsampled_seq_length\n-        )\n-\n-        # decoder\n-        self._check_hidden_states_for_generate(\n-            num_sequences_in_output,\n-            output.decoder_hidden_states,\n-            min_length=1,\n-            max_length=output.sequences.shape[-1],\n-            config=config,\n-            use_cache=use_cache,\n-        )\n-\n     # overwritten from parent due to the inability to work when non-text inputs are not passed AND because the input is\n     # `input_features`\n     def test_lm_head_model_random_no_beam_search_generate(self):"
        },
        {
            "sha": "7aacf5171921bed680ba9e3b680b61d2acfdb9fb",
            "filename": "tests/models/whisper/test_modeling_tf_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -527,48 +527,6 @@ def test_attention_outputs(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_sequences=1):\n-        batch_size, mel, seq_length = input_ids.shape\n-        subsampled_seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n-        num_sequences_in_output = batch_size * num_return_sequences\n-        gen_len = (\n-            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n-        )\n-\n-        # scores\n-        self._check_scores(num_sequences_in_output, output.scores, length=gen_len, config=config)\n-\n-        # Attentions\n-        # encoder\n-        self._check_encoder_attention_for_generate(\n-            output.encoder_attentions, batch_size, config, subsampled_seq_length\n-        )\n-        # decoder\n-        self._check_attentions_for_generate(\n-            num_sequences_in_output,\n-            output.decoder_attentions,\n-            min_length=1,\n-            max_length=output.sequences.shape[-1],\n-            config=config,\n-            use_cache=use_cache,\n-        )\n-\n-        # Hidden States\n-        # encoder\n-        self._check_encoder_hidden_states_for_generate(\n-            output.encoder_hidden_states, batch_size, config, subsampled_seq_length\n-        )\n-\n-        # decoder\n-        self._check_hidden_states_for_generate(\n-            num_sequences_in_output,\n-            output.decoder_hidden_states,\n-            min_length=1,\n-            max_length=output.sequences.shape[-1],\n-            config=config,\n-            use_cache=use_cache,\n-        )\n-\n     # overwritten from parent due to the inability to work when non-text inputs are not passed AND because the input is\n     # `input_features`\n     def test_lm_head_model_random_no_beam_search_generate(self):"
        },
        {
            "sha": "916517add7b2c728918766ddf8b0f1fe3fcbecd2",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -1607,6 +1607,11 @@ def test_labels_sequence_max_length_error_after_changing_config(self):\n     def test_generate_compile_model_forward(self):\n         pass\n \n+    # TODO (joao, eustache): fix me :)\n+    @unittest.skip(reason=\"A CUDA exception is thrown when storing extra outputs\")\n+    def test_generate_compilation_all_outputs(self):\n+        pass\n+\n \n @require_torch\n @require_torchaudio"
        },
        {
            "sha": "d2eefced08fa921c72e70c0d4174e924f262873c",
            "filename": "tests/models/xlm/test_modeling_xlm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 38,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -473,50 +473,24 @@ def test_xlm_for_multiple_choice(self):\n         self.model_tester.create_and_check_xlm_for_multiple_choice(*config_and_inputs)\n \n     def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n-        self.assertIsInstance(attentions, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n+        # adds PAD dummy token, expected shape is off by 1\n+        prompt_length += 1\n+        output_length += 1\n+        super()._check_attentions_for_generate(\n+            batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n         )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_attentions in enumerate(attentions):\n-            # adds PAD dummy token\n-            tgt_len = min_length + idx + 1\n-            src_len = min_length + idx + 1\n-\n-            expected_shape = (\n-                batch_size * num_beam_groups,\n-                config.num_attention_heads,\n-                tgt_len,\n-                src_len,\n-            )\n-            # check attn size\n-            self.assertListEqual(\n-                [layer_attention.shape for layer_attention in iter_attentions], [expected_shape] * len(iter_attentions)\n-            )\n \n     def _check_hidden_states_for_generate(\n-        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n     ):\n-        self.assertIsInstance(hidden_states, tuple)\n-        self.assertListEqual(\n-            [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n-            [True] * len(hidden_states),\n+        # adds PAD dummy token, expected shape is off by 1\n+        prompt_length += 1\n+        output_length += 1\n+        super()._check_hidden_states_for_generate(\n+            batch_size, hidden_states, prompt_length, output_length, config, use_cache\n         )\n-        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n-\n-        for idx, iter_hidden_states in enumerate(hidden_states):\n-            # adds PAD dummy token\n-            seq_len = min_length + idx + 1\n-            expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n-            # check hidden size\n-            self.assertListEqual(\n-                [layer_hidden_states.shape for layer_hidden_states in iter_hidden_states],\n-                [expected_shape] * len(iter_hidden_states),\n-            )\n-        pass\n \n     @slow\n     def test_model_from_pretrained(self):"
        },
        {
            "sha": "4636efed10b129267075d156c74f8229971e02c9",
            "filename": "tests/models/xlnet/test_modeling_xlnet.py",
            "status": "modified",
            "additions": 15,
            "deletions": 20,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2ac0916a7902e1683d708805270142257a254a/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py?ref=be2ac0916a7902e1683d708805270142257a254a",
            "patch": "@@ -636,57 +636,52 @@ def _mock_init_weights(self, module):\n                 weight.data.fill_(3)\n \n     def _check_hidden_states_for_generate(\n-        self, batch_size, hidden_states, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, hidden_states, prompt_length, output_length, config, use_cache=False\n     ):\n         self.assertIsInstance(hidden_states, tuple)\n         self.assertListEqual(\n             [isinstance(iter_hidden_states, tuple) for iter_hidden_states in hidden_states],\n             [True] * len(hidden_states),\n         )\n-        self.assertEqual(len(hidden_states), (max_length - min_length) * num_beam_groups)\n+        self.assertEqual(len(hidden_states), (output_length - prompt_length))\n \n-        for idx, iter_hidden_states in enumerate(hidden_states):\n+        for generated_length, iter_hidden_states in enumerate(hidden_states):\n             # check hidden size\n             for i, layer_hidden_states in enumerate(iter_hidden_states):\n                 # every 2nd tensor is from extra stream\n                 if i % 2 != 0:\n-                    seq_len = 1\n+                    model_output_length = 1\n                 else:\n                     # for first item dummy PAD token is appended so need one more\n                     # else offset+dummy_token when using cache\n-                    seq_len = (min_length + 1) if idx == 0 else 3\n+                    model_output_length = (prompt_length + 1) if generated_length == 0 else 3\n \n-                expected_shape = (batch_size * num_beam_groups, seq_len, config.hidden_size)\n+                expected_shape = (batch_size, model_output_length, config.hidden_size)\n                 self.assertEqual(layer_hidden_states.shape, expected_shape)\n \n     def _check_attentions_for_generate(\n-        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+        self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n         self.assertIsInstance(attentions, tuple)\n         self.assertListEqual(\n             [isinstance(iter_attentions, tuple) for iter_attentions in attentions], [True] * len(attentions)\n         )\n-        self.assertEqual(len(attentions), (max_length - min_length) * num_beam_groups)\n+        self.assertEqual(len(attentions), (output_length - prompt_length))\n \n-        for idx, attentions_item in enumerate(attentions):\n+        for generated_length, attentions_item in enumerate(attentions):\n             for iter_attentions in attentions_item:\n-                tgt_len = min_length\n+                model_input_length = prompt_length\n \n                 # for first item dummy PAD token is appended so need one more\n                 # every token after consists of offset+dummy_token length when using cache\n-                if idx == 0:\n-                    tgt_len += 1\n+                if generated_length == 0:\n+                    model_input_length += 1\n                 else:\n-                    tgt_len = 3\n+                    model_input_length = 3\n \n-                src_len = min_length + idx + 1\n+                query_length = prompt_length + generated_length + 1\n \n-                expected_shape = (\n-                    batch_size * num_beam_groups,\n-                    config.num_attention_heads,\n-                    tgt_len,\n-                    src_len,\n-                )\n+                expected_shape = (batch_size, config.num_attention_heads, model_input_length, query_length)\n                 # check attn size\n                 self.assertListEqual(\n                     [layer_attention.shape for layer_attention in iter_attentions],"
        }
    ],
    "stats": {
        "total": 1304,
        "additions": 383,
        "deletions": 921
    }
}