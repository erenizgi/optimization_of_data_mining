{
    "author": "LoserCheems",
    "message": "Add Doge model (#35891)\n\n* Add Doge Model\n\n* Fix code quality\n\n* Rollback an error commit\n\n* Fix config for open-source weights\n\n* Revert \"Fix config for open-source weights\"\n\nThis reverts commit 229cdcac10a6a4274d1dd13b729bc14c98eb0c76.\n\n* Add modular_doge\n\n* Update Doge inherits from Llama\n\n* Fix import bug\n\n* [docs] Add usage of doge model\n\n* Fix Doge import pretrainedconfig from modeling_utils to configuration_utils\n\n* [docs] remove trust remote code from doge\n\n* Fix dynamo bug in doge model\n\n* Update docstrings\n\n* Import apply_rotary_pos_emb and repeat_kv from Llama\n\n* Fix all nits\n\n* Fix code quality\n\n* Fix some bugs\n\n* Fix code quality\n\n* Remove inherited `_update_causal_mask` from Llama\nThis leads to incorrect weight initialization.\n\n* Fix the wrong tensor orderings in DogeCDMoE\n\n* Fix attention mask bug\nWe have to provide attention_mask for dynamic mask computation\n\n* Modify most implementations to inherit from Llama\nBut there are two problems:\n1. `flex_attention_forward` is not updated properly\n2. `Example` error in the forward method of DogeForCausalLM\n\n* Modify CDMoE for batch efficient implementation\n\n* Uniform MoE configuration names, just like QwenMoE\n\n* Fix code quality\n\n* Fix code quality\n\n* Fix code quality\n\n* Add tp plan of CDMoE Module\n\n* Hybird DMA with sliding window\n\n* Update valid tokens greater than window size\n\n* Fix code quality\n\n* Add `convert_doge_weights_to_hf`\n\n* Fix STATE_DICT_MAPPING in convert_doge_weights_to_hf.py\n\n* Fix nits in modular_doge\n\n* Fix code quality\n\n* Fix all nits\n\n* Fix all nits\n\n* Make sure the attention function is updated inside the class\n\n* Fix code quality issues in the Doge model and add a test for it\n\n* Fix `test_generate`\n\n* Fix code quality\n\n* Fix nits fllowing suggestions\n\n* Fix code quality\n\n* Fix code quality issues\n\n* Fix nits\n\n* Fix code quality nits\n\n* Fix the missing parameters in the configuration.\n\n* Fix the missing parameters in the configuration.\n\n* Fix nits\n\n* Add initialization of attention\n\n* Fix last nits\n\n* Simplify dynamic mask generation logic\n\n* Rename router_logits to gate_logits for matching latest changes of MixtralModel\n\n* Rename typings for matching latest changes of MixtralModel\n\n* Fixes typo in comment\n\n* Update src/transformers/models/doge/modular_doge.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Fix code quality issues to match other modular\n\n* Fix code quality issues to match other modular\n\n* Fix the static compilation errors\n\n* Update model weights link\n\n* Fix code quality issues to match other modular\n\n* reapply modular and support for new outputs\n\n* style\n\n* simplify a lot\n\n* fix import location\n\n* reapply modular\n\n* fix\n\n* fix integration test\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
    "files": [
        {
            "sha": "e8a0e3312167faf0e57f7fb981e68bd93d46d946",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -429,6 +429,8 @@\n         title: DiffLlama\n       - local: model_doc/distilbert\n         title: DistilBERT\n+      - local: model_doc/doge\n+        title: Doge\n       - local: model_doc/dots1\n         title: dots1\n       - local: model_doc/dpr"
        },
        {
            "sha": "76ffc390d5906dd70829e535ad786964241d0048",
            "filename": "docs/source/en/model_doc/doge.md",
            "status": "added",
            "additions": 103,
            "deletions": 0,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/docs%2Fsource%2Fen%2Fmodel_doc%2Fdoge.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/docs%2Fsource%2Fen%2Fmodel_doc%2Fdoge.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdoge.md?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -0,0 +1,103 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Doge\n+\n+\n+## Overview\n+\n+Doge is a series of small language models based on the [Doge](https://github.com/SmallDoges/small-doge) architecture, aiming to combine the advantages of state-space and self-attention algorithms, calculate dynamic masks from cached value states using the zero-order hold method, and solve the problem of existing mainstream language models getting lost in context. It uses the `wsd_scheduler` scheduler to pre-train on the `smollm-corpus`, and can continue training on new datasets or add sparse activation feedforward networks from stable stage checkpoints.\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/refs%2Fpr%2F426/transformers/model_doc/doge_architecture.png\" alt=\"drawing\" width=\"600\"/>\n+\n+As shown in the figure below, the sequence transformation part of the Doge architecture uses `Dynamic Mask Attention`, which can be understood as using self-attention related to value states during training, and using state-space without past state decay during inference, to solve the problem of existing Transformers or SSMs getting lost in long text. The state transformation part of Doge uses `Cross Domain Mixture of Experts`, which consists of dense linear layers and sparse embedding layers, and can additionally increase sparse parameters to continue training from dense weight checkpoints without retraining the entire model, thereby reducing the cost of continuous iteration of the model. In addition, Doge also uses `RMSNorm` and `Residual` with learnable parameters to adapt the gradient range of deep models.\n+\n+Checkout all Doge model checkpoints [here](https://huggingface.co/collections/SmallDoge/doge-slm-679cc991f027c4a3abbded4a).\n+\n+\n+## Usage\n+\n+<details>\n+<summary>Using Doge-Base for text generation</summary>\n+\n+```python\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-20M\")\n+model = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-20M\")\n+inputs = tokenizer(\"Hey how are you doing?\", return_tensors=\"pt\")\n+\n+outputs = model.generate(**inputs, max_new_tokens=100)\n+print(tokenizer.batch_decode(outputs))\n+```\n+</details>\n+\n+<details>\n+<summary>Using Doge-Instruct for question answering</summary>\n+\n+```python\n+from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, TextStreamer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-20M-Instruct\")\n+model = AutoModelForCausalLM.from_pretrained(\"SmallDoge/Doge-20M-Instruct\")\n+\n+generation_config = GenerationConfig(\n+      max_new_tokens=100, \n+      use_cache=True, \n+      do_sample=True, \n+      temperature=0.8, \n+      top_p=0.9,\n+      repetition_penalty=1.0\n+)\n+steamer = TextStreamer(tokenizer=tokenizer, skip_prompt=True)\n+\n+prompt = \"Hi, how are you doing today?\"\n+conversation = [\n+      {\"role\": \"user\", \"content\": prompt}\n+]\n+inputs = tokenizer.apply_chat_template(\n+    conversation=conversation,\n+    tokenize=True,\n+    return_tensors=\"pt\",\n+)\n+\n+outputs = model.generate(\n+    inputs, \n+    tokenizer=tokenizer,\n+    generation_config=generation_config, \n+    streamer=steamer\n+)\n+```\n+</details>\n+\n+## DogeConfig\n+\n+[[autodoc]] DogeConfig\n+\n+## DogeModel\n+\n+[[autodoc]] DogeModel\n+    - forward\n+\n+## DogeForCausalLM\n+\n+[[autodoc]] DogeForCausalLM\n+    - forward\n+\n+## DogeForSequenceClassification\n+\n+[[autodoc]] DogeForSequenceClassification\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "89cbbd03a4c142307a54eb2b0e8e14db7814062e",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -112,6 +112,7 @@\n         (\"dinov2\", \"Dinov2Config\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersConfig\"),\n         (\"distilbert\", \"DistilBertConfig\"),\n+        (\"doge\", \"DogeConfig\"),\n         (\"donut-swin\", \"DonutSwinConfig\"),\n         (\"dots1\", \"Dots1Config\"),\n         (\"dpr\", \"DPRConfig\"),\n@@ -493,6 +494,7 @@\n         (\"dinov2_with_registers\", \"DINOv2 with Registers\"),\n         (\"distilbert\", \"DistilBERT\"),\n         (\"dit\", \"DiT\"),\n+        (\"doge\", \"Doge\"),\n         (\"donut-swin\", \"DonutSwin\"),\n         (\"dots1\", \"dots1\"),\n         (\"dpr\", \"DPR\"),"
        },
        {
            "sha": "5e8db32d0a6aff96bbdc2bf684112faaa9562bc3",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -105,6 +105,7 @@\n         (\"dinov2\", \"Dinov2Model\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersModel\"),\n         (\"distilbert\", \"DistilBertModel\"),\n+        (\"doge\", \"DogeModel\"),\n         (\"donut-swin\", \"DonutSwinModel\"),\n         (\"dots1\", \"Dots1Model\"),\n         (\"dpr\", \"DPRQuestionEncoder\"),\n@@ -576,6 +577,7 @@\n         (\"dbrx\", \"DbrxForCausalLM\"),\n         (\"deepseek_v3\", \"DeepseekV3ForCausalLM\"),\n         (\"diffllama\", \"DiffLlamaForCausalLM\"),\n+        (\"doge\", \"DogeForCausalLM\"),\n         (\"dots1\", \"Dots1ForCausalLM\"),\n         (\"electra\", \"ElectraForCausalLM\"),\n         (\"emu3\", \"Emu3ForCausalLM\"),\n@@ -1105,6 +1107,7 @@\n         (\"deberta-v2\", \"DebertaV2ForSequenceClassification\"),\n         (\"diffllama\", \"DiffLlamaForSequenceClassification\"),\n         (\"distilbert\", \"DistilBertForSequenceClassification\"),\n+        (\"doge\", \"DogeForSequenceClassification\"),\n         (\"electra\", \"ElectraForSequenceClassification\"),\n         (\"ernie\", \"ErnieForSequenceClassification\"),\n         (\"ernie_m\", \"ErnieMForSequenceClassification\"),"
        },
        {
            "sha": "91aeca0340f162439b6b7485f325f5343802dd6e",
            "filename": "src/transformers/models/doge/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2F__init__.py?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -0,0 +1,28 @@\n+# coding=utf-8\n+# Copyright 2025 Jingze Shi and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_doge import *\n+    from .modeling_doge import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "06bd87aa07f172032a9a3c7edc7aa334fb8e79bf",
            "filename": "src/transformers/models/doge/configuration_doge.py",
            "status": "added",
            "additions": 241,
            "deletions": 0,
            "changes": 241,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -0,0 +1,241 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/doge/modular_doge.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_doge.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Jingze Shi and the HuggingFace Inc. team. All rights reserved.\n+#\n+# The Doge family of small language models is trained by SmallDoge Team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class DogeConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DogeModel`]. It is used to instantiate an Doge\n+    model according to the specified arguments, defining the model architecture like [SmallDoge/Doge-320M](https://huggingface.co/SmallDoge/Doge-320M).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32768):\n+            Vocabulary size of the Doge2 model. Defines the number of different tokens that can be represented by the `inputs_ids` passed when calling [`DogeModel`]\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        hidden_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for each sequence transformation and state transformation module.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings.\n+            NOTE: if you apply new rope type and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value accordingly.\n+            Doge family of small models use `{ 'rope_type': 'dynamic', 'factor': 4.0, 'original_max_position_embeddings': 2048 }` as the default value.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope', 'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings.\n+                    In most scaling types, a `factor` of x will enable the model to handle sequences of length x * original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'.\n+                    The original max position embeddings used during pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation.\n+                    If unspecified, it defaults to value recommended by the implementation, using the `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<`original_max_position_embeddings`).\n+                    Must be a list of numbers with the same length as the hidden size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<`original_max_position_embeddings`).\n+                    Must be a list of numbers with the same length as the hidden size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention.\n+            If `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used.\n+            When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group.\n+            For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf).\n+            If it is not specified, will default to `num_attention_heads`.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        sliding_window (`int`, *optional*):\n+            Sliding window attention window size. If not specified, will default to `None`.\n+        keep_window_size (`int`, *optional*, defaults to 2048):\n+            The window size of tokens that are not dynamically masked, and dynamic masking is only performed when the sequence length exceeds this value.\n+        is_moe (`bool`, *optional*, defaults to `False`):\n+            Whether to use the Cross Domain Mixture of Experts, if `True`, the MoE will inherit the MLP to initialize.\n+        num_experts (`int`, *optional*, defaults to 16384):\n+            Number of routed experts in the model. This is only used when `is_moe=True`.\n+        num_experts_per_tok (`int`, *optional*, defaults to 64):\n+            Number of selected experts to route per-token.\n+        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n+            Whether to normalize the topk probabilities.\n+        output_router_logits (`bool`, *optional*, defaults to `False`):\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n+            allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n+            The aux loss factor for the total loss.\n+\n+    ```python\n+    >>> from transformers import DogeConfig, DogeModel\n+\n+    >>> # Initializing a Doge-320M style configuration\n+    >>> configuration = DogeConfig()\n+\n+    >>> # Initializing a model from the Doge-320M style configuration\n+    >>> model = DogeModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"doge\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `DogeModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.dt_proj\": \"rowwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n+        \"layers.*.input_residual.weight\": \"sequence_parallel\",\n+        \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n+        \"layers.*.post_attention_residual.weight\": \"sequence_parallel\",\n+        \"norm.weight\": \"sequence_parallel\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.router_gate\": \"colwise_rep\",\n+        \"layers.*.mlp.down_embed\": \"rowwise_rep\",\n+        \"layers.*.mlp.up_embed\": \"rowwise_rep\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=32768,\n+        hidden_size=1024,\n+        intermediate_size=2048,\n+        num_hidden_layers=32,\n+        hidden_dropout=0.0,\n+        hidden_act=\"silu\",\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-06,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n+        max_position_embeddings=2048,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        num_attention_heads=8,\n+        num_key_value_heads=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        mlp_bias=False,\n+        sliding_window=None,\n+        keep_window_size=2048,\n+        is_moe=False,\n+        num_experts=16384,\n+        num_experts_per_tok=64,\n+        norm_topk_prob=False,\n+        output_router_logits=False,\n+        router_aux_loss_coef=0.001,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+\n+        self.hidden_dropout = hidden_dropout\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+\n+        self.max_position_embeddings = max_position_embeddings\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.sliding_window = sliding_window\n+        self.keep_window_size = keep_window_size\n+        self.is_moe = is_moe\n+        self.num_experts = num_experts\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.norm_topk_prob = norm_topk_prob\n+        self.output_router_logits = output_router_logits\n+        self.router_aux_loss_coef = router_aux_loss_coef\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            self.num_key_value_heads = num_attention_heads\n+\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"DogeConfig\"]"
        },
        {
            "sha": "cde4350a15c4bd24bbc42da73bc320380cf25e25",
            "filename": "src/transformers/models/doge/convert_doge_weights_to_hf.py",
            "status": "added",
            "additions": 126,
            "deletions": 0,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2Fconvert_doge_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2Fconvert_doge_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconvert_doge_weights_to_hf.py?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -0,0 +1,126 @@\n+import argparse\n+import json\n+import os\n+import re\n+\n+import torch\n+from safetensors.torch import load_file\n+\n+from transformers import DogeConfig, DogeForCausalLM\n+\n+\n+# fmt: off\n+# `None` means we drop the key\n+STATE_DICT_MAPPING = {\n+    # CausalLM keys\n+    r\"^lm_head.weight\": r\"lm_head.weight\",\n+\n+    # Model keys\n+    r\"^model.word_embed.weight\": r\"model.embed_tokens.weight\",\n+    r\"^model.rotary_emb.rotary_emb\": r\"model.rotary_emb.rotary_emb\",\n+    r\"^model.final_layernorm.weight\": r\"model.norm.weight\",\n+\n+    # Layers keys\n+    r\"^model.layers.(\\d+).pre_layernorm.weight\": r\"model.layers.\\1.input_layernorm.weight\",\n+    r\"^model.layers.(\\d+).pre_residual.weight\": r\"model.layers.\\1.input_residual\",\n+    r\"^model.layers.(\\d+).post_layernorm.weight\": r\"model.layers.\\1.post_attention_layernorm.weight\",\n+    r\"^model.layers.(\\d+).post_residual.weight\": r\"model.layers.\\1.post_attention_residual\",\n+\n+    # Attention keys\n+    r\"^model.layers.(\\d+).self_attn.q_proj.weight\": r\"model.layers.\\1.self_attn.q_proj.weight\",\n+    r\"^model.layers.(\\d+).self_attn.k_proj.weight\": r\"model.layers.\\1.self_attn.k_proj.weight\",\n+    r\"^model.layers.(\\d+).self_attn.v_proj.weight\": r\"model.layers.\\1.self_attn.v_proj.weight\",\n+    r\"^model.layers.(\\d+).self_attn.A\": r\"model.layers.\\1.self_attn.A\",\n+    r\"^model.layers.(\\d+).self_attn.dt_proj.weight\": r\"model.layers.\\1.self_attn.dt_proj.weight\",\n+    r\"^model.layers.(\\d+).self_attn.o_proj.weight\": r\"model.layers.\\1.self_attn.o_proj.weight\",\n+\n+    # Feedforward keys\n+    r\"^model.layers.(\\d+).feed_forward.gate_proj.weight\": r\"model.layers.\\1.mlp.gate_proj.weight\",\n+    r\"^model.layers.(\\d+).feed_forward.up_proj.weight\": r\"model.layers.\\1.mlp.up_proj.weight\",\n+    r\"^model.layers.(\\d+).feed_forward.down_proj.weight\": r\"model.layers.\\1.mlp.down_proj.weight\",\n+    r\"^model.layers.(\\d+).feed_forward.router_gate.weight\": r\"model.layers.\\1.mlp.router_gate.weight\",\n+    r\"^model.layers.(\\d+).feed_forward.router_gate.bias\": None,\n+    r\"^model.layers.(\\d+).feed_forward.down_embed.weight\": r\"model.layers.\\1.mlp.down_embed.weight\",\n+    r\"^model.layers.(\\d+).feed_forward.up_embed.weight\": r\"model.layers.\\1.mlp.up_embed.weight\",\n+}\n+# fmt: on\n+\n+\n+def load_weights(input_dir: str):\n+    safetensor_files = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if x.endswith(\".safetensors\")]\n+\n+    all_weights = {}\n+\n+    if safetensor_files:\n+        if len(safetensor_files) == 1:\n+            tensors = load_file(safetensor_files[0])\n+            all_weights.update(tensors)\n+            return all_weights\n+        safetensor_files = sorted(safetensor_files, key=lambda x: int(x.rsplit(\"-\", 3)[1]))\n+        for file in safetensor_files:\n+            tensors = load_file(file)\n+            all_weights.update(tensors)\n+        return all_weights\n+\n+    else:\n+        raise ValueError(\"No .safetensors or .bin files found in the specified directory.\")\n+\n+\n+def map_old_key_to_new(old_key):\n+    for pattern, replacement in STATE_DICT_MAPPING.items():\n+        if replacement is None:\n+            if re.fullmatch(pattern, old_key):\n+                return None\n+        else:\n+            new_key, n_replace = re.subn(pattern, replacement, old_key)\n+            # Early exit of the loop\n+            if n_replace > 0:\n+                return new_key\n+\n+    raise ValueError(f\"Key: {old_key} could not be mapped (check the mapping).\")\n+\n+\n+def convert_state_dict(original_state_dict: dict, config: DogeConfig):\n+    new_dict = {}\n+\n+    for old_key, value in original_state_dict.items():\n+        new_key = map_old_key_to_new(old_key)\n+        if new_key is None:\n+            continue\n+        new_dict[new_key] = value\n+    return new_dict\n+\n+\n+def convert_doge_model(input_dir, output_dir):\n+    # Load and convert config\n+    with open(os.path.join(input_dir, \"config.json\")) as f:\n+        config = json.load(f)\n+    config = DogeConfig(**config)\n+    config.save_pretrained(output_dir)\n+\n+    # Load and convert weights\n+    original_state_dict = load_weights(input_dir)\n+    new_dict = convert_state_dict(original_state_dict, config)\n+    with torch.device(\"meta\"):\n+        model = DogeForCausalLM(config)\n+    if config.tie_word_embeddings:\n+        new_dict[\"lm_head.weight\"] = new_dict[\"model.embed_tokens.weight\"]\n+    model.load_state_dict(new_dict, strict=True, assign=True)\n+    model.save_pretrained(output_dir)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"input_dir\",\n+        type=str,\n+        help=\"Location of the local folder copied from the Hub.\",\n+    )\n+    parser.add_argument(\n+        \"output_dir\",\n+        type=str,\n+        help=\"Location to write HF model.\",\n+    )\n+\n+    args = parser.parse_args()\n+    convert_doge_model(args.input_dir, args.output_dir)"
        },
        {
            "sha": "17b33bc418feafbfdd7eb2343875e14e4857ad8c",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "added",
            "additions": 947,
            "deletions": 0,
            "changes": 947,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -0,0 +1,947 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/doge/modular_doge.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_doge.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Jingze Shi and the HuggingFace Inc. team. All rights reserved.\n+#\n+# The Doge family of small language models is trained by SmallDoge Team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...integrations.flex_attention import compile_friendly_flex_attention\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    MoeCausalLMOutputWithPast,\n+    MoeModelOutputWithPast,\n+    SequenceClassifierOutputWithPast,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import AttentionInterface, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from .configuration_doge import DogeConfig\n+\n+\n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class DogeRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        DogeRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class DogeRotaryEmbedding(nn.Module):\n+    def __init__(self, config: DogeConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def flex_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+    scaling: Optional[float] = None,\n+    softcap: Optional[float] = None,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+) -> tuple[torch.Tensor, torch.Tensor]:\n+    block_mask = None\n+    causal_mask = None\n+    if isinstance(attention_mask, BlockMask):\n+        block_mask = attention_mask\n+    else:\n+        causal_mask = attention_mask\n+\n+    if causal_mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n+        if softcap is not None:\n+            score = softcap * torch.tanh(score / softcap)\n+        if causal_mask is not None:\n+            score = score + causal_mask[batch_idx][head_idx][q_idx][kv_idx]\n+        if head_mask is not None:\n+            score = score + head_mask[batch_idx][head_idx][0][0]\n+        return score\n+\n+    attn_output, attention_weights = compile_friendly_flex_attention(\n+        query,\n+        key,\n+        value,\n+        score_mod=score_mod,\n+        block_mask=block_mask,\n+        enable_gqa=True,\n+        scale=scaling,\n+        # Last time checked on PyTorch == 2.5.1: Flex Attention always computes the lse regardless.\n+        # For simplification, we thus always return it as no additional computations are introduced.\n+        return_lse=True,\n+    )\n+    # lse is returned in float32\n+    attention_weights = attention_weights.to(value.dtype)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attention_weights\n+\n+\n+ALL_ATTENTION_FUNCTIONS = AttentionInterface()\n+ALL_ATTENTION_FUNCTIONS[\"doge_flex_attention\"] = flex_attention_forward\n+\n+\n+class DogeAttention(nn.Module):\n+    def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.keep_window_size = config.keep_window_size\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        # dynamic mask for the QK^T attention weights matrix\n+        self.A = nn.Parameter(torch.zeros(config.num_key_value_heads))\n+        self.dt_proj = nn.Linear(\n+            config.num_key_value_heads * self.head_dim, config.num_key_value_heads, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.q_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.k_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # calculate dynamic mask from value_states\n+        dt_states = self.dt_proj(\n+            value_states.transpose(1, 2).reshape(value_states.shape[0], value_states.shape[-2], -1)\n+        )\n+        dt_states = torch.exp(self.A * F.softplus(dt_states)).transpose(-1, -2)\n+        attn_mask = self.prepare_dynamic_mask(\n+            hidden_states=hidden_states,\n+            dt_states=dt_states,\n+            keep_window_size=self.keep_window_size,\n+            attention_mask=attention_mask,\n+        )\n+        attn_mask = repeat_kv(attn_mask, self.num_key_value_groups)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=attn_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+    def prepare_dynamic_mask(\n+        self,\n+        hidden_states: torch.Tensor,\n+        dt_states: torch.Tensor,\n+        keep_window_size: int = 2048,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        \"\"\"\n+        The core idea of DMA is to calculate the dynamic attention mask to mask the tokens that should be masked, so as to form sparse attention.\n+\n+        Combine `dt_states` with `attention_mask` to generate the final `attn_mask`.\n+\n+        Args:\n+            hidden_states (`torch.Tensor`): The input hidden_states, used to determine the minimum value of the current input precision.\n+            dt_states (`torch.Tensor`): dt_states of shape `(batch_size, num_heads, key_sequence_length)`.\n+            keep_window_size (`int`): The window size of tokens that are not dynamically masked, and dynamic masking is only performed when the sequence length exceeds this value.\n+            attention_mask (`torch.Tensor`, *optional*): attention mask of shape `(batch_size, 1, query_sequence_length, key_sequence_length)`.\n+        \"\"\"\n+        min_dtype = torch.finfo(hidden_states.dtype).min\n+        dtype = hidden_states.dtype\n+        attn_mask = dt_states[:, :, None, :].expand(\n+            -1, -1, hidden_states.shape[1], -1\n+        )  # [batch_size, num_heads, query_len, key_len]\n+        if attention_mask is not None and not isinstance(attention_mask, BlockMask):\n+            if attention_mask.dtype == torch.bool:\n+                dtype = hidden_states.dtype\n+                attention_mask = torch.where(\n+                    attention_mask, torch.tensor(0.0, device=attention_mask.device, dtype=dtype), min_dtype\n+                )\n+            attn_mask = attn_mask.masked_fill(attention_mask[:, :, :, : attn_mask.shape[-1]] != 0, min_dtype)\n+        if attn_mask.shape[-1] > keep_window_size:\n+            active_mask = torch.zeros_like(attn_mask, dtype=dtype, device=attn_mask.device)\n+            topk_indices = torch.topk(attn_mask, keep_window_size, dim=-1, largest=True, sorted=False).indices\n+            active_mask = active_mask.scatter(-1, topk_indices, 1.0)\n+            attn_mask = attn_mask.masked_fill(active_mask == 0.0, min_dtype)\n+        return attn_mask\n+\n+\n+class DogeMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class DogeCDMoE(nn.Module):\n+    def __init__(self, config: DogeConfig):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+        self.num_experts = config.num_experts\n+        self.num_keys = math.floor(math.sqrt(self.num_experts))\n+        self.top_k = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n+\n+        # shared expert\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+\n+        # router gate for retrieval experts\n+        self.router_gate = nn.Linear(self.hidden_size, self.num_keys * 2, bias=False)\n+\n+        # routed experts\n+        self.down_embed = nn.Embedding(self.num_experts, self.hidden_size)\n+        self.up_embed = nn.Embedding(self.num_experts, self.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        bsz, seq_len, _ = hidden_states.shape\n+\n+        # get routing logits with router gate\n+        router_logits = self.router_gate(hidden_states).view(2, bsz * seq_len, -1)\n+\n+        # get experts with the highest routing logits\n+        (scores_x, scores_y), (indices_x, indices_y) = router_logits.topk(self.num_keys, dim=-1)\n+        all_scores = scores_x.unsqueeze(-1) + scores_y.unsqueeze(-2)\n+        all_indices = indices_x.unsqueeze(-1) * self.num_keys + indices_y.unsqueeze(-2)\n+        all_scores = all_scores.view(*all_scores.shape[:-2], -1)\n+        all_indices = all_indices.view(*all_indices.shape[:-2], -1)\n+        scores, position_indices = all_scores.topk(self.top_k, dim=-1)\n+        indices = all_indices.gather(-1, position_indices)\n+        routing_weights = F.softmax(scores, dim=-1)\n+        if self.norm_topk_prob:\n+            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+\n+        # mix routed experts states with shared expert states\n+        down_embed = self.down_embed(indices)\n+        up_embed = self.up_embed(indices)\n+        experts_weights = torch.matmul(down_embed, hidden_states.view(bsz * seq_len, -1, 1)).view(bsz * seq_len, -1)\n+        experts_weights = self.act_fn(experts_weights) * routing_weights\n+        experts_states = torch.matmul(experts_weights.view(bsz * seq_len, 1, -1), up_embed).view(bsz, seq_len, -1)\n+        hidden_states = self.down_proj(self.act_fn(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))\n+        hidden_states = hidden_states + experts_states\n+        return hidden_states, router_logits\n+\n+\n+class DogeDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.hidden_dropout = config.hidden_dropout\n+\n+        self.input_layernorm = DogeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.self_attn = DogeAttention(config=config, layer_idx=layer_idx)\n+        self.input_residual = nn.Parameter(torch.ones(config.hidden_size))\n+\n+        self.post_attention_layernorm = DogeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.mlp = DogeMLP(config) if not config.is_moe else DogeCDMoE(config)\n+        self.post_attention_residual = nn.Parameter(torch.ones(config.hidden_size))\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        # sequence transformation\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = F.dropout(hidden_states, p=self.hidden_dropout, training=self.training)\n+        hidden_states = self.input_residual * residual + hidden_states\n+\n+        # state transformation\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = F.dropout(hidden_states, p=self.hidden_dropout, training=self.training)\n+        hidden_states = self.post_attention_residual * residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class DogePreTrainedModel(PreTrainedModel):\n+    config_class = DogeConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"DogeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = False\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = False\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(DogeCDMoE, index=1),\n+        \"hidden_states\": DogeDecoderLayer,\n+        \"attentions\": DogeAttention,\n+    }\n+    _supports_flash_attn_3 = False\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, DogeRMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+        if isinstance(module, DogeAttention):\n+            if hasattr(module, \"A\"):\n+                module.A.data.zero_()\n+        elif isinstance(module, DogeDecoderLayer):\n+            if hasattr(module, \"input_residual\"):\n+                module.input_residual.data.fill_(1.0)\n+            if hasattr(module, \"post_attention_residual\"):\n+                module.post_attention_residual.data.fill_(1.0)\n+\n+\n+@auto_docstring\n+class DogeModel(DogePreTrainedModel):\n+    def __init__(self, config: DogeConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [DogeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = DogeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = DogeRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    num_keys: Optional[int] = None,\n+    top_k: int = 2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `router_gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [2, batch_size * sequence_length, num_keys].\n+        num_experts:\n+            Number of experts\n+        num_keys:\n+            Number of keys\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    compute_dtype = gate_logits[0].dtype\n+    compute_device = gate_logits[0].device\n+    all_expert_indices = []\n+    all_routing_weights = []\n+\n+    for layer_gate_logits in gate_logits:\n+        layer_gate_logits = layer_gate_logits.to(compute_device)\n+\n+        (scores_x, scores_y), (indices_x, indices_y) = layer_gate_logits.topk(num_keys, dim=-1)\n+\n+        all_scores = scores_x.unsqueeze(-1) + scores_y.unsqueeze(-2)\n+        all_indices = indices_x.unsqueeze(-1) * num_keys + indices_y.unsqueeze(-2)\n+        all_scores = all_scores.view(*all_scores.shape[:-2], -1)\n+        all_indices = all_indices.view(*all_indices.shape[:-2], -1)\n+\n+        _, position_indices = all_scores.topk(top_k, dim=-1)\n+        expert_indices = all_indices.gather(-1, position_indices)\n+\n+        routing_weights = F.softmax(all_scores, dim=-1)\n+\n+        all_expert_indices.append(expert_indices)\n+        all_routing_weights.append(routing_weights)\n+    all_expert_indices = torch.cat(all_expert_indices, dim=0)\n+    all_routing_weights = torch.cat(all_routing_weights, dim=0)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        all_expert_indices = all_expert_indices.view(-1)\n+        tokens_per_expert = torch.zeros(num_experts, dtype=compute_dtype, device=compute_device)\n+        pad = torch.ones_like(all_expert_indices, dtype=compute_dtype, device=compute_device)\n+        tokens_per_expert = tokens_per_expert.scatter_add_(0, all_expert_indices, pad) / all_expert_indices.shape[0]\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(all_routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = len(gate_logits)\n+\n+        #  Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k))\n+            .reshape(-1)\n+            .to(compute_device)\n+        )\n+        all_expert_indices = all_expert_indices.view(-1)[expert_attention_mask.bool()]\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.zeros(num_experts, dtype=compute_dtype, device=compute_device)\n+        pad = torch.ones_like(all_expert_indices, dtype=compute_dtype, device=compute_device)\n+        tokens_per_expert = tokens_per_expert.scatter_add_(0, all_expert_indices, pad) / torch.sum(\n+            expert_attention_mask\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(all_routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert)\n+    return overall_loss * num_experts\n+\n+\n+@auto_docstring\n+class DogeForCausalLM(DogePreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = DogeModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.router_aux_loss_coef = config.router_aux_loss_coef\n+        self.num_experts = config.num_experts\n+        self.num_experts_per_tok = config.num_experts_per_tok\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        output_router_logits: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, DogeForCausalLM\n+\n+        >>> model = DogeForCausalLM.from_pretrained(\"SmallDoge/Doge-320M\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-320M\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        output_router_logits = (\n+            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs: MoeModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        aux_loss = None\n+        if output_router_logits:\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.num_experts,\n+                math.floor(math.sqrt(self.num_experts)),\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n+\n+        return MoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            router_logits=outputs.router_logits,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Doge Model transformer with a sequence classification head on top (linear layer).\n+\n+    [`DogeForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n+    (e.g. GPT-2) do.\n+\n+    Since it does classification on the last token, it requires to know the position of the last token. If a\n+    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n+    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n+    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n+    each row of the batch).\n+    \"\"\"\n+)\n+class DogeForSequenceClassification(DogePreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.model = DogeModel(config)\n+        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> SequenceClassifierOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+\n+        transformer_outputs: BaseModelOutputWithPast = self.model(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+        hidden_states = transformer_outputs.last_hidden_state\n+        logits = self.score(hidden_states)\n+\n+        if input_ids is not None:\n+            batch_size = input_ids.shape[0]\n+        else:\n+            batch_size = inputs_embeds.shape[0]\n+\n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n+        if self.config.pad_token_id is None:\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n+        else:\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n+        return SequenceClassifierOutputWithPast(\n+            loss=loss,\n+            logits=pooled_logits,\n+            past_key_values=transformer_outputs.past_key_values,\n+            hidden_states=transformer_outputs.hidden_states,\n+            attentions=transformer_outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"DogeForCausalLM\", \"DogeModel\", \"DogePreTrainedModel\", \"DogeForSequenceClassification\"]"
        },
        {
            "sha": "208989aff833aaa792ed2ee730e3e75a17dabeb5",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "added",
            "additions": 799,
            "deletions": 0,
            "changes": 799,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -0,0 +1,799 @@\n+# coding=utf-8\n+# Copyright 2025 Jingze Shi and the HuggingFace Inc. team. All rights reserved.\n+#\n+# The Doge family of small language models is trained by SmallDoge Team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Doge model.\"\"\"\n+\n+import math\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...configuration_utils import PretrainedConfig\n+from ...integrations.flex_attention import compile_friendly_flex_attention\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_rope_utils import rope_config_validation\n+from ...modeling_utils import AttentionInterface\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, is_torch_flex_attn_available\n+from ...utils.generic import OutputRecorder\n+from ..llama.modeling_llama import (\n+    LlamaForSequenceClassification,\n+    LlamaMLP,\n+    LlamaPreTrainedModel,\n+    LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+    repeat_kv,\n+)\n+from ..mixtral.modeling_mixtral import MixtralForCausalLM, MixtralModel\n+\n+\n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+\n+class DogeConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DogeModel`]. It is used to instantiate an Doge\n+    model according to the specified arguments, defining the model architecture like [SmallDoge/Doge-320M](https://huggingface.co/SmallDoge/Doge-320M).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32768):\n+            Vocabulary size of the Doge2 model. Defines the number of different tokens that can be represented by the `inputs_ids` passed when calling [`DogeModel`]\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        hidden_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for each sequence transformation and state transformation module.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings.\n+            NOTE: if you apply new rope type and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value accordingly.\n+            Doge family of small models use `{ 'rope_type': 'dynamic', 'factor': 4.0, 'original_max_position_embeddings': 2048 }` as the default value.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope', 'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings.\n+                    In most scaling types, a `factor` of x will enable the model to handle sequences of length x * original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'.\n+                    The original max position embeddings used during pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation.\n+                    If unspecified, it defaults to value recommended by the implementation, using the `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<`original_max_position_embeddings`).\n+                    Must be a list of numbers with the same length as the hidden size divided by the number of attention heads divided by 2\n+                `long_factor` (`List[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<`original_max_position_embeddings`).\n+                    Must be a list of numbers with the same length as the hidden size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention.\n+            If `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used.\n+            When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group.\n+            For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf).\n+            If it is not specified, will default to `num_attention_heads`.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mlp_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n+        sliding_window (`int`, *optional*):\n+            Sliding window attention window size. If not specified, will default to `None`.\n+        keep_window_size (`int`, *optional*, defaults to 2048):\n+            The window size of tokens that are not dynamically masked, and dynamic masking is only performed when the sequence length exceeds this value.\n+        is_moe (`bool`, *optional*, defaults to `False`):\n+            Whether to use the Cross Domain Mixture of Experts, if `True`, the MoE will inherit the MLP to initialize.\n+        num_experts (`int`, *optional*, defaults to 16384):\n+            Number of routed experts in the model. This is only used when `is_moe=True`.\n+        num_experts_per_tok (`int`, *optional*, defaults to 64):\n+            Number of selected experts to route per-token.\n+        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n+            Whether to normalize the topk probabilities.\n+        output_router_logits (`bool`, *optional*, defaults to `False`):\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n+            allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n+            The aux loss factor for the total loss.\n+\n+    ```python\n+    >>> from transformers import DogeConfig, DogeModel\n+\n+    >>> # Initializing a Doge-320M style configuration\n+    >>> configuration = DogeConfig()\n+\n+    >>> # Initializing a model from the Doge-320M style configuration\n+    >>> model = DogeModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"doge\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `DogeModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.dt_proj\": \"rowwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n+        \"layers.*.input_residual.weight\": \"sequence_parallel\",\n+        \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n+        \"layers.*.post_attention_residual.weight\": \"sequence_parallel\",\n+        \"norm.weight\": \"sequence_parallel\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.router_gate\": \"colwise_rep\",\n+        \"layers.*.mlp.down_embed\": \"rowwise_rep\",\n+        \"layers.*.mlp.up_embed\": \"rowwise_rep\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=32768,\n+        hidden_size=1024,\n+        intermediate_size=2048,\n+        num_hidden_layers=32,\n+        hidden_dropout=0.0,\n+        hidden_act=\"silu\",\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-06,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n+        max_position_embeddings=2048,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        num_attention_heads=8,\n+        num_key_value_heads=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        mlp_bias=False,\n+        sliding_window=None,\n+        keep_window_size=2048,\n+        is_moe=False,\n+        num_experts=16384,\n+        num_experts_per_tok=64,\n+        norm_topk_prob=False,\n+        output_router_logits=False,\n+        router_aux_loss_coef=0.001,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+\n+        self.hidden_dropout = hidden_dropout\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+\n+        self.max_position_embeddings = max_position_embeddings\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.mlp_bias = mlp_bias\n+        self.sliding_window = sliding_window\n+        self.keep_window_size = keep_window_size\n+        self.is_moe = is_moe\n+        self.num_experts = num_experts\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.norm_topk_prob = norm_topk_prob\n+        self.output_router_logits = output_router_logits\n+        self.router_aux_loss_coef = router_aux_loss_coef\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            self.num_key_value_heads = num_attention_heads\n+\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+class DogeRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class DogeRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+def flex_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+    scaling: Optional[float] = None,\n+    softcap: Optional[float] = None,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+) -> tuple[torch.Tensor, torch.Tensor]:\n+    block_mask = None\n+    causal_mask = None\n+    if isinstance(attention_mask, BlockMask):\n+        block_mask = attention_mask\n+    else:\n+        causal_mask = attention_mask\n+\n+    if causal_mask is not None:\n+        causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n+\n+    def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n+        if softcap is not None:\n+            score = softcap * torch.tanh(score / softcap)\n+        if causal_mask is not None:\n+            score = score + causal_mask[batch_idx][head_idx][q_idx][kv_idx]\n+        if head_mask is not None:\n+            score = score + head_mask[batch_idx][head_idx][0][0]\n+        return score\n+\n+    attn_output, attention_weights = compile_friendly_flex_attention(\n+        query,\n+        key,\n+        value,\n+        score_mod=score_mod,\n+        block_mask=block_mask,\n+        enable_gqa=True,\n+        scale=scaling,\n+        # Last time checked on PyTorch == 2.5.1: Flex Attention always computes the lse regardless.\n+        # For simplification, we thus always return it as no additional computations are introduced.\n+        return_lse=True,\n+    )\n+    # lse is returned in float32\n+    attention_weights = attention_weights.to(value.dtype)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attention_weights\n+\n+\n+ALL_ATTENTION_FUNCTIONS = AttentionInterface()\n+ALL_ATTENTION_FUNCTIONS[\"doge_flex_attention\"] = flex_attention_forward\n+\n+\n+class DogeAttention(nn.Module):\n+    def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.keep_window_size = config.keep_window_size\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        # dynamic mask for the QK^T attention weights matrix\n+        self.A = nn.Parameter(torch.zeros(config.num_key_value_heads))\n+        self.dt_proj = nn.Linear(\n+            config.num_key_value_heads * self.head_dim, config.num_key_value_heads, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.q_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.k_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        key_states = self.k_norm(self.k_proj(hidden_states).view(hidden_shape)).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # calculate dynamic mask from value_states\n+        dt_states = self.dt_proj(\n+            value_states.transpose(1, 2).reshape(value_states.shape[0], value_states.shape[-2], -1)\n+        )\n+        dt_states = torch.exp(self.A * F.softplus(dt_states)).transpose(-1, -2)\n+        attn_mask = self.prepare_dynamic_mask(\n+            hidden_states=hidden_states,\n+            dt_states=dt_states,\n+            keep_window_size=self.keep_window_size,\n+            attention_mask=attention_mask,\n+        )\n+        attn_mask = repeat_kv(attn_mask, self.num_key_value_groups)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=attn_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+    def prepare_dynamic_mask(\n+        self,\n+        hidden_states: torch.Tensor,\n+        dt_states: torch.Tensor,\n+        keep_window_size: int = 2048,\n+        attention_mask: Optional[torch.Tensor] = None,\n+    ):\n+        \"\"\"\n+        The core idea of DMA is to calculate the dynamic attention mask to mask the tokens that should be masked, so as to form sparse attention.\n+\n+        Combine `dt_states` with `attention_mask` to generate the final `attn_mask`.\n+\n+        Args:\n+            hidden_states (`torch.Tensor`): The input hidden_states, used to determine the minimum value of the current input precision.\n+            dt_states (`torch.Tensor`): dt_states of shape `(batch_size, num_heads, key_sequence_length)`.\n+            keep_window_size (`int`): The window size of tokens that are not dynamically masked, and dynamic masking is only performed when the sequence length exceeds this value.\n+            attention_mask (`torch.Tensor`, *optional*): attention mask of shape `(batch_size, 1, query_sequence_length, key_sequence_length)`.\n+        \"\"\"\n+        min_dtype = torch.finfo(hidden_states.dtype).min\n+        dtype = hidden_states.dtype\n+        attn_mask = dt_states[:, :, None, :].expand(\n+            -1, -1, hidden_states.shape[1], -1\n+        )  # [batch_size, num_heads, query_len, key_len]\n+        if attention_mask is not None and not isinstance(attention_mask, BlockMask):\n+            if attention_mask.dtype == torch.bool:\n+                dtype = hidden_states.dtype\n+                attention_mask = torch.where(\n+                    attention_mask, torch.tensor(0.0, device=attention_mask.device, dtype=dtype), min_dtype\n+                )\n+            attn_mask = attn_mask.masked_fill(attention_mask[:, :, :, : attn_mask.shape[-1]] != 0, min_dtype)\n+        if attn_mask.shape[-1] > keep_window_size:\n+            active_mask = torch.zeros_like(attn_mask, dtype=dtype, device=attn_mask.device)\n+            topk_indices = torch.topk(attn_mask, keep_window_size, dim=-1, largest=True, sorted=False).indices\n+            active_mask = active_mask.scatter(-1, topk_indices, 1.0)\n+            attn_mask = attn_mask.masked_fill(active_mask == 0.0, min_dtype)\n+        return attn_mask\n+\n+\n+class DogeMLP(LlamaMLP):\n+    pass\n+\n+\n+class DogeCDMoE(nn.Module):\n+    def __init__(self, config: DogeConfig):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+        self.num_experts = config.num_experts\n+        self.num_keys = math.floor(math.sqrt(self.num_experts))\n+        self.top_k = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n+\n+        # shared expert\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+\n+        # router gate for retrieval experts\n+        self.router_gate = nn.Linear(self.hidden_size, self.num_keys * 2, bias=False)\n+\n+        # routed experts\n+        self.down_embed = nn.Embedding(self.num_experts, self.hidden_size)\n+        self.up_embed = nn.Embedding(self.num_experts, self.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        bsz, seq_len, _ = hidden_states.shape\n+\n+        # get routing logits with router gate\n+        router_logits = self.router_gate(hidden_states).view(2, bsz * seq_len, -1)\n+\n+        # get experts with the highest routing logits\n+        (scores_x, scores_y), (indices_x, indices_y) = router_logits.topk(self.num_keys, dim=-1)\n+        all_scores = scores_x.unsqueeze(-1) + scores_y.unsqueeze(-2)\n+        all_indices = indices_x.unsqueeze(-1) * self.num_keys + indices_y.unsqueeze(-2)\n+        all_scores = all_scores.view(*all_scores.shape[:-2], -1)\n+        all_indices = all_indices.view(*all_indices.shape[:-2], -1)\n+        scores, position_indices = all_scores.topk(self.top_k, dim=-1)\n+        indices = all_indices.gather(-1, position_indices)\n+        routing_weights = F.softmax(scores, dim=-1)\n+        if self.norm_topk_prob:\n+            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+\n+        # mix routed experts states with shared expert states\n+        down_embed = self.down_embed(indices)\n+        up_embed = self.up_embed(indices)\n+        experts_weights = torch.matmul(down_embed, hidden_states.view(bsz * seq_len, -1, 1)).view(bsz * seq_len, -1)\n+        experts_weights = self.act_fn(experts_weights) * routing_weights\n+        experts_states = torch.matmul(experts_weights.view(bsz * seq_len, 1, -1), up_embed).view(bsz, seq_len, -1)\n+        hidden_states = self.down_proj(self.act_fn(self.gate_proj(hidden_states)) * self.up_proj(hidden_states))\n+        hidden_states = hidden_states + experts_states\n+        return hidden_states, router_logits\n+\n+\n+class DogeDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.hidden_dropout = config.hidden_dropout\n+\n+        self.input_layernorm = DogeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.self_attn = DogeAttention(config=config, layer_idx=layer_idx)\n+        self.input_residual = nn.Parameter(torch.ones(config.hidden_size))\n+\n+        self.post_attention_layernorm = DogeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.mlp = DogeMLP(config) if not config.is_moe else DogeCDMoE(config)\n+        self.post_attention_residual = nn.Parameter(torch.ones(config.hidden_size))\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        # sequence transformation\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = F.dropout(hidden_states, p=self.hidden_dropout, training=self.training)\n+        hidden_states = self.input_residual * residual + hidden_states\n+\n+        # state transformation\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = F.dropout(hidden_states, p=self.hidden_dropout, training=self.training)\n+        hidden_states = self.post_attention_residual * residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+class DogePreTrainedModel(LlamaPreTrainedModel):\n+    _supports_flash_attn_3 = False\n+    _supports_flash_attn_2 = False\n+    _supports_static_cache = False\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(DogeCDMoE, index=1),\n+        \"hidden_states\": DogeDecoderLayer,\n+        \"attentions\": DogeAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        super()._init_weights(module)\n+\n+        if isinstance(module, DogeAttention):\n+            if hasattr(module, \"A\"):\n+                module.A.data.zero_()\n+        elif isinstance(module, DogeDecoderLayer):\n+            if hasattr(module, \"input_residual\"):\n+                module.input_residual.data.fill_(1.0)\n+            if hasattr(module, \"post_attention_residual\"):\n+                module.post_attention_residual.data.fill_(1.0)\n+\n+\n+class DogeModel(MixtralModel):\n+    pass\n+\n+\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    num_keys: Optional[int] = None,\n+    top_k: int = 2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://arxiv.org/abs/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `router_gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [2, batch_size * sequence_length, num_keys].\n+        num_experts:\n+            Number of experts\n+        num_keys:\n+            Number of keys\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    compute_dtype = gate_logits[0].dtype\n+    compute_device = gate_logits[0].device\n+    all_expert_indices = []\n+    all_routing_weights = []\n+\n+    for layer_gate_logits in gate_logits:\n+        layer_gate_logits = layer_gate_logits.to(compute_device)\n+\n+        (scores_x, scores_y), (indices_x, indices_y) = layer_gate_logits.topk(num_keys, dim=-1)\n+\n+        all_scores = scores_x.unsqueeze(-1) + scores_y.unsqueeze(-2)\n+        all_indices = indices_x.unsqueeze(-1) * num_keys + indices_y.unsqueeze(-2)\n+        all_scores = all_scores.view(*all_scores.shape[:-2], -1)\n+        all_indices = all_indices.view(*all_indices.shape[:-2], -1)\n+\n+        _, position_indices = all_scores.topk(top_k, dim=-1)\n+        expert_indices = all_indices.gather(-1, position_indices)\n+\n+        routing_weights = F.softmax(all_scores, dim=-1)\n+\n+        all_expert_indices.append(expert_indices)\n+        all_routing_weights.append(routing_weights)\n+    all_expert_indices = torch.cat(all_expert_indices, dim=0)\n+    all_routing_weights = torch.cat(all_routing_weights, dim=0)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        all_expert_indices = all_expert_indices.view(-1)\n+        tokens_per_expert = torch.zeros(num_experts, dtype=compute_dtype, device=compute_device)\n+        pad = torch.ones_like(all_expert_indices, dtype=compute_dtype, device=compute_device)\n+        tokens_per_expert = tokens_per_expert.scatter_add_(0, all_expert_indices, pad) / all_expert_indices.shape[0]\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(all_routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = len(gate_logits)\n+\n+        #  Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k))\n+            .reshape(-1)\n+            .to(compute_device)\n+        )\n+        all_expert_indices = all_expert_indices.view(-1)[expert_attention_mask.bool()]\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.zeros(num_experts, dtype=compute_dtype, device=compute_device)\n+        pad = torch.ones_like(all_expert_indices, dtype=compute_dtype, device=compute_device)\n+        tokens_per_expert = tokens_per_expert.scatter_add_(0, all_expert_indices, pad) / torch.sum(\n+            expert_attention_mask\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(all_routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert)\n+    return overall_loss * num_experts\n+\n+\n+class DogeForCausalLM(MixtralForCausalLM):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = DogeModel(config)\n+        self.num_experts = config.num_experts\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        output_router_logits: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, DogeForCausalLM\n+\n+        >>> model = DogeForCausalLM.from_pretrained(\"SmallDoge/Doge-320M\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-320M\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        output_router_logits = (\n+            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs: MoeModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        aux_loss = None\n+        if output_router_logits:\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.num_experts,\n+                math.floor(math.sqrt(self.num_experts)),\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n+\n+        return MoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            router_logits=outputs.router_logits,\n+        )\n+\n+\n+class DogeForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+__all__ = [\n+    \"DogeConfig\",\n+    \"DogeForCausalLM\",\n+    \"DogeModel\",\n+    \"DogePreTrainedModel\",\n+    \"DogeForSequenceClassification\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/doge/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/tests%2Fmodels%2Fdoge%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/tests%2Fmodels%2Fdoge%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdoge%2F__init__.py?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1"
        },
        {
            "sha": "4add82b077f3f095572785fe6c74f364a6d9b05e",
            "filename": "tests/models/doge/test_modeling_doge.py",
            "status": "added",
            "additions": 373,
            "deletions": 0,
            "changes": 373,
            "blob_url": "https://github.com/huggingface/transformers/blob/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py?ref=d8590b4b0c84094277c1a7ffc2e8a9f3baa7f3a1",
            "patch": "@@ -0,0 +1,373 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Doge model.\"\"\"\n+\n+import unittest\n+\n+from transformers import AutoTokenizer, DogeConfig, is_torch_available, set_seed\n+from transformers.testing_utils import (\n+    require_read_token,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        DogeForCausalLM,\n+        DogeForSequenceClassification,\n+        DogeModel,\n+    )\n+\n+\n+class DogeModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=8,\n+        seq_length=16,\n+        is_training=True,\n+        use_input_mask=True,\n+        use_token_type_ids=False,\n+        use_labels=True,\n+        vocab_size=128,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=64,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=512,\n+        type_vocab_size=16,\n+        type_sequence_label_size=2,\n+        initializer_range=0.02,\n+        num_labels=3,\n+        pad_token_id=0,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_token_type_ids = use_token_type_ids\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.type_vocab_size = type_vocab_size\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_labels = num_labels\n+        self.pad_token_id = pad_token_id\n+        self.scope = scope\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        input_mask = None\n+        if self.use_input_mask:\n+            input_mask = torch.tril(torch.ones_like(input_ids).to(torch_device))\n+\n+        token_type_ids = None\n+        if self.use_token_type_ids:\n+            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n+\n+        sequence_labels = None\n+        token_labels = None\n+        if self.use_labels:\n+            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+            token_labels = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels\n+\n+    def get_config(self):\n+        return DogeConfig(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            hidden_act=self.hidden_act,\n+            max_position_embeddings=self.max_position_embeddings,\n+            type_vocab_size=self.type_vocab_size,\n+            is_decoder=False,\n+            initializer_range=self.initializer_range,\n+            pad_token_id=self.pad_token_id,\n+        )\n+\n+    def create_and_check_model(self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels):\n+        model = DogeModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask)\n+        result = model(input_ids)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_model_as_decoder(\n+        self,\n+        config,\n+        input_ids,\n+        token_type_ids,\n+        input_mask,\n+        sequence_labels,\n+        token_labels,\n+        encoder_hidden_states,\n+        encoder_attention_mask,\n+    ):\n+        config.add_cross_attention = True\n+        model = DogeModel(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(\n+            input_ids,\n+            attention_mask=input_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+        )\n+        result = model(\n+            input_ids,\n+            attention_mask=input_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+        )\n+        result = model(input_ids, attention_mask=input_mask)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_for_causal_lm(\n+        self,\n+        config,\n+        input_ids,\n+        token_type_ids,\n+        input_mask,\n+        sequence_labels,\n+        token_labels,\n+        encoder_hidden_states,\n+        encoder_attention_mask,\n+    ):\n+        model = DogeForCausalLM(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n+\n+    def create_and_check_decoder_model_past_large_inputs(\n+        self,\n+        config,\n+        input_ids,\n+        token_type_ids,\n+        input_mask,\n+        sequence_labels,\n+        token_labels,\n+        encoder_hidden_states,\n+        encoder_attention_mask,\n+    ):\n+        config.is_decoder = True\n+        config.add_cross_attention = True\n+        model = DogeForCausalLM(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        # first forward pass\n+        outputs = model(\n+            input_ids,\n+            attention_mask=input_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            use_cache=True,\n+        )\n+        past_key_values = outputs.past_key_values\n+\n+        # create hypothetical multiple next token and extent to next_input_ids\n+        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n+        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n+\n+        # append to next input_ids and\n+        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n+        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n+\n+        output_from_no_past = model(\n+            next_input_ids,\n+            attention_mask=next_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            output_hidden_states=True,\n+        )[\"hidden_states\"][0]\n+        output_from_past = model(\n+            next_tokens,\n+            attention_mask=next_attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_values=past_key_values,\n+            output_hidden_states=True,\n+        )[\"hidden_states\"][0]\n+\n+        # select random slice\n+        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n+        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n+        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n+\n+        self.parent.assertTrue(output_from_past_slice.shape[1] == next_tokens.shape[1])\n+\n+        # test that outputs are equal for slice\n+        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            token_type_ids,\n+            input_mask,\n+            sequence_labels,\n+            token_labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class DogeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            DogeModel,\n+            DogeForCausalLM,\n+            DogeForSequenceClassification,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    all_generative_model_classes = (DogeForCausalLM,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": DogeModel,\n+            \"text-classification\": DogeForSequenceClassification,\n+            \"text-generation\": DogeForCausalLM,\n+            \"zero-shot\": DogeForSequenceClassification,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    has_attentions = False\n+    test_headmasking = False\n+    test_pruning = False\n+    test_torchscript = False\n+    fx_compatible = False\n+\n+    # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n+    # This is because we are hitting edge cases with the causal_mask buffer\n+    model_split_percents = [0.5, 0.7, 0.8]\n+\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = DogeForCausalLM if is_torch_available() else None\n+\n+    def setUp(self):\n+        self.model_tester = DogeModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=DogeConfig, hidden_size=32)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_doge_sequence_classification_model(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n+        model = DogeForSequenceClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n+        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    def test_doge_sequence_classification_model_for_single_label(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        config.problem_type = \"single_label_classification\"\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor([self.model_tester.batch_size], self.model_tester.type_sequence_label_size)\n+        model = DogeForSequenceClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n+        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    def test_doge_sequence_classification_model_for_multi_label(self):\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        config.problem_type = \"multi_label_classification\"\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        sequence_labels = ids_tensor(\n+            [self.model_tester.batch_size, config.num_labels], self.model_tester.type_sequence_label_size\n+        ).to(torch.float)\n+        model = DogeForSequenceClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=attention_mask, labels=sequence_labels)\n+        self.assertEqual(result.logits.shape, (self.model_tester.batch_size, self.model_tester.num_labels))\n+\n+    @unittest.skip(reason=\"Doge buffers include complex numbers, which breaks this test\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n+\n+@require_torch_accelerator\n+class DogeIntegrationTest(unittest.TestCase):\n+    # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n+    # Depending on the hardware we get different logits / generations\n+    cuda_compute_capability_major_version = None\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        if is_torch_available() and torch.cuda.is_available():\n+            # 8 is for A100 / A10 and 7 for T4\n+            cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n+\n+    @slow\n+    @require_read_token\n+    def test_Doge_20M_hard(self):\n+        \"\"\"\n+        An integration test for Doge-20M. It tests against a long output to ensure the subtle numerical differences\n+        \"\"\"\n+        EXPECTED_TEXT = \"Here's everything I know about dogs. Dogs is the best animal in the world. It is a very popular and popular dog in the United States. It is a very popular\"\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"SmallDoge/Doge-20M\")\n+        model = DogeForCausalLM.from_pretrained(\"SmallDoge/Doge-20M\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n+        input_text = [\"Here's everything I know about dogs. Dogs is the best animal in the\"]\n+        set_seed(0)\n+        model_inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+\n+        generated_ids = model.generate(**model_inputs, max_new_tokens=20, do_sample=False)\n+        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(generated_text, EXPECTED_TEXT)"
        }
    ],
    "stats": {
        "total": 2624,
        "additions": 2624,
        "deletions": 0
    }
}