{
    "author": "zucchini-nlp",
    "message": "VLM: add more modularity (#34175)\n\n* update\r\n\r\n* fix tests + fix copies\r\n\r\n* fix tests once more",
    "sha": "5077bc034fd806a8b0155e71b963596a765a7758",
    "files": [
        {
            "sha": "50b3d4c6a89533aa32e6b8853b8969229a342b73",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=5077bc034fd806a8b0155e71b963596a765a7758",
            "patch": "@@ -273,6 +273,20 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_m\n     def get_image_features(\n         self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n     ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n         image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n         # this is not memory efficient at all (output_hidden_states=True) will save all the hidden stated.\n         selected_image_feature = image_outputs.hidden_states[vision_feature_layer]"
        },
        {
            "sha": "0cbda9cfd64b7493a1af1c0ad421287a0bda1e1f",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 57,
            "deletions": 28,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=5077bc034fd806a8b0155e71b963596a765a7758",
            "patch": "@@ -705,6 +705,57 @@ def pack_image_features(self, image_features, image_sizes, vision_feature_select\n         feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features.device)\n         return image_features, feature_lens\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: int,\n+        vision_feature_select_strategy: str,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n+                Actual image size of each images (H, W).\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_patches, image_length, embed_dim)`).\n+        \"\"\"\n+        # ! infer image_num_patches from image_sizes\n+        image_num_patches = [\n+            image_size_to_num_patches(\n+                image_size=imsize,\n+                grid_pinpoints=self.config.image_grid_pinpoints,\n+                patch_size=self.config.vision_config.image_size,\n+            )\n+            for imsize in image_sizes\n+        ]\n+        if pixel_values.dim() == 5:\n+            # stacked if input is (batch_size, num_patches, num_channels, height, width)\n+            _pixel_values_list = [pix_val[:num_patch] for pix_val, num_patch in zip(pixel_values, image_num_patches)]\n+            pixel_values = torch.cat(_pixel_values_list, dim=0)\n+        elif pixel_values.dim() != 4:\n+            # otherwise has to be stacked from list of (num_patches, num_channels, height, width)\n+            raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n+\n+        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+        selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        if vision_feature_select_strategy == \"default\":\n+            selected_image_feature = selected_image_feature[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_image_feature = selected_image_feature\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        image_features = torch.split(image_features, image_num_patches, dim=0)\n+        return image_features\n+\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaNextCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -796,34 +847,12 @@ def forward(\n             ) or (input_ids.shape[-1] == 1 and pixel_values is not None)\n \n         if pixel_values is not None and pixel_values.size(0) > 0:\n-            # ! infer image_num_patches from image_sizes\n-            image_num_patches = [\n-                image_size_to_num_patches(\n-                    image_size=imsize,\n-                    grid_pinpoints=self.config.image_grid_pinpoints,\n-                    patch_size=self.config.vision_config.image_size,\n-                )\n-                for imsize in image_sizes\n-            ]\n-            # figure out if pixel_values is concatenated or stacked\n-            if pixel_values.dim() == 5:\n-                # stacking when input is (batch_size, num_patches, num_channels, height, width)\n-                _pixel_values_list = [\n-                    pix_val[:num_patch] for pix_val, num_patch in zip(pixel_values, image_num_patches)\n-                ]\n-                pixel_values = torch.cat(_pixel_values_list, dim=0)\n-            elif pixel_values.dim() != 4:\n-                # otherwise has to be stacked from list of (num_patches, num_channels, height, width)\n-                raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n-\n-            image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-            selected_image_feature = image_features.hidden_states[vision_feature_layer]\n-            if vision_feature_select_strategy == \"default\":\n-                selected_image_feature = selected_image_feature[:, 1:]\n-            elif vision_feature_select_strategy == \"full\":\n-                selected_image_feature = selected_image_feature\n-            image_features = self.multi_modal_projector(selected_image_feature)\n-            image_features = torch.split(image_features, image_num_patches, dim=0)\n+            image_features = self.get_image_features(\n+                pixel_values,\n+                image_sizes,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n \n             # NOTE we only support multimodal_patch_merge_type == \"spatial_unpad\"\n             image_features, feature_lens = self.pack_image_features("
        },
        {
            "sha": "96f4373afd9ec64bc7f73b8aaf335d3e2b935df1",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 89,
            "deletions": 40,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=5077bc034fd806a8b0155e71b963596a765a7758",
            "patch": "@@ -744,6 +744,57 @@ def pack_image_features(self, image_features, image_sizes, vision_feature_select\n         feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features.device)\n         return image_features, feature_lens\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: int,\n+        vision_feature_select_strategy: str,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n+                Actual image size of each images (H, W).\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_patches, image_length, embed_dim)`).\n+        \"\"\"\n+        # ! infer image_num_patches from image_sizes\n+        image_num_patches = [\n+            image_size_to_num_patches(\n+                image_size=imsize,\n+                grid_pinpoints=self.config.image_grid_pinpoints,\n+                patch_size=self.config.vision_config.image_size,\n+            )\n+            for imsize in image_sizes\n+        ]\n+        if pixel_values.dim() == 5:\n+            # stacked if input is (batch_size, num_patches, num_channels, height, width)\n+            _pixel_values_list = [pix_val[:num_patch] for pix_val, num_patch in zip(pixel_values, image_num_patches)]\n+            pixel_values = torch.cat(_pixel_values_list, dim=0)\n+        elif pixel_values.dim() != 4:\n+            # otherwise has to be stacked from list of (num_patches, num_channels, height, width)\n+            raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n+\n+        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+        selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        if vision_feature_select_strategy == \"default\":\n+            selected_image_feature = selected_image_feature[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_image_feature = selected_image_feature\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        image_features = torch.split(image_features, image_num_patches, dim=0)\n+        return image_features\n+\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaNextVideoCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -883,7 +934,12 @@ def forward(\n \n         image_features = feature_lens = None\n         if pixel_values is not None and pixel_values.size(0) > 0:\n-            image_features = self._get_image_features(pixel_values, image_sizes)\n+            image_features = self.get_image_features(\n+                pixel_values,\n+                image_sizes,\n+                vision_feature_layer=self.vision_feature_layer,\n+                vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            )\n             image_features, feature_lens = self.pack_image_features(\n                 image_features,\n                 image_sizes,\n@@ -893,7 +949,11 @@ def forward(\n \n         video_features = video_feature_lens = None\n         if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n-            video_features = self._get_video_features(pixel_values_videos)\n+            video_features = self.get_video_features(\n+                pixel_values_videos,\n+                vision_feature_layer=self.vision_feature_layer,\n+                vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            )\n             video_features = [feature.flatten(0, 1) for feature in video_features]\n             video_feature_lens = [feature.size(0) for feature in video_features]\n             video_features = torch.cat(video_features, dim=0)\n@@ -1080,46 +1140,35 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    def _get_image_features(self, pixel_values, image_sizes):\n-        # ! infer image_num_patches from image_sizes\n-        image_num_patches = [\n-            image_size_to_num_patches(\n-                image_size=imsize,\n-                grid_pinpoints=self.config.image_grid_pinpoints,\n-                patch_size=self.config.vision_config.image_size,\n-            )\n-            for imsize in image_sizes\n-        ]\n-        if pixel_values.dim() == 5:\n-            # stacked if input is (batch_size, num_patches, num_channels, height, width)\n-            _pixel_values_list = [pix_val[:num_patch] for pix_val, num_patch in zip(pixel_values, image_num_patches)]\n-            pixel_values = torch.cat(_pixel_values_list, dim=0)\n-        elif pixel_values.dim() != 4:\n-            # otherwise has to be stacked from list of (num_patches, num_channels, height, width)\n-            raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n-\n-        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_image_feature = image_features.hidden_states[self.vision_feature_layer]\n-        if self.vision_feature_select_strategy == \"default\":\n-            selected_image_feature = selected_image_feature[:, 1:]\n-        elif self.vision_feature_select_strategy == \"full\":\n-            selected_image_feature = selected_image_feature\n-        image_features = self.multi_modal_projector(selected_image_feature)\n-        image_features = torch.split(image_features, image_num_patches, dim=0)\n-        return image_features\n+    def get_video_features(\n+        self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+    ):\n+        \"\"\"\n+        Obtains video last hidden states from the vision tower and apply multimodal projection.\n \n-    def _get_video_features(self, pixel_values):\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n+               The tensors corresponding to the input video.\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            video_features (List[`torch.Tensor`]): List of video feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_videos, video_length, embed_dim)`).\n+        \"\"\"\n         batch_size, frames, channels, height, width = pixel_values.shape\n         pixel_values = pixel_values.reshape(batch_size * frames, channels, height, width)\n-        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_image_feature = image_features.hidden_states[self.vision_feature_layer]\n-        if self.vision_feature_select_strategy == \"default\":\n-            selected_image_feature = selected_image_feature[:, 1:]\n-        elif self.vision_feature_select_strategy == \"full\":\n-            selected_image_feature = selected_image_feature\n+        video_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+        selected_video_features = video_features.hidden_states[vision_feature_layer]\n+        if vision_feature_select_strategy == \"default\":\n+            selected_video_features = selected_video_features[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_video_features = selected_video_features\n \n         # Same as image features except that video has pooling layer\n-        image_features = self.vision_resampler(selected_image_feature)\n-        image_features = self.multi_modal_projector(image_features)\n-        image_features = torch.split(image_features, frames, dim=0)\n-        return image_features\n+        video_features = self.vision_resampler(selected_video_features)\n+        video_features = self.multi_modal_projector(video_features)\n+        video_features = torch.split(video_features, frames, dim=0)\n+        return video_features"
        },
        {
            "sha": "c1ed7571941b9e4511f140a0f6103d34a1a4e7c9",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 66,
            "deletions": 17,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=5077bc034fd806a8b0155e71b963596a765a7758",
            "patch": "@@ -225,7 +225,30 @@ def __init__(self, config: LlavaNextVideoConfig, **super_kwargs):\n         self.vision_resampler = LlavaNextVideoPooler(config)\n         self.post_init()\n \n-    def _get_image_features(self, pixel_values, image_sizes):\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: int,\n+        vision_feature_select_strategy: str,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n+                Actual image size of each images (H, W).\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_patches, image_length, embed_dim)`).\n+        \"\"\"\n         # ! infer image_num_patches from image_sizes\n         image_num_patches = [\n             image_size_to_num_patches(\n@@ -244,30 +267,47 @@ def _get_image_features(self, pixel_values, image_sizes):\n             raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n \n         image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_image_feature = image_features.hidden_states[self.vision_feature_layer]\n-        if self.vision_feature_select_strategy == \"default\":\n+        selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        if vision_feature_select_strategy == \"default\":\n             selected_image_feature = selected_image_feature[:, 1:]\n-        elif self.vision_feature_select_strategy == \"full\":\n+        elif vision_feature_select_strategy == \"full\":\n             selected_image_feature = selected_image_feature\n         image_features = self.multi_modal_projector(selected_image_feature)\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n-    def _get_video_features(self, pixel_values):\n+    def get_video_features(\n+        self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+    ):\n+        \"\"\"\n+        Obtains video last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n+               The tensors corresponding to the input video.\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            video_features (List[`torch.Tensor`]): List of video feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_videos, video_length, embed_dim)`).\n+        \"\"\"\n         batch_size, frames, channels, height, width = pixel_values.shape\n         pixel_values = pixel_values.reshape(batch_size * frames, channels, height, width)\n-        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-        selected_image_feature = image_features.hidden_states[self.vision_feature_layer]\n-        if self.vision_feature_select_strategy == \"default\":\n-            selected_image_feature = selected_image_feature[:, 1:]\n-        elif self.vision_feature_select_strategy == \"full\":\n-            selected_image_feature = selected_image_feature\n+        video_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+        selected_video_features = video_features.hidden_states[vision_feature_layer]\n+        if vision_feature_select_strategy == \"default\":\n+            selected_video_features = selected_video_features[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_video_features = selected_video_features\n \n         # Same as image features except that video has pooling layer\n-        image_features = self.vision_resampler(selected_image_feature)\n-        image_features = self.multi_modal_projector(image_features)\n-        image_features = torch.split(image_features, frames, dim=0)\n-        return image_features\n+        video_features = self.vision_resampler(selected_video_features)\n+        video_features = self.multi_modal_projector(video_features)\n+        video_features = torch.split(video_features, frames, dim=0)\n+        return video_features\n \n     @replace_return_docstrings(output_type=LlavaNextVideoCausalLMOutputWithPast, config_class=\"LlavaNextVideoConfig\")\n     def forward(\n@@ -407,7 +447,12 @@ def forward(\n \n         image_features = feature_lens = None\n         if pixel_values is not None and pixel_values.size(0) > 0:\n-            image_features = self._get_image_features(pixel_values, image_sizes)\n+            image_features = self.get_image_features(\n+                pixel_values,\n+                image_sizes,\n+                vision_feature_layer=self.vision_feature_layer,\n+                vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            )\n             image_features, feature_lens = self.pack_image_features(\n                 image_features,\n                 image_sizes,\n@@ -417,7 +462,11 @@ def forward(\n \n         video_features = video_feature_lens = None\n         if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n-            video_features = self._get_video_features(pixel_values_videos)\n+            video_features = self.get_video_features(\n+                pixel_values_videos,\n+                vision_feature_layer=self.vision_feature_layer,\n+                vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            )\n             video_features = [feature.flatten(0, 1) for feature in video_features]\n             video_feature_lens = [feature.size(0) for feature in video_features]\n             video_features = torch.cat(video_features, dim=0)"
        },
        {
            "sha": "946688bfcf07f4a9d9fdb3d71f2b62ea0a1f994b",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 99,
            "deletions": 43,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=5077bc034fd806a8b0155e71b963596a765a7758",
            "patch": "@@ -481,6 +481,91 @@ def apply_pooling(self, image_features):\n         image_features = image_features.view(batch_frames, -1, dim)\n         return image_features\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: int,\n+        vision_feature_select_strategy: str,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n+                Actual image size of each images (H, W).\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_patches, image_length, embed_dim)`).\n+        \"\"\"\n+        # ! infer image_num_patches from image_sizes\n+        image_num_patches = [\n+            image_size_to_num_patches(\n+                image_size=imsize,\n+                grid_pinpoints=self.config.image_grid_pinpoints,\n+                patch_size=self.config.vision_config.image_size,\n+            )\n+            for imsize in image_sizes\n+        ]\n+        if pixel_values.dim() == 5:\n+            # stacked if input is (batch_size, num_patches, num_channels, height, width)\n+            _pixel_values_list = [pix_val[:num_patch] for pix_val, num_patch in zip(pixel_values, image_num_patches)]\n+            pixel_values = torch.cat(_pixel_values_list, dim=0)\n+        elif pixel_values.dim() != 4:\n+            # otherwise has to be stacked from list of (num_patches, num_channels, height, width)\n+            raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n+\n+        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+        selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        if vision_feature_select_strategy == \"default\":\n+            selected_image_feature = selected_image_feature[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_image_feature = selected_image_feature\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        image_features = torch.split(image_features, image_num_patches, dim=0)\n+        return image_features\n+\n+    def get_video_features(\n+        self, pixel_values: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+    ):\n+        \"\"\"\n+        Obtains video last hidden states from the vision tower, apply multimodal projection and pooling.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n+               The tensors corresponding to the input video.\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            video_features (List[`torch.Tensor`]): List of video feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_videos, video_length, embed_dim)`).\n+        \"\"\"\n+        batch_size, frames, channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.view(batch_size * frames, channels, height, width)\n+        video_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+        selected_video_feature = video_features.hidden_states[vision_feature_layer]\n+\n+        if vision_feature_select_strategy == \"default\":\n+            selected_video_feature = selected_video_feature[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_video_feature = selected_video_feature\n+        video_features = self.multi_modal_projector(selected_video_feature)\n+\n+        video_features = self.apply_pooling(video_features)\n+        video_features = video_features.reshape(batch_size, frames * video_features.shape[1], -1)\n+\n+        return video_features\n+\n     @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -580,35 +665,12 @@ def forward(\n \n         # Images are processed with Anyres\n         if pixel_values is not None:\n-            image_num_patches = [\n-                image_size_to_num_patches(\n-                    image_size=imsize,\n-                    grid_pinpoints=self.config.image_grid_pinpoints,\n-                    patch_size=self.config.vision_config.image_size,\n-                )\n-                for imsize in image_sizes\n-            ]\n-\n-            # unpad extra patches and concatenate them\n-            if pixel_values.dim() == 5:\n-                _pixel_values_list = [\n-                    pix_val[:num_patch] for pix_val, num_patch in zip(pixel_values, image_num_patches)\n-                ]\n-                # [batch_size*frames*num_patches, num_channels, height, width] where frames=1 for images\n-                pixel_values = torch.cat(_pixel_values_list, dim=0)\n-            elif pixel_values.dim() != 4:\n-                raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n-\n-            image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-            selected_image_feature = image_features.hidden_states[vision_feature_layer]\n-\n-            if vision_feature_select_strategy == \"default\":\n-                selected_image_feature = selected_image_feature[:, 1:]\n-            elif vision_feature_select_strategy == \"full\":\n-                selected_image_feature = selected_image_feature\n-            image_features = self.multi_modal_projector(selected_image_feature)\n-\n-            image_features = torch.split(image_features, image_num_patches, dim=0)\n+            image_features = self.get_image_features(\n+                pixel_values,\n+                image_sizes,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n             image_features, feature_lens = self.pack_image_features(\n                 image_features,\n                 image_sizes,\n@@ -632,20 +694,14 @@ def forward(\n \n         # Video are simply embedded and further pooled to decrease seq len\n         if pixel_values_videos is not None:\n-            batch_size, frames, channels, height, width = pixel_values_videos.shape\n-            pixel_values_videos = pixel_values_videos.view(batch_size * frames, channels, height, width)\n-            video_features = self.vision_tower(pixel_values_videos, output_hidden_states=True)\n-            selected_video_feature = video_features.hidden_states[vision_feature_layer]\n-\n-            if vision_feature_select_strategy == \"default\":\n-                selected_video_feature = selected_video_feature[:, 1:]\n-            elif vision_feature_select_strategy == \"full\":\n-                selected_video_feature = selected_video_feature\n-            video_features = self.multi_modal_projector(selected_video_feature)\n-\n-            video_features = self.apply_pooling(video_features)\n-            video_features = video_features.reshape(batch_size, frames * video_features.shape[1], -1)\n-            image_newline = self.image_newline[None, None, :].repeat(batch_size, 1, 1).to(video_features.device)\n+            video_features = self.get_video_features(\n+                pixel_values_videos,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n+            image_newline = (\n+                self.image_newline[None, None, :].repeat(video_features.shape[0], 1, 1).to(video_features.device)\n+            )\n             video_features = torch.cat((video_features, image_newline), dim=1)\n             video_features = video_features.flatten(0, 1)\n             n_video_tokens = (input_ids == self.config.video_token_index).sum().item()"
        },
        {
            "sha": "e198dab420abe8f7386a9946455463737be8208b",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 17,
            "deletions": 4,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=5077bc034fd806a8b0155e71b963596a765a7758",
            "patch": "@@ -392,6 +392,22 @@ def _update_causal_mask(\n                 )\n         return causal_mask\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_outputs = self.vision_tower(pixel_values)\n+        selected_image_feature = image_outputs.last_hidden_state\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        image_features = image_features / (self.config.hidden_size**0.5)\n+        return image_features\n+\n     @add_start_docstrings_to_model_forward(PALIGEMMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=PaliGemmaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -477,10 +493,7 @@ def forward(\n \n         # Merge text and images\n         if pixel_values is not None:\n-            image_outputs = self.vision_tower(pixel_values.to(inputs_embeds.dtype))\n-            selected_image_feature = image_outputs.last_hidden_state\n-            image_features = self.multi_modal_projector(selected_image_feature)\n-            image_features = image_features / (self.config.hidden_size**0.5)\n+            image_features = self.get_image_features(pixel_values)\n \n             special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)"
        },
        {
            "sha": "c4ec1b5196929a8e3cdd968f6b9c6f972ea046d4",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 138,
            "deletions": 124,
            "changes": 262,
            "blob_url": "https://github.com/huggingface/transformers/blob/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=5077bc034fd806a8b0155e71b963596a765a7758",
            "patch": "@@ -23,7 +23,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_outputs import BaseModelOutputWithPooling, ModelOutput\n+from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -355,41 +355,59 @@ def _merge_input_ids_with_visual_features(\n \n         return final_embedding, final_attention_mask, final_labels, position_ids, final_input_ids\n \n-    def _get_vision_features(\n-        self,\n-        pixel_values_images: Optional[torch.FloatTensor] = None,\n-        pixel_values_videos: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[int] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n-    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n-        if pixel_values_images is None and pixel_values_videos is None:\n-            raise ValueError(\"You have to specify `pixel_values_images` or `pixel_values_videos`\")\n+    def get_image_features(\n+        self, pixel_values_images: torch.FloatTensor, vision_feature_layer: int, vision_feature_select_strategy: str\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n-        # videos do not need to select features and it's always \"full\" (as it is done in the orig implementation)\n-        if pixel_values_videos is not None:\n-            batch_size_vid, num_frames, channels, height, width = pixel_values_videos.shape\n+        Args:\n+            pixel_values_images (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n \n-            pixel_values = pixel_values_videos.reshape(batch_size_vid * num_frames, channels, height, width)\n-            video_outputs = self.video_tower(pixel_values, output_hidden_states=True)\n-            video_outputs = video_outputs.hidden_states[vision_feature_layer].squeeze(1)\n+        image_outputs = self.image_tower(pixel_values_images, output_hidden_states=True)\n+        image_outputs = image_outputs.hidden_states[vision_feature_layer].squeeze(1)\n+\n+        if vision_feature_select_strategy == \"default\":\n+            image_outputs = image_outputs[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            image_outputs = image_outputs\n         else:\n-            video_outputs = None\n-            num_frames = 0\n+            raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n \n-        if pixel_values_images is not None:\n-            image_outputs = self.image_tower(pixel_values_images, output_hidden_states=True)\n-            image_outputs = image_outputs.hidden_states[vision_feature_layer].squeeze(1)\n+        image_features = self.multi_modal_projector(image_outputs)\n \n-            if vision_feature_select_strategy == \"default\":\n-                image_outputs = image_outputs[:, 1:]\n-            elif vision_feature_select_strategy == \"full\":\n-                image_outputs = image_outputs\n-            else:\n-                raise ValueError(f\"Unexpected select feature strategy: {self.config.vision_feature_select_strategy}\")\n-        else:\n-            image_outputs = None\n+        return image_features\n \n-        return image_outputs, video_outputs, num_frames\n+    def get_video_features(self, pixel_values_videos: torch.FloatTensor, vision_feature_layer: int):\n+        \"\"\"\n+        Obtains video last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n+               The tensors corresponding to the input videos.\n+            vision_feature_layer (`int`):\n+                The index of the layer to select the vision feature.\n+        Returns:\n+            video_features (`torch.Tensor`): Video feature tensor of shape `(num_videos * num_frames, image_length, embed_dim)`).\n+            frames (`int`): Number of frames the videos have.\n+        \"\"\"\n+        batch_size_vid, num_frames, channels, height, width = pixel_values_videos.shape\n+\n+        pixel_values = pixel_values_videos.reshape(batch_size_vid * num_frames, channels, height, width)\n+        video_outputs = self.video_tower(pixel_values, output_hidden_states=True)\n+        video_features = video_outputs.hidden_states[vision_feature_layer].squeeze(1)\n+        video_features = self.multi_modal_projector(video_features)\n+\n+        return video_features, num_frames\n \n     @add_start_docstrings_to_model_forward(VIDEO_LLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=VideoLlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n@@ -534,110 +552,106 @@ def forward(\n             )\n             legacy_processing = inputs_not_expanded or pixels_present\n \n-        if pixel_values_images is not None or pixel_values_videos is not None:\n-            image_outputs, video_outputs, num_frames = self._get_vision_features(\n-                pixel_values_images=pixel_values_images,\n-                pixel_values_videos=pixel_values_videos,\n+        image_features = None\n+        if pixel_values_images is not None:\n+            image_features = self.get_image_features(\n+                pixel_values_images,\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n \n-            image_features = video_features = None\n-            if image_outputs is not None:\n-                image_features = self.multi_modal_projector(image_outputs)\n-            if video_outputs is not None:\n-                video_features = self.multi_modal_projector(video_outputs)\n-\n-            if legacy_processing:\n-                logger.warning_once(\n-                    \"Expanding inputs for image tokens in Video-LLaVa should be done in processing. \"\n-                    \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n-                    \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n-                    \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n-                )\n-                if input_ids.shape[1] != 1:\n-                    for features, frames in ((image_features, 1), (video_features, num_frames)):\n-                        if features is not None:\n-                            (\n-                                inputs_embeds,\n-                                attention_mask,\n-                                labels,\n-                                position_ids,\n-                                input_ids,\n-                            ) = self._merge_input_ids_with_visual_features(\n-                                features,\n-                                inputs_embeds,\n-                                input_ids,\n-                                attention_mask,\n-                                labels,\n-                                num_frames=frames,\n-                            )\n-                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n-                else:\n-                    # Retrieve the first layer to inspect the logits and mask out the hidden states\n-                    # that are set to 0\n-                    first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n-\n-                    # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n-                    batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n-\n-                    target_length = input_ids.shape[1]\n-                    past_length = first_layer_past_key_value.shape[-1]\n-\n-                    extended_attention_mask = torch.ones(\n-                        (attention_mask.shape[0], past_length),\n-                        dtype=attention_mask.dtype,\n-                        device=attention_mask.device,\n-                    )\n+        video_features = None\n+        if pixel_values_videos is not None:\n+            video_features, num_frames = self.get_video_features(\n+                pixel_values_videos=pixel_values_videos, vision_feature_layer=vision_feature_layer\n+            )\n \n-                    # Filter out only the tokens that can be un-attended, this can happen\n-                    # if one uses Llava + Fused modules where the cache on the\n-                    # first iteration is already big enough, or if one passes custom cache\n-                    valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n-                    new_batch_index = batch_index[valid_indices]\n-                    new_non_attended_tokens = non_attended_tokens[valid_indices]\n+        if legacy_processing:\n+            logger.warning_once(\n+                \"Expanding inputs for image tokens in Video-LLaVa should be done in processing. \"\n+                \"Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly \"\n+                \"with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. \"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+            )\n+            if input_ids.shape[1] != 1:\n+                for features, frames in ((image_features, 1), (video_features, num_frames)):\n+                    if features is not None:\n+                        (\n+                            inputs_embeds,\n+                            attention_mask,\n+                            labels,\n+                            position_ids,\n+                            input_ids,\n+                        ) = self._merge_input_ids_with_visual_features(\n+                            features,\n+                            inputs_embeds,\n+                            input_ids,\n+                            attention_mask,\n+                            labels,\n+                            num_frames=frames,\n+                        )\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)\n+            else:\n+                # Retrieve the first layer to inspect the logits and mask out the hidden states\n+                # that are set to 0\n+                first_layer_past_key_value = past_key_values[0][0][:, :, :, 0]\n+\n+                # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n+                batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n+                target_length = input_ids.shape[1]\n+                past_length = first_layer_past_key_value.shape[-1]\n+                extended_attention_mask = torch.ones(\n+                    (attention_mask.shape[0], past_length),\n+                    dtype=attention_mask.dtype,\n+                    device=attention_mask.device,\n+                )\n \n-                    # Zero-out the places where we don't need to attend\n-                    extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n+                # Filter out only the tokens that can be un-attended, this can happen\n+                # if one uses Llava + Fused modules where the cache on the\n+                # first iteration is already big enough, or if one passes custom cache\n+                valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n+                new_batch_index = batch_index[valid_indices]\n+                new_non_attended_tokens = non_attended_tokens[valid_indices]\n \n-                    attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n-                    position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n-                    cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[\n-                        -target_length:\n-                    ]\n+                # Zero-out the places where we don't need to attend\n+                extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n+                attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n+                position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n+                cache_position = torch.arange(attention_mask.shape[1], device=attention_mask.device)[-target_length:]\n \n-            # TODO: @raushan retain only the new behavior after v4.47\n-            else:\n-                if image_outputs is not None:\n-                    n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n-                    n_image_features = image_features.shape[1]\n-                    if n_image_tokens != n_image_features:\n-                        raise ValueError(\n-                            f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                        )\n-                    special_image_mask = (\n-                        (input_ids == self.config.image_token_index)\n-                        .unsqueeze(-1)\n-                        .expand_as(inputs_embeds)\n-                        .to(inputs_embeds.device)\n+        # TODO: @raushan retain only the new behavior after v4.47\n+        else:\n+            if pixel_values_images is not None:\n+                n_image_tokens = (input_ids == self.config.image_token_index).sum(dim=-1)[0].item()\n+                n_image_features = image_features.shape[1]\n+                if n_image_tokens != n_image_features:\n+                    raise ValueError(\n+                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                     )\n-                    image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                    inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-                if video_outputs is not None:\n-                    n_video_tokens = (input_ids == self.config.video_token_index).sum(dim=-1)[0].item()\n-                    n_video_features = video_features.shape[1]\n-                    if n_video_tokens != n_video_features:\n-                        raise ValueError(\n-                            f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                        )\n-                    special_image_mask = (\n-                        (input_ids == self.config.video_token_index)\n-                        .unsqueeze(-1)\n-                        .expand_as(inputs_embeds)\n-                        .to(inputs_embeds.device)\n+                special_image_mask = (\n+                    (input_ids == self.config.image_token_index)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n+                )\n+                image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+            if pixel_values_videos is not None:\n+                n_video_tokens = (input_ids == self.config.video_token_index).sum(dim=-1)[0].item()\n+                n_video_features = video_features.shape[1]\n+                if n_video_tokens != n_video_features:\n+                    raise ValueError(\n+                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                     )\n-                    video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                    inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+                special_image_mask = (\n+                    (input_ids == self.config.video_token_index)\n+                    .unsqueeze(-1)\n+                    .expand_as(inputs_embeds)\n+                    .to(inputs_embeds.device)\n+                )\n+                video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "dd7baa34406fb01322ef064e7b98ed7831fc9cda",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5077bc034fd806a8b0155e71b963596a765a7758/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=5077bc034fd806a8b0155e71b963596a765a7758",
            "patch": "@@ -275,6 +275,17 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_m\n \n     # Ignore copy\n     def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: List[int]):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            vision_feature_layers (`List[int]`):\n+                The list og indexes of the layers to select the vision feature.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n         image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n \n         # For VIP-llava, the image features are computed this way"
        }
    ],
    "stats": {
        "total": 747,
        "additions": 491,
        "deletions": 256
    }
}