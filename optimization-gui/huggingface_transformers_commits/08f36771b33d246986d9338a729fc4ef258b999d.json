{
    "author": "Cyrilvallez",
    "message": "Fix `init empty weights` without accelerate (#37337)\n\n* add the integration\n\n* Update accelerate.py\n\n* Update accelerate.py\n\n* add find_tied_params as well\n\n* Update accelerate.py\n\n* add where copied from\n\n* simplify\n\n* add error",
    "sha": "08f36771b33d246986d9338a729fc4ef258b999d",
    "files": [
        {
            "sha": "83efac9661af0423901f26783c76194aa7c205d6",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "added",
            "additions": 196,
            "deletions": 0,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/08f36771b33d246986d9338a729fc4ef258b999d/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08f36771b33d246986d9338a729fc4ef258b999d/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=08f36771b33d246986d9338a729fc4ef258b999d",
            "patch": "@@ -0,0 +1,196 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Since, https://github.com/huggingface/transformers/pull/36963, loading is always performed with models on meta\n+device. But since the `init_empty_weights` and `find_tied_parameters` functions are from accelerate, and accelerate is\n+somewhat still a soft dependency, we copy the functions here to be used natively in Transformers.\n+\n+The `init_empty_weights` and `init_on_device` functions were copied from `accelerate.big_modeling.py`, and the\n+`find_tied_parameters` was copied from `accelerate.utils.modeling.py`\n+\"\"\"\n+\n+from contextlib import contextmanager\n+\n+from ..utils import is_torch_available, logging\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn as nn\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@contextmanager\n+def init_empty_weights(include_buffers: bool = False):\n+    \"\"\"\n+    A context manager under which models are initialized with all parameters on the meta device, therefore creating an\n+    empty model. Useful when just initializing the model would blow the available RAM.\n+\n+    Args:\n+        include_buffers (`bool`, *optional*):\n+            Whether or not to also put all buffers on the meta device while initializing.\n+\n+    Example:\n+\n+    ```python\n+    import torch.nn as nn\n+    from accelerate import init_empty_weights\n+\n+    # Initialize a model with 100 billions parameters in no time and without using any RAM.\n+    with init_empty_weights():\n+        tst = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])\n+    ```\n+\n+    <Tip warning={true}>\n+\n+    Any model created under this context manager has no weights. As such you can't do something like\n+    `model.to(some_device)` with it. To load weights inside your empty model, see [`load_checkpoint_and_dispatch`].\n+    Make sure to overwrite the default device_map param for [`load_checkpoint_and_dispatch`], otherwise dispatch is not\n+    called.\n+\n+    </Tip>\n+    \"\"\"\n+    with init_on_device(torch.device(\"meta\"), include_buffers=include_buffers) as f:\n+        yield f\n+\n+\n+@contextmanager\n+def init_on_device(device: \"torch.device\", include_buffers: bool = False):\n+    \"\"\"\n+    A context manager under which models are initialized with all parameters on the specified device.\n+\n+    Args:\n+        device (`torch.device`):\n+            Device to initialize all parameters on.\n+        include_buffers (`bool`, *optional*):\n+            Whether or not to also put all buffers on the meta device while initializing.\n+\n+    Example:\n+\n+    ```python\n+    import torch.nn as nn\n+    from accelerate import init_on_device\n+\n+    with init_on_device(device=torch.device(\"cuda\")):\n+        tst = nn.Linear(100, 100)  # on `cuda` device\n+    ```\n+    \"\"\"\n+    if include_buffers:\n+        with device:\n+            yield\n+        return\n+\n+    old_register_parameter = nn.Module.register_parameter\n+    if include_buffers:\n+        old_register_buffer = nn.Module.register_buffer\n+\n+    def register_empty_parameter(module, name, param):\n+        old_register_parameter(module, name, param)\n+        if param is not None:\n+            param_cls = type(module._parameters[name])\n+            kwargs = module._parameters[name].__dict__\n+            kwargs[\"requires_grad\"] = param.requires_grad\n+            module._parameters[name] = param_cls(module._parameters[name].to(device), **kwargs)\n+\n+    def register_empty_buffer(module, name, buffer, persistent=True):\n+        old_register_buffer(module, name, buffer, persistent=persistent)\n+        if buffer is not None:\n+            module._buffers[name] = module._buffers[name].to(device)\n+\n+    # Patch tensor creation\n+    if include_buffers:\n+        tensor_constructors_to_patch = {\n+            torch_function_name: getattr(torch, torch_function_name)\n+            for torch_function_name in [\"empty\", \"zeros\", \"ones\", \"full\"]\n+        }\n+    else:\n+        tensor_constructors_to_patch = {}\n+\n+    def patch_tensor_constructor(fn):\n+        def wrapper(*args, **kwargs):\n+            kwargs[\"device\"] = device\n+            return fn(*args, **kwargs)\n+\n+        return wrapper\n+\n+    try:\n+        nn.Module.register_parameter = register_empty_parameter\n+        if include_buffers:\n+            nn.Module.register_buffer = register_empty_buffer\n+        for torch_function_name in tensor_constructors_to_patch.keys():\n+            setattr(torch, torch_function_name, patch_tensor_constructor(getattr(torch, torch_function_name)))\n+        yield\n+    finally:\n+        nn.Module.register_parameter = old_register_parameter\n+        if include_buffers:\n+            nn.Module.register_buffer = old_register_buffer\n+        for torch_function_name, old_torch_function in tensor_constructors_to_patch.items():\n+            setattr(torch, torch_function_name, old_torch_function)\n+\n+\n+def find_tied_parameters(model: \"nn.Module\", **kwargs):\n+    \"\"\"\n+    Find the tied parameters in a given model.\n+\n+    <Tip warning={true}>\n+\n+    The signature accepts keyword arguments, but they are for the recursive part of this function and you should ignore\n+    them.\n+\n+    </Tip>\n+\n+    Args:\n+        model (`torch.nn.Module`): The model to inspect.\n+\n+    Returns:\n+        List[List[str]]: A list of lists of parameter names being all tied together.\n+\n+    Example:\n+\n+    ```py\n+    >>> from collections import OrderedDict\n+    >>> import torch.nn as nn\n+\n+    >>> model = nn.Sequential(OrderedDict([(\"linear1\", nn.Linear(4, 4)), (\"linear2\", nn.Linear(4, 4))]))\n+    >>> model.linear2.weight = model.linear1.weight\n+    >>> find_tied_parameters(model)\n+    [['linear1.weight', 'linear2.weight']]\n+    ```\n+    \"\"\"\n+\n+    # get ALL model parameters and thier names\n+    all_named_parameters = dict(model.named_parameters(remove_duplicate=False))\n+\n+    # get ONLY unique named parameters,\n+    # if parameter is tied and have multiple names, it will be included only once\n+    no_duplicate_named_parameters = dict(model.named_parameters(remove_duplicate=True))\n+\n+    # the difference of the two sets will give us the tied parameters\n+    tied_param_names = set(all_named_parameters.keys()) - set(no_duplicate_named_parameters.keys())\n+\n+    # 'tied_param_names' contains the names of parameters that are tied in the model, but we do not know\n+    # which names refer to the same parameter. To identify this, we need to group them together.\n+    tied_param_groups = {}\n+    for tied_param_name in tied_param_names:\n+        tied_param = all_named_parameters[tied_param_name]\n+        for param_name, param in no_duplicate_named_parameters.items():\n+            # compare if parameters are the same, if so, group thier names together\n+            if param is tied_param:\n+                if param_name not in tied_param_groups:\n+                    tied_param_groups[param_name] = []\n+                tied_param_groups[param_name].append(tied_param_name)\n+\n+    return [sorted([weight] + list(set(tied))) for weight, tied in tied_param_groups.items()]"
        },
        {
            "sha": "a462d1e348ef6396e1210b10097cd92981d479c5",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/08f36771b33d246986d9338a729fc4ef258b999d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08f36771b33d246986d9338a729fc4ef258b999d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=08f36771b33d246986d9338a729fc4ef258b999d",
            "patch": "@@ -57,6 +57,7 @@\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig, GenerationMixin\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n+from .integrations.accelerate import find_tied_parameters, init_empty_weights\n from .integrations.deepspeed import _load_state_dict_into_zero3_model, is_deepspeed_available\n from .integrations.flash_attention import flash_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n@@ -131,12 +132,11 @@\n \n \n if is_accelerate_available():\n-    from accelerate import dispatch_model, infer_auto_device_map, init_empty_weights\n+    from accelerate import dispatch_model, infer_auto_device_map\n     from accelerate.hooks import add_hook_to_module\n     from accelerate.utils import (\n         check_tied_parameters_on_same_device,\n         extract_model_from_parallel,\n-        find_tied_parameters,\n         get_balanced_memory,\n         get_max_memory,\n         load_offloaded_weights,\n@@ -4135,6 +4135,10 @@ def from_pretrained(\n         if device_map is not None:\n             if is_deepspeed_zero3_enabled():\n                 raise ValueError(\"DeepSpeed Zero-3 is not compatible with passing a `device_map`.\")\n+            if not is_accelerate_available():\n+                raise ValueError(\n+                    \"Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`\"\n+                )\n \n         # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\n         if load_in_4bit or load_in_8bit:"
        }
    ],
    "stats": {
        "total": 204,
        "additions": 202,
        "deletions": 2
    }
}