{
    "author": "rziga",
    "message": "Add MM Grounding DINO (#37925)\n\n* first commit\n\nAdded modular implementation for MM Grounding DINO from starting point created by add-new-model-like. Added conversion script from mmdetection to huggingface.\n\nTODO: Some tests are failing so that needs to be fixed.\n\n* fixed a bug with modular definition of MMGroundingDinoForObjectDetection where box and class heads were not correctly assigned to inner model\n\n* cleaned up a hack in the conversion script\n\n* Fixed the expected values in integration tests\n\nCross att masking and cpu-gpu consistency tests are still failing however.\n\n* changes for make style and quality\n\n* add documentation\n\n* clean up contrastive embedding\n\n* add mm grounding dino to loss mapping\n\n* add model link to config docstring\n\n* hack fix for mm grounding dino consistency tests\n\n* add special cases for unused config attr check\n\n* add all models and update docs\n\n* update model doc to the new style\n\n* Use super_kwargs for modular config\n\n* Move init to the _init_weights function\n\n* Add copied from for tests\n\n* fixup\n\n* update typehints\n\n* Fix-copies for tests\n\n* fix-copies\n\n* Fix init test\n\n* fix snippets in docs\n\n* fix consistency\n\n* fix consistency\n\n* update conversion script\n\n* fix nits in readme and remove old comments from conversion script\n\n* add license\n\n* remove unused config args\n\n* remove unnecessary if/else in model init\n\n* fix quality\n\n* Update references\n\n* fix test\n\n* fixup\n\n---------\n\nCo-authored-by: qubvel <qubvel@gmail.com>",
    "sha": "3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
    "files": [
        {
            "sha": "6277ab85bb8e6487347ca0be50bc10cd4b3e1742",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -1051,6 +1051,8 @@\n         title: Mistral3\n       - local: model_doc/mllama\n         title: mllama\n+      - local: model_doc/mm-grounding-dino\n+        title: MM Grounding DINO\n       - local: model_doc/nougat\n         title: Nougat\n       - local: model_doc/omdet-turbo"
        },
        {
            "sha": "d129093498f37bb5b48151d26f9da5f4069a2310",
            "filename": "docs/source/en/model_doc/mm-grounding-dino.md",
            "status": "added",
            "additions": 124,
            "deletions": 0,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/docs%2Fsource%2Fen%2Fmodel_doc%2Fmm-grounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/docs%2Fsource%2Fen%2Fmodel_doc%2Fmm-grounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmm-grounding-dino.md?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -0,0 +1,124 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+           <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# MM Grounding DINO\n+\n+[MM Grounding DINO](https://arxiv.org/abs/2401.02361) model was proposed in [An Open and Comprehensive Pipeline for Unified Object Grounding and Detection](https://arxiv.org/abs/2401.02361) by Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang>.\n+\n+MM Grounding DINO improves upon the [Grounding DINO](https://huggingface.co/docs/transformers/model_doc/grounding-dino) by improving the contrastive class head and removing the parameter sharing in the decoder, improving zero-shot detection performance on both COCO (50.6(+2.2) AP) and LVIS (31.9(+11.8) val AP and 41.4(+12.6) minival AP).\n+\n+You can find all the original MM Grounding DINO checkpoints under the [MM Grounding DINO](https://huggingface.co/collections/openmmlab-community/mm-grounding-dino-688cbde05b814c4e2832f9df) collection. This model also supports LLMDet inference. You can find LLMDet checkpoints under the [LLMDet](https://huggingface.co/collections/iSEE-Laboratory/llmdet-688475906dc235d5f1dc678e) collection.\n+\n+> [!TIP]\n+> Click on the MM Grounding DINO models in the right sidebar for more examples of how to apply MM Grounding DINO to different MM Grounding DINO tasks.\n+\n+The example below demonstrates how to generate text based on an image with the [`AutoModelForZeroShotObjectDetection`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForZeroShotObjectDetection, AutoProcessor\n+from transformers.image_utils import load_image\n+\n+\n+# Prepare processor and model\n+model_id = \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\"\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n+\n+# Prepare inputs\n+image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = load_image(image_url)\n+text_labels = [[\"a cat\", \"a remote control\"]]\n+inputs = processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n+\n+# Run inference\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+\n+# Postprocess outputs\n+results = processor.post_process_grounded_object_detection(\n+    outputs,\n+    threshold=0.4,\n+    target_sizes=[(image.height, image.width)]\n+)\n+\n+# Retrieve the first image result\n+result = results[0]\n+for box, score, labels in zip(result[\"boxes\"], result[\"scores\"], result[\"labels\"]):\n+    box = [round(x, 2) for x in box.tolist()]\n+    print(f\"Detected {labels} with confidence {round(score.item(), 3)} at location {box}\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Notes\n+\n+- Here's a table of models and their object detection performance results on COCO (results from [official repo](https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/README.md)):\n+\n+    |                                                              Model                                                             | Backbone |      Pre-Train Data      |   Style   |  COCO mAP  |\n+    | ------------------------------------------------------------------------------------------------------------------------------ | -------- | ------------------------ | --------- | ---------- |\n+    |  [mm_grounding_dino_tiny_o365v1_goldg](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg)                       |  Swin-T  |        O365,GoldG        | Zero-shot | 50.4(+2.3) |\n+    |  [mm_grounding_dino_tiny_o365v1_goldg_grit](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_grit)             |  Swin-T  |     O365,GoldG,GRIT      | Zero-shot | 50.5(+2.1) |\n+    |  [mm_grounding_dino_tiny_o365v1_goldg_v3det](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det)           |  Swin-T  |     O365,GoldG,V3Det     | Zero-shot | 50.6(+2.2) |\n+    |  [mm_grounding_dino_tiny_o365v1_goldg_grit_v3det](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_grit_v3det) |  Swin-T  |  O365,GoldG,GRIT,V3Det   | Zero-shot | 50.4(+2.0) |\n+    |  [mm_grounding_dino_base_o365v1_goldg_v3det](https://huggingface.co/openmmlab-community/mm_grounding_dino_base_o365v1_goldg_v3det)           |  Swin-B  |     O365,GoldG,V3Det     | Zero-shot |    52.5    |\n+    |  [mm_grounding_dino_base_all](https://huggingface.co/openmmlab-community/mm_grounding_dino_base_all)                                         |  Swin-B  |         O365,ALL         |     -     |    59.5    |\n+    |  [mm_grounding_dino_large_o365v2_oiv6_goldg](https://huggingface.co/openmmlab-community/mm_grounding_dino_large_o365v2_oiv6_goldg)           |  Swin-L  | O365V2,OpenImageV6,GoldG | Zero-shot |    53.0    |\n+    |  [mm_grounding_dino_large_all](https://huggingface.co/openmmlab-community/mm_grounding_dino_large_all)                                       |  Swin-L  |  O365V2,OpenImageV6,ALL  |     -     |    60.3    |\n+\n+- Here's a table of MM Grounding DINO tiny models and their object detection performance on LVIS (results from [official repo](https://github.com/open-mmlab/mmdetection/blob/main/configs/mm_grounding_dino/README.md)):\n+\n+    |                                                              Model                                                             |    Pre-Train Data     | MiniVal APr | MiniVal APc | MiniVal APf | MiniVal AP  | Val1.0 APr | Val1.0 APc | Val1.0 APf |  Val1.0 AP  |\n+    | ------------------------------------------------------------------------------------------------------------------------------ | --------------------- | ----------- | ----------- | ----------- | ----------- | ---------- | ---------- | ---------- | ----------- |\n+    |  [mm_grounding_dino_tiny_o365v1_goldg](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg)                       |      O365,GoldG       |    28.1     |    30.2     |    42.0     | 35.7(+6.9)  |    17.1    |    22.4    |    36.5    | 27.0(+6.9)  |\n+    |  [mm_grounding_dino_tiny_o365v1_goldg_grit](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_grit)             |    O365,GoldG,GRIT    |    26.6     |    32.4     |    41.8     | 36.5(+7.7)  |    17.3    |    22.6    |    36.4    | 27.1(+7.0)  |\n+    |  [mm_grounding_dino_tiny_o365v1_goldg_v3det](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det)           |   O365,GoldG,V3Det    |    33.0     |    36.0     |    45.9     | 40.5(+11.7) |    21.5    |    25.5    |    40.2    | 30.6(+10.5) |\n+    |  [mm_grounding_dino_tiny_o365v1_goldg_grit_v3det](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_grit_v3det) | O365,GoldG,GRIT,V3Det |    34.2     |    37.4     |    46.2     | 41.4(+12.6) |    23.6    |    27.6    |    40.5    | 31.9(+11.8) |\n+\n+\n+- This implementation also supports inference for [LLMDet](https://github.com/iSEE-Laboratory/LLMDet). Here's a table of LLMDet models and their performance on LVIS (results from [official repo](https://github.com/iSEE-Laboratory/LLMDet)):\n+\n+    |                             Model                         | Pre-Train Data            |  MiniVal APr | MiniVal APc | MiniVal APf | MiniVal AP  | Val1.0 APr | Val1.0 APc | Val1.0 APf |  Val1.0 AP  |\n+    | --------------------------------------------------------- | -------------------------------------------- | ------------ | ----------- | ----------- | ----------- | ---------- | ---------- | ---------- | ----------- |\n+    | [llmdet_tiny](https://huggingface.co/iSEE-Laboratory/llmdet_tiny)   | (O365,GoldG,GRIT,V3Det) + GroundingCap-1M    | 44.7         | 37.3        | 39.5        | 50.7        | 34.9       | 26.0       | 30.1       | 44.3        |\n+    | [llmdet_base](https://huggingface.co/iSEE-Laboratory/llmdet_base)   | (O365,GoldG,V3Det) + GroundingCap-1M         | 48.3         | 40.8        | 43.1        | 54.3        | 38.5       | 28.2       | 34.3       | 47.8        |\n+    | [llmdet_large](https://huggingface.co/iSEE-Laboratory/llmdet_large) | (O365V2,OpenImageV6,GoldG) + GroundingCap-1M | 51.1         | 45.1        | 46.1        | 56.6        | 42.0       | 31.6       | 38.8       | 50.2        |\n+\n+\n+## MMGroundingDinoConfig\n+\n+[[autodoc]] MMGroundingDinoConfig\n+\n+## MMGroundingDinoModel\n+\n+[[autodoc]] MMGroundingDinoModel\n+    - forward\n+\n+## MMGroundingDinoForObjectDetection\n+\n+[[autodoc]] MMGroundingDinoForObjectDetection\n+    - forward"
        },
        {
            "sha": "b73e08991a8e2ed3bba9dadd17a380b1b694501d",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -158,6 +158,7 @@ def ForTokenClassification(logits: torch.Tensor, labels, config, **kwargs):\n     \"ConditionalDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"DabDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"GroundingDinoForObjectDetection\": GroundingDinoForObjectDetectionLoss,\n+    \"MMGroundingDinoForObjectDetection\": GroundingDinoForObjectDetectionLoss,\n     \"ConditionalDetrForSegmentation\": DeformableDetrForSegmentationLoss,\n     \"RTDetrForObjectDetection\": RTDetrForObjectDetectionLoss,\n     \"RTDetrV2ForObjectDetection\": RTDetrForObjectDetectionLoss,"
        },
        {
            "sha": "129c5ea300b00c9580ffbe297ef43611babb1367",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -244,6 +244,7 @@\n         (\"mixtral\", \"MixtralConfig\"),\n         (\"mlcd\", \"MLCDVisionConfig\"),\n         (\"mllama\", \"MllamaConfig\"),\n+        (\"mm-grounding-dino\", \"MMGroundingDinoConfig\"),\n         (\"mobilebert\", \"MobileBertConfig\"),\n         (\"mobilenet_v1\", \"MobileNetV1Config\"),\n         (\"mobilenet_v2\", \"MobileNetV2Config\"),\n@@ -657,6 +658,7 @@\n         (\"mlcd\", \"MLCD\"),\n         (\"mllama\", \"Mllama\"),\n         (\"mluke\", \"mLUKE\"),\n+        (\"mm-grounding-dino\", \"MM Grounding DINO\"),\n         (\"mms\", \"MMS\"),\n         (\"mobilebert\", \"MobileBERT\"),\n         (\"mobilenet_v1\", \"MobileNetV1\"),"
        },
        {
            "sha": "5786be0ab1e6eb082525152e0c53649d88bb75ad",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -130,6 +130,7 @@\n             (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"mlcd\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"mllama\", (\"MllamaImageProcessor\",)),\n+            (\"mm-grounding-dino\", (\"GroundingDinoImageProcessor\", \"GroundingDinoImageProcessorFast\")),\n             (\"mobilenet_v1\", (\"MobileNetV1ImageProcessor\", \"MobileNetV1ImageProcessorFast\")),\n             (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\", \"MobileNetV2ImageProcessorFast\")),\n             (\"mobilevit\", (\"MobileViTImageProcessor\", \"MobileViTImageProcessorFast\")),"
        },
        {
            "sha": "fb86b5687e46c877c738c163627df691d8d1dc6f",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -233,6 +233,7 @@\n         (\"mixtral\", \"MixtralModel\"),\n         (\"mlcd\", \"MLCDVisionModel\"),\n         (\"mllama\", \"MllamaModel\"),\n+        (\"mm-grounding-dino\", \"MMGroundingDinoModel\"),\n         (\"mobilebert\", \"MobileBertModel\"),\n         (\"mobilenet_v1\", \"MobileNetV1Model\"),\n         (\"mobilenet_v2\", \"MobileNetV2Model\"),\n@@ -1057,6 +1058,7 @@\n     [\n         # Model for Zero Shot Object Detection mapping\n         (\"grounding-dino\", \"GroundingDinoForObjectDetection\"),\n+        (\"mm-grounding-dino\", \"MMGroundingDinoForObjectDetection\"),\n         (\"omdet-turbo\", \"OmDetTurboForObjectDetection\"),\n         (\"owlv2\", \"Owlv2ForObjectDetection\"),\n         (\"owlvit\", \"OwlViTForObjectDetection\"),"
        },
        {
            "sha": "71a2d1d38f0b5040f267cca72aac0f8e53788a06",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -100,6 +100,7 @@\n         (\"mgp-str\", \"MgpstrProcessor\"),\n         (\"mistral3\", \"PixtralProcessor\"),\n         (\"mllama\", \"MllamaProcessor\"),\n+        (\"mm-grounding-dino\", \"GroundingDinoProcessor\"),\n         (\"moonshine\", \"Wav2Vec2Processor\"),\n         (\"oneformer\", \"OneFormerProcessor\"),\n         (\"owlv2\", \"Owlv2Processor\"),"
        },
        {
            "sha": "b623010e7b1cf5a6555f6fbe0f6f419b4ed3e6ac",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -430,6 +430,7 @@\n         ),\n         (\"mllama\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"mluke\", (\"MLukeTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"mm-grounding-dino\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"mobilebert\", (\"MobileBertTokenizer\", \"MobileBertTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"modernbert\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"moonshine\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),"
        },
        {
            "sha": "72257c2ad40072337696a98ad8ada72dcf05b539",
            "filename": "src/transformers/models/mm_grounding_dino/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2F__init__.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_mm_grounding_dino import *\n+    from .modeling_mm_grounding_dino import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "42e3af33a184d27e0ecc8684478f9d7d1d34f517",
            "filename": "src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py",
            "status": "added",
            "additions": 292,
            "deletions": 0,
            "changes": 292,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -0,0 +1,292 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/mm_grounding_dino/modular_mm_grounding_dino.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_mm_grounding_dino.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ...utils.backbone_utils import verify_backbone_config_arguments\n+from ..auto import CONFIG_MAPPING\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class MMGroundingDinoConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MMGroundingDinoModel`]. It is used to instantiate a\n+    MM Grounding DINO model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the MM Grounding DINO tiny architecture\n+    [openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        backbone_config (`PretrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n+            The configuration of the backbone model.\n+        backbone (`str`, *optional*):\n+            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n+            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`\n+            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.\n+        use_pretrained_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to use pretrained weights for the backbone.\n+        use_timm_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to load `backbone` from the timm library. If `False`, the backbone is loaded from the transformers\n+            library.\n+        backbone_kwargs (`dict`, *optional*):\n+            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n+            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `BertConfig`):\n+            The config object or dictionary of the text backbone.\n+        num_queries (`int`, *optional*, defaults to 900):\n+            Number of object queries, i.e. detection slots. This is the maximal number of objects\n+            [`MMGroundingDinoModel`] can detect in a single image.\n+        encoder_layers (`int`, *optional*, defaults to 6):\n+            Number of encoder layers.\n+        encoder_ffn_dim (`int`, *optional*, defaults to 2048):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        encoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        decoder_layers (`int`, *optional*, defaults to 6):\n+            Number of decoder layers.\n+        decoder_ffn_dim (`int`, *optional*, defaults to 2048):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        decoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+        activation_function (`str` or `function`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        d_model (`int`, *optional*, defaults to 256):\n+            Dimension of the layers.\n+        dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        activation_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for activations inside the fully connected layer.\n+        auxiliary_loss (`bool`, *optional*, defaults to `False`):\n+            Whether auxiliary decoding losses (loss at each decoder layer) are to be used.\n+        position_embedding_type (`str`, *optional*, defaults to `\"sine\"`):\n+            Type of position embeddings to be used on top of the image features. One of `\"sine\"` or `\"learned\"`.\n+        num_feature_levels (`int`, *optional*, defaults to 4):\n+            The number of input feature levels.\n+        encoder_n_points (`int`, *optional*, defaults to 4):\n+            The number of sampled keys in each feature level for each attention head in the encoder.\n+        decoder_n_points (`int`, *optional*, defaults to 4):\n+            The number of sampled keys in each feature level for each attention head in the decoder.\n+        two_stage (`bool`, *optional*, defaults to `True`):\n+            Whether to apply a two-stage deformable DETR, where the region proposals are also generated by a variant of\n+            Grounding DINO, which are further fed into the decoder for iterative bounding box refinement.\n+        class_cost (`float`, *optional*, defaults to 1.0):\n+            Relative weight of the classification error in the Hungarian matching cost.\n+        bbox_cost (`float`, *optional*, defaults to 5.0):\n+            Relative weight of the L1 error of the bounding box coordinates in the Hungarian matching cost.\n+        giou_cost (`float`, *optional*, defaults to 2.0):\n+            Relative weight of the generalized IoU loss of the bounding box in the Hungarian matching cost.\n+        bbox_loss_coefficient (`float`, *optional*, defaults to 5.0):\n+            Relative weight of the L1 bounding box loss in the object detection loss.\n+        giou_loss_coefficient (`float`, *optional*, defaults to 2.0):\n+            Relative weight of the generalized IoU loss in the object detection loss.\n+        focal_alpha (`float`, *optional*, defaults to 0.25):\n+            Alpha parameter in the focal loss.\n+        disable_custom_kernels (`bool`, *optional*, defaults to `False`):\n+            Disable the use of custom CUDA and CPU kernels. This option is necessary for the ONNX export, as custom\n+            kernels are not supported by PyTorch ONNX export.\n+        max_text_len (`int`, *optional*, defaults to 256):\n+            The maximum length of the text input.\n+        text_enhancer_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the text enhancer.\n+        fusion_droppath (`float`, *optional*, defaults to 0.1):\n+            The droppath ratio for the fusion module.\n+        fusion_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the fusion module.\n+        embedding_init_target (`bool`, *optional*, defaults to `True`):\n+            Whether to initialize the target with Embedding weights.\n+        query_dim (`int`, *optional*, defaults to 4):\n+            The dimension of the query vector.\n+        positional_embedding_temperature (`float`, *optional*, defaults to 20):\n+            The temperature for Sine Positional Embedding that is used together with vision backbone.\n+        init_std (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import MMGroundingDinoConfig, MMGroundingDinoModel\n+\n+    >>> # Initializing a MM Grounding DINO configuration\n+    >>> configuration = MMGroundingDinoConfig()\n+\n+    >>> # Initializing a model (with random weights) from the configuration\n+    >>> model = MMGroundingDinoModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"mm-grounding-dino\"\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"encoder_attention_heads\",\n+    }\n+\n+    def __init__(\n+        self,\n+        backbone_config=None,\n+        backbone=None,\n+        use_pretrained_backbone=False,\n+        use_timm_backbone=False,\n+        backbone_kwargs=None,\n+        text_config=None,\n+        num_queries=900,\n+        encoder_layers=6,\n+        encoder_ffn_dim=2048,\n+        encoder_attention_heads=8,\n+        decoder_layers=6,\n+        decoder_ffn_dim=2048,\n+        decoder_attention_heads=8,\n+        is_encoder_decoder=True,\n+        activation_function=\"relu\",\n+        d_model=256,\n+        dropout=0.1,\n+        attention_dropout=0.0,\n+        activation_dropout=0.0,\n+        auxiliary_loss=False,\n+        position_embedding_type=\"sine\",\n+        num_feature_levels=4,\n+        encoder_n_points=4,\n+        decoder_n_points=4,\n+        two_stage=True,\n+        class_cost=1.0,\n+        bbox_cost=5.0,\n+        giou_cost=2.0,\n+        bbox_loss_coefficient=5.0,\n+        giou_loss_coefficient=2.0,\n+        focal_alpha=0.25,\n+        disable_custom_kernels=False,\n+        # other parameters\n+        max_text_len=256,\n+        text_enhancer_dropout=0.0,\n+        fusion_droppath=0.1,\n+        fusion_dropout=0.0,\n+        embedding_init_target=True,\n+        query_dim=4,\n+        positional_embedding_temperature=20,\n+        init_std=0.02,\n+        layer_norm_eps=1e-5,\n+        **kwargs,\n+    ):\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+        if backbone_config is None and backbone is None:\n+            logger.info(\"`backbone_config` is `None`. Initializing the config with the default `Swin` backbone.\")\n+            backbone_config = CONFIG_MAPPING[\"swin\"](\n+                window_size=7,\n+                image_size=224,\n+                embed_dim=96,\n+                depths=[2, 2, 6, 2],\n+                num_heads=[3, 6, 12, 24],\n+                out_indices=[2, 3, 4],\n+            )\n+        elif isinstance(backbone_config, dict):\n+            backbone_model_type = backbone_config.pop(\"model_type\")\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            backbone_config = config_class.from_dict(backbone_config)\n+\n+        verify_backbone_config_arguments(\n+            use_timm_backbone=use_timm_backbone,\n+            use_pretrained_backbone=use_pretrained_backbone,\n+            backbone=backbone,\n+            backbone_config=backbone_config,\n+            backbone_kwargs=backbone_kwargs,\n+        )\n+\n+        if text_config is None:\n+            text_config = {}\n+            logger.info(\"text_config is None. Initializing the text config with default values (`BertConfig`).\")\n+\n+        self.backbone_config = backbone_config\n+        self.backbone = backbone\n+        self.use_pretrained_backbone = use_pretrained_backbone\n+        self.use_timm_backbone = use_timm_backbone\n+        self.backbone_kwargs = backbone_kwargs\n+        self.num_queries = num_queries\n+        self.d_model = d_model\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.encoder_layers = encoder_layers\n+        self.encoder_attention_heads = encoder_attention_heads\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.decoder_layers = decoder_layers\n+        self.decoder_attention_heads = decoder_attention_heads\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.activation_dropout = activation_dropout\n+        self.activation_function = activation_function\n+        self.auxiliary_loss = auxiliary_loss\n+        self.position_embedding_type = position_embedding_type\n+        # deformable attributes\n+        self.num_feature_levels = num_feature_levels\n+        self.encoder_n_points = encoder_n_points\n+        self.decoder_n_points = decoder_n_points\n+        self.two_stage = two_stage\n+        # Hungarian matcher\n+        self.class_cost = class_cost\n+        self.bbox_cost = bbox_cost\n+        self.giou_cost = giou_cost\n+        # Loss coefficients\n+        self.bbox_loss_coefficient = bbox_loss_coefficient\n+        self.giou_loss_coefficient = giou_loss_coefficient\n+        self.focal_alpha = focal_alpha\n+        self.disable_custom_kernels = disable_custom_kernels\n+        # Text backbone\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"bert\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"bert\"]()\n+\n+        self.text_config = text_config\n+        self.max_text_len = max_text_len\n+\n+        # Text Enhancer\n+        self.text_enhancer_dropout = text_enhancer_dropout\n+        # Fusion\n+        self.fusion_droppath = fusion_droppath\n+        self.fusion_dropout = fusion_dropout\n+        # Others\n+        self.embedding_init_target = embedding_init_target\n+        self.query_dim = query_dim\n+        self.positional_embedding_temperature = positional_embedding_temperature\n+        self.init_std = init_std\n+        self.layer_norm_eps = layer_norm_eps\n+\n+    @property\n+    def num_attention_heads(self) -> int:\n+        return self.encoder_attention_heads\n+\n+    @property\n+    def hidden_size(self) -> int:\n+        return self.d_model\n+\n+\n+__all__ = [\"MMGroundingDinoConfig\"]"
        },
        {
            "sha": "e985fdfef3f7afbe3d8bf38ff1436134ca39471d",
            "filename": "src/transformers/models/mm_grounding_dino/convert_mm_grounding_dino_to_hf.py",
            "status": "added",
            "additions": 504,
            "deletions": 0,
            "changes": 504,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconvert_mm_grounding_dino_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconvert_mm_grounding_dino_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconvert_mm_grounding_dino_to_hf.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -0,0 +1,504 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import re\n+\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers.models.bert.tokenization_bert import BertTokenizer\n+from transformers.models.grounding_dino.image_processing_grounding_dino import GroundingDinoImageProcessor\n+from transformers.models.grounding_dino.processing_grounding_dino import GroundingDinoProcessor\n+from transformers.models.mm_grounding_dino.configuration_mm_grounding_dino import MMGroundingDinoConfig\n+from transformers.models.mm_grounding_dino.modeling_mm_grounding_dino import MMGroundingDinoForObjectDetection\n+from transformers.models.swin.configuration_swin import SwinConfig\n+\n+\n+MODEL_NAME_TO_CHECKPOINT_URL_MAPPING = {\n+    \"mm_grounding_dino_tiny_o365v1_goldg\": \"https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg/grounding_dino_swin-t_pretrain_obj365_goldg_20231122_132602-4ea751ce.pth\",\n+    \"mm_grounding_dino_tiny_o365v1_goldg_grit\": \"https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_20231128_200818-169cc352.pth\",\n+    \"mm_grounding_dino_tiny_o365v1_goldg_v3det\": \"https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_v3det_20231218_095741-e316e297.pth\",\n+    \"mm_grounding_dino_tiny_o365v1_goldg_grit_v3det\": \"https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det/grounding_dino_swin-t_pretrain_obj365_goldg_grit9m_v3det_20231204_095047-b448804b.pth\",\n+    \"mm_grounding_dino_base_o365v1_goldg_v3det\": \"https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-b_pretrain_obj365_goldg_v3det/grounding_dino_swin-b_pretrain_obj365_goldg_v3de-f83eef00.pth\",\n+    \"mm_grounding_dino_base_all\": \"https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-b_pretrain_all/grounding_dino_swin-b_pretrain_all-f9818a7c.pth\",\n+    \"mm_grounding_dino_large_o365v2_oiv6_goldg\": \"https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-l_pretrain_obj365_goldg/grounding_dino_swin-l_pretrain_obj365_goldg-34dcdc53.pth\",\n+    \"mm_grounding_dino_large_all\": \"https://download.openmmlab.com/mmdetection/v3.0/mm_grounding_dino/grounding_dino_swin-l_pretrain_all/grounding_dino_swin-l_pretrain_all-56d69e78.pth\",\n+    \"llmdet_tiny\": \"https://huggingface.co/fushh7/LLMDet/resolve/main/tiny.pth?download=true\",\n+    \"llmdet_base\": \"https://huggingface.co/fushh7/LLMDet/resolve/main/base.pth?download=true\",\n+    \"llmdet_large\": \"https://huggingface.co/fushh7/LLMDet/resolve/main/large.pth?download=true\",\n+}\n+\n+\n+MODEL_NAME_TO_EXPECTED_OUTPUT_MAPPING = {\n+    \"mm_grounding_dino_tiny_o365v1_goldg\": {\n+        \"scores\": torch.tensor([0.7722, 0.7584, 0.7984, 0.7163]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.5212, 0.1594, 0.5792, 0.3895],\n+                [0.5424, 0.0513, 0.9996, 0.7757],\n+                [0.0629, 0.1526, 0.2746, 0.2447],\n+                [0.0091, 0.1127, 0.4945, 0.9911],\n+            ]\n+        ),\n+    },\n+    \"mm_grounding_dino_tiny_o365v1_goldg_grit\": {\n+        \"scores\": torch.tensor([0.7865, 0.7180, 0.7665, 0.8177]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.0084, 0.1129, 0.4940, 0.9895],\n+                [0.5214, 0.1597, 0.5786, 0.3875],\n+                [0.5413, 0.0507, 0.9998, 0.7768],\n+                [0.0631, 0.1527, 0.2740, 0.2449],\n+            ]\n+        ),\n+    },\n+    \"mm_grounding_dino_tiny_o365v1_goldg_v3det\": {\n+        \"scores\": torch.tensor([0.5690, 0.5553, 0.6075, 0.5775]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.5393, 0.0502, 0.9989, 0.7763],\n+                [0.0090, 0.1125, 0.4950, 0.9895],\n+                [0.5207, 0.1589, 0.5794, 0.3889],\n+                [0.0625, 0.1519, 0.2750, 0.2446],\n+            ]\n+        ),\n+    },\n+    \"mm_grounding_dino_tiny_o365v1_goldg_grit_v3det\": {\n+        \"scores\": torch.tensor([0.8381, 0.8204, 0.7970, 0.7175]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.0099, 0.1129, 0.4942, 0.9903],\n+                [0.5413, 0.0506, 0.9998, 0.7753],\n+                [0.0626, 0.1527, 0.2744, 0.2443],\n+                [0.5211, 0.1596, 0.5790, 0.3890],\n+            ]\n+        ),\n+    },\n+    \"mm_grounding_dino_base_o365v1_goldg_v3det\": {\n+        \"scores\": torch.tensor([0.8418, 0.8364, 0.8342, 0.7885]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.5427, 0.0502, 0.9996, 0.7770],\n+                [0.0628, 0.1529, 0.2747, 0.2448],\n+                [0.0085, 0.1132, 0.4947, 0.9898],\n+                [0.5208, 0.1597, 0.5787, 0.3910],\n+            ]\n+        ),\n+    },\n+    \"mm_grounding_dino_base_all\": {\n+        \"scores\": torch.tensor([0.4713]),\n+        \"boxes\": torch.tensor([[0.5423, 0.0507, 0.9998, 0.7761]]),\n+    },\n+    \"mm_grounding_dino_large_o365v2_oiv6_goldg\": {\n+        \"scores\": torch.tensor([0.7824, 0.8275, 0.7715, 0.8211]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.0082, 0.1133, 0.4945, 0.9889],\n+                [0.5410, 0.0508, 0.9998, 0.7771],\n+                [0.0632, 0.1526, 0.2740, 0.2439],\n+                [0.5205, 0.1599, 0.5787, 0.3906],\n+            ]\n+        ),\n+    },\n+    \"mm_grounding_dino_large_all\": {\n+        \"scores\": torch.tensor([0.7373, 0.6208, 0.6913, 0.4523]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.5424, 0.0509, 0.9997, 0.7765],\n+                [0.0632, 0.1529, 0.2744, 0.2447],\n+                [0.0121, 0.1125, 0.4947, 0.9884],\n+                [0.5206, 0.1597, 0.5789, 0.3933],\n+            ]\n+        ),\n+    },\n+    \"llmdet_tiny\": {\n+        \"scores\": torch.tensor([0.7262, 0.7552, 0.7656, 0.8207]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.0114, 0.1132, 0.4947, 0.9854],\n+                [0.5387, 0.0513, 0.9992, 0.7765],\n+                [0.5212, 0.1605, 0.5788, 0.3890],\n+                [0.0634, 0.1536, 0.2743, 0.2440],\n+            ]\n+        ),\n+    },\n+    \"llmdet_base\": {\n+        \"scores\": torch.tensor([0.8646, 0.7567, 0.6978, 0.8084]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.0632, 0.1529, 0.2745, 0.2438],\n+                [0.5420, 0.0512, 0.9989, 0.7774],\n+                [0.0110, 0.1134, 0.4950, 0.9875],\n+                [0.5209, 0.1602, 0.5789, 0.3908],\n+            ]\n+        ),\n+    },\n+    \"llmdet_large\": {\n+        \"scores\": torch.tensor([0.7107, 0.8626, 0.7458, 0.8166]),\n+        \"boxes\": torch.tensor(\n+            [\n+                [0.0147, 0.1128, 0.4957, 0.9858],\n+                [0.0634, 0.1528, 0.2744, 0.2447],\n+                [0.5414, 0.0511, 0.9997, 0.7776],\n+                [0.5209, 0.1602, 0.5792, 0.3916],\n+            ]\n+        ),\n+    },\n+}\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # vision backbone\n+    r\"backbone.patch_embed.projection.(weight|bias)\":                                                               r\"model.backbone.conv_encoder.model.embeddings.patch_embeddings.projection.\\1\",\n+    r\"backbone.patch_embed.norm.(weight|bias)\":                                                                     r\"model.backbone.conv_encoder.model.embeddings.norm.\\1\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).attn.w_msa.(relative_position_bias_table|relative_position_index)\":        r\"model.backbone.conv_encoder.model.encoder.layers.\\1.blocks.\\2.attention.self.\\3\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).norm1.(weight|bias)\":                                                      r\"model.backbone.conv_encoder.model.encoder.layers.\\1.blocks.\\2.layernorm_before.\\3\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).attn.w_msa.(query|key|value).(weight|bias)\":                               r\"model.backbone.conv_encoder.model.encoder.layers.\\1.blocks.\\2.attention.self.\\3.\\4\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).attn.w_msa.proj.(weight|bias)\":                                            r\"model.backbone.conv_encoder.model.encoder.layers.\\1.blocks.\\2.attention.output.dense.\\3\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).norm2.(weight|bias)\":                                                      r\"model.backbone.conv_encoder.model.encoder.layers.\\1.blocks.\\2.layernorm_after.\\3\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).ffn.layers.0.0.(weight|bias)\":                                             r\"model.backbone.conv_encoder.model.encoder.layers.\\1.blocks.\\2.intermediate.dense.\\3\",\n+    r\"backbone.stages.(\\d+).blocks.(\\d+).ffn.layers.1.(weight|bias)\":                                               r\"model.backbone.conv_encoder.model.encoder.layers.\\1.blocks.\\2.output.dense.\\3\",\n+    r\"backbone.stages.(\\d+).downsample.reduction.weight\":                                                           r\"model.backbone.conv_encoder.model.encoder.layers.\\1.downsample.reduction.weight\",\n+    r\"backbone.stages.(\\d+).downsample.norm.(weight|bias)\":                                                         r\"model.backbone.conv_encoder.model.encoder.layers.\\1.downsample.norm.\\2\",\n+    r\"backbone.norms.(\\d+).(weight|bias)\":                                                                            r\"model.backbone.conv_encoder.model.hidden_states_norms.stage\\1.\\2\",\n+    r\"neck.convs.(\\d+).conv.(weight|bias)\":                                                                         r\"model.input_proj_vision.\\1.0.\\2\",\n+    r\"neck.convs.(\\d+).gn.(weight|bias)\":                                                                           r\"model.input_proj_vision.\\1.1.\\2\",\n+    r\"neck.extra_convs.(\\d+).conv.(weight|bias)\":                                                                   r\"model.input_proj_vision.\\1.0.\\2\",\n+    r\"neck.extra_convs.(\\d+).gn.(weight|bias)\":                                                                     r\"model.input_proj_vision.\\1.1.\\2\",\n+    # text backbone\n+    r\"language_model.language_backbone.body.model.(.*)\":                                                            r\"model.text_backbone.\\1\",\n+    r\"text_feat_map.(weight|bias)\":                                                                                 r\"model.text_projection.\\1\",\n+    # encoder\n+    r\"encoder.fusion_layers.(\\d+).gamma_v\":                                                                         r\"model.encoder.layers.\\1.fusion_layer.vision_param\",\n+    r\"encoder.fusion_layers.(\\d+).gamma_l\":                                                                         r\"model.encoder.layers.\\1.fusion_layer.text_param\",\n+    r\"encoder.fusion_layers.(\\d+).layer_norm_v.(weight|bias)\":                                                      r\"model.encoder.layers.\\1.fusion_layer.layer_norm_vision.\\2\",\n+    r\"encoder.fusion_layers.(\\d+).attn.v_proj.(weight|bias)\":                                                       r\"model.encoder.layers.\\1.fusion_layer.attn.vision_proj.\\2\",\n+    r\"encoder.fusion_layers.(\\d+).attn.values_v_proj.(weight|bias)\":                                                r\"model.encoder.layers.\\1.fusion_layer.attn.values_vision_proj.\\2\",\n+    r\"encoder.fusion_layers.(\\d+).attn.out_v_proj.(weight|bias)\":                                                   r\"model.encoder.layers.\\1.fusion_layer.attn.out_vision_proj.\\2\",\n+    r\"encoder.fusion_layers.(\\d+).layer_norm_l.(weight|bias)\":                                                      r\"model.encoder.layers.\\1.fusion_layer.layer_norm_text.\\2\",\n+    r\"encoder.fusion_layers.(\\d+).attn.l_proj.(weight|bias)\":                                                       r\"model.encoder.layers.\\1.fusion_layer.attn.text_proj.\\2\",\n+    r\"encoder.fusion_layers.(\\d+).attn.values_l_proj.(weight|bias)\":                                                r\"model.encoder.layers.\\1.fusion_layer.attn.values_text_proj.\\2\",\n+    r\"encoder.fusion_layers.(\\d+).attn.out_l_proj.(weight|bias)\":                                                   r\"model.encoder.layers.\\1.fusion_layer.attn.out_text_proj.\\2\",\n+    r\"encoder.layers.(\\d+).self_attn.(sampling_offsets|attention_weights|value_proj|output_proj).(weight|bias)\":    r\"model.encoder.layers.\\1.deformable_layer.self_attn.\\2.\\3\",\n+    r\"encoder.layers.(\\d+).norms.0.(weight|bias)\":                                                                  r\"model.encoder.layers.\\1.deformable_layer.self_attn_layer_norm.\\2\",\n+    r\"encoder.layers.(\\d+).ffn.layers.0.0.(weight|bias)\":                                                           r\"model.encoder.layers.\\1.deformable_layer.fc1.\\2\",\n+    r\"encoder.layers.(\\d+).ffn.layers.1.(weight|bias)\":                                                             r\"model.encoder.layers.\\1.deformable_layer.fc2.\\2\",\n+    r\"encoder.layers.(\\d+).norms.1.(weight|bias)\":                                                                  r\"model.encoder.layers.\\1.deformable_layer.final_layer_norm.\\2\",\n+    r\"encoder.text_layers.(\\d+).self_attn.attn.(query|key|value)_proj_(weight|bias)\":                               r\"model.encoder.layers.\\1.text_enhancer_layer.self_attn.\\2.\\3\",\n+    r\"encoder.text_layers.(\\d+).self_attn.attn.out_proj.(weight|bias)\":                                             r\"model.encoder.layers.\\1.text_enhancer_layer.self_attn.out_proj.\\2\",\n+    r\"encoder.text_layers.(\\d+).norms.0.(weight|bias)\":                                                             r\"model.encoder.layers.\\1.text_enhancer_layer.layer_norm_before.\\2\",\n+    r\"encoder.text_layers.(\\d+).ffn.layers.0.0.(weight|bias)\":                                                      r\"model.encoder.layers.\\1.text_enhancer_layer.fc1.\\2\",\n+    r\"encoder.text_layers.(\\d+).ffn.layers.1.(weight|bias)\":                                                        r\"model.encoder.layers.\\1.text_enhancer_layer.fc2.\\2\",\n+    r\"encoder.text_layers.(\\d+).norms.1.(weight|bias)\":                                                             r\"model.encoder.layers.\\1.text_enhancer_layer.layer_norm_after.\\2\",\n+    r\"encoder.bbox_head.cls_branch.bias\":                                                                           r\"model.encoder_output_class_embed.bias\",\n+    r\"encoder.bbox_head.reg_branch.0.(weight|bias)\":                                                                r\"model.encoder_output_bbox_embed.layers.0.\\1\",\n+    r\"encoder.bbox_head.reg_branch.2.(weight|bias)\":                                                                r\"model.encoder_output_bbox_embed.layers.1.\\1\",\n+    r\"encoder.bbox_head.reg_branch.4.(weight|bias)\":                                                                r\"model.encoder_output_bbox_embed.layers.2.\\1\",\n+    # decoder\n+    r\"decoder.norm.(weight|bias)\":                                                                                  r\"model.decoder.layer_norm.\\1\",\n+    r\"decoder.ref_point_head.layers.(\\d+).(weight|bias)\":                                                           r\"model.decoder.reference_points_head.layers.\\1.\\2\",\n+    r\"decoder.layers.(\\d+).self_attn.attn.(query|key|value)_proj_(weight|bias)\":                                    r\"model.decoder.layers.\\1.self_attn.\\2.\\3\",\n+    r\"decoder.layers.(\\d+).self_attn.attn.out_proj.(weight|bias)\":                                                  r\"model.decoder.layers.\\1.self_attn.out_proj.\\2\",\n+    r\"decoder.layers.(\\d+).norms.0.(weight|bias)\":                                                                  r\"model.decoder.layers.\\1.self_attn_layer_norm.\\2\",\n+    r\"decoder.layers.(\\d+).cross_attn_text.attn.(query|key|value)_proj_(weight|bias)\":                              r\"model.decoder.layers.\\1.encoder_attn_text.\\2.\\3\",\n+    r\"decoder.layers.(\\d+).cross_attn_text.attn.out_proj.(weight|bias)\":                                            r\"model.decoder.layers.\\1.encoder_attn_text.out_proj.\\2\",\n+    r\"decoder.layers.(\\d+).norms.1.(weight|bias)\":                                                                  r\"model.decoder.layers.\\1.encoder_attn_text_layer_norm.\\2\",\n+    r\"decoder.layers.(\\d+).cross_attn.(sampling_offsets|attention_weights|value_proj|output_proj).(weight|bias)\":   r\"model.decoder.layers.\\1.encoder_attn.\\2.\\3\",\n+    r\"decoder.layers.(\\d+).norms.2.(weight|bias)\":                                                                  r\"model.decoder.layers.\\1.encoder_attn_layer_norm.\\2\",\n+    r\"decoder.layers.(\\d+).ffn.layers.0.0.(weight|bias)\":                                                           r\"model.decoder.layers.\\1.fc1.\\2\",\n+    r\"decoder.layers.(\\d+).ffn.layers.1.(weight|bias)\":                                                             r\"model.decoder.layers.\\1.fc2.\\2\",\n+    r\"decoder.layers.(\\d+).norms.3.(weight|bias)\":                                                                  r\"model.decoder.layers.\\1.final_layer_norm.\\2\",\n+    r\"decoder.bbox_head.cls_branches.(\\d+).bias\":                                                                   r\"model.decoder.class_embed.\\1.bias\",\n+    r\"decoder.bbox_head.reg_branches.(\\d+).0.(weight|bias)\":                                                        r\"model.decoder.bbox_embed.\\1.layers.0.\\2\",\n+    r\"decoder.bbox_head.reg_branches.(\\d+).2.(weight|bias)\":                                                        r\"model.decoder.bbox_embed.\\1.layers.1.\\2\",\n+    r\"decoder.bbox_head.reg_branches.(\\d+).4.(weight|bias)\":                                                        r\"model.decoder.bbox_embed.\\1.layers.2.\\2\",\n+    # other\n+    r\"level_embed\":                                                                                                 r\"model.level_embed\",\n+    r\"query_embedding.weight\":                                                                                      r\"model.query_position_embeddings.weight\",\n+    r\"memory_trans_fc.(weight|bias)\":                                                                               r\"model.enc_output.\\1\",\n+    r\"memory_trans_norm.(weight|bias)\":                                                                             r\"model.enc_output_norm.\\1\",\n+    r\"bbox_head.cls_branches.(\\d+).bias\":                                                                           r\"class_embed.\\1.bias\",\n+    r\"bbox_head.reg_branches.(\\d+).0.(weight|bias)\":                                                                r\"bbox_embed.\\1.layers.0.\\2\",\n+    r\"bbox_head.reg_branches.(\\d+).2.(weight|bias)\":                                                                r\"bbox_embed.\\1.layers.1.\\2\",\n+    r\"bbox_head.reg_branches.(\\d+).4.(weight|bias)\":                                                                r\"bbox_embed.\\1.layers.2.\\2\",\n+}\n+# fmt: on\n+\n+\n+def get_mm_grounding_dino_config(model_name: str) -> MMGroundingDinoConfig:\n+    if \"tiny\" in model_name:\n+        swin_image_size = 224\n+        swin_window_size = 7\n+        swin_embed_dim = 96\n+        swin_depths = (2, 2, 6, 2)\n+        swin_num_heads = (3, 6, 12, 24)\n+        swin_out_features = [\"stage2\", \"stage3\", \"stage4\"]\n+        num_feature_levels = 4\n+    elif \"base\" in model_name:\n+        swin_image_size = 384\n+        swin_window_size = 12\n+        swin_embed_dim = 128\n+        swin_depths = (2, 2, 18, 2)\n+        swin_num_heads = (4, 8, 16, 32)\n+        swin_out_features = [\"stage2\", \"stage3\", \"stage4\"]\n+        num_feature_levels = 4\n+    elif \"large\" in model_name:\n+        swin_image_size = 384\n+        swin_window_size = 12\n+        swin_embed_dim = 192\n+        swin_depths = (2, 2, 18, 2)\n+        swin_num_heads = (6, 12, 24, 48)\n+        swin_out_features = [\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n+        num_feature_levels = 5\n+    else:\n+        raise ValueError(\n+            f\"Model name: {model_name} is not supported. Only `tiny`, `base` and `large` models are currently supported.\"\n+        )\n+\n+    backbone_config = SwinConfig(\n+        image_size=swin_image_size,\n+        window_size=swin_window_size,\n+        embed_dim=swin_embed_dim,\n+        depths=swin_depths,\n+        num_heads=swin_num_heads,\n+        out_features=swin_out_features,\n+    )\n+\n+    model_config = MMGroundingDinoConfig(\n+        backbone_config=backbone_config,\n+        num_feature_levels=num_feature_levels,\n+    )\n+\n+    return model_config\n+\n+\n+def get_mm_grounding_dino_processor() -> GroundingDinoProcessor:\n+    img_processor = GroundingDinoImageProcessor()\n+    txt_processor = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n+    processor = GroundingDinoProcessor(img_processor, txt_processor)\n+    return processor\n+\n+\n+# Copied from: https://github.com/iSEE-Laboratory/LLMDet/blob/96ec8c82a9d97b170db759e043afd5b81445d0f1/hf_model/mmdet2groundingdino_swint.py#L8C1-L13C13\n+def correct_unfold_reduction_order(x: torch.Tensor) -> torch.Tensor:\n+    out_channel, in_channel = x.shape\n+    x = x.reshape(out_channel, in_channel // 4, 4).transpose(1, 2)\n+    x = x[:, [0, 2, 1, 3], :]\n+    x = x.reshape(out_channel, in_channel)\n+    return x\n+\n+\n+# Copied from: https://github.com/iSEE-Laboratory/LLMDet/blob/96ec8c82a9d97b170db759e043afd5b81445d0f1/hf_model/mmdet2groundingdino_swint.py#L15C1-L20C13\n+def correct_unfold_norm_order(x: torch.Tensor) -> torch.Tensor:\n+    in_channel = x.shape[0]\n+    x = x.reshape(in_channel // 4, 4).transpose(0, 1)\n+    x = x[[0, 2, 1, 3], :]\n+    x = x.reshape(in_channel)\n+    return x\n+\n+\n+def preprocess_old_state(state_dict: dict, config: MMGroundingDinoConfig) -> dict:\n+    \"\"\"\n+    Preprocesses old state dict to enable 1-1 mapping:\n+        - split qkv projections in Swin backbone\n+        - reorder reduction and norm parameters in Swin backbone\n+        - shift output norm indices in Swin backbone\n+        - shift output proj indices in neck\n+        - split q,k,v projections in text self and cross attentions in encoder and decoder\n+        - duplicate detection head parameters for decoder and encoder\n+    \"\"\"\n+    new_state_dict = state_dict.copy()\n+    for k in state_dict:\n+        if k.startswith(\"backbone\"):\n+            if \"downsample.reduction\" in k:\n+                new_state_dict[k] = correct_unfold_reduction_order(new_state_dict.pop(k))\n+            elif \"downsample.norm\" in k:\n+                new_state_dict[k] = correct_unfold_norm_order(new_state_dict.pop(k))\n+            elif \"w_msa.qkv\" in k:\n+                q_param, k_param, v_param = new_state_dict.pop(k).chunk(3)\n+                new_state_dict[k.replace(\"qkv\", \"query\")] = q_param\n+                new_state_dict[k.replace(\"qkv\", \"key\")] = k_param\n+                new_state_dict[k.replace(\"qkv\", \"value\")] = v_param\n+            elif \"backbone.norm\" in k:\n+                match = re.match(r\"backbone.norm(\\d+).(weight|bias)\", k)\n+                new_state_dict[f\"backbone.norms.{int(match.group(1)) + 1}.{match.group(2)}\"] = new_state_dict.pop(k)\n+        elif k.startswith(\"neck.extra_convs\"):\n+            num_normal_convs = len(config.backbone_config.out_indices)\n+            if \"gn\" in k:\n+                match = re.match(r\"neck.extra_convs.(\\d+).gn.(weight|bias)\", k)\n+                new_state_dict[f\"neck.extra_convs.{num_normal_convs + int(match.group(1))}.gn.{match.group(2)}\"] = (\n+                    new_state_dict.pop(k)\n+                )\n+            elif \"conv\" in k:\n+                match = re.match(r\"neck.extra_convs.(\\d+).conv.(weight|bias)\", k)\n+                new_state_dict[f\"neck.extra_convs.{num_normal_convs + int(match.group(1))}.conv.{match.group(2)}\"] = (\n+                    new_state_dict.pop(k)\n+                )\n+        elif k.startswith(\"encoder\"):\n+            if \"self_attn.attn.in_proj\" in k:\n+                q_param, k_param, v_param = new_state_dict.pop(k).chunk(3)\n+                new_state_dict[k.replace(\"in\", \"query\")] = q_param\n+                new_state_dict[k.replace(\"in\", \"key\")] = k_param\n+                new_state_dict[k.replace(\"in\", \"value\")] = v_param\n+        elif k.startswith(\"decoder\"):\n+            if \"self_attn.attn.in_proj\" in k or \"cross_attn_text.attn.in_proj\" in k:\n+                q_param, k_param, v_param = new_state_dict.pop(k).chunk(3)\n+                new_state_dict[k.replace(\"in\", \"query\")] = q_param\n+                new_state_dict[k.replace(\"in\", \"key\")] = k_param\n+                new_state_dict[k.replace(\"in\", \"value\")] = v_param\n+        elif k.startswith(\"bbox_head\"):\n+            num_decoder_layers = config.decoder_layers\n+            match = re.match(r\"bbox_head.(cls|reg)_branches.(\\d+).(.*)\", k)\n+            cls_or_reg = match.group(1)\n+            layer_idx = int(match.group(2))\n+            suffix = match.group(3)\n+            if layer_idx < num_decoder_layers:\n+                new_key = f\"decoder.bbox_head.{cls_or_reg}_branches.{layer_idx}.{suffix}\"\n+                new_state_dict[new_key] = new_state_dict[k]  # copy\n+            else:\n+                new_key = f\"encoder.bbox_head.{cls_or_reg}_branch.{suffix}\"\n+                new_state_dict[new_key] = new_state_dict.pop(k)  # move\n+\n+        # remove unused params\n+        if (\n+            k == \"dn_query_generator.label_embedding.weight\"\n+            or k == \"language_model.language_backbone.body.model.embeddings.position_ids\"\n+            or k == \"image_seperate.weight\"\n+            or k.startswith(\"lmm\")\n+            or k.startswith(\"connector\")\n+            or k.startswith(\"region_connector\")\n+            or k.startswith(\"ref_point_head\")\n+        ):\n+            new_state_dict.pop(k)\n+\n+    return new_state_dict\n+\n+\n+# Copied from transformers/models/siglip2/convert_siglip2_to_hf.py\n+def convert_old_keys_to_new_keys(state_dict_keys: list) -> dict:\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def convert_mm_to_hf_state(original_state: dict, hf_cfg: MMGroundingDinoConfig) -> dict:\n+    original_state = preprocess_old_state(original_state, hf_cfg)\n+    original_state_keys = list(original_state.keys())\n+    original_to_hf_key_map = convert_old_keys_to_new_keys(original_state_keys)\n+\n+    hf_state = {}\n+    for original_key in original_state_keys:\n+        hf_key = original_to_hf_key_map[original_key]\n+        hf_state[hf_key] = original_state.pop(original_key)\n+\n+    return hf_state\n+\n+\n+def prepare_test_inputs():\n+    image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(image_url, stream=True).raw)\n+    text = [[\"cat\", \"remote\"]]\n+    return image, text\n+\n+\n+@torch.no_grad()\n+def convert_mm_grounding_dino_checkpoint(\n+    model_name: str,\n+    verify_outputs: bool,\n+    push_to_hub: bool,\n+    hub_user_name: str,\n+) -> tuple[MMGroundingDinoConfig, dict]:\n+    # Load original state\n+    checkpoint_url = MODEL_NAME_TO_CHECKPOINT_URL_MAPPING[model_name]\n+    print(f\"Loading checkpoint from: {checkpoint_url}\")\n+    ckpt = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=\"cpu\")\n+    mm_state = ckpt[\"state_dict\"]\n+\n+    # Create hf model and processor\n+    print(\"Creating model...\")\n+    hf_cfg = get_mm_grounding_dino_config(model_name)\n+    hf_state = convert_mm_to_hf_state(mm_state, hf_cfg)\n+    hf_model = MMGroundingDinoForObjectDetection(hf_cfg).eval()\n+    hf_model.load_state_dict(hf_state)\n+    hf_processor = get_mm_grounding_dino_processor()\n+\n+    # Verify outputs if needed\n+    if verify_outputs:\n+        print(\"Running inference to verify outputs...\")\n+        image, text = prepare_test_inputs()\n+        model_inputs = hf_processor(images=image, text=text, return_tensors=\"pt\")\n+        model_outputs = hf_model(**model_inputs)\n+        results = hf_processor.post_process_grounded_object_detection(\n+            model_outputs,\n+            model_inputs.input_ids,\n+            box_threshold=0.4,\n+            text_threshold=0.3,\n+        )\n+        result = results[0]\n+        print(result)\n+        expected = MODEL_NAME_TO_EXPECTED_OUTPUT_MAPPING[model_name]\n+        for key in expected:\n+            torch.testing.assert_close(result[key], expected[key], atol=1e-3, rtol=1e-3)\n+        print(\"Outputs match.\")\n+\n+    # Push to hub if needed\n+    if push_to_hub:\n+        print(\"Pushing to hub...\")\n+        hub_url = f\"{hub_user_name}/{model_name}\"\n+        hf_model.push_to_hub(hub_url)\n+        hf_processor.push_to_hub(hub_url)\n+        print(f\"Pushed to huggingface hub at: {hub_url}.\")\n+\n+    return hf_cfg, hf_state\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--model-name\",\n+        required=True,\n+        type=str,\n+        choices=list(MODEL_NAME_TO_CHECKPOINT_URL_MAPPING.keys()),\n+        help=\"URL to the original mm grounding dino checkpoint.\",\n+    )\n+    parser.add_argument(\"--hub-user-name\", type=str, help=\"User name on the huggingface hub.\")\n+    parser.add_argument(\"--push-to-hub\", action=\"store_true\", help=\"Whether to push model to hub or not.\")\n+    parser.add_argument(\n+        \"--verify-outputs\", action=\"store_true\", help=\"Whether to verify that model output is correct or not.\"\n+    )\n+    return parser.parse_args()\n+\n+\n+if __name__ == \"__main__\":\n+    args = parse_args()\n+    convert_mm_grounding_dino_checkpoint(\n+        args.model_name,\n+        args.verify_outputs,\n+        args.push_to_hub,\n+        args.hub_user_name,\n+    )"
        },
        {
            "sha": "e75456769e8414490d6c3685d137a5982a7317c6",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "added",
            "additions": 2611,
            "deletions": 0,
            "changes": 2611,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1"
        },
        {
            "sha": "6fc13df410b78d55514528b7cf958753cd2c6b6a",
            "filename": "src/transformers/models/mm_grounding_dino/modular_mm_grounding_dino.py",
            "status": "added",
            "additions": 434,
            "deletions": 0,
            "changes": 434,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -0,0 +1,434 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+\n+import torch\n+from torch import nn\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ...utils.backbone_utils import verify_backbone_config_arguments\n+from ..auto import CONFIG_MAPPING\n+from ..auto.modeling_auto import AutoModel\n+from ..grounding_dino.configuration_grounding_dino import GroundingDinoConfig\n+from ..grounding_dino.modeling_grounding_dino import (\n+    GroundingDinoContrastiveEmbedding,\n+    GroundingDinoConvEncoder,\n+    GroundingDinoConvModel,\n+    GroundingDinoDecoder,\n+    GroundingDinoEncoder,\n+    GroundingDinoForObjectDetection,\n+    GroundingDinoMLPPredictionHead,\n+    GroundingDinoModel,\n+    GroundingDinoPreTrainedModel,\n+    build_position_encoding,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class MMGroundingDinoConfig(GroundingDinoConfig, PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MMGroundingDinoModel`]. It is used to instantiate a\n+    MM Grounding DINO model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the MM Grounding DINO tiny architecture\n+    [openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det](https://huggingface.co/openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        backbone_config (`PretrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n+            The configuration of the backbone model.\n+        backbone (`str`, *optional*):\n+            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n+            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`\n+            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.\n+        use_pretrained_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to use pretrained weights for the backbone.\n+        use_timm_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to load `backbone` from the timm library. If `False`, the backbone is loaded from the transformers\n+            library.\n+        backbone_kwargs (`dict`, *optional*):\n+            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n+            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `BertConfig`):\n+            The config object or dictionary of the text backbone.\n+        num_queries (`int`, *optional*, defaults to 900):\n+            Number of object queries, i.e. detection slots. This is the maximal number of objects\n+            [`MMGroundingDinoModel`] can detect in a single image.\n+        encoder_layers (`int`, *optional*, defaults to 6):\n+            Number of encoder layers.\n+        encoder_ffn_dim (`int`, *optional*, defaults to 2048):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        encoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        decoder_layers (`int`, *optional*, defaults to 6):\n+            Number of decoder layers.\n+        decoder_ffn_dim (`int`, *optional*, defaults to 2048):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        decoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether the model is used as an encoder/decoder or not.\n+        activation_function (`str` or `function`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        d_model (`int`, *optional*, defaults to 256):\n+            Dimension of the layers.\n+        dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        activation_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for activations inside the fully connected layer.\n+        auxiliary_loss (`bool`, *optional*, defaults to `False`):\n+            Whether auxiliary decoding losses (loss at each decoder layer) are to be used.\n+        position_embedding_type (`str`, *optional*, defaults to `\"sine\"`):\n+            Type of position embeddings to be used on top of the image features. One of `\"sine\"` or `\"learned\"`.\n+        num_feature_levels (`int`, *optional*, defaults to 4):\n+            The number of input feature levels.\n+        encoder_n_points (`int`, *optional*, defaults to 4):\n+            The number of sampled keys in each feature level for each attention head in the encoder.\n+        decoder_n_points (`int`, *optional*, defaults to 4):\n+            The number of sampled keys in each feature level for each attention head in the decoder.\n+        two_stage (`bool`, *optional*, defaults to `True`):\n+            Whether to apply a two-stage deformable DETR, where the region proposals are also generated by a variant of\n+            Grounding DINO, which are further fed into the decoder for iterative bounding box refinement.\n+        class_cost (`float`, *optional*, defaults to 1.0):\n+            Relative weight of the classification error in the Hungarian matching cost.\n+        bbox_cost (`float`, *optional*, defaults to 5.0):\n+            Relative weight of the L1 error of the bounding box coordinates in the Hungarian matching cost.\n+        giou_cost (`float`, *optional*, defaults to 2.0):\n+            Relative weight of the generalized IoU loss of the bounding box in the Hungarian matching cost.\n+        bbox_loss_coefficient (`float`, *optional*, defaults to 5.0):\n+            Relative weight of the L1 bounding box loss in the object detection loss.\n+        giou_loss_coefficient (`float`, *optional*, defaults to 2.0):\n+            Relative weight of the generalized IoU loss in the object detection loss.\n+        focal_alpha (`float`, *optional*, defaults to 0.25):\n+            Alpha parameter in the focal loss.\n+        disable_custom_kernels (`bool`, *optional*, defaults to `False`):\n+            Disable the use of custom CUDA and CPU kernels. This option is necessary for the ONNX export, as custom\n+            kernels are not supported by PyTorch ONNX export.\n+        max_text_len (`int`, *optional*, defaults to 256):\n+            The maximum length of the text input.\n+        text_enhancer_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the text enhancer.\n+        fusion_droppath (`float`, *optional*, defaults to 0.1):\n+            The droppath ratio for the fusion module.\n+        fusion_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the fusion module.\n+        embedding_init_target (`bool`, *optional*, defaults to `True`):\n+            Whether to initialize the target with Embedding weights.\n+        query_dim (`int`, *optional*, defaults to 4):\n+            The dimension of the query vector.\n+        positional_embedding_temperature (`float`, *optional*, defaults to 20):\n+            The temperature for Sine Positional Embedding that is used together with vision backbone.\n+        init_std (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import MMGroundingDinoConfig, MMGroundingDinoModel\n+\n+    >>> # Initializing a MM Grounding DINO configuration\n+    >>> configuration = MMGroundingDinoConfig()\n+\n+    >>> # Initializing a model (with random weights) from the configuration\n+    >>> model = MMGroundingDinoModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"mm-grounding-dino\"\n+\n+    def __init__(\n+        self,\n+        backbone_config=None,\n+        backbone=None,\n+        use_pretrained_backbone=False,\n+        use_timm_backbone=False,\n+        backbone_kwargs=None,\n+        text_config=None,\n+        num_queries=900,\n+        encoder_layers=6,\n+        encoder_ffn_dim=2048,\n+        encoder_attention_heads=8,\n+        decoder_layers=6,\n+        decoder_ffn_dim=2048,\n+        decoder_attention_heads=8,\n+        is_encoder_decoder=True,\n+        activation_function=\"relu\",\n+        d_model=256,\n+        dropout=0.1,\n+        attention_dropout=0.0,\n+        activation_dropout=0.0,\n+        auxiliary_loss=False,\n+        position_embedding_type=\"sine\",\n+        num_feature_levels=4,\n+        encoder_n_points=4,\n+        decoder_n_points=4,\n+        two_stage=True,\n+        class_cost=1.0,\n+        bbox_cost=5.0,\n+        giou_cost=2.0,\n+        bbox_loss_coefficient=5.0,\n+        giou_loss_coefficient=2.0,\n+        focal_alpha=0.25,\n+        disable_custom_kernels=False,\n+        # other parameters\n+        max_text_len=256,\n+        text_enhancer_dropout=0.0,\n+        fusion_droppath=0.1,\n+        fusion_dropout=0.0,\n+        embedding_init_target=True,\n+        query_dim=4,\n+        positional_embedding_temperature=20,\n+        init_std=0.02,\n+        layer_norm_eps=1e-5,\n+        **kwargs,\n+    ):\n+        PretrainedConfig.__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+        if backbone_config is None and backbone is None:\n+            logger.info(\"`backbone_config` is `None`. Initializing the config with the default `Swin` backbone.\")\n+            backbone_config = CONFIG_MAPPING[\"swin\"](\n+                window_size=7,\n+                image_size=224,\n+                embed_dim=96,\n+                depths=[2, 2, 6, 2],\n+                num_heads=[3, 6, 12, 24],\n+                out_indices=[2, 3, 4],\n+            )\n+        elif isinstance(backbone_config, dict):\n+            backbone_model_type = backbone_config.pop(\"model_type\")\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            backbone_config = config_class.from_dict(backbone_config)\n+\n+        verify_backbone_config_arguments(\n+            use_timm_backbone=use_timm_backbone,\n+            use_pretrained_backbone=use_pretrained_backbone,\n+            backbone=backbone,\n+            backbone_config=backbone_config,\n+            backbone_kwargs=backbone_kwargs,\n+        )\n+\n+        if text_config is None:\n+            text_config = {}\n+            logger.info(\"text_config is None. Initializing the text config with default values (`BertConfig`).\")\n+\n+        self.backbone_config = backbone_config\n+        self.backbone = backbone\n+        self.use_pretrained_backbone = use_pretrained_backbone\n+        self.use_timm_backbone = use_timm_backbone\n+        self.backbone_kwargs = backbone_kwargs\n+        self.num_queries = num_queries\n+        self.d_model = d_model\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.encoder_layers = encoder_layers\n+        self.encoder_attention_heads = encoder_attention_heads\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.decoder_layers = decoder_layers\n+        self.decoder_attention_heads = decoder_attention_heads\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.activation_dropout = activation_dropout\n+        self.activation_function = activation_function\n+        self.auxiliary_loss = auxiliary_loss\n+        self.position_embedding_type = position_embedding_type\n+        # deformable attributes\n+        self.num_feature_levels = num_feature_levels\n+        self.encoder_n_points = encoder_n_points\n+        self.decoder_n_points = decoder_n_points\n+        self.two_stage = two_stage\n+        # Hungarian matcher\n+        self.class_cost = class_cost\n+        self.bbox_cost = bbox_cost\n+        self.giou_cost = giou_cost\n+        # Loss coefficients\n+        self.bbox_loss_coefficient = bbox_loss_coefficient\n+        self.giou_loss_coefficient = giou_loss_coefficient\n+        self.focal_alpha = focal_alpha\n+        self.disable_custom_kernels = disable_custom_kernels\n+        # Text backbone\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"bert\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"bert\"]()\n+\n+        self.text_config = text_config\n+        self.max_text_len = max_text_len\n+\n+        # Text Enhancer\n+        self.text_enhancer_dropout = text_enhancer_dropout\n+        # Fusion\n+        self.fusion_droppath = fusion_droppath\n+        self.fusion_dropout = fusion_dropout\n+        # Others\n+        self.embedding_init_target = embedding_init_target\n+        self.query_dim = query_dim\n+        self.positional_embedding_temperature = positional_embedding_temperature\n+        self.init_std = init_std\n+        self.layer_norm_eps = layer_norm_eps\n+\n+\n+class MMGroundingDinoContrastiveEmbedding(GroundingDinoContrastiveEmbedding):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.bias = nn.Parameter(torch.tensor(0.0))\n+\n+    def forward(\n+        self,\n+        vision_hidden_state: torch.FloatTensor,\n+        text_hidden_state: torch.FloatTensor,\n+        text_token_mask: torch.BoolTensor,\n+    ) -> torch.FloatTensor:\n+        res = vision_hidden_state @ text_hidden_state.transpose(-1, -2)\n+        res = res / math.sqrt(vision_hidden_state.shape[-1])\n+        res = res + self.bias\n+        res.masked_fill_(~text_token_mask[:, None, :], float(\"-inf\"))\n+\n+        # padding to max_text_len\n+        new_res = torch.full((*res.shape[:-1], self.max_text_len), float(\"-inf\"), device=res.device)\n+        new_res[..., : res.shape[-1]] = res\n+\n+        return new_res\n+\n+\n+class MMGroundingDinoPreTrainedModel(GroundingDinoPreTrainedModel):\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, MMGroundingDinoContrastiveEmbedding):\n+            nn.init.constant_(module.bias, -math.log((1 - 0.01) / 0.01))\n+\n+\n+class MMGroundingDinoConvEncoder(GroundingDinoConvEncoder):\n+    pass\n+\n+\n+class MMGroundingDinoConvModel(GroundingDinoConvModel):\n+    pass\n+\n+\n+class MMGroundingDinoEncoder(GroundingDinoEncoder):\n+    pass\n+\n+\n+class MMGroundingDinoDecoder(GroundingDinoDecoder):\n+    pass\n+\n+\n+class MMGroundingDinoModel(GroundingDinoModel, MMGroundingDinoPreTrainedModel):\n+    def __init__(self, config: MMGroundingDinoConfig):\n+        MMGroundingDinoPreTrainedModel.__init__(config)\n+\n+        # Create backbone + positional encoding\n+        backbone = MMGroundingDinoConvEncoder(config)\n+        position_embeddings = build_position_encoding(config)\n+        self.backbone = MMGroundingDinoConvModel(backbone, position_embeddings)\n+\n+        # Create input projection layers\n+        num_backbone_outs = len(backbone.intermediate_channel_sizes)\n+        input_proj_list = []\n+        for i in range(num_backbone_outs):\n+            in_channels = backbone.intermediate_channel_sizes[i]\n+            input_proj_list.append(\n+                nn.Sequential(\n+                    nn.Conv2d(in_channels, config.d_model, kernel_size=1),\n+                    nn.GroupNorm(32, config.d_model),\n+                )\n+            )\n+        for _ in range(config.num_feature_levels - num_backbone_outs):\n+            input_proj_list.append(\n+                nn.Sequential(\n+                    nn.Conv2d(in_channels, config.d_model, kernel_size=3, stride=2, padding=1),\n+                    nn.GroupNorm(32, config.d_model),\n+                )\n+            )\n+            in_channels = config.d_model\n+        self.input_proj_vision = nn.ModuleList(input_proj_list)\n+\n+        # Create text backbone\n+        self.text_backbone = AutoModel.from_config(config.text_config, add_pooling_layer=False)\n+        self.text_projection = nn.Linear(config.text_config.hidden_size, config.d_model)\n+\n+        if config.embedding_init_target or not config.two_stage:\n+            self.query_position_embeddings = nn.Embedding(config.num_queries, config.d_model)\n+\n+        self.encoder = MMGroundingDinoEncoder(config)\n+        self.decoder = MMGroundingDinoDecoder(config)\n+\n+        self.level_embed = nn.Parameter(torch.Tensor(config.num_feature_levels, config.d_model))\n+\n+        self.enc_output = nn.Linear(config.d_model, config.d_model)\n+        self.enc_output_norm = nn.LayerNorm(config.d_model, config.layer_norm_eps)\n+        self.encoder_output_bbox_embed = MMGroundingDinoMLPPredictionHead(\n+            input_dim=config.d_model, hidden_dim=config.d_model, output_dim=4, num_layers=3\n+        )\n+        self.encoder_output_class_embed = MMGroundingDinoContrastiveEmbedding(config)\n+\n+        self.post_init()\n+\n+\n+class MMGroundingDinoMLPPredictionHead(GroundingDinoMLPPredictionHead):\n+    pass\n+\n+\n+class MMGroundingDinoForObjectDetection(GroundingDinoForObjectDetection, MMGroundingDinoPreTrainedModel):\n+    _tied_weights_keys = [\n+        r\"bbox_embed\\.[1-9]\\d*\",\n+        r\"model\\.decoder\\.bbox_embed\\.[0-9]\\d*\",\n+        r\"class_embed\\.[1-9]\\d*\",\n+        r\"model\\.decoder\\.class_embed\\.[0-9]\\d*\",\n+    ]\n+\n+    def __init__(self, config: MMGroundingDinoConfig):\n+        MMGroundingDinoPreTrainedModel.__init__(config)\n+\n+        self.model = MMGroundingDinoModel(config)\n+\n+        self.class_embed = nn.ModuleList(\n+            [MMGroundingDinoContrastiveEmbedding(config) for _ in range(config.decoder_layers)]\n+        )\n+\n+        self.bbox_embed = nn.ModuleList(\n+            [\n+                MMGroundingDinoMLPPredictionHead(\n+                    input_dim=config.d_model, hidden_dim=config.d_model, output_dim=4, num_layers=3\n+                )\n+                for _ in range(config.decoder_layers)\n+            ]\n+        )\n+\n+        # hack for box-refinement\n+        self.model.decoder.bbox_embed = self.bbox_embed\n+        # hack implementation for two-stage\n+        self.model.decoder.class_embed = self.class_embed\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+\n+__all__ = [\n+    \"MMGroundingDinoConfig\",\n+    \"MMGroundingDinoForObjectDetection\",\n+    \"MMGroundingDinoModel\",\n+    \"MMGroundingDinoPreTrainedModel\",\n+]"
        },
        {
            "sha": "1e0a0a49cd7cd73c72f61ea91e3593d5d16a54a2",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -818,7 +818,9 @@ def test_grounding_dino_loss(self):\n         prompt = \". \".join(id2label.values()) + \".\"\n \n         text_inputs = tokenizer([prompt, prompt], return_tensors=\"pt\")\n-        image_inputs = image_processor(images=ds[\"image\"], annotations=ds[\"annotations\"], return_tensors=\"pt\")\n+        image_inputs = image_processor(\n+            images=list(ds[\"image\"]), annotations=list(ds[\"annotations\"]), return_tensors=\"pt\"\n+        )\n \n         # Passing auxiliary_loss=True to compare with the expected loss\n         model = GroundingDinoForObjectDetection.from_pretrained("
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/mm_grounding_dino/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/tests%2Fmodels%2Fmm_grounding_dino%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/tests%2Fmodels%2Fmm_grounding_dino%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmm_grounding_dino%2F__init__.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1"
        },
        {
            "sha": "1d380bc3e09723f709f27acabda015ed9d2209fa",
            "filename": "tests/models/mm_grounding_dino/test_modeling_mm_grounding_dino.py",
            "status": "added",
            "additions": 871,
            "deletions": 0,
            "changes": 871,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -0,0 +1,871 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch MM Grounding DINO model.\"\"\"\n+\n+import collections\n+import inspect\n+import math\n+import re\n+import unittest\n+\n+from datasets import load_dataset\n+\n+from transformers import (\n+    MMGroundingDinoConfig,\n+    SwinConfig,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.file_utils import cached_property\n+from transformers.testing_utils import (\n+    is_flaky,\n+    require_timm,\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import MMGroundingDinoConfig, MMGroundingDinoForObjectDetection, MMGroundingDinoModel\n+    from transformers.pytorch_utils import id_tensor_storage\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoProcessor\n+\n+\n+# Copied from tests.models.grounding_dino.test_modeling_grounding_dino.generate_fake_bounding_boxes\n+def generate_fake_bounding_boxes(n_boxes):\n+    \"\"\"Generate bounding boxes in the format (center_x, center_y, width, height)\"\"\"\n+    # Validate the input\n+    if not isinstance(n_boxes, int):\n+        raise TypeError(\"n_boxes must be an integer\")\n+    if n_boxes <= 0:\n+        raise ValueError(\"n_boxes must be a positive integer\")\n+\n+    # Generate random bounding boxes in the format (center_x, center_y, width, height)\n+    bounding_boxes = torch.rand((n_boxes, 4))\n+\n+    # Extract the components\n+    center_x = bounding_boxes[:, 0]\n+    center_y = bounding_boxes[:, 1]\n+    width = bounding_boxes[:, 2]\n+    height = bounding_boxes[:, 3]\n+\n+    # Ensure width and height do not exceed bounds\n+    width = torch.min(width, torch.tensor(1.0))\n+    height = torch.min(height, torch.tensor(1.0))\n+\n+    # Ensure the bounding box stays within the normalized space\n+    center_x = torch.where(center_x - width / 2 < 0, width / 2, center_x)\n+    center_x = torch.where(center_x + width / 2 > 1, 1 - width / 2, center_x)\n+    center_y = torch.where(center_y - height / 2 < 0, height / 2, center_y)\n+    center_y = torch.where(center_y + height / 2 > 1, 1 - height / 2, center_y)\n+\n+    # Combine back into bounding boxes\n+    bounding_boxes = torch.stack([center_x, center_y, width, height], dim=1)\n+\n+    return bounding_boxes\n+\n+\n+# Copied from tests.models.grounding_dino.test_modeling_grounding_dino.GroundingDinoModelTester with GroundingDino->MMGroundingDino\n+class MMGroundingDinoModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=4,\n+        is_training=True,\n+        use_labels=True,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=4,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        num_queries=2,\n+        num_channels=3,\n+        image_size=98,\n+        n_targets=8,\n+        num_labels=2,\n+        num_feature_levels=4,\n+        encoder_n_points=2,\n+        decoder_n_points=6,\n+        max_text_len=7,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.num_queries = num_queries\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.n_targets = n_targets\n+        self.num_labels = num_labels\n+        self.num_feature_levels = num_feature_levels\n+        self.encoder_n_points = encoder_n_points\n+        self.decoder_n_points = decoder_n_points\n+        self.max_text_len = max_text_len\n+\n+        # we also set the expected seq length for both encoder and decoder\n+        self.encoder_seq_length_vision = (\n+            math.ceil(self.image_size / 8) ** 2\n+            + math.ceil(self.image_size / 16) ** 2\n+            + math.ceil(self.image_size / 32) ** 2\n+            + math.ceil(self.image_size / 64) ** 2\n+        )\n+\n+        self.encoder_seq_length_text = self.max_text_len\n+\n+        self.decoder_seq_length = self.num_queries\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        pixel_mask = torch.ones([self.batch_size, self.image_size, self.image_size], device=torch_device)\n+\n+        # When using `MMGroundingDino` the text input template is '{label1}. {label2}. {label3. ... {labelN}.'\n+        # Therefore to avoid errors when running tests with `labels` `input_ids` have to follow this structure.\n+        # Otherwise when running `build_label_maps` it will throw an error when trying to split the input_ids into segments.\n+        input_ids = torch.tensor([101, 3869, 1012, 11420, 3869, 1012, 102], device=torch_device)\n+        input_ids = input_ids.unsqueeze(0).expand(self.batch_size, -1)\n+\n+        labels = None\n+        if self.use_labels:\n+            # labels is a list of Dict (each Dict being the labels for a given example in the batch)\n+            labels = []\n+            for i in range(self.batch_size):\n+                target = {}\n+                target[\"class_labels\"] = torch.randint(\n+                    high=self.num_labels, size=(self.n_targets,), device=torch_device\n+                )\n+                target[\"boxes\"] = generate_fake_bounding_boxes(self.n_targets).to(torch_device)\n+                target[\"masks\"] = torch.rand(self.n_targets, self.image_size, self.image_size, device=torch_device)\n+                labels.append(target)\n+\n+        config = self.get_config()\n+        return config, pixel_values, pixel_mask, input_ids, labels\n+\n+    def get_config(self):\n+        swin_config = SwinConfig(\n+            window_size=7,\n+            embed_dim=8,\n+            depths=[1, 1, 1, 1],\n+            num_heads=[1, 1, 1, 1],\n+            image_size=self.image_size,\n+            out_features=[\"stage2\", \"stage3\", \"stage4\"],\n+            out_indices=[2, 3, 4],\n+        )\n+        text_backbone = {\n+            \"hidden_size\": 8,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 2,\n+            \"intermediate_size\": 8,\n+            \"max_position_embeddings\": 8,\n+            \"model_type\": \"bert\",\n+        }\n+        return MMGroundingDinoConfig(\n+            d_model=self.hidden_size,\n+            encoder_layers=self.num_hidden_layers,\n+            decoder_layers=self.num_hidden_layers,\n+            encoder_attention_heads=self.num_attention_heads,\n+            decoder_attention_heads=self.num_attention_heads,\n+            encoder_ffn_dim=self.intermediate_size,\n+            decoder_ffn_dim=self.intermediate_size,\n+            dropout=self.hidden_dropout_prob,\n+            attention_dropout=self.attention_probs_dropout_prob,\n+            num_queries=self.num_queries,\n+            num_labels=self.num_labels,\n+            num_feature_levels=self.num_feature_levels,\n+            encoder_n_points=self.encoder_n_points,\n+            decoder_n_points=self.decoder_n_points,\n+            use_timm_backbone=False,\n+            backbone_config=swin_config,\n+            max_text_len=self.max_text_len,\n+            text_config=text_backbone,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, pixel_values, pixel_mask, input_ids, labels = self.prepare_config_and_inputs()\n+        inputs_dict = {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask, \"input_ids\": input_ids}\n+        return config, inputs_dict\n+\n+    def create_and_check_model(self, config, pixel_values, pixel_mask, input_ids, labels):\n+        model = MMGroundingDinoModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask, input_ids=input_ids)\n+\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.num_queries, self.hidden_size))\n+\n+    def create_and_check_object_detection_head_model(self, config, pixel_values, pixel_mask, input_ids, labels):\n+        model = MMGroundingDinoForObjectDetection(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask, input_ids=input_ids)\n+\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_queries, config.max_text_len))\n+        self.parent.assertEqual(result.pred_boxes.shape, (self.batch_size, self.num_queries, 4))\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask, input_ids=input_ids, labels=labels)\n+\n+        self.parent.assertEqual(result.loss.shape, ())\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_queries, config.max_text_len))\n+        self.parent.assertEqual(result.pred_boxes.shape, (self.batch_size, self.num_queries, 4))\n+\n+\n+@require_torch\n+# Copied from tests.models.grounding_dino.test_modeling_grounding_dino.GroundingDinoModelTest with Grounding->MMGrounding\n+class MMGroundingDinoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (MMGroundingDinoModel, MMGroundingDinoForObjectDetection) if is_torch_available() else ()\n+    is_encoder_decoder = True\n+    test_torchscript = False\n+    test_pruning = False\n+    test_head_masking = False\n+    test_missing_keys = False\n+    pipeline_model_mapping = (\n+        {\n+            \"image-feature-extraction\": MMGroundingDinoModel,\n+            \"zero-shot-object-detection\": MMGroundingDinoForObjectDetection,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    # special case for head models\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n+\n+        if return_labels:\n+            if model_class.__name__ == \"MMGroundingDinoForObjectDetection\":\n+                labels = []\n+                for i in range(self.model_tester.batch_size):\n+                    target = {}\n+                    target[\"class_labels\"] = torch.ones(\n+                        size=(self.model_tester.n_targets,), device=torch_device, dtype=torch.long\n+                    )\n+                    target[\"boxes\"] = torch.ones(\n+                        self.model_tester.n_targets, 4, device=torch_device, dtype=torch.float\n+                    )\n+                    target[\"masks\"] = torch.ones(\n+                        self.model_tester.n_targets,\n+                        self.model_tester.image_size,\n+                        self.model_tester.image_size,\n+                        device=torch_device,\n+                        dtype=torch.float,\n+                    )\n+                    labels.append(target)\n+                inputs_dict[\"labels\"] = labels\n+\n+        return inputs_dict\n+\n+    def setUp(self):\n+        self.model_tester = MMGroundingDinoModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=MMGroundingDinoConfig,\n+            has_text_modality=False,\n+            common_properties=[\"d_model\", \"encoder_attention_heads\", \"decoder_attention_heads\"],\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_object_detection_head_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_object_detection_head_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"MMGrounding DINO does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"MMGrounding DINO does not have a get_input_embeddings method\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"MMGrounding DINO does not use token embeddings\")\n+    def test_resize_tokens_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Feed forward chunking is not implemented\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions[-1]\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions[-1]\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            self.assertListEqual(\n+                list(attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_attention_heads,\n+                    self.model_tester.num_feature_levels,\n+                    self.model_tester.encoder_n_points,\n+                ],\n+            )\n+            out_len = len(outputs)\n+\n+            correct_outlen = 12\n+\n+            # loss is at first position\n+            if \"labels\" in inputs_dict:\n+                correct_outlen += 1  # loss is added to beginning\n+            # Object Detection model returns pred_logits and pred_boxes and input_ids\n+            if model_class.__name__ == \"MMGroundingDinoForObjectDetection\":\n+                correct_outlen += 3\n+\n+            self.assertEqual(out_len, correct_outlen)\n+\n+            # decoder attentions\n+            decoder_attentions = outputs.decoder_attentions[0]\n+            self.assertIsInstance(decoder_attentions, (list, tuple))\n+            self.assertEqual(len(decoder_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(decoder_attentions[0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, self.model_tester.num_queries, self.model_tester.num_queries],\n+            )\n+\n+            # cross attentions\n+            cross_attentions = outputs.decoder_attentions[-1]\n+            self.assertIsInstance(cross_attentions, (list, tuple))\n+            self.assertEqual(len(cross_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(cross_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_attention_heads,\n+                    self.model_tester.num_feature_levels,\n+                    self.model_tester.decoder_n_points,\n+                ],\n+            )\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            self.assertEqual(out_len + 3, len(outputs))\n+\n+            self_attentions = outputs.encoder_attentions[-1]\n+\n+            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(self_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_attention_heads,\n+                    self.model_tester.num_feature_levels,\n+                    self.model_tester.encoder_n_points,\n+                ],\n+            )\n+\n+    # overwrite since hidden_states are called encoder_text_hidden_states\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_vision_hidden_states\n+\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n+            )\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            seq_len = self.model_tester.encoder_seq_length_vision\n+\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-2:]),\n+                [seq_len, self.model_tester.hidden_size],\n+            )\n+\n+            hidden_states = outputs.encoder_text_hidden_states\n+\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            seq_len = self.model_tester.encoder_seq_length_text\n+\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-2:]),\n+                [seq_len, self.model_tester.hidden_size],\n+            )\n+\n+            hidden_states = outputs.decoder_hidden_states\n+\n+            self.assertIsInstance(hidden_states, (list, tuple))\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+            seq_len = getattr(self.model_tester, \"seq_length\", None)\n+            decoder_seq_length = getattr(self.model_tester, \"decoder_seq_length\", seq_len)\n+\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-2:]),\n+                [decoder_seq_length, self.model_tester.hidden_size],\n+            )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    # removed retain_grad and grad on decoder_hidden_states, as queries don't require grad\n+    def test_retain_grad_hidden_states_attentions(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.output_hidden_states = True\n+        config.output_attentions = True\n+\n+        # no need to test all models as different heads yield the same functionality\n+        model_class = self.all_model_classes[0]\n+        model = model_class(config)\n+        model.to(torch_device)\n+\n+        inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+        outputs = model(**inputs)\n+\n+        output = outputs[0]\n+\n+        encoder_hidden_states = outputs.encoder_vision_hidden_states[0]\n+        encoder_attentions = outputs.encoder_attentions[0][0]\n+        encoder_hidden_states.retain_grad()\n+        encoder_attentions.retain_grad()\n+\n+        cross_attentions = outputs.decoder_attentions[-1][0]\n+        cross_attentions.retain_grad()\n+\n+        output.flatten()[0].backward(retain_graph=True)\n+\n+        self.assertIsNotNone(encoder_hidden_states.grad)\n+        self.assertIsNotNone(encoder_attentions.grad)\n+        self.assertIsNotNone(cross_attentions.grad)\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\", \"input_ids\"]\n+            self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n+\n+    def test_different_timm_backbone(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # let's pick a random timm backbone\n+        config.backbone = \"tf_mobilenetv3_small_075\"\n+        config.use_timm_backbone = True\n+        config.backbone_config = None\n+        config.backbone_kwargs = {\"in_chans\": 3, \"out_indices\": (2, 3, 4)}\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if model_class.__name__ == \"MMGroundingDinoForObjectDetection\":\n+                expected_shape = (\n+                    self.model_tester.batch_size,\n+                    self.model_tester.num_queries,\n+                    config.max_text_len,\n+                )\n+                self.assertEqual(outputs.logits.shape, expected_shape)\n+\n+            self.assertTrue(outputs)\n+\n+    @require_timm\n+    def test_hf_backbone(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # Load a pretrained HF checkpoint as backbone\n+        config.backbone = \"microsoft/resnet-18\"\n+        config.backbone_config = None\n+        config.use_timm_backbone = False\n+        config.use_pretrained_backbone = True\n+        config.backbone_kwargs = {\"out_indices\": [2, 3, 4]}\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if model_class.__name__ == \"MMGroundingDinoForObjectDetection\":\n+                expected_shape = (\n+                    self.model_tester.batch_size,\n+                    self.model_tester.num_queries,\n+                    config.max_text_len,\n+                )\n+                self.assertEqual(outputs.logits.shape, expected_shape)\n+\n+            self.assertTrue(outputs)\n+\n+    # Ignore copy\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    if (\n+                        \"level_embed\" in name\n+                        or \"sampling_offsets.bias\" in name\n+                        or \"text_param\" in name\n+                        or \"vision_param\" in name\n+                        or \"value_proj\" in name\n+                        or \"output_proj\" in name\n+                        or \"reference_points\" in name\n+                        or \"vision_proj\" in name\n+                        or \"text_proj\" in name\n+                        or (\"class_embed\" in name and \"bias\" in name)\n+                    ):\n+                        continue\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    # Copied from tests.models.deformable_detr.test_modeling_deformable_detr.DeformableDetrModelTest.test_two_stage_training with DeformableDetr->MMGroundingDino\n+    def test_two_stage_training(self):\n+        model_class = MMGroundingDinoForObjectDetection\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        config.two_stage = True\n+        config.auxiliary_loss = True\n+        config.with_box_refine = True\n+\n+        model = model_class(config)\n+        model.to(torch_device)\n+        model.train()\n+        inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+        loss = model(**inputs).loss\n+        loss.backward()\n+\n+    def test_tied_weights_keys(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.tie_word_embeddings = True\n+        for model_class in self.all_model_classes:\n+            model_tied = model_class(config)\n+\n+            ptrs = collections.defaultdict(list)\n+            for name, tensor in model_tied.state_dict().items():\n+                ptrs[id_tensor_storage(tensor)].append(name)\n+\n+            # These are all the pointers of shared tensors.\n+            tied_params = [names for _, names in ptrs.items() if len(names) > 1]\n+\n+            tied_weight_keys = model_tied._tied_weights_keys if model_tied._tied_weights_keys is not None else []\n+            # Detect we get a hit for each key\n+            for key in tied_weight_keys:\n+                if not any(re.search(key, p) for group in tied_params for p in group):\n+                    raise ValueError(f\"{key} is not a tied weight key for {model_class}.\")\n+\n+            # Removed tied weights found from tied params -> there should only be one left after\n+            for key in tied_weight_keys:\n+                for i in range(len(tied_params)):\n+                    tied_params[i] = [p for p in tied_params[i] if re.search(key, p) is None]\n+\n+            # MMGroundingDino when sharing weights also uses the shared ones in MMGroundingDinoDecoder\n+            # Therefore, differently from DeformableDetr, we expect the group lens to be 2\n+            # one for self.bbox_embed in MMGroundingDinoForObejectDetection and another one\n+            # in the decoder\n+            tied_params = [group for group in tied_params if len(group) > 2]\n+            self.assertListEqual(\n+                tied_params,\n+                [],\n+                f\"Missing `_tied_weights_keys` for {model_class}: add all of {tied_params} except one.\",\n+            )\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+def prepare_text():\n+    text = \"a cat.\"\n+    return text\n+\n+\n+@require_timm\n+@require_vision\n+@slow\n+class MMGroundingDinoModelIntegrationTests(unittest.TestCase):\n+    @cached_property\n+    def default_processor(self):\n+        return (\n+            AutoProcessor.from_pretrained(\"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\")\n+            if is_vision_available()\n+            else None\n+        )\n+\n+    def test_inference_object_detection_head(self):\n+        model = MMGroundingDinoForObjectDetection.from_pretrained(\n+            \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\"\n+        ).to(torch_device)\n+\n+        processor = self.default_processor\n+        image = prepare_img()\n+        text = prepare_text()\n+        encoding = processor(images=image, text=text, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**encoding)\n+\n+        expected_shape_logits = torch.Size((1, model.config.num_queries, model.config.d_model))\n+        self.assertEqual(outputs.logits.shape, expected_shape_logits)\n+\n+        expected_boxes = torch.tensor(\n+            [[0.7666, 0.4142, 0.4590], [0.2557, 0.5480, 0.4812], [0.5049, 0.5133, 0.9767]]\n+        ).to(torch_device)\n+        expected_logits = torch.tensor(\n+            [[-5.1160, -0.2143, -0.2089], [-5.0592, -0.4269, -0.4169], [-4.9087, -1.7608, -1.7372]]\n+        ).to(torch_device)\n+\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=1e-3, atol=1e-3)\n+\n+        expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n+        self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=1e-4, atol=1e-4)\n+\n+        # verify postprocessing\n+        results = processor.image_processor.post_process_object_detection(\n+            outputs, threshold=0.35, target_sizes=[(image.height, image.width)]\n+        )[0]\n+        expected_scores = torch.tensor([0.4480, 0.3973]).to(torch_device)\n+        expected_slice_boxes = torch.tensor([343.7321, 23.8182, 637.5044, 373.8593]).to(torch_device)\n+\n+        self.assertEqual(len(results[\"scores\"]), 2)\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes, rtol=1e-2, atol=1e-2)\n+\n+        # verify grounded postprocessing\n+        expected_labels = [\"a cat\", \"a cat\"]\n+        results = processor.post_process_grounded_object_detection(\n+            outputs=outputs,\n+            input_ids=encoding.input_ids,\n+            threshold=0.35,\n+            text_threshold=0.3,\n+            target_sizes=[(image.height, image.width)],\n+        )[0]\n+\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes, rtol=1e-2, atol=1e-2)\n+        self.assertListEqual(results[\"text_labels\"], expected_labels)\n+\n+    @require_torch_accelerator\n+    @is_flaky()\n+    def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n+        processor = self.default_processor\n+        image = prepare_img()\n+        text = prepare_text()\n+        encoding = processor(images=image, text=text, return_tensors=\"pt\")\n+\n+        # 1. run model on CPU\n+        model = MMGroundingDinoForObjectDetection.from_pretrained(\n+            \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\"\n+        )\n+        # HACK: the issue happens during top-k (k=900) after the encoder\n+        # there are some flips between cpu and gpu query ordering (idxs 195<->196 and 267<->268 on my machine)\n+        # which causes different query position embedding assingments\n+        # which in turn significantly changes the decoder pass due to self attention\n+        model.config.num_queries = 100\n+        model.model.query_position_embeddings.weight.data = model.model.query_position_embeddings.weight.data[:100]\n+\n+        with torch.no_grad():\n+            cpu_outputs = model(**encoding)\n+\n+        # 2. run model on GPU\n+        model.to(torch_device)\n+        encoding = encoding.to(torch_device)\n+        with torch.no_grad():\n+            gpu_outputs = model(**encoding)\n+\n+        # 3. assert equivalence\n+        for key in cpu_outputs.keys():\n+            torch.testing.assert_close(cpu_outputs[key], gpu_outputs[key].cpu(), rtol=1e-3, atol=1e-3)\n+\n+        expected_logits = torch.tensor(\n+            [[-5.0188, -1.0069, -1.0005], [-5.1177, -1.0537, -1.0444], [-5.3986, -2.4935, -2.4716]]\n+        )\n+        torch.testing.assert_close(cpu_outputs.logits[0, :3, :3], expected_logits, rtol=1e-3, atol=1e-3)\n+\n+        # assert postprocessing\n+        results_cpu = processor.image_processor.post_process_object_detection(\n+            cpu_outputs, threshold=0.35, target_sizes=[(image.height, image.width)]\n+        )[0]\n+\n+        result_gpu = processor.image_processor.post_process_object_detection(\n+            gpu_outputs, threshold=0.35, target_sizes=[(image.height, image.width)]\n+        )[0]\n+\n+        torch.testing.assert_close(results_cpu[\"scores\"], result_gpu[\"scores\"].cpu(), rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(results_cpu[\"boxes\"], result_gpu[\"boxes\"].cpu(), rtol=1e-3, atol=1e-3)\n+\n+    @is_flaky()\n+    def test_cross_attention_mask(self):\n+        model = MMGroundingDinoForObjectDetection.from_pretrained(\n+            \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\"\n+        ).to(torch_device)\n+        # HACK: the issue happens during top-k (k=900) after the encoder\n+        # there are some flips between cpu and gpu query ordering\n+        # which causes different query position embedding assingments\n+        # which in turn significantly changes the decoder pass due to self attention\n+        model.config.num_queries = 100\n+        model.model.query_position_embeddings.weight.data = model.model.query_position_embeddings.weight.data[:100]\n+\n+        processor = self.default_processor\n+        image = prepare_img()\n+        text1 = \"a cat.\"\n+        text2 = \"a remote control.\"\n+        text_batched = [text1, text2]\n+\n+        encoding1 = processor(images=image, text=text1, return_tensors=\"pt\").to(torch_device)\n+        encoding2 = processor(images=image, text=text2, return_tensors=\"pt\").to(torch_device)\n+        # If we batch the text and cross attention masking is working the batched result should be equal to\n+        # The singe text result\n+        encoding_batched = processor(\n+            images=[image] * len(text_batched), text=text_batched, padding=\"longest\", return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs1 = model(**encoding1)\n+            outputs2 = model(**encoding2)\n+            outputs_batched = model(**encoding_batched)\n+\n+        torch.testing.assert_close(outputs1.logits, outputs_batched.logits[:1], rtol=1e-3, atol=1e-3)\n+        # For some reason 12 elements are > 1e-3, but the rest are fine\n+        self.assertTrue(torch.allclose(outputs2.logits, outputs_batched.logits[1:], atol=1.8e-3))\n+\n+    def test_mm_grounding_dino_loss(self):\n+        ds = load_dataset(\"EduardoPacheco/aquarium-sample\", split=\"train\")\n+        image_processor = self.default_processor.image_processor\n+        tokenizer = self.default_processor.tokenizer\n+        id2label = {0: \"fish\", 1: \"jellyfish\", 2: \"penguins\", 3: \"sharks\", 4: \"puffins\", 5: \"stingrays\", 6: \"starfish\"}\n+        prompt = \". \".join(id2label.values()) + \".\"\n+\n+        text_inputs = tokenizer([prompt, prompt], return_tensors=\"pt\")\n+        image_inputs = image_processor(\n+            images=list(ds[\"image\"]), annotations=list(ds[\"annotations\"]), return_tensors=\"pt\"\n+        )\n+\n+        # Passing auxiliary_loss=True to compare with the expected loss\n+        model = MMGroundingDinoForObjectDetection.from_pretrained(\n+            \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\",\n+            auxiliary_loss=True,\n+        )\n+        # Interested in the loss only\n+        model.eval()\n+        with torch.no_grad():\n+            outputs = model(**text_inputs, **image_inputs)\n+\n+        # Loss differs by CPU and GPU, also this can be changed in future.\n+        expected_loss_dict = {\n+            \"loss_ce\": torch.tensor(1.1799),\n+            \"loss_bbox\": torch.tensor(0.2348),\n+            \"loss_giou\": torch.tensor(0.5834),\n+            \"loss_ce_0\": torch.tensor(1.1199),\n+            \"loss_bbox_0\": torch.tensor(0.3083),\n+            \"loss_giou_0\": torch.tensor(0.6555),\n+            \"loss_ce_1\": torch.tensor(1.2075),\n+            \"loss_bbox_1\": torch.tensor(0.2641),\n+            \"loss_giou_1\": torch.tensor(0.6073),\n+            \"loss_ce_2\": torch.tensor(1.2915),\n+            \"loss_bbox_2\": torch.tensor(0.2616),\n+            \"loss_giou_2\": torch.tensor(0.5730),\n+            \"loss_ce_3\": torch.tensor(1.0243),\n+            \"loss_bbox_3\": torch.tensor(0.2799),\n+            \"loss_giou_3\": torch.tensor(0.6326),\n+            \"loss_ce_4\": torch.tensor(1.2019),\n+            \"loss_bbox_4\": torch.tensor(0.2430),\n+            \"loss_giou_4\": torch.tensor(0.5679),\n+            \"loss_ce_enc\": torch.tensor(10.2381),\n+            \"loss_bbox_enc\": torch.tensor(0.2886),\n+            \"loss_giou_enc\": torch.tensor(0.6335),\n+        }\n+\n+        expected_loss = torch.tensor(52.4340)\n+\n+        for key in expected_loss_dict:\n+            self.assertTrue(torch.allclose(outputs.loss_dict[key], expected_loss_dict[key], atol=1e-3))\n+\n+        self.assertTrue(torch.allclose(outputs.loss, expected_loss, atol=1e-3))"
        },
        {
            "sha": "79904b8c2abd38c97d56a977d9ee1c8b73739f3f",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3951d4ad5d426b3e062e81fb71bf747cdfb357c1/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=3951d4ad5d426b3e062e81fb71bf747cdfb357c1",
            "patch": "@@ -221,6 +221,14 @@\n         \"giou_cost\",\n         \"giou_loss_coefficient\",\n     ],\n+    \"MMGroundingDinoConfig\": [\n+        \"bbox_cost\",\n+        \"bbox_loss_coefficient\",\n+        \"class_cost\",\n+        \"focal_alpha\",\n+        \"giou_cost\",\n+        \"giou_loss_coefficient\",\n+    ],\n     \"RTDetrConfig\": [\n         \"eos_coefficient\",\n         \"focal_loss_alpha\","
        }
    ],
    "stats": {
        "total": 4885,
        "additions": 4884,
        "deletions": 1
    }
}