{
    "author": "gante",
    "message": "[gemma 3] multimodal checkpoints + AutoModelForCausalLM (#36741)",
    "sha": "7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7",
    "files": [
        {
            "sha": "c06017fd76e44a2a6051b519869a014aee83a773",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7",
            "patch": "@@ -444,6 +444,11 @@ def from_config(cls, config, **kwargs):\n             f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\n         )\n \n+    @classmethod\n+    def _prepare_config_for_auto_class(cls, config: PretrainedConfig) -> PretrainedConfig:\n+        \"\"\"Additional autoclass-specific config post-loading manipulation. May be overridden in subclasses.\"\"\"\n+        return config\n+\n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n         config = kwargs.pop(\"config\", None)\n@@ -539,6 +544,10 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n             if kwargs_orig.get(\"quantization_config\", None) is not None:\n                 kwargs[\"quantization_config\"] = kwargs_orig[\"quantization_config\"]\n \n+        # AutoClass-specific config manipulation\n+        config = copy.deepcopy(config)\n+        config = cls._prepare_config_for_auto_class(config)\n+\n         has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\n         has_local_code = type(config) in cls._model_mapping.keys()\n         trust_remote_code = resolve_trust_remote_code("
        },
        {
            "sha": "f08fc2fa04839b92393b1975aa0faa0a933fdd49",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7",
            "patch": "@@ -17,6 +17,7 @@\n import warnings\n from collections import OrderedDict\n \n+from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n from .auto_factory import (\n     _BaseAutoBackboneClass,\n@@ -1658,6 +1659,17 @@ class _AutoModelWithLMHead(_BaseAutoModelClass):\n class AutoModelForCausalLM(_BaseAutoModelClass):\n     _model_mapping = MODEL_FOR_CAUSAL_LM_MAPPING\n \n+    @classmethod\n+    def _prepare_config_for_auto_class(cls, config: PretrainedConfig) -> PretrainedConfig:\n+        \"\"\"\n+        Additional autoclass-specific config post-loading manipulation. In this specific autoclass, if the config has\n+        a nested text decoder section, uses that section instead.\n+\n+        Under the hood, multimodal models mapped by AutoModelForCausalLM assume the text decoder receives its own\n+        config, rather than the config for the whole model. This is used e.g. to load the text-only part of a VLM.\n+        \"\"\"\n+        return config.get_text_config(decoder=True)\n+\n \n AutoModelForCausalLM = auto_class_update(AutoModelForCausalLM, head_doc=\"causal language modeling\")\n "
        },
        {
            "sha": "a5a44330d819e4c97276a597a81a2a2da8be6aa7",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=7c233980f4a8b8d4a8426e2072cb9dcdfaefc4f7",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Gemma3 model.\"\"\"\n \n+import tempfile\n import unittest\n \n import pytest\n@@ -339,6 +340,18 @@ def test_initialization(self):\n     def test_flex_attention_with_grads(self):\n         pass\n \n+    def test_automodelforcausallm(self):\n+        \"\"\"\n+        Regression test for #36741 -- make sure `AutoModelForCausalLM` works with a Gemma3 config, i.e. that\n+        `AutoModelForCausalLM.from_pretrained` pulls the text config before loading the model\n+        \"\"\"\n+        config = self.model_tester.get_config()\n+        model = Gemma3ForConditionalGeneration(config)\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            model.save_pretrained(tmp_dir)\n+            for_causal_lm = AutoModelForCausalLM.from_pretrained(tmp_dir)\n+            self.assertIsInstance(for_causal_lm, Gemma3ForCausalLM)\n+\n \n @slow\n @require_torch_gpu"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 34,
        "deletions": 0
    }
}