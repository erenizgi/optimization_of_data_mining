{
    "author": "SunMarc",
    "message": "Align TP check (#38328)\n\nalign tp check",
    "sha": "051a8acc9a5a004dbaacd7156f77846e52ce9a5f",
    "files": [
        {
            "sha": "5e972b3423630a109cb654ad9a9c1070baf2c45c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/051a8acc9a5a004dbaacd7156f77846e52ce9a5f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/051a8acc9a5a004dbaacd7156f77846e52ce9a5f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=051a8acc9a5a004dbaacd7156f77846e52ce9a5f",
            "patch": "@@ -2084,7 +2084,7 @@ def post_init(self):\n             if plan := getattr(module, \"_tp_plan\", None):\n                 self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.copy().items()})\n \n-        if self._tp_plan is not None and is_torch_greater_or_equal(\"2.5\"):\n+        if self._tp_plan is not None and is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n             for _, v in self._tp_plan.items():\n                 if v not in ALL_PARALLEL_STYLES:\n                     raise ValueError("
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}