{
    "author": "S1ro1",
    "message": "Feat: add warnings for unused keys and rules in tensor parallel (#37893)\n\nFeat: tensor parallel plan verification",
    "sha": "7b5e327c6e4c809c86f5db9f08860291e3401ca8",
    "files": [
        {
            "sha": "5b8f8acb5a60e77a756e72cdd0a5ad3b2bfe5f78",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b5e327c6e4c809c86f5db9f08860291e3401ca8/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b5e327c6e4c809c86f5db9f08860291e3401ca8/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=7b5e327c6e4c809c86f5db9f08860291e3401ca8",
            "patch": "@@ -670,3 +670,34 @@ def shard_and_distribute_module(\n     setattr(module_to_tp, param_type, param)\n     # module_to_tp.load_state_dict({param_type: param}, strict=False, assign=True)\n     return param\n+\n+\n+def verify_tp_plan(expected_keys: list[str], tp_plan: Optional[dict[str, str]]):\n+    \"\"\"\n+    Verify the TP plan of the model, log a warning if the layers that were not sharded and the rules that were not applied.\n+    \"\"\"\n+\n+    if tp_plan is None:\n+        return\n+\n+    generic_keys = {re.sub(r\"\\d+\", \"*\", key) for key in expected_keys}\n+    unsharded_layers = set(generic_keys)\n+    unused_rules = tp_plan\n+\n+    for key in generic_keys:\n+        param_name, _ = key.rsplit(\".\", 1) if \".\" in key else key\n+        generic_param_name = re.sub(r\"\\d+\", \"*\", param_name)\n+\n+        if generic_param_name in tp_plan:\n+            unused_rules.pop(generic_param_name)\n+            unsharded_layers.discard(key)\n+        elif \".\" in generic_param_name and (parent_param_name := generic_param_name.rsplit(\".\", 1)[0]) in tp_plan:\n+            unused_rules.pop(parent_param_name)\n+            unsharded_layers.discard(key)\n+        else:\n+            pass  # we couldn't find the rule for this parameter, so it's not sharded\n+\n+    if len(unused_rules) > 0:\n+        logger.warning(f\"The following TP rules were not applied on any of the layers: {unused_rules}\")\n+    if len(unsharded_layers) > 0:\n+        logger.warning(f\"The following layers were not sharded: {', '.join(unsharded_layers)}\")"
        },
        {
            "sha": "459cd7aca55d0ba0d26ff2aa60a06cfe6c54c6de",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b5e327c6e4c809c86f5db9f08860291e3401ca8/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b5e327c6e4c809c86f5db9f08860291e3401ca8/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=7b5e327c6e4c809c86f5db9f08860291e3401ca8",
            "patch": "@@ -64,6 +64,7 @@\n from .integrations.tensor_parallel import (\n     SUPPORTED_TP_STYLES,\n     shard_and_distribute_module,\n+    verify_tp_plan,\n )\n from .loss.loss_utils import LOSS_MAPPING\n from .pytorch_utils import (  # noqa: F401\n@@ -4974,6 +4975,9 @@ def _load_pretrained_model(\n         if hf_quantizer is not None:\n             expected_keys = hf_quantizer.update_expected_keys(model_to_load, expected_keys, checkpoint_keys)\n \n+        if logger.level >= logging.WARNING:\n+            verify_tp_plan(expected_keys, getattr(model_to_load, \"_tp_plan\", None))\n+\n         # Warmup cuda to load the weights much faster on devices\n         if device_map is not None and not is_hqq_or_quark:\n             expanded_device_map = expand_device_map(device_map, expected_keys)"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 35,
        "deletions": 0
    }
}