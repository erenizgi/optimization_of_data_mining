{
    "author": "aroun-coumar",
    "message": "This PR contains additional changes for #33143 (#33581)\n\n* fix: Fix optimizer bug in ModelCard\r\n\r\n* fix: fix W293\r\n\r\n* Fixes in modelcard.py for issue #33143\r\n\r\n---------\r\n\r\nCo-authored-by: moontidef <53668275+relic-yuexi@users.noreply.github.com>",
    "sha": "326b2bad1c1834b826f5b0460b6e43706e993386",
    "files": [
        {
            "sha": "57cd971dd79a53fb2acc953ba2e920a0073a7e87",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/326b2bad1c1834b826f5b0460b6e43706e993386/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/326b2bad1c1834b826f5b0460b6e43706e993386/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=326b2bad1c1834b826f5b0460b6e43706e993386",
            "patch": "@@ -874,13 +874,17 @@ def extract_hyperparameters_from_trainer(trainer):\n     if total_eval_batch_size != hyperparameters[\"eval_batch_size\"]:\n         hyperparameters[\"total_eval_batch_size\"] = total_eval_batch_size\n \n-    if trainer.args.adafactor:\n-        hyperparameters[\"optimizer\"] = \"Adafactor\"\n-    else:\n-        hyperparameters[\"optimizer\"] = (\n-            f\"Adam with betas=({trainer.args.adam_beta1},{trainer.args.adam_beta2}) and\"\n-            f\" epsilon={trainer.args.adam_epsilon}\"\n-        )\n+    if trainer.args.optim:\n+        optimizer_name = trainer.args.optim\n+        optimizer_args = trainer.args.optim_args if trainer.args.optim_args else \"No additional optimizer arguments\"\n+\n+        if \"adam\" in optimizer_name.lower():\n+            hyperparameters[\"optimizer\"] = (\n+                f\"Use {optimizer_name} with betas=({trainer.args.adam_beta1},{trainer.args.adam_beta2}) and\"\n+                f\" epsilon={trainer.args.adam_epsilon} and optimizer_args={optimizer_args}\"\n+            )\n+        else:\n+            hyperparameters[\"optimizer\"] = f\"Use {optimizer_name} and the args are:\\n{optimizer_args}\"\n \n     hyperparameters[\"lr_scheduler_type\"] = trainer.args.lr_scheduler_type.value\n     if trainer.args.warmup_ratio != 0.0:"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 11,
        "deletions": 7
    }
}