{
    "author": "ArthurZucker",
    "message": "Fixes the BC (#39636)\n\n* fix\n\n* update\n\n* Update src/transformers/utils/generic.py\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\n\n* fixup\n\n* fixes\n\n* fix more models\n\n* fix fix fix\n\n* add embedding to more models\n\n* update\n\n* update\n\n* fix\n\n---------\n\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>",
    "sha": "4f17bf0572fbeb380db0b4e95c54b28e7b4bc52b",
    "files": [
        {
            "sha": "e13d3bbe287542800a3d080d97d4cd59e0e0d945",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f17bf0572fbeb380db0b4e95c54b28e7b4bc52b/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f17bf0572fbeb380db0b4e95c54b28e7b4bc52b/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=4f17bf0572fbeb380db0b4e95c54b28e7b4bc52b",
            "patch": "@@ -1032,6 +1032,8 @@ def wrapper(self, *args, **kwargs):\n         def make_capture_wrapper(module, orig_forward, key, index):\n             @wraps(orig_forward)\n             def wrapped_forward(*args, **kwargs):\n+                if key == \"hidden_states\" and len(collected_outputs[key]) == 0:\n+                    collected_outputs[key] += (args[0],)\n                 output = orig_forward(*args, **kwargs)\n                 if not isinstance(output, tuple):\n                     collected_outputs[key] += (output,)\n@@ -1065,18 +1067,19 @@ def wrapped_forward(*args, **kwargs):\n                         monkey_patched_layers.append((module, original_forward))\n \n         outputs = func(self, *args, **kwargs)\n-\n         # Restore original forward methods\n         for module, original_forward in monkey_patched_layers:\n             module.forward = original_forward\n \n         # Inject collected outputs into model output\n         for key in collected_outputs:\n             if key == \"hidden_states\":\n+                collected_outputs[key] = collected_outputs[key][:-1]\n                 if hasattr(outputs, \"vision_hidden_states\"):\n                     collected_outputs[key] += (outputs.vision_hidden_states,)\n                 elif hasattr(outputs, \"last_hidden_state\"):\n                     collected_outputs[key] += (outputs.last_hidden_state,)\n+\n                 outputs[key] = collected_outputs[key]\n             elif key == \"attentions\":\n                 if isinstance(capture_flags[key], list) and len(capture_flags[key]) == 2:"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 4,
        "deletions": 1
    }
}