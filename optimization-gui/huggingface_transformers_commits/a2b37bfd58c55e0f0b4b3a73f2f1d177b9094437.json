{
    "author": "akintunero",
    "message": "Fix typo: 'casual' -> 'causal' in code and documentation (#40371) (#40407)",
    "sha": "a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
    "files": [
        {
            "sha": "b5ce450a9c7fd9cb843840ff2383bee80b137de7",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -394,7 +394,7 @@ def from_encoder_decoder_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "4a27c23c3c69ae928c73273c9397d5f5aad2b1c0",
            "filename": "src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -841,7 +841,7 @@ def from_encoder_decoder_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "7e5343d200499e1f3b8ba26f8d70924c2999a2fc",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -392,7 +392,7 @@ def from_encoder_decoder_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "2c084ee114d762de16d11710cb05ec49d41e3676",
            "filename": "src/transformers/models/mistral/modeling_flax_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -239,8 +239,8 @@ def setup(self):\n         self.k_proj = nn.Dense(self.num_key_value_heads * self.head_dim, use_bias=False, dtype=self.dtype)\n         self.v_proj = nn.Dense(self.num_key_value_heads * self.head_dim, use_bias=False, dtype=self.dtype)\n         self.o_proj = nn.Dense(self.hidden_size, use_bias=False, dtype=self.dtype)\n-        casual_mask = make_causal_mask(jnp.ones((1, config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\")\n-        self.causal_mask = jnp.triu(casual_mask, k=-(config.sliding_window or 0))\n+        causal_mask = make_causal_mask(jnp.ones((1, config.max_position_embeddings), dtype=\"bool\"), dtype=\"bool\")\n+        self.causal_mask = jnp.triu(causal_mask, k=-(config.sliding_window or 0))\n         self.rotary_emb = FlaxMistralRotaryEmbedding(self.config, dtype=self.dtype)\n \n     def _split_heads(self, hidden_states, num_heads):"
        },
        {
            "sha": "c8df024e4e5adea314264310d380734c7490a83a",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -1606,7 +1606,7 @@ def from_sub_models_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {text_encoder_pretrained_model_name_or_path} as a text_encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False\n@@ -1633,7 +1633,7 @@ def from_sub_models_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {audio_encoder_pretrained_model_name_or_path} as an audio_encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "28ef14ee15fe117de8ac26a14301b1b6b1d9e9e2",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -1492,7 +1492,7 @@ def from_sub_models_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {text_encoder_pretrained_model_name_or_path} as a text_encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False\n@@ -1519,7 +1519,7 @@ def from_sub_models_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {audio_encoder_pretrained_model_name_or_path} as an audio_encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "3614c5d4981b4e010f9ba7732ad537865357046d",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -867,7 +867,7 @@ def from_encoder_decoder_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "9a519cd9a5dcc63c75b7befabb57dad801f65a92",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -263,7 +263,7 @@ def from_encoder_decoder_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "a59c799cc04afa51c0f2156baa06a88683f1dbf6",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -806,7 +806,7 @@ def from_encoder_decoder_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "ef2ea2109987beabf5f8dc7b758d055f0a543bbf",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -393,7 +393,7 @@ def from_encoder_decoder_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "d9f2c593a026933df8999d361020e7ad72ee9a7f",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -382,7 +382,7 @@ def from_encoder_decoder_pretrained(\n                 if encoder_config.is_decoder is True or encoder_config.add_cross_attention is True:\n                     logger.info(\n                         f\"Initializing {encoder_pretrained_model_name_or_path} as a encoder model \"\n-                        \"from a decoder model. Cross-attention and casual mask are disabled.\"\n+                        \"from a decoder model. Cross-attention and causal mask are disabled.\"\n                     )\n                     encoder_config.is_decoder = False\n                     encoder_config.add_cross_attention = False"
        },
        {
            "sha": "04d81393cf4a3bd5ce2f4accf5be2eb7441cb7e4",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -328,7 +328,7 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_for_casual_lm(self):\n+    def test_for_causal_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_causal_lm(*config_and_inputs)\n "
        },
        {
            "sha": "ebe72705bbd7a347bac6a5e31dac53e01b423e1e",
            "filename": "tests/models/cpmant/test_modeling_cpmant.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -192,7 +192,7 @@ def test_inference_masked_lm(self):\n @require_torch\n class CpmAntForCausalLMlIntegrationTest(unittest.TestCase):\n     @tooslow\n-    def test_inference_casual(self):\n+    def test_inference_causal(self):\n         texts = \"今天天气真好！\"\n         model_path = \"openbmb/cpm-ant-10b\"\n         model = CpmAntForCausalLM.from_pretrained(model_path)"
        },
        {
            "sha": "cc78f7bf7c1d8686f29ead4a9fe58ef1d6348664",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -303,7 +303,7 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_for_casual_lm(self):\n+    def test_for_causal_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_causal_lm(*config_and_inputs)\n "
        },
        {
            "sha": "866c40c466641f6c10b368ae59365d75ca4a3e60",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -373,7 +373,7 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_for_casual_lm(self):\n+    def test_for_causal_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_causal_lm(*config_and_inputs)\n "
        },
        {
            "sha": "7af18a15b3e8541e0cba29d65e1aaa2174570e83",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=a2b37bfd58c55e0f0b4b3a73f2f1d177b9094437",
            "patch": "@@ -335,7 +335,7 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    def test_for_casual_lm(self):\n+    def test_for_causal_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_causal_lm(*config_and_inputs)\n "
        }
    ],
    "stats": {
        "total": 38,
        "additions": 19,
        "deletions": 19
    }
}