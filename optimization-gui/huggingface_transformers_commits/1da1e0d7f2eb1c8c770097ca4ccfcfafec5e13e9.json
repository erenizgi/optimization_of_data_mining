{
    "author": "Cyrilvallez",
    "message": "Support for easier multimodal use of modular (#35056)\n\n* update modular and add examples\r\n\r\n* style\r\n\r\n* improve example comments\r\n\r\n* style\r\n\r\n* fix small logic issue for imports\r\n\r\n* fix relative order issue when files do not make sense\r\n\r\n* Improve comments\r\n\r\n* trigger CIs",
    "sha": "1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
    "files": [
        {
            "sha": "a64eb17861a1c2afc5f3118513b26583605d5975",
            "filename": "examples/modular-transformers/image_processing_new_imgproc_model.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fimage_processing_new_imgproc_model.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -36,7 +36,7 @@\n \n class ImgprocModelImageProcessor(BaseImageProcessor):\n     r\"\"\"\n-    Constructs a NEW_IMGPROC_MODEL image processor.\n+    Constructs a IMGPROC_MODEL image processor.\n \n     Args:\n         do_resize (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "d6c16c697437d8844c9ce818eac98dbfedd1de5f",
            "filename": "examples/modular-transformers/modeling_from_uppercase_model.py",
            "status": "added",
            "additions": 357,
            "deletions": 0,
            "changes": 357,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -0,0 +1,357 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_from_uppercase_model.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_from_uppercase_model.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from typing import Optional, Tuple\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...pytorch_utils import is_torch_greater_or_equal_than_2_2\n+from ...utils import is_flash_attn_2_available, is_flash_attn_greater_or_equal_2_10, logging\n+from .configuration_from_uppercase_model import FromUppercaseModelConfig\n+\n+\n+if is_flash_attn_2_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class FromUppercaseModelAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+\n+    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n+        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        causal_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        bsz, tgt_len, embed_dim = hidden_states.size()\n+\n+        # get query proj\n+        query_states = self.q_proj(hidden_states) * self.scale\n+        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+\n+        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n+        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        key_states = key_states.view(*proj_shape)\n+        value_states = value_states.view(*proj_shape)\n+\n+        src_len = key_states.size(1)\n+        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n+\n+        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n+            raise ValueError(\n+                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n+                f\" {attn_weights.size()}\"\n+            )\n+\n+        # apply the causal_attention_mask first\n+        if causal_attention_mask is not None:\n+            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n+                raise ValueError(\n+                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n+                    f\" {causal_attention_mask.size()}\"\n+                )\n+            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n+            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n+\n+        if attention_mask is not None:\n+            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n+                raise ValueError(\n+                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n+                )\n+            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n+            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n+\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+        if output_attentions:\n+            # this operation is a bit akward, but it's required to\n+            # make sure that attn_weights keeps its gradient.\n+            # In order to do so, attn_weights have to reshaped\n+            # twice and have to be reused in the following\n+            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n+            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        else:\n+            attn_weights_reshaped = None\n+\n+        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+\n+        attn_output = torch.bmm(attn_probs, value_states)\n+\n+        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights_reshaped\n+\n+\n+class FromUppercaseModelFlashAttention2(FromUppercaseModelAttention):\n+    \"\"\"\n+    FromUppercaseModelAttention flash attention module. This module inherits from `FromUppercaseModelAttention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        causal_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        output_attentions = False\n+\n+        batch_size, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # Flash attention requires the input to have the shape\n+        # batch_size x seq_length x head_dim x hidden_dim\n+        # therefore we just need to keep the original shape\n+        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+\n+        dropout_rate = self.dropout if self.training else 0.0\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32.\n+\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            dropout=dropout_rate,\n+            is_causal=causal_attention_mask is not None,\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights\n+\n+\n+class FromUppercaseModelSdpaAttention(FromUppercaseModelAttention):\n+    \"\"\"\n+    SDPA attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `FromUppercaseModelAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from FromUppercaseModelAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        causal_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"FromUppercaseModelModel is using FromUppercaseModelSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not \"\n+                \"support `output_attentions=True`. Falling back to the manual attention implementation, but specifying \"\n+                \"the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can \"\n+                'be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                causal_attention_mask=causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+        # FROM_UPPERCASE_MODEL text model uses both `causal_attention_mask` and `attention_mask`\n+        if attention_mask is not None and causal_attention_mask is not None:\n+            attn_mask = attention_mask + causal_attention_mask\n+        elif causal_attention_mask is not None:\n+            attn_mask = causal_attention_mask\n+        else:\n+            attn_mask = attention_mask\n+\n+        bsz, tgt_len, embed_dim = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if not is_torch_greater_or_equal_than_2_2 and query_states.device.type == \"cuda\" and attn_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # FROM_UPPERCASE_MODEL text model uses both `causal_attention_mask` and `attention_mask` sequentially.\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=attn_mask,\n+            dropout_p=self.dropout if self.training else 0.0,\n+            scale=self.scale,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, None\n+\n+\n+class FromUppercaseModelMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+FROM_UPPERCASE_MODEL_ATTENTION_CLASSES = {\n+    \"eager\": FromUppercaseModelAttention,\n+    \"sdpa\": FromUppercaseModelSdpaAttention,\n+    \"flash_attention_2\": FromUppercaseModelFlashAttention2,\n+}\n+\n+\n+class FromUppercaseModelEncoderLayer(nn.Module):\n+    def __init__(self, config: FromUppercaseModelConfig):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.self_attn = FROM_UPPERCASE_MODEL_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = FromUppercaseModelMLP(config)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        causal_attention_mask: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.FloatTensor]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+                `(config.encoder_attention_heads,)`.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            causal_attention_mask=causal_attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs"
        },
        {
            "sha": "02876996e0e5cdf9311e9de81c382374ba71f3fe",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "added",
            "additions": 1017,
            "deletions": 0,
            "changes": 1017,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -0,0 +1,1017 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_multimodal1.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_multimodal1.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import math\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_greater_or_equal_2_10,\n+    logging,\n+)\n+from .configuration_multimodal1 import Multimodal1TextConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Multimodal1TextRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Multimodal1TextRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Multimodal1TextRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        dim=None,\n+        max_position_embeddings=2048,\n+        base=10000,\n+        device=None,\n+        scaling_factor=1.0,\n+        rope_type=\"default\",\n+        config: Optional[Multimodal1TextConfig] = None,\n+    ):\n+        super().__init__()\n+        # TODO (joao): remove the `if` below, only used for BC\n+        self.rope_kwargs = {}\n+        if config is None:\n+            logger.warning_once(\n+                \"`Multimodal1TextRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n+            )\n+            self.rope_kwargs = {\n+                \"rope_type\": rope_type,\n+                \"factor\": scaling_factor,\n+                \"dim\": dim,\n+                \"base\": base,\n+                \"max_position_embeddings\": max_position_embeddings,\n+            }\n+            self.rope_type = rope_type\n+            self.max_seq_len_cached = max_position_embeddings\n+            self.original_max_seq_len = max_position_embeddings\n+        else:\n+            # BC: \"rope_type\" was originally \"type\"\n+            if config.rope_scaling is not None:\n+                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            else:\n+                self.rope_type = \"default\"\n+            self.max_seq_len_cached = config.max_position_embeddings\n+            self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class Multimodal1TextMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+class Multimodal1TextAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Multimodal1TextConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = getattr(config, \"head_dim\", self.hidden_size // self.num_heads)\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n+        self.rope_theta = config.rope_theta\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n+\n+        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n+        self.rotary_emb = Multimodal1TextRotaryEmbedding(config=self.config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n+\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            attn_weights = attn_weights + causal_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class Multimodal1TextFlashAttention2(Multimodal1TextAttention):\n+    \"\"\"\n+    Multimodal1Text flash attention module. This module inherits from `Multimodal1TextAttention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if isinstance(past_key_value, StaticCache):\n+            raise ValueError(\n+                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n+                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n+            )\n+\n+        output_attentions = False\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # Flash attention requires the input to have the shape\n+        # batch_size x seq_length x head_dim x hidden_dim\n+        # therefore we just need to keep the original shape\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        dropout_rate = self.attention_dropout if self.training else 0.0\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32. (Multimodal1TextRMSNorm handles it correctly)\n+\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            position_ids=position_ids,\n+            dropout=dropout_rate,\n+            sliding_window=getattr(self, \"sliding_window\", None),\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            is_causal=self.is_causal,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+class Multimodal1TextSdpaAttention(Multimodal1TextAttention):\n+    \"\"\"\n+    Multimodal1Text attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `Multimodal1TextAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from Multimodal1TextAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"Multimodal1TextModel is using Multimodal1TextSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+\n+        if position_embeddings is None:\n+            logger.warning_once(\n+                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n+                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n+                \"removed and `position_embeddings` will be mandatory.\"\n+            )\n+            cos, sin = self.rotary_emb(value_states, position_ids)\n+        else:\n+            cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n+MULTIMODAL1_TEXT_ATTENTION_CLASSES = {\n+    \"eager\": Multimodal1TextAttention,\n+    \"flash_attention_2\": Multimodal1TextFlashAttention2,\n+    \"sdpa\": Multimodal1TextSdpaAttention,\n+}\n+\n+\n+class Multimodal1TextDecoderLayer(nn.Module):\n+    def __init__(self, config: Multimodal1TextConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = MULTIMODAL1_TEXT_ATTENTION_CLASSES[config._attn_implementation](\n+            config=config, layer_idx=layer_idx\n+        )\n+\n+        self.mlp = Multimodal1TextMLP(config)\n+        self.input_layernorm = Multimodal1TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Multimodal1TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs,\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (present_key_value,)\n+\n+        return outputs\n+\n+\n+MULTIMODAL1_TEXT_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Multimodal1TextConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Multimodal1Text Model outputting raw hidden-states without any specific head on top.\",\n+    MULTIMODAL1_TEXT_START_DOCSTRING,\n+)\n+class Multimodal1TextPreTrainedModel(PreTrainedModel):\n+    config_class = Multimodal1TextConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Multimodal1TextDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+MULTIMODAL1_TEXT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Multimodal1Text Model outputting raw hidden-states without any specific head on top.\",\n+    MULTIMODAL1_TEXT_START_DOCSTRING,\n+)\n+class Multimodal1TextModel(Multimodal1TextPreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Multimodal1TextDecoderLayer`]\n+\n+    Args:\n+        config: Multimodal1TextConfig\n+    \"\"\"\n+\n+    def __init__(self, config: Multimodal1TextConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Multimodal1TextDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Multimodal1TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Multimodal1TextRotaryEmbedding(config=config)\n+\n+        self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @add_start_docstrings_to_model_forward(MULTIMODAL1_TEXT_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        # kept for BC (non `Cache` `past_key_values` inputs)\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            return_legacy_cache = True\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n+            else:\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+                logger.warning_once(\n+                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n+                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n+                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n+                )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        next_decoder_cache = None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    decoder_layer.__call__,\n+                    hidden_states,\n+                    causal_mask,\n+                    position_ids,\n+                    past_key_values,\n+                    output_attentions,\n+                    use_cache,\n+                    cache_position,\n+                    position_embeddings,\n+                )\n+            else:\n+                layer_outputs = decoder_layer(\n+                    hidden_states,\n+                    attention_mask=causal_mask,\n+                    position_ids=position_ids,\n+                    past_key_value=past_key_values,\n+                    output_attentions=output_attentions,\n+                    use_cache=use_cache,\n+                    cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if use_cache:\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask"
        },
        {
            "sha": "b10b11b671af95ea8109622fda1fd4fe1523e8a3",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "added",
            "additions": 705,
            "deletions": 0,
            "changes": 705,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -0,0 +1,705 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_multimodal2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_multimodal2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from transformers.utils import add_start_docstrings\n+\n+from ...activations import ACT2FN\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import is_torch_greater_or_equal_than_2_2\n+from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_2_available,\n+    is_flash_attn_greater_or_equal_2_10,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n+from .configuration_multimodal2 import Multimodal2Config, Multimodal2VisionConfig\n+\n+\n+if is_flash_attn_2_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Multimodal2VisionAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+\n+    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n+        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        causal_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        bsz, tgt_len, embed_dim = hidden_states.size()\n+\n+        # get query proj\n+        query_states = self.q_proj(hidden_states) * self.scale\n+        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+\n+        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n+        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n+        key_states = key_states.view(*proj_shape)\n+        value_states = value_states.view(*proj_shape)\n+\n+        src_len = key_states.size(1)\n+        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n+\n+        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n+            raise ValueError(\n+                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n+                f\" {attn_weights.size()}\"\n+            )\n+\n+        # apply the causal_attention_mask first\n+        if causal_attention_mask is not None:\n+            if causal_attention_mask.size() != (bsz, 1, tgt_len, src_len):\n+                raise ValueError(\n+                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is\"\n+                    f\" {causal_attention_mask.size()}\"\n+                )\n+            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + causal_attention_mask\n+            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n+\n+        if attention_mask is not None:\n+            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n+                raise ValueError(\n+                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n+                )\n+            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n+            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n+\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+        if output_attentions:\n+            # this operation is a bit akward, but it's required to\n+            # make sure that attn_weights keeps its gradient.\n+            # In order to do so, attn_weights have to reshaped\n+            # twice and have to be reused in the following\n+            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n+            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n+        else:\n+            attn_weights_reshaped = None\n+\n+        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+\n+        attn_output = torch.bmm(attn_probs, value_states)\n+\n+        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights_reshaped\n+\n+\n+class Multimodal2VisionSdpaAttention(Multimodal2VisionAttention):\n+    \"\"\"\n+    SDPA attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `Multimodal2VisionAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from Multimodal2VisionAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        causal_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"Multimodal2VisionModel is using Multimodal2VisionSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not \"\n+                \"support `output_attentions=True`. Falling back to the manual attention implementation, but specifying \"\n+                \"the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can \"\n+                'be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                causal_attention_mask=causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+        # MULTIMODAL2_VISION text model uses both `causal_attention_mask` and `attention_mask`\n+        if attention_mask is not None and causal_attention_mask is not None:\n+            attn_mask = attention_mask + causal_attention_mask\n+        elif causal_attention_mask is not None:\n+            attn_mask = causal_attention_mask\n+        else:\n+            attn_mask = attention_mask\n+\n+        bsz, tgt_len, embed_dim = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if not is_torch_greater_or_equal_than_2_2 and query_states.device.type == \"cuda\" and attn_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # MULTIMODAL2_VISION text model uses both `causal_attention_mask` and `attention_mask` sequentially.\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=attn_mask,\n+            dropout_p=self.dropout if self.training else 0.0,\n+            scale=self.scale,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, None\n+\n+\n+class Multimodal2VisionFlashAttention2(Multimodal2VisionAttention):\n+    \"\"\"\n+    Multimodal2VisionAttention flash attention module. This module inherits from `Multimodal2VisionAttention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        causal_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        output_attentions = False\n+\n+        batch_size, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # Flash attention requires the input to have the shape\n+        # batch_size x seq_length x head_dim x hidden_dim\n+        # therefore we just need to keep the original shape\n+        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+\n+        dropout_rate = self.dropout if self.training else 0.0\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32.\n+\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            dropout=dropout_rate,\n+            is_causal=causal_attention_mask is not None,\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights\n+\n+\n+class Multimodal2VisionMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+MULTIMODAL2_VISION_ATTENTION_CLASSES = {\n+    \"eager\": Multimodal2VisionAttention,\n+    \"sdpa\": Multimodal2VisionSdpaAttention,\n+    \"flash_attention_2\": Multimodal2VisionFlashAttention2,\n+}\n+\n+\n+class Multimodal2VisionEncoderLayer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.self_attn = MULTIMODAL2_VISION_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = Multimodal2VisionMLP(config)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        causal_attention_mask: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.FloatTensor]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+                `(config.encoder_attention_heads,)`.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            causal_attention_mask=causal_attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class Multimodal2VisionEncoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`Multimodal2VisionEncoderLayer`].\n+\n+    Args:\n+        config: Multimodal2VisionConfig\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([Multimodal2VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        causal_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Causal mask for the text model. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        hidden_states = inputs_embeds\n+        for idx, encoder_layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    causal_attention_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    causal_attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+        )\n+\n+\n+class Multimodal2VisionEmbeddings(nn.Module):\n+    def __init__(self, config: Multimodal2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.image_size = config.image_size\n+        self.patch_size = config.patch_size\n+\n+        self.class_embedding = nn.Parameter(torch.randn(self.embed_dim))\n+\n+        self.patch_embedding = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=self.embed_dim,\n+            kernel_size=self.patch_size,\n+            stride=self.patch_size,\n+            bias=False,\n+        )\n+\n+        self.num_patches = (self.image_size // self.patch_size) ** 2\n+        self.num_positions = self.num_patches + 1\n+        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n+        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embedding(self.position_ids)\n+\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model\" f\" ({self.image_size}*{self.image_size}).\"\n+            )\n+        target_dtype = self.patch_embedding.weight.dtype\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n+        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n+\n+        class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n+        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n+        return embeddings\n+\n+\n+MULTIMODAL2_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`Multimodal2ImageProcessor.__call__`] for details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n+            Whether to interpolate the pre-trained position encodings.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+class Multimodal2VisionTransformer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = Multimodal2VisionEmbeddings(config)\n+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.encoder = Multimodal2VisionEncoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+\n+    @add_start_docstrings_to_model_forward(MULTIMODAL2_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Multimodal2VisionConfig)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n+        hidden_states = self.pre_layrnorm(hidden_states)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        if not return_dict:\n+            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class Multimodal2VisionPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = Multimodal2Config\n+    base_model_prefix = \"multimodal2_vision\"\n+    supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, Multimodal2VisionMLP):\n+            pass\n+\n+\n+MULTIMODAL2_VISION_START_DOCSTRING = \"doc\"\n+\n+\n+@add_start_docstrings(\"New doc\", MULTIMODAL2_VISION_START_DOCSTRING)\n+class Multimodal2VisionModel(Multimodal2VisionPreTrainedModel):\n+    config_class = Multimodal2VisionConfig\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\"Multimodal2VisionEncoderLayer\"]\n+\n+    def __init__(self, config: Multimodal2VisionConfig):\n+        super().__init__(config)\n+        self.vision_model = Multimodal2VisionTransformer(config)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_model.embeddings.patch_embedding\n+\n+    @add_start_docstrings_to_model_forward(MULTIMODAL2_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Multimodal2VisionConfig)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Multimodal2VisionModel\n+\n+        >>> model = Multimodal2VisionModel.from_pretrained(\"openai/multimodal2-vit-base-patch32\")\n+        >>> processor = AutoProcessor.from_pretrained(\"openai/multimodal2-vit-base-patch32\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        return self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )"
        },
        {
            "sha": "d303d328e887d6778f8bf423ef735885d8c080a6",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -265,7 +265,7 @@ def _update_causal_mask(\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = inputs_embeds.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -358,9 +358,9 @@ def forward(\n         ```python\n         >>> from PIL import Image\n         >>> import requests\n-        >>> from transformers import AutoProcessor, NewTaskModelForConditionalGeneration\n+        >>> from transformers import AutoProcessor, NewTaskModelForNewTask\n \n-        >>> model = NewTaskModelForConditionalGeneration.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n+        >>> model = NewTaskModelForNewTask.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n         >>> processor = AutoProcessor.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n \n         >>> prompt = \"answer en Where is the cow standing?\""
        },
        {
            "sha": "ef3044e7ee2cf023f0d23fed2ea4da678f06b98d",
            "filename": "examples/modular-transformers/modular_from_uppercase_model.py",
            "status": "added",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodular_from_uppercase_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodular_from_uppercase_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_from_uppercase_model.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -0,0 +1,6 @@\n+from transformers.models.clip.modeling_clip import CLIPEncoderLayer\n+\n+\n+# Check if we can correctly grab dependencies with correct naming from all UPPERCASE old model\n+class FromUppercaseModelEncoderLayer(CLIPEncoderLayer):\n+    pass"
        },
        {
            "sha": "8f8eaf91a371065beff347e9b5f1c7f58f1b614b",
            "filename": "examples/modular-transformers/modular_multimodal1.py",
            "status": "added",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodular_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodular_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_multimodal1.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -0,0 +1,6 @@\n+from transformers.models.llama.modeling_llama import LlamaModel\n+\n+\n+# Check that we can correctly change the prefix (here add Text part at the end of the name)\n+class Multimodal1TextModel(LlamaModel):\n+    pass"
        },
        {
            "sha": "bc11e0b2869fb9ab758347f49ee145560120dafb",
            "filename": "examples/modular-transformers/modular_multimodal2.py",
            "status": "added",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodular_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/examples%2Fmodular-transformers%2Fmodular_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_multimodal2.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -0,0 +1,88 @@\n+\"\"\"\n+Here, because clip is not consistent with the use of the \"Text\" and \"Vision\" prefixes, we cannot simply use\n+```\n+class Multimodal2VisionModel(CLIPVisionModel):\n+    pass\n+```\n+with the hope that all dependencies will be renamed as `Multimodal2VisionClass`. For this reason, if we want consistency and\n+use the \"Vision\" part everywhere, we need to overwrite the intermediate classes and add the prefix everytime.\n+This adds noise to the modular, but is unfortunately unavoidable.\n+\"\"\"\n+\n+from torch import nn\n+\n+from transformers.models.clip.modeling_clip import (\n+    CLIPMLP,\n+    CLIPAttention,\n+    CLIPEncoder,\n+    CLIPEncoderLayer,\n+    CLIPFlashAttention2,\n+    CLIPPreTrainedModel,\n+    CLIPSdpaAttention,\n+    CLIPVisionModel,\n+    CLIPVisionTransformer,\n+)\n+from transformers.utils import add_start_docstrings\n+\n+\n+class Multimodal2VisionAttention(CLIPAttention):\n+    pass\n+\n+\n+# Check that adding the second base class correctly set the parent, even though in clip it does not have the \"Vision\" part\n+class Multimodal2VisionSdpaAttention(CLIPSdpaAttention, Multimodal2VisionAttention):\n+    pass\n+\n+\n+# Check that adding the second base class correctly set the parent, even though in clip it does not have the \"Vision\" part\n+class Multimodal2VisionFlashAttention2(CLIPFlashAttention2, Multimodal2VisionAttention):\n+    pass\n+\n+\n+MULTIMODAL2_VISION_ATTENTION_CLASSES = {\n+    \"eager\": Multimodal2VisionAttention,\n+    \"sdpa\": Multimodal2VisionSdpaAttention,\n+    \"flash_attention_2\": Multimodal2VisionFlashAttention2,\n+}\n+\n+\n+class Multimodal2VisionMLP(CLIPMLP):\n+    pass\n+\n+\n+class Multimodal2VisionEncoderLayer(CLIPEncoderLayer):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.self_attn = MULTIMODAL2_VISION_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.mlp = Multimodal2VisionMLP(config)\n+\n+\n+class Multimodal2VisionEncoder(CLIPEncoder):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList([Multimodal2VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+\n+\n+# Finally here the `Vision` part was correct in CLIP, but we still need to tell it that the encoder arg should use it as well\n+class Multimodal2VisionTransformer(CLIPVisionTransformer):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.encoder = Multimodal2VisionEncoder(config)\n+\n+\n+class Multimodal2VisionPreTrainedModel(CLIPPreTrainedModel):\n+    def _init_weights(self, module):\n+        if isinstance(module, Multimodal2VisionMLP):\n+            pass\n+\n+\n+MULTIMODAL2_VISION_START_DOCSTRING = \"doc\"\n+\n+\n+# Here the only arg `self.vision_model = CLIPVisionTransformer(config)` in CLIPVisionModel already has the \"Vision\" part, so\n+# no need to overwrite it, it will look for `Multimodal2VisionTransformer` which has already being redefined above\n+# Note: we may want to redefine decorator as well for full consistency, as CLIP does not use \"CLIP_VISION_START_DOCSTRING\" but only\n+# \"CLIP_START_DOCSTRING\"\n+@add_start_docstrings(\"New doc\", MULTIMODAL2_VISION_START_DOCSTRING)\n+class Multimodal2VisionModel(CLIPVisionModel, Multimodal2VisionPreTrainedModel):\n+    _no_split_modules = [\"Multimodal2VisionEncoderLayer\"]"
        },
        {
            "sha": "346f386ba698f273fc209ce6f3caff62cdffcbb2",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -20,7 +20,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n from ...configuration_utils import PretrainedConfig\n \n "
        },
        {
            "sha": "58836a5631c2c02ed9c4b99a648875a41f3eb5bd",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -27,7 +27,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache\n from ...generation import GenerationMixin\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,"
        },
        {
            "sha": "c042669e1ed5c3070dfdffafde888848517f2199",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -14,7 +14,6 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import ("
        },
        {
            "sha": "5ecffc8719bec904d797aafb6a8e22f72a25a6cf",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -34,7 +34,6 @@\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,"
        },
        {
            "sha": "cf1a0cfd95ca0ad46681a5623859cc2c8a716bb1",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 242,
            "deletions": 96,
            "changes": 338,
            "blob_url": "https://github.com/huggingface/transformers/blob/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=1da1e0d7f2eb1c8c770097ca4ccfcfafec5e13e9",
            "patch": "@@ -18,7 +18,7 @@\n import os\n import re\n from abc import ABC, abstractmethod\n-from collections import defaultdict, deque\n+from collections import Counter, defaultdict, deque\n from typing import Dict, Set\n \n import libcst as cst\n@@ -48,7 +48,7 @@ def get_module_source_from_name(module_name: str) -> str:\n     # Extract the source code from the module name\n     spec = importlib.util.find_spec(module_name)\n     if spec is None or spec.origin is None:\n-        return f\"Module {module_name} not found\"\n+        raise ValueError(f\"Cannot open file associated with {module_name} module.\")\n \n     with open(spec.origin, \"r\", encoding=\"utf-8\") as file:\n         source_code = file.read()\n@@ -58,20 +58,40 @@ def get_module_source_from_name(module_name: str) -> str:\n def preserve_case_replace(text, patterns: dict, default_name: str):\n     # Create a regex pattern to match all variations\n     regex_pattern = \"|\".join(re.escape(key) for key in patterns.keys())\n-    compiled_regex = re.compile(regex_pattern, re.IGNORECASE)\n+    compiled_regex = re.compile(f\"({regex_pattern})(.|$)\", re.IGNORECASE | re.DOTALL)\n \n     def replace(match):\n-        word = match.group(0)\n-        result = patterns.get(word, default_name)\n-        return result\n+        matched_pattern = match.group(1)\n+        next_char = match.group(2)\n+        new_pattern = patterns.get(matched_pattern, default_name)\n+\n+        # In this case, the cased old model did not respect CamelCase and was all UPPERCASE, so we need to rely on next char\n+        # The heuristic is: if next char is not a letter, then it is not part of a model name and result should be `new_name`.upper()\n+        if len(patterns) == 2 and matched_pattern.isupper():\n+            if not next_char.isalpha():\n+                # `new_name.upper()` is just the other entry for `matched_pattern.lower()`, uppercased\n+                new_pattern = patterns[matched_pattern.lower()].upper()\n+\n+        return new_pattern + next_char\n \n     return compiled_regex.sub(replace, text)\n \n \n-def convert_to_camelcase(text, old_name: str, default_old_name: str):\n-    # Regex pattern to match consecutive uppercase letters and lowercase the first set\n-    result = re.sub(rf\"^({old_name})(?=[a-z]+)\", lambda m: default_old_name, text, flags=re.IGNORECASE, count=1)\n-    return result\n+def get_cased_name(lowercase_name: str) -> str:\n+    \"\"\"From a model name in lowercase in the format `my_model`, return the cased name in the format `MyModel`.\"\"\"\n+    if lowercase_name in CONFIG_MAPPING_NAMES:\n+        return CONFIG_MAPPING_NAMES[lowercase_name].replace(\"Config\", \"\")\n+    else:\n+        return \"\".join(x.title() for x in lowercase_name.split(\"_\"))\n+\n+\n+def get_lowercase_name(cased_name: str) -> str:\n+    \"\"\"From a model name in Camelcase in the format `MyModel`, return the lowercase name in the format `my_model`.\"\"\"\n+    inverse_mapping = {value: key for key, value in CONFIG_MAPPING_NAMES.items()}\n+    if cased_name + \"Config\" in inverse_mapping:\n+        return inverse_mapping[cased_name + \"Config\"]\n+    else:\n+        return \"_\".join([s.lower() for s in re.findall(r\"[A-Z][^A-Z]*\", cased_name)])\n \n \n class ReplaceNameTransformer(m.MatcherDecoratableTransformer):\n@@ -84,43 +104,47 @@ class ReplaceNameTransformer(m.MatcherDecoratableTransformer):\n         - LLaMa -> MyNewModel       abd     MyNewModel      -> Llama\n     \"\"\"\n \n-    def __init__(\n-        self,\n-        old_name,\n-        new_name,\n-        given_old_name=None,\n-        given_new_name=None,\n-    ):\n+    def __init__(self, old_name: str, new_name: str, original_new_model_name: str = \"\", only_doc: bool = False):\n         super().__init__()\n         self.old_name = old_name\n         self.new_name = new_name\n-        self.default_name = \"\".join(x.title() for x in new_name.split(\"_\"))\n-        if self.new_name in CONFIG_MAPPING_NAMES:\n-            self.default_name = CONFIG_MAPPING_NAMES[self.new_name].replace(\n-                \"Config\", \"\"\n-            )  # the best source of truth for class names. Could also just use the ones de\n+        self.cased_new_name = get_cased_name(self.new_name)\n+        self.cased_old_name = get_cased_name(self.old_name)\n         self.patterns = {\n             old_name: new_name,\n             old_name.upper(): new_name.upper(),\n-            \"\".join(x.title() for x in old_name.split(\"_\")): self.default_name,\n+            # For some old models, `self.cased_old_name` == `old_name.upper()` in which case this overwrite previous entry\n+            self.cased_old_name: self.cased_new_name,\n         }\n-        if given_old_name is not None and given_new_name is not None and given_old_name not in self.patterns:\n-            self.patterns[given_old_name] = given_new_name\n-        if self.old_name in CONFIG_MAPPING_NAMES:\n-            self.default_old_name = CONFIG_MAPPING_NAMES[self.old_name].replace(\"Config\", \"\")\n-            if self.default_old_name.isupper():\n-                self.default_old_name = self.default_old_name.capitalize()\n-\n-    @m.leave(m.Name() | m.SimpleString() | m.Comment())\n-    def replace_name(self, original_node, updated_node):\n+        # In case new_name is a prefix alias, and not the original new model name\n+        self.original_new_model_name = original_new_model_name\n+        self.only_doc = only_doc\n+\n+    def _replace_name(self, original_node, updated_node):\n         if re.findall(r\"# Copied from\", updated_node.value):\n             return cst.RemoveFromParent()\n-        update = preserve_case_replace(updated_node.value, self.patterns, self.default_name)\n+        update = preserve_case_replace(updated_node.value, self.patterns, self.cased_new_name)\n         return updated_node.with_changes(value=update)\n \n-    def leave_ClassDef(self, original_node, updated_node):\n-        new_name = convert_to_camelcase(updated_node.name.value, self.old_name, self.default_old_name)\n-        return updated_node.with_changes(name=cst.Name(new_name))\n+    @m.leave(m.SimpleString() | m.Comment())\n+    def replace_name(self, original_node, updated_node):\n+        return self._replace_name(original_node, updated_node)\n+\n+    def leave_Name(self, original_node, updated_node):\n+        if not self.only_doc:\n+            return self._replace_name(original_node, updated_node)\n+        return updated_node\n+\n+    def leave_ImportFrom(self, original_node, updated_node):\n+        \"\"\"The imports from other file types (configuration, processing etc) should use original model name.\"\"\"\n+        if self.original_new_model_name != self.new_name and m.matches(updated_node.module, m.Name()):\n+            patterns = \"|\".join(ALL_FILE_TYPES)\n+            regex = rf\"({patterns})_{self.new_name}\"\n+            new_source = re.sub(\n+                regex, lambda m: f\"{m.group(1)}_{self.original_new_model_name}\", updated_node.module.value\n+            )\n+            updated_node = updated_node.with_changes(module=updated_node.module.with_changes(value=new_source))\n+        return updated_node\n \n \n DOCSTRING_NODE = m.SimpleStatementLine(\n@@ -760,10 +784,12 @@ def compute_relative_order(self, missing_dependencies: set[str]) -> dict[str, in\n                 remaining_dependencies.remove(dep)\n                 relative_order[dep] = idx\n                 idx += 1\n-            # Add the class itself\n-            remaining_dependencies.remove(class_name)\n-            relative_order[class_name] = idx\n-            idx += 1\n+            # Add the class itself (it can sometimes already be present if the order of classes in the source file\n+            # does not make sense, i.e. a class is used somewhere before being defined like in `rt_detr`...)\n+            if class_name in remaining_dependencies:\n+                remaining_dependencies.remove(class_name)\n+                relative_order[class_name] = idx\n+                idx += 1\n \n         # Now add what still remains\n         remaining_dependencies = tuple(remaining_dependencies)\n@@ -859,7 +885,24 @@ def visit_and_merge_dependencies(\n         return mapper\n \n \n-def replace_class_node(mapper: ModelFileMapper, class_node: cst.ClassDef, renamed_super_class: str):\n+def common_partial_suffix(str1: str, str2: str) -> str:\n+    \"\"\"Return the biggest common suffix between 2 strings. If one string is a full suffix of the other string,\n+    we do not consider it a common suffix and return `\"\"`\"\"\"\n+    common_suffix = \"\"\n+    for i in range(1, min(len(str1), len(str2)) + 1):\n+        if str1[-i] == str2[-i]:\n+            common_suffix = str1[-i] + common_suffix\n+        else:\n+            break\n+    # We do not allow full string suffix\n+    if common_suffix == str1 or common_suffix == str2:\n+        common_suffix = \"\"\n+    return common_suffix\n+\n+\n+def replace_class_node(\n+    mapper: ModelFileMapper, class_node: cst.ClassDef, renamed_super_class: str, original_super_class: str\n+):\n     \"\"\"\n     Replace a class node which inherits from another modeling class. This function works in the following way:\n     - start from the base class node of the inherited class (a cst.Node)\n@@ -889,6 +932,36 @@ def replace_class_node(mapper: ModelFileMapper, class_node: cst.ClassDef, rename\n         raise ValueError(f\"Could not parse the name of the bases for {class_node.name.value}\")\n \n     original_node = mapper.classes[renamed_super_class]\n+    # Always use the new name of the class (in case we use e.g. `ColPaliForRetrieval` inheriting from `PaliGemmaForConditionalGeneration`)\n+    new_name = class_node.name\n+\n+    # If the new class name is different from the renamed super class name, we need to update the docstrings/comments accordingly\n+    if new_name.value != renamed_super_class:\n+        common_suffix = common_partial_suffix(new_name.value, renamed_super_class)\n+        # Note that this works even without common prefix, in which case it does not replace anything\n+        old, new = renamed_super_class.replace(common_suffix, \"\"), new_name.value.replace(common_suffix, \"\")\n+        temp_module = cst.Module(body=[original_node])\n+        original_node = temp_module.visit(\n+            ReplaceNameTransformer(get_lowercase_name(old), get_lowercase_name(new), only_doc=True)\n+        ).body[0]\n+\n+    # If we explicitly passed a new base with common suffix to an old base, it is for switching the prefix\n+    # e.g. if the \"natural\" parent class is `PreTrainedModel` but we wanted to rename it to `PreTrainedVisionModel`\n+    additional_bases = [base for base in all_bases if base != original_super_class]\n+    new_bases = []\n+    for original_base in original_node.bases:\n+        new_base = original_base\n+        # we only potentially switch base for Name-based bases, not Attribute\n+        if m.matches(original_base.value, m.Name()):\n+            original_base_name = original_base.value.value\n+            for additional_base_name in additional_bases:\n+                suffix = common_partial_suffix(original_base_name, additional_base_name)\n+                if len(suffix) > 0 and suffix[0].isupper():\n+                    new_name_node = original_base.value.with_changes(value=additional_base_name)\n+                    new_base = original_base.with_changes(value=new_name_node)\n+                    break\n+        new_bases.append(new_base)\n+\n     original_methods = {\n         f.name.value if hasattr(f, \"name\") else mapper.python_module.code_for_node(f): f\n         for f in original_node.body.body\n@@ -942,12 +1015,17 @@ def replace_class_node(mapper: ModelFileMapper, class_node: cst.ClassDef, rename\n         if m.matches(func, DOCSTRING_NODE):  # This processes the docstring of the class!\n             # Extract the original docstring\n             updated_docstring = func.body[0].value.value\n-            original_docstring = docstring_node[0].body[0].value.value\n-            merged_doc = merge_docstrings(original_docstring, updated_docstring)\n-            # Update the docstring in the original function\n-            docstring_node = [\n-                docstring_node[0].with_changes(body=[cst.Expr(value=cst.SimpleString(value=merged_doc))])\n-            ]\n+            if len(docstring_node) == 0:  # If the original docstring is empty, just create one from the updated.\n+                docstring_node = [\n+                    cst.SimpleStatementLine(body=[cst.Expr(value=cst.SimpleString(value=updated_docstring))])\n+                ]\n+            else:\n+                original_docstring = docstring_node[0].body[0].value.value\n+                merged_doc = merge_docstrings(original_docstring, updated_docstring)\n+                # Update the docstring in the original function\n+                docstring_node = [\n+                    docstring_node[0].with_changes(body=[cst.Expr(value=cst.SimpleString(value=merged_doc))])\n+                ]\n         if name not in original_methods and func is not None and isinstance(func, cst.FunctionDef):\n             end_meth.append(func)\n         if m.matches(func, m.SimpleStatementLine(body=[m.Assign()])):\n@@ -970,10 +1048,10 @@ def replace_class_node(mapper: ModelFileMapper, class_node: cst.ClassDef, rename\n \n     # Use decorators redefined in `modular_xxx.py` if any\n     new_decorators = class_node.decorators if len(class_node.decorators) > 0 else original_node.decorators\n-    # Always use the new name of the class (in case we use e.g. `ColPaliForRetrieval` inheriting from `PaliGemmaForConditionalGeneration`)\n-    name = class_node.name\n \n-    return original_node.with_changes(body=new_replacement_body, decorators=new_decorators, name=name)\n+    return original_node.with_changes(\n+        body=new_replacement_body, decorators=new_decorators, bases=new_bases, name=new_name\n+    )\n \n \n TYPE_TO_FILE_TYPE = {\n@@ -1014,14 +1092,18 @@ def find_file_type(class_name: str) -> str:\n IMPORTS_TO_SKIP_IN_MODULAR = (\"auto.modeling_auto\",)\n \n \n-def append_new_import_node(node: cst.CSTNode, unused_imports: set[str], imports_to_keep: list[cst.CSTNode]):\n-    \"\"\"Insert the new `node` to the list of `imports_to_keep` in-place, if it is not part of the `unused_imports`.\"\"\"\n+def append_new_import_node(\n+    node: cst.CSTNode, unused_imports: set[str], added_names: set, imports_to_keep: list[cst.CSTNode]\n+):\n+    \"\"\"Insert the new `node` to the list of `imports_to_keep` in-place, if it is not part of the `unused_imports` or `added_names`.\n+    Also modifies `added_names` in-place accordingly.\"\"\"\n     import_node = node.body[0]\n     names_to_keep = []\n     for name in import_node.names:\n         name_value = name.evaluated_name\n-        if name_value not in unused_imports:\n+        if name_value not in unused_imports and name_value not in added_names:\n             names_to_keep.append(name.with_changes(comma=cst.MaybeSentinel.DEFAULT))\n+            added_names.add(name_value)\n     if len(names_to_keep) > 0:\n         new_node = node.with_changes(body=[import_node.with_changes(names=names_to_keep)])\n         imports_to_keep.append(new_node)\n@@ -1036,40 +1118,38 @@ def get_needed_imports(body: dict[str, dict], all_imports: list[cst.CSTNode]) ->\n     wrapper = MetadataWrapper(cst.Module(body=all_imports + new_body))\n     scopes = set(wrapper.resolve(ScopeProvider).values())\n     unused_imports = set()\n-    import_ref_count = {}\n+    import_ref_count = defaultdict(lambda: 0)\n     for scope in scopes:\n         for assignment in scope.assignments:\n             node = assignment.node\n             if isinstance(assignment, cst.metadata.Assignment) and isinstance(node, (cst.Import, cst.ImportFrom)):\n                 ref_count = len(assignment.references)\n                 name = assignment.name\n-                # Similar imports may be redefined, and only used between their 1st and 2nd definition\n-                # so if we already have a ref count > 0, the imports is actually used\n-                if (ref_count == 0 and import_ref_count.get(name, -1) <= 0) or name in body.keys():\n-                    unused_imports.add(name)\n-                import_ref_count[name] = ref_count\n+                import_ref_count[name] = max(ref_count, import_ref_count[name])\n+    # Similar imports may be redefined, and only used between their 1st and 2nd definition so if we already have\n+    # a ref count > 0 at any point, the imports is actually used\n+    unused_imports = {name for name, count in import_ref_count.items() if count <= 0 or name in body.keys()}\n \n     imports_to_keep = []\n+    # We need to keep track of which names were already imported, because some import may be duplicated from multiple sources\n+    # or be both protected and unprotected due to inconsistency between models\n+    added_names = set()\n     existing_protected_statements = set()  # str repr of the import nodes - does not work with the nodes directly\n     for node in all_imports:\n         if m.matches(node, m.If()):  # handle safe imports\n             new_statements = []\n             for stmt_node in node.body.body:\n-                append_new_import_node(stmt_node, unused_imports, new_statements)\n+                append_new_import_node(stmt_node, unused_imports, added_names, new_statements)\n             new_statements = [stmt for stmt in new_statements if str(stmt) not in existing_protected_statements]\n             if len(new_statements) > 0:\n                 new_node = node.with_changes(body=node.body.with_changes(body=new_statements))\n                 imports_to_keep.append(new_node)\n                 existing_protected_statements.update({str(stmt) for stmt in new_statements})\n         else:\n-            append_new_import_node(node, unused_imports, imports_to_keep)\n+            append_new_import_node(node, unused_imports, added_names, imports_to_keep)\n \n     protected_import_nodes = [node for node in imports_to_keep if m.matches(node, m.If())]\n     usual_import_nodes = [node for node in imports_to_keep if not m.matches(node, m.If())]\n-    # If the same import is both protected and unprotected, only keep the protected one\n-    for protected_node in protected_import_nodes:\n-        for stmt_node in protected_node.body.body:\n-            usual_import_nodes = [node for node in usual_import_nodes if node.body[0] != stmt_node.body[0]]\n \n     # Protected imports always appear at the end of all imports\n     return usual_import_nodes + protected_import_nodes\n@@ -1102,12 +1182,10 @@ class ModularFileMapper(ModuleMapper):\n     Calling the method `create_modules()` after visit will create all modules based on this modular file.\n     \"\"\"\n \n-    def __init__(self, python_module, new_name, given_old_name=None, given_new_name=None):\n+    def __init__(self, python_module, new_name):\n         super().__init__(python_module)\n         # fmt: off\n         self.model_name = new_name  # name of the model being defined. Should be in the format of `llama` or `layout_xlm` or `phi3`\n-        self.given_old_name = given_old_name\n-        self.given_new_name = given_new_name\n \n         self.model_specific_imported_objects: Dict[str, str] = {}  # e.g. {\"LlamaModel\": \"transformers.models.llama.modeling_llama\"}\n         self.model_specific_modules: Dict[str, cst.Module] = {}  # e.g. {\"transformers.models.llama.modeling_llama\": cst.Module}\n@@ -1191,11 +1269,11 @@ def leave_Module(self, node):\n         # 1. for each modeling file found in the imports, rename it with the new model name, visit it, and update dependencies\n         self.visited_modules = {}\n         self.renamers = {}\n+        name_prefixes = self.infer_new_model_name()\n         for file, module in self.model_specific_modules.items():\n             file_model_name = file.split(\".\")[-2]\n-            renamer = ReplaceNameTransformer(\n-                file_model_name, self.model_name, self.given_old_name, self.given_new_name\n-            )\n+            new_name = name_prefixes[file]\n+            renamer = ReplaceNameTransformer(file_model_name, new_name, self.model_name)\n             renamed_module = module.visit(renamer)\n             self.visited_modules[file] = ModelFileMapper.visit_and_merge_dependencies(\n                 renamed_module,\n@@ -1288,6 +1366,87 @@ def compute_relative_order(self, missing_dependencies: set) -> dict[str, int]:\n \n         return relative_order\n \n+    def infer_new_model_name(self) -> dict:\n+        \"\"\"Infer whether we are using a model name prefix different from the usual model name as defined from the filename.\n+        This is useful e.g. when we define a new multi-modal model, and only the text part inherits from `LlamaModel`,\n+        so we have something like:\n+        ```python\n+        class NewModelNameTextDecoderLayer(LlamaDecoderLayer):\n+            pass\n+        ```\n+        with the `Text` prefix added to the model name.\n+        However, in case of multiple prefix used, we raise a warning and use the most frequent prefix, to avoid parsing\n+        the same file multiple times and inconsistencies in the objects added from dependencies.\n+        If the new prefix collides with a prefix of another class in the file where we are importing from, then we also\n+        raise a warning, and use the default prefix (model name) to avoid collisions in dependencies.\n+        \"\"\"\n+        prefix_model_name_mapping = defaultdict(Counter)\n+        cased_default_name = get_cased_name(self.model_name)\n+        # Iterate over all new classes to get modeling super classes\n+        for class_name, class_node in self.classes.items():\n+            modeling_bases = [\n+                k.value.value for k in class_node.bases if k.value.value in self.model_specific_imported_objects\n+            ]\n+            if len(modeling_bases) > 1:\n+                raise ValueError(\n+                    f\"{class_name} was defined with more than 1 model-specific super class. This is unsupported. We found {*modeling_bases,}.\"\n+                )\n+            if len(modeling_bases) == 1:\n+                filename = self.model_specific_imported_objects[modeling_bases[0]]\n+                cased_model_name = cased_default_name  # the default name prefix\n+                suffix = common_partial_suffix(class_name, modeling_bases[0])\n+                if len(suffix) > 0 and suffix[0].isupper():\n+                    cased_model_name = class_name.replace(suffix, \"\")\n+                prefix_model_name_mapping[filename].update([cased_model_name])\n+\n+        # Check if we found multiple prefixes for some modeling files\n+        final_name_mapping = {}\n+        for file, prefixes_counter in prefix_model_name_mapping.items():\n+            if len(prefixes_counter) > 1:\n+                _, total = prefixes_counter.most_common(1)[0]\n+                most_used_entities = [name for name, count in prefixes_counter.most_common() if count == total]\n+                # if the default name is in the pool of equally used prefixes, use it, otherwise last encountered\n+                final_name = cased_default_name if cased_default_name in most_used_entities else most_used_entities[-1]\n+            else:\n+                final_name = list(prefixes_counter)[0]\n+            # Check if the prefix can be used without collisions in the names\n+            old_cased_model_name = get_cased_name(file.split(\".\")[-2])\n+            old_model_name_prefix = final_name.replace(cased_default_name, old_cased_model_name)\n+            # Raise adequate warning depending on the situation\n+            has_prefix_collision = f\"\\nclass {old_model_name_prefix}\" in get_module_source_from_name(file)\n+            if final_name != cased_default_name and has_prefix_collision:\n+                if len(prefixes_counter) > 1:\n+                    logger.warning(\n+                        f\"We detected multiple prefix names when inheriting from {file}: {*set(prefixes_counter),}. However, the \"\n+                        f\"most used one, '{final_name}', is already present in the source file and will likely cause consistency \"\n+                        f\"issues. For this reason we fallback to the default prefix '{cased_default_name}' when grabbing args \"\n+                        \"and dependencies. Make sure to subclass the intermediate classes with the prefix you want (if different \"\n+                        f\"from '{cased_default_name}') or use a single prefix in all the modular (best).\"\n+                    )\n+                else:\n+                    logger.warning(\n+                        f\"We detected the use of the new default prefix {final_name} when inheriting from {file}. However, it is \"\n+                        \"already present in the source file and will likely cause consistency issues. For this reason we fallback \"\n+                        f\"to the default prefix '{cased_default_name}' when grabbing args and dependencies. Make sure to subclass \"\n+                        f\"the intermediate classes with the prefix you want (if different from '{cased_default_name}')\"\n+                    )\n+                final_name = cased_default_name\n+            elif len(prefixes_counter) > 1:\n+                logger.warning(\n+                    f\"We detected multiple prefix names when inheriting from {file}: {*set(prefixes_counter),}. We will only \"\n+                    f\"use the most used '{final_name}' prefix when grabbing args and dependencies. Make sure to subclass the \"\n+                    f\"intermediate classes with the prefix you want (if different from '{final_name}') or use a single prefix \"\n+                    \"in all the modular (best).\"\n+                )\n+            final_name_mapping[file] = get_lowercase_name(final_name)\n+\n+        # Check we are not missing imported files\n+        for file in self.model_specific_modules.keys():\n+            if file not in final_name_mapping.keys():\n+                final_name_mapping[file] = self.model_name\n+\n+        return final_name_mapping\n+\n \n def check_dependencies_and_create_import_node(\n     file_type: str, new_dependencies: set[str], mapper: ModuleMapper, new_name: str\n@@ -1338,11 +1497,11 @@ def get_class_node_and_dependencies(\n     class node based on the inherited classes if needed. Also returns any new imports of a new class defined in\n     the modular that we nay need.\n     \"\"\"\n-    bases = [k.value.value for k in node.bases if k.value.value in modular_mapper.model_specific_imported_objects]\n-    if len(bases) > 1:\n-        raise ValueError(\n-            f\"{class_name} was defined with more than 1 model-specific super class. This is unsupported. We found {*bases,}.\"\n-        )\n+    # An exception was already raised if this has len > 1\n+    model_specific_bases = [\n+        k.value.value for k in node.bases if k.value.value in modular_mapper.model_specific_imported_objects\n+    ]\n+    super_class = model_specific_bases[0] if len(model_specific_bases) == 1 else None\n \n     file_type = find_file_type(class_name)\n     file_to_update = files[file_type]\n@@ -1352,19 +1511,17 @@ class node based on the inherited classes if needed. Also returns any new import\n     imported_objects = modular_mapper.imported_objects_per_file[file_type]\n \n     # We need to replace the class node with the transformers (modeling file) super class node\n-    if len(bases) == 1:\n-        super_class = bases[0]\n+    if super_class is not None:\n         super_file_name = modular_mapper.model_specific_imported_objects[super_class]\n \n         # Get the mapper corresponding to the inherited class\n         mapper = modular_mapper.visited_modules[super_file_name]\n         # Rename the super class according to the exact same rule we used when renaming the whole module\n         renamer = modular_mapper.renamers[super_file_name]\n-        renamed_super_class = preserve_case_replace(super_class, renamer.patterns, renamer.default_name)\n-        renamed_super_class = convert_to_camelcase(renamed_super_class, renamer.old_name, renamer.default_old_name)\n+        renamed_super_class = preserve_case_replace(super_class, renamer.patterns, renamer.cased_new_name)\n \n         # Create the new class node\n-        updated_node = replace_class_node(mapper, node, renamed_super_class)\n+        updated_node = replace_class_node(mapper, node, renamed_super_class, super_class)\n \n         # Grab all immediate dependencies of the new node\n         new_node_dependencies = augmented_dependencies_for_class_node(updated_node, mapper, imported_objects)\n@@ -1468,7 +1625,7 @@ def create_modules(modular_mapper: ModularFileMapper) -> dict[str, cst.Module]:\n     return files\n \n \n-def convert_modular_file(modular_file, old_model_name=None, new_model_name=None, cst_transformers=None):\n+def convert_modular_file(modular_file):\n     pattern = re.search(r\"modular_(.*)(?=\\.py$)\", modular_file)\n     output = {}\n     if pattern is not None:\n@@ -1478,8 +1635,7 @@ def convert_modular_file(modular_file, old_model_name=None, new_model_name=None,\n             code = file.read()\n         module = cst.parse_module(code)\n         wrapper = MetadataWrapper(module)\n-        if cst_transformers is None:\n-            cst_transformers = ModularFileMapper(module, model_name, old_model_name, new_model_name)\n+        cst_transformers = ModularFileMapper(module, model_name)\n         wrapper.visit(cst_transformers)\n         for file, module in create_modules(cst_transformers).items():\n             if module != {}:\n@@ -1522,20 +1678,10 @@ def save_modeling_file(modular_file, converted_file):\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--files_to_parse\",\n-        default=[\"src/transformers/models/starcoder2/modular_starcoder2.py\"],\n+        default=[\"src/transformers/models/gemma/modular_gemma.py\"],\n         nargs=\"+\",\n         help=\"A list of `modular_xxxx` files that should be converted to single model file\",\n     )\n-    parser.add_argument(\n-        \"--old_model_name\",\n-        required=False,\n-        help=\"The name of the model from which the copying is done in CamelCase. If not provided is inferred from modular-file\",\n-    )\n-    parser.add_argument(\n-        \"--new_model_name\",\n-        required=False,\n-        help=\"The name of the new model being added in CamelCase. If not provided is inferred from modular-file\",\n-    )\n     args = parser.parse_args()\n     if args.files_to_parse == [\"all\"]:\n         args.files_to_parse = glob.glob(\"src/transformers/models/**/modular_*.py\", recursive=True)\n@@ -1544,5 +1690,5 @@ def save_modeling_file(modular_file, converted_file):\n     for file_name in find_priority_list(args.files_to_parse):\n         print(f\"Converting {file_name} to a single model single file format\")\n         module_path = file_name.replace(\"/\", \".\").replace(\".py\", \"\").replace(\"src.\", \"\")\n-        converted_files = convert_modular_file(file_name, args.old_model_name, args.new_model_name)\n+        converted_files = convert_modular_file(file_name)\n         converter = save_modeling_file(file_name, converted_files)"
        }
    ],
    "stats": {
        "total": 2529,
        "additions": 2425,
        "deletions": 104
    }
}