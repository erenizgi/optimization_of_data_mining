{
    "author": "geetu040",
    "message": "Create and Expose SamVisionModel as public for better accessibility (#36493)\n\n* move encoder below\n\n* auto modeling\n\n* write SamVisionTester\n\n* fix vision attention shape\n\n* fix SamVisionTest\n\n* minor changes to SamVisionTest\n\n* Revert \"fix vision attention shape\"\n\nThis reverts commit d2a4083ae5704716e33351aed03af8f3cc45f3ae.\n\n* fix attention output shape in new tests\n\n* remove encoder examples\n\n* run modular on got_ocr2\n\n* code formatting\n\n* fix got_ocr2\n\n* ruff fixes\n\n* code quality\n\n* add sam_vision in auto modeling and auto configuration\n\n* remove composite test\n\n* updated index.md\n\n* add TFSamVisionEncoder to __init__\n\n* fix public TFSamVisionEncoder\n\n* remove outdated todo comment\n\n* set test_torch_exportable\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* rename: VisionEncoder -> VisionModel\n\n* bring back original SamVisionEncoder\n\n* rename back: VisionEncoderOutput -> VisionModelOutput\n\n* undo changes in SamModelTester\n\n* reuse SamVisionEncoder in SamVisionModel\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
    "files": [
        {
            "sha": "58cbfbfb2190e600d0bcb65960b43781283bec1b",
            "filename": "docs/source/en/model_doc/sam.md",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -149,12 +149,24 @@ alt=\"drawing\" width=\"900\"/>\n [[autodoc]] SamImageProcessor\n \n \n+## SamVisionModel\n+\n+[[autodoc]] SamVisionModel\n+    - forward\n+\n+\n ## SamModel\n \n [[autodoc]] SamModel\n     - forward\n \n \n+## TFSamVisionModel\n+\n+[[autodoc]] TFSamVisionModel\n+    - call\n+\n+\n ## TFSamModel\n \n [[autodoc]] TFSamModel"
        },
        {
            "sha": "82b57928f3ab6d7f375327b5eca470503094c3e1",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -3589,6 +3589,7 @@\n         [\n             \"SamModel\",\n             \"SamPreTrainedModel\",\n+            \"SamVisionModel\",\n         ]\n     )\n     _import_structure[\"models.seamless_m4t\"].extend(\n@@ -4757,6 +4758,7 @@\n         [\n             \"TFSamModel\",\n             \"TFSamPreTrainedModel\",\n+            \"TFSamVisionModel\",\n         ]\n     )\n     _import_structure[\"models.segformer\"].extend(\n@@ -8431,6 +8433,7 @@\n         from .models.sam import (\n             SamModel,\n             SamPreTrainedModel,\n+            SamVisionModel,\n         )\n         from .models.seamless_m4t import (\n             SeamlessM4TCodeHifiGan,\n@@ -9372,6 +9375,7 @@\n         from .models.sam import (\n             TFSamModel,\n             TFSamPreTrainedModel,\n+            TFSamVisionModel,\n         )\n         from .models.segformer import (\n             TFSegformerDecodeHead,"
        },
        {
            "sha": "9937b55a8b0f2a13fa647ecc56e31fe6e1a854da",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -273,6 +273,7 @@\n         (\"rt_detr_v2\", \"RTDetrV2Config\"),\n         (\"rwkv\", \"RwkvConfig\"),\n         (\"sam\", \"SamConfig\"),\n+        (\"sam_vision_model\", \"SamVisionConfig\"),\n         (\"seamless_m4t\", \"SeamlessM4TConfig\"),\n         (\"seamless_m4t_v2\", \"SeamlessM4Tv2Config\"),\n         (\"segformer\", \"SegformerConfig\"),\n@@ -630,6 +631,7 @@\n         (\"rt_detr_v2\", \"RT-DETRv2\"),\n         (\"rwkv\", \"RWKV\"),\n         (\"sam\", \"SAM\"),\n+        (\"sam_vision_model\", \"SamVisionModel\"),\n         (\"seamless_m4t\", \"SeamlessM4T\"),\n         (\"seamless_m4t_v2\", \"SeamlessM4Tv2\"),\n         (\"segformer\", \"SegFormer\"),\n@@ -773,6 +775,7 @@\n         (\"chinese_clip_vision_model\", \"chinese_clip\"),\n         (\"rt_detr_resnet\", \"rt_detr\"),\n         (\"granitevision\", \"llava_next\"),\n+        (\"sam_vision_model\", \"sam\"),\n     ]\n )\n "
        },
        {
            "sha": "5df90ff5a876c8689f20cfbf2f7681a80e258c03",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -249,6 +249,7 @@\n         (\"rt_detr_v2\", \"RTDetrV2Model\"),\n         (\"rwkv\", \"RwkvModel\"),\n         (\"sam\", \"SamModel\"),\n+        (\"sam_vision_model\", \"SamVisionModel\"),\n         (\"seamless_m4t\", \"SeamlessM4TModel\"),\n         (\"seamless_m4t_v2\", \"SeamlessM4Tv2Model\"),\n         (\"segformer\", \"SegformerModel\"),"
        },
        {
            "sha": "67b69e2c61090c8f97122beb8fd077602124e916",
            "filename": "src/transformers/models/auto/modeling_tf_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_tf_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_tf_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_tf_auto.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -80,6 +80,7 @@\n         (\"roberta-prelayernorm\", \"TFRobertaPreLayerNormModel\"),\n         (\"roformer\", \"TFRoFormerModel\"),\n         (\"sam\", \"TFSamModel\"),\n+        (\"sam_vision_model\", \"TFSamVisionModel\"),\n         (\"segformer\", \"TFSegformerModel\"),\n         (\"speech_to_text\", \"TFSpeech2TextModel\"),\n         (\"swiftformer\", \"TFSwiftFormerModel\"),"
        },
        {
            "sha": "0f4a2b0893abde34374c5c0acc694c20a3de3083",
            "filename": "src/transformers/models/sam/configuration_sam.py",
            "status": "modified",
            "additions": 19,
            "deletions": 1,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fconfiguration_sam.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -183,9 +183,27 @@ class SamVisionConfig(PretrainedConfig):\n         mlp_dim (`int`, *optional*):\n             The dimensionality of the MLP layer in the Transformer encoder. If `None`, defaults to `mlp_ratio *\n             hidden_size`.\n-    \"\"\"\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import (\n+    ...     SamVisionConfig,\n+    ...     SamVisionModel,\n+    ... )\n+\n+    >>> # Initializing a SamVisionConfig with `\"facebook/sam-vit-huge\"` style configuration\n+    >>> configuration = SamVisionConfig()\n+\n+    >>> # Initializing a SamVisionModel (with random weights) from the `\"facebook/sam-vit-huge\"` style configuration\n+    >>> model = SamVisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n \n     base_config_key = \"vision_config\"\n+    model_type = \"sam_vision_model\"\n \n     def __init__(\n         self,"
        },
        {
            "sha": "50e0e86ddee078f2f1b914dd3a037fd0acbb1ef3",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 63,
            "deletions": 2,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -27,7 +27,13 @@\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import ModelOutput, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n from .configuration_sam import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig\n \n \n@@ -1280,6 +1286,61 @@ def _init_weights(self, module):\n \"\"\"\n \n \n+SAM_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`SamProcessor`]. See [`SamProcessor.__call__`] for\n+            details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The vision model from Sam without any head or projection on top.\"\"\",\n+    SAM_START_DOCSTRING,\n+)\n+class SamVisionModel(SamPreTrainedModel):\n+    config_class = SamVisionConfig\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: SamVisionConfig):\n+        super().__init__(config)\n+        self.vision_encoder = SamVisionEncoder(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_encoder.patch_embed\n+\n+    @add_start_docstrings_to_model_forward(SAM_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=SamVisionEncoderOutput, config_class=SamVisionConfig)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, SamVisionEncoderOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        return self.vision_encoder(\n+            pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+\n @add_start_docstrings(\n     \"Segment Anything Model (SAM) for generating segmentation masks, given an input image and \",\n     \" optional 2D location and bounding boxes.\",\n@@ -1522,4 +1583,4 @@ def forward(\n         )\n \n \n-__all__ = [\"SamModel\", \"SamPreTrainedModel\"]\n+__all__ = [\"SamVisionModel\", \"SamModel\", \"SamPreTrainedModel\"]"
        },
        {
            "sha": "d0462499c5acccec2dc50d4bfbf31c0d6b913945",
            "filename": "src/transformers/models/sam/modeling_tf_sam.py",
            "status": "modified",
            "additions": 72,
            "deletions": 2,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -30,7 +30,13 @@\n from ...modeling_tf_outputs import TFBaseModelOutput\n from ...modeling_tf_utils import TFModelInputType, TFPreTrainedModel, keras, shape_list, unpack_inputs\n from ...tf_utils import flatten, functional_layernorm\n-from ...utils import ModelOutput, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n from .configuration_sam import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig\n \n \n@@ -1400,6 +1406,70 @@ class TFSamPreTrainedModel(TFPreTrainedModel):\n \"\"\"\n \n \n+SAM_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`tf.Tensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`SamProcessor`]. See [`SamProcessor.__call__`] for\n+            details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The vision model from Sam without any head or projection on top.\"\"\",\n+    SAM_START_DOCSTRING,\n+)\n+class TFSamVisionModel(TFSamPreTrainedModel):\n+    config_class = SamVisionConfig\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: SamVisionConfig, **kwargs):\n+        super().__init__(config, **kwargs)\n+        self.vision_encoder = TFSamVisionEncoder(config, name=\"vision_encoder\")\n+\n+    def build(self, input_shape=None):\n+        if self.built:\n+            return\n+        self.built = True\n+        if getattr(self, \"vision_encoder\", None) is not None:\n+            with tf.name_scope(self.vision_encoder.name):\n+                self.vision_encoder.build(None)\n+\n+    def get_input_embeddings(self):\n+        return self.vision_encoder.patch_embed\n+\n+    @unpack_inputs\n+    @add_start_docstrings_to_model_forward(SAM_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=TFSamVisionEncoderOutput, config_class=SamVisionConfig)\n+    def call(\n+        self,\n+        pixel_values: TFModelInputType | None = None,\n+        output_attentions: bool | None = None,\n+        output_hidden_states: bool | None = None,\n+        return_dict: bool | None = None,\n+        training: bool = False,\n+        **kwargs,\n+    ) -> TFSamVisionEncoderOutput | Tuple[tf.Tensor]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        return self.vision_encoder(\n+            pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            training=training,\n+        )\n+\n+\n @add_start_docstrings(\n     \"Segment Anything Model (SAM) for generating segmentation masks, given an input image and \",\n     \" optional 2D location and bounding boxes.\",\n@@ -1653,4 +1723,4 @@ def build(self, input_shape=None):\n                 self.mask_decoder.build(None)\n \n \n-__all__ = [\"TFSamModel\", \"TFSamPreTrainedModel\"]\n+__all__ = [\"TFSamVisionModel\", \"TFSamModel\", \"TFSamPreTrainedModel\"]"
        },
        {
            "sha": "79b2fd4e232736e906e1c61ab25598138c96890b",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -8836,6 +8836,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class SamVisionModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class SeamlessM4TCodeHifiGan(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "985445fbba567b87bea76a81e1c980cdaf4b2daf",
            "filename": "src/transformers/utils/dummy_tf_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Futils%2Fdummy_tf_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/src%2Ftransformers%2Futils%2Fdummy_tf_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_tf_objects.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -2375,6 +2375,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"tf\"])\n \n \n+class TFSamVisionModel(metaclass=DummyObject):\n+    _backends = [\"tf\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"tf\"])\n+\n+\n class TFSegformerDecodeHead(metaclass=DummyObject):\n     _backends = [\"tf\"]\n "
        },
        {
            "sha": "0f19b29d9026d3f74332d945c71ede92c7a01c19",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 231,
            "deletions": 1,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -32,13 +32,243 @@\n     import torch\n     from torch import nn\n \n-    from transformers import SamModel, SamProcessor\n+    from transformers import SamModel, SamProcessor, SamVisionModel\n \n \n if is_vision_available():\n     from PIL import Image\n \n \n+class SamVisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        hidden_size=36,\n+        intermediate_size=72,\n+        projection_dim=62,\n+        output_channels=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        num_channels=3,\n+        image_size=24,\n+        patch_size=2,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-06,\n+        dropout=0.0,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        initializer_factor=1.0,\n+        qkv_bias=True,\n+        mlp_ratio=4.0,\n+        use_abs_pos=True,\n+        use_rel_pos=True,\n+        rel_pos_zero_init=False,\n+        window_size=14,\n+        global_attn_indexes=[2, 5, 8, 11],\n+        num_pos_feats=16,\n+        mlp_dim=None,\n+        batch_size=2,\n+    ):\n+        self.parent = parent\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.projection_dim = projection_dim\n+        self.output_channels = output_channels\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.initializer_factor = initializer_factor\n+        self.qkv_bias = qkv_bias\n+        self.mlp_ratio = mlp_ratio\n+        self.use_abs_pos = use_abs_pos\n+        self.use_rel_pos = use_rel_pos\n+        self.rel_pos_zero_init = rel_pos_zero_init\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.num_pos_feats = num_pos_feats\n+        self.mlp_dim = mlp_dim\n+        self.batch_size = batch_size\n+\n+        # in ViT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + 1\n+\n+    def get_config(self):\n+        return SamVisionConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            projection_dim=self.projection_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            initializer_range=self.initializer_range,\n+            initializer_factor=self.initializer_factor,\n+            output_channels=self.output_channels,\n+            qkv_bias=self.qkv_bias,\n+            mlp_ratio=self.mlp_ratio,\n+            use_abs_pos=self.use_abs_pos,\n+            use_rel_pos=self.use_rel_pos,\n+            rel_pos_zero_init=self.rel_pos_zero_init,\n+            window_size=self.window_size,\n+            global_attn_indexes=self.global_attn_indexes,\n+            num_pos_feats=self.num_pos_feats,\n+            mlp_dim=self.mlp_dim,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = SamVisionModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        output_size = self.image_size // self.patch_size\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape, (self.batch_size, self.output_channels, output_size, output_size)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class SamVisionModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as SAM's vision encoder does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (SamVisionModel,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torchscript = False\n+    test_torch_exportable = True\n+\n+    def setUp(self):\n+        self.model_tester = SamVisionModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=SamVisionConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"SAM's vision encoder does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        expected_attention_shape = (\n+            self.model_tester.batch_size * self.model_tester.num_attention_heads,\n+            196,\n+            196,\n+        )\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            self.assertListEqual(\n+                list(attentions[0].shape[-4:]),\n+                list(expected_attention_shape),\n+            )\n+\n+    @unittest.skip(reason=\"SamVisionModel does not support training\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamVisionModel does not support training\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamVisionModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamVisionModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_to_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamVisionModel does not support training\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in create_and_check_model tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_compile_dynamic(self):\n+        self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n+\n+\n class SamPromptEncoderTester:\n     def __init__(\n         self,"
        },
        {
            "sha": "305f429ea0e6112111dd035df3cf1bc4a047313f",
            "filename": "tests/models/sam/test_modeling_tf_sam.py",
            "status": "modified",
            "additions": 192,
            "deletions": 1,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0710e9b1e87a486fee62f3a93fbfb6cb306072dc/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py?ref=0710e9b1e87a486fee62f3a93fbfb6cb306072dc",
            "patch": "@@ -34,13 +34,204 @@\n if is_tf_available():\n     import tensorflow as tf\n \n-    from transformers import SamProcessor, TFSamModel\n+    from transformers import SamProcessor, TFSamModel, TFSamVisionModel\n     from transformers.modeling_tf_utils import keras\n \n if is_vision_available():\n     from PIL import Image\n \n \n+class TFSamVisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        hidden_size=36,\n+        intermediate_size=72,\n+        projection_dim=62,\n+        output_channels=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        num_channels=3,\n+        image_size=24,\n+        patch_size=2,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-06,\n+        dropout=0.0,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        initializer_factor=1.0,\n+        qkv_bias=True,\n+        mlp_ratio=4.0,\n+        use_abs_pos=True,\n+        use_rel_pos=True,\n+        rel_pos_zero_init=False,\n+        window_size=14,\n+        global_attn_indexes=[2, 5, 8, 11],\n+        num_pos_feats=16,\n+        mlp_dim=None,\n+        batch_size=2,\n+    ):\n+        self.parent = parent\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.projection_dim = projection_dim\n+        self.output_channels = output_channels\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.initializer_factor = initializer_factor\n+        self.qkv_bias = qkv_bias\n+        self.mlp_ratio = mlp_ratio\n+        self.use_abs_pos = use_abs_pos\n+        self.use_rel_pos = use_rel_pos\n+        self.rel_pos_zero_init = rel_pos_zero_init\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.num_pos_feats = num_pos_feats\n+        self.mlp_dim = mlp_dim\n+        self.batch_size = batch_size\n+\n+    def get_config(self):\n+        return SamVisionConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            projection_dim=self.projection_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            initializer_range=self.initializer_range,\n+            initializer_factor=self.initializer_factor,\n+            output_channels=self.output_channels,\n+            qkv_bias=self.qkv_bias,\n+            mlp_ratio=self.mlp_ratio,\n+            use_abs_pos=self.use_abs_pos,\n+            use_rel_pos=self.use_rel_pos,\n+            rel_pos_zero_init=self.rel_pos_zero_init,\n+            window_size=self.window_size,\n+            global_attn_indexes=self.global_attn_indexes,\n+            num_pos_feats=self.num_pos_feats,\n+            mlp_dim=self.mlp_dim,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = TFSamVisionModel(config=config)\n+        result = model(pixel_values)\n+        output_size = self.image_size // self.patch_size\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape, (self.batch_size, self.output_channels, output_size, output_size)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_tf\n+class TFSamVisionModelTest(TFModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as SAM's vision encoder does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (TFSamVisionModel,) if is_tf_available() else ()\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_onnx = False\n+\n+    def setUp(self):\n+        self.model_tester = TFSamVisionModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=SamVisionConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"SAM's vision encoder does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_common_attributes(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (keras.layers.Layer))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, keras.layers.Dense))\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.call)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        expected_attention_shape = (\n+            self.model_tester.batch_size * self.model_tester.num_attention_heads,\n+            196,\n+            196,\n+        )\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            self.assertListEqual(\n+                list(attentions[0].shape[-4:]),\n+                list(expected_attention_shape),\n+            )\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in create_and_check_model tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+\n class TFSamPromptEncoderTester:\n     def __init__(\n         self,"
        }
    ],
    "stats": {
        "total": 619,
        "additions": 612,
        "deletions": 7
    }
}