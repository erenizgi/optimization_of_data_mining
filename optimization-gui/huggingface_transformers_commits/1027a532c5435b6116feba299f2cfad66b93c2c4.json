{
    "author": "winglian",
    "message": "add a callback hook right before the optimizer step (#33444)",
    "sha": "1027a532c5435b6116feba299f2cfad66b93c2c4",
    "files": [
        {
            "sha": "f815c50d597f27a51a54e73144676f3d9e79b56e",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1027a532c5435b6116feba299f2cfad66b93c2c4/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1027a532c5435b6116feba299f2cfad66b93c2c4/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1027a532c5435b6116feba299f2cfad66b93c2c4",
            "patch": "@@ -2417,6 +2417,8 @@ def _inner_training_loop(\n                         else:\n                             grad_norm = _grad_norm\n \n+                    self.control = self.callback_handler.on_pre_optimizer_step(args, self.state, self.control)\n+\n                     self.optimizer.step()\n \n                     self.control = self.callback_handler.on_optimizer_step(args, self.state, self.control)"
        },
        {
            "sha": "d457a65993db42c274bc5d32d86548f2a9fbaae4",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1027a532c5435b6116feba299f2cfad66b93c2c4/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1027a532c5435b6116feba299f2cfad66b93c2c4/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=1027a532c5435b6116feba299f2cfad66b93c2c4",
            "patch": "@@ -344,6 +344,12 @@ def on_step_begin(self, args: TrainingArguments, state: TrainerState, control: T\n         \"\"\"\n         pass\n \n+    def on_pre_optimizer_step(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n+        \"\"\"\n+        Event called before the optimizer step but after gradient clipping. Useful for monitoring gradients.\n+        \"\"\"\n+        pass\n+\n     def on_optimizer_step(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n         \"\"\"\n         Event called after the optimizer step but before gradients are zeroed out. Useful for monitoring gradients.\n@@ -475,6 +481,9 @@ def on_step_begin(self, args: TrainingArguments, state: TrainerState, control: T\n         control.should_save = False\n         return self.call_event(\"on_step_begin\", args, state, control)\n \n+    def on_pre_optimizer_step(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n+        return self.call_event(\"on_pre_optimizer_step\", args, state, control)\n+\n     def on_optimizer_step(self, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n         return self.call_event(\"on_optimizer_step\", args, state, control)\n "
        },
        {
            "sha": "0d1e6645f9a59f3cb340944bccf70834e092e2f9",
            "filename": "tests/trainer/test_trainer_callback.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1027a532c5435b6116feba299f2cfad66b93c2c4/tests%2Ftrainer%2Ftest_trainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1027a532c5435b6116feba299f2cfad66b93c2c4/tests%2Ftrainer%2Ftest_trainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_callback.py?ref=1027a532c5435b6116feba299f2cfad66b93c2c4",
            "patch": "@@ -78,6 +78,9 @@ def on_epoch_end(self, args, state, control, **kwargs):\n     def on_step_begin(self, args, state, control, **kwargs):\n         self.events.append(\"on_step_begin\")\n \n+    def on_pre_optimizer_step(self, args, state, control, **kwargs):\n+        self.events.append(\"on_pre_optimizer_step\")\n+\n     def on_optimizer_step(self, args, state, control, **kwargs):\n         self.events.append(\"on_optimizer_step\")\n \n@@ -151,7 +154,7 @@ def get_expected_events(self, trainer):\n             expected_events.append(\"on_epoch_begin\")\n             for _ in range(train_dl_len):\n                 step += 1\n-                expected_events += [\"on_step_begin\", \"on_optimizer_step\", \"on_step_end\"]\n+                expected_events += [\"on_step_begin\", \"on_pre_optimizer_step\", \"on_optimizer_step\", \"on_step_end\"]\n                 if step % trainer.args.logging_steps == 0:\n                     expected_events.append(\"on_log\")\n                 if trainer.args.eval_strategy == IntervalStrategy.STEPS and step % trainer.args.eval_steps == 0:"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 15,
        "deletions": 1
    }
}